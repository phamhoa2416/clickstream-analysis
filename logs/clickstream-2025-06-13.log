2025-06-13 22:16:55.891 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 35935 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-13 22:16:55.893 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-13 22:16:56.820 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-13 22:16:56.824 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-13 22:16:56.826 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-13 22:16:56.826 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-13 22:16:56.894 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-13 22:16:56.895 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 939 ms
2025-06-13 22:16:57.454 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-13 22:16:57.559 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-13 22:16:57.616 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:16:57.617 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-13 22:16:57.617 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:16:57.617 [main INFO ] org.apache.spark.SparkContext - Submitted application: ClickstreamProcessor
2025-06-13 22:16:57.630 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-13 22:16:57.639 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-13 22:16:57.639 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-13 22:16:57.688 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-13 22:16:57.688 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-13 22:16:57.688 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-13 22:16:57.688 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-13 22:16:57.689 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-13 22:16:57.833 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 41797.
2025-06-13 22:16:57.850 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-13 22:16:57.873 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-13 22:16:57.887 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-13 22:16:57.887 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-13 22:16:57.889 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-13 22:16:57.896 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-8374be4e-d1bb-4358-bca5-a11aa6f35416
2025-06-13 22:16:57.923 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-13 22:16:57.933 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-13 22:16:57.959 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @2882ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-13 22:16:58.042 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-13 22:16:58.051 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-13 22:16:58.064 [main INFO ] org.sparkproject.jetty.server.Server - Started @2987ms
2025-06-13 22:16:58.083 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@79110c13{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:16:58.083 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-13 22:16:58.092 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1aee6d14{/,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.144 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-13 22:16:58.148 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-13 22:16:58.161 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34157.
2025-06-13 22:16:58.161 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:34157
2025-06-13 22:16:58.163 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-13 22:16:58.168 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 34157, None)
2025-06-13 22:16:58.171 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:34157 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 34157, None)
2025-06-13 22:16:58.174 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 34157, None)
2025-06-13 22:16:58.176 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 34157, None)
2025-06-13 22:16:58.199 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1aee6d14{/,null,STOPPED,@Spark}
2025-06-13 22:16:58.199 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12270a01{/jobs,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.200 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@646d58cd{/jobs/json,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.200 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e3ee457{/jobs/job,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.200 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fb2c2f3{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.201 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d2a8819{/stages,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.201 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b64bf61{/stages/json,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.202 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66abb2fa{/stages/stage,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.202 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133b712{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.203 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70f91ae3{/stages/pool,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.203 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c2a3f0c{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.203 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d93ff21{/storage,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.204 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ca4c88a{/storage/json,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.204 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55397d15{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.204 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24ac6fef{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.205 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@227b9277{/environment,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.205 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b56d8a7{/environment/json,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.206 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6de5ad56{/executors,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.206 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cfb94fd{/executors/json,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.207 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44a44a04{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.207 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a6fc1bc{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.212 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@360a3106{/static,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.212 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12a0d249{/,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.213 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c6c84fa{/api,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.213 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c9ef37b{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.214 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c8758d1{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.215 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22ff11ef{/metrics/json,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.368 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-13 22:16:58.369 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-13 22:16:58.378 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-13 22:16:58.392 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15f11bfb{/SQL,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.392 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2bf4fa1{/SQL/json,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.393 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5fafa76d{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.393 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e62e227{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.394 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@539dd2d0{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:16:58.929 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-13 22:16:59.127 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-13 22:16:59.162 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-13 22:16:59.208 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-13 22:16:59.208 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-13 22:16:59.208 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749827819207
2025-06-13 22:16:59.602 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-13 22:16:59.604 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-13 22:16:59.604 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-13 22:16:59.604 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-13 22:16:59.609 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-13 22:16:59.615 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-13 22:16:59.624 [main INFO ] com.example.Application - Started Application in 3.965 seconds (JVM running for 4.547)
2025-06-13 22:16:59.912 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-13 22:16:59.912 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-13 22:16:59.913 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-13 22:16:59.996 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-13 22:17:00.009 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ec8389c{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-13 22:17:00.010 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c1d0f2c{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-13 22:17:00.010 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a3f7064{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-13 22:17:00.010 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6232f94c{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-13 22:17:00.011 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7efd1441{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:17:12.760 [http-nio-8080-exec-1 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:17:13.724 [http-nio-8080-exec-1 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `event_id` cannot be resolved. Did you mean one of the following? [`eventId`, `eventName`, `eventParams`, `userId`, `appId`].;
'Filter ((((isnotnull('event_id) AND isnotnull('event_name)) AND isnotnull('event_time)) AND isnotnull('user_id)) AND isnotnull('session_id))
+- LocalRelation [appId#0, eventId#1, eventName#2, eventParams#3, eventTimestamp#4, pageUrl#5, platform#6, sessionId#7, userId#8]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:258)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:211)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:4201)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1712)
	at com.example.clickstream.validation.EventValidator.validateEvents(EventValidator.java:27)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:39)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-13 22:17:14.813 [http-nio-8080-exec-2 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:17:14.823 [http-nio-8080-exec-2 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `event_id` cannot be resolved. Did you mean one of the following? [`eventId`, `eventName`, `eventParams`, `userId`, `appId`].;
'Filter ((((isnotnull('event_id) AND isnotnull('event_name)) AND isnotnull('event_time)) AND isnotnull('user_id)) AND isnotnull('session_id))
+- LocalRelation [appId#18, eventId#19, eventName#20, eventParams#21, eventTimestamp#22, pageUrl#23, platform#24, sessionId#25, userId#26]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:258)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:211)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:4201)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1712)
	at com.example.clickstream.validation.EventValidator.validateEvents(EventValidator.java:27)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:39)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-13 22:17:16.398 [http-nio-8080-exec-3 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:17:16.407 [http-nio-8080-exec-3 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `event_id` cannot be resolved. Did you mean one of the following? [`eventId`, `eventName`, `eventParams`, `userId`, `appId`].;
'Filter ((((isnotnull('event_id) AND isnotnull('event_name)) AND isnotnull('event_time)) AND isnotnull('user_id)) AND isnotnull('session_id))
+- LocalRelation [appId#36, eventId#37, eventName#38, eventParams#39, eventTimestamp#40, pageUrl#41, platform#42, sessionId#43, userId#44]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:258)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:211)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:4201)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1712)
	at com.example.clickstream.validation.EventValidator.validateEvents(EventValidator.java:27)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:39)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-13 22:17:17.112 [http-nio-8080-exec-4 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:17:17.121 [http-nio-8080-exec-4 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `event_id` cannot be resolved. Did you mean one of the following? [`eventId`, `eventName`, `eventParams`, `userId`, `appId`].;
'Filter ((((isnotnull('event_id) AND isnotnull('event_name)) AND isnotnull('event_time)) AND isnotnull('user_id)) AND isnotnull('session_id))
+- LocalRelation [appId#54, eventId#55, eventName#56, eventParams#57, eventTimestamp#58, pageUrl#59, platform#60, sessionId#61, userId#62]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:258)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:211)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:4201)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1712)
	at com.example.clickstream.validation.EventValidator.validateEvents(EventValidator.java:27)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:39)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-13 22:17:49.804 [http-nio-8080-exec-5 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:17:49.811 [http-nio-8080-exec-5 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `event_id` cannot be resolved. Did you mean one of the following? [`eventId`, `eventName`, `eventParams`, `userId`, `appId`].;
'Filter ((((isnotnull('event_id) AND isnotnull('event_name)) AND isnotnull('event_time)) AND isnotnull('user_id)) AND isnotnull('session_id))
+- LocalRelation [appId#72, eventId#73, eventName#74, eventParams#75, eventTimestamp#76, pageUrl#77, platform#78, sessionId#79, userId#80]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:258)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:211)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:4201)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1712)
	at com.example.clickstream.validation.EventValidator.validateEvents(EventValidator.java:27)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:39)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-13 22:18:57.295 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-13 22:18:57.295 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:18:57.301 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:18:57.302 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext already stopped.
2025-06-13 22:18:57.302 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@79110c13{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:19:00.598 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 37697 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-13 22:19:00.601 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-13 22:19:01.154 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-13 22:19:01.160 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-13 22:19:01.160 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-13 22:19:01.160 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-13 22:19:01.207 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-13 22:19:01.207 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 590 ms
2025-06-13 22:19:01.644 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-13 22:19:01.688 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-13 22:19:01.717 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:19:01.717 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-13 22:19:01.717 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:19:01.717 [main INFO ] org.apache.spark.SparkContext - Submitted application: ClickstreamProcessor
2025-06-13 22:19:01.725 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-13 22:19:01.730 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-13 22:19:01.731 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-13 22:19:01.749 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-13 22:19:01.749 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-13 22:19:01.749 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-13 22:19:01.749 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-13 22:19:01.749 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-13 22:19:01.826 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 36901.
2025-06-13 22:19:01.834 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-13 22:19:01.846 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-13 22:19:01.852 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-13 22:19:01.852 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-13 22:19:01.854 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-13 22:19:01.858 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-4ed59bce-5ece-468a-bb9d-c6991aa5b1fe
2025-06-13 22:19:01.873 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-13 22:19:01.880 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-13 22:19:01.890 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1861ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-13 22:19:01.927 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-13 22:19:01.931 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-13 22:19:01.937 [main INFO ] org.sparkproject.jetty.server.Server - Started @1908ms
2025-06-13 22:19:01.948 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@a579e4a{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:19:01.948 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-13 22:19:01.955 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2168def{/,null,AVAILABLE,@Spark}
2025-06-13 22:19:01.984 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-13 22:19:01.987 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-13 22:19:01.994 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40441.
2025-06-13 22:19:01.994 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:40441
2025-06-13 22:19:01.995 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-13 22:19:01.998 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 40441, None)
2025-06-13 22:19:01.999 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:40441 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 40441, None)
2025-06-13 22:19:02.002 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 40441, None)
2025-06-13 22:19:02.003 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 40441, None)
2025-06-13 22:19:02.017 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@2168def{/,null,STOPPED,@Spark}
2025-06-13 22:19:02.018 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44106e25{/jobs,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.019 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5649f55{/jobs/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.019 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@646d58cd{/jobs/job,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.019 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12532e37{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.020 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e3ee457{/stages,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.020 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fb2c2f3{/stages/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.021 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7846913f{/stages/stage,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.021 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60b553f{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.021 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66abb2fa{/stages/pool,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.022 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133b712{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.022 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70f91ae3{/storage,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.022 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c2a3f0c{/storage/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.023 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d93ff21{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.023 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ca4c88a{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.023 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55397d15{/environment,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.024 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24ac6fef{/environment/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.024 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@227b9277{/executors,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.024 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b56d8a7{/executors/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.025 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6de5ad56{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.025 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cfb94fd{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.028 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44a44a04{/static,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.028 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f4fc83f{/,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.029 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16e4db59{/api,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.029 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4feec184{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.030 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3278d065{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.031 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18dbc1b{/metrics/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.117 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-13 22:19:02.117 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-13 22:19:02.120 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-13 22:19:02.124 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50c2ef56{/SQL,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.125 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3514237f{/SQL/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.125 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27210a3b{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.126 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@84a9f65{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.126 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71df5f30{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:19:02.432 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-13 22:19:02.604 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-13 22:19:02.623 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-13 22:19:02.643 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-13 22:19:02.643 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-13 22:19:02.643 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749827942642
2025-06-13 22:19:02.793 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-13 22:19:02.796 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-13 22:19:02.796 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-13 22:19:02.796 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-13 22:19:02.802 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-13 22:19:02.806 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-13 22:19:02.815 [main INFO ] com.example.Application - Started Application in 2.388 seconds (JVM running for 2.786)
2025-06-13 22:19:03.220 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-13 22:19:03.220 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-13 22:19:03.221 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-13 22:19:03.259 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-13 22:19:03.268 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33d9a3c2{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-13 22:19:03.269 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5616c1f5{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:03.269 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3656f269{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-13 22:19:03.269 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57f458b4{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:03.270 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55f0a4d4{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:19:07.375 [http-nio-8080-exec-1 WARN ] o.s.w.s.m.s.DefaultHandlerExceptionResolver - Resolved [org.springframework.web.HttpMediaTypeNotSupportedException: Content type 'text/plain;charset=UTF-8' not supported]
2025-06-13 22:19:09.100 [http-nio-8080-exec-2 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:19:09.889 [http-nio-8080-exec-2 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `event` cannot be resolved. Did you mean one of the following? [`eventId`, `eventName`, `appId`, `userId`, `eventParams`].;
'Filter ((((isnotnull('event) AND isnotnull(eventName#2)) AND isnotnull(eventTimestamp#4)) AND isnotnull(userId#8)) AND isnotnull(sessionId#7))
+- LocalRelation [appId#0, eventId#1, eventName#2, eventParams#3, eventTimestamp#4, pageUrl#5, platform#6, sessionId#7, userId#8]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:258)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:211)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:4201)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1712)
	at com.example.clickstream.validation.EventValidator.validateEvents(EventValidator.java:27)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:39)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-13 22:19:12.359 [http-nio-8080-exec-3 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:19:12.372 [http-nio-8080-exec-3 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `event` cannot be resolved. Did you mean one of the following? [`eventId`, `eventName`, `appId`, `userId`, `eventParams`].;
'Filter ((((isnotnull('event) AND isnotnull(eventName#20)) AND isnotnull(eventTimestamp#22)) AND isnotnull(userId#26)) AND isnotnull(sessionId#25))
+- LocalRelation [appId#18, eventId#19, eventName#20, eventParams#21, eventTimestamp#22, pageUrl#23, platform#24, sessionId#25, userId#26]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:258)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:211)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:4201)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1712)
	at com.example.clickstream.validation.EventValidator.validateEvents(EventValidator.java:27)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:39)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-13 22:19:13.971 [http-nio-8080-exec-4 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:19:13.981 [http-nio-8080-exec-4 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `event` cannot be resolved. Did you mean one of the following? [`eventId`, `eventName`, `appId`, `userId`, `eventParams`].;
'Filter ((((isnotnull('event) AND isnotnull(eventName#38)) AND isnotnull(eventTimestamp#40)) AND isnotnull(userId#44)) AND isnotnull(sessionId#43))
+- LocalRelation [appId#36, eventId#37, eventName#38, eventParams#39, eventTimestamp#40, pageUrl#41, platform#42, sessionId#43, userId#44]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:258)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:211)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:4201)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1712)
	at com.example.clickstream.validation.EventValidator.validateEvents(EventValidator.java:27)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:39)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-13 22:19:14.973 [http-nio-8080-exec-5 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:19:14.982 [http-nio-8080-exec-5 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `event` cannot be resolved. Did you mean one of the following? [`eventId`, `eventName`, `appId`, `userId`, `eventParams`].;
'Filter ((((isnotnull('event) AND isnotnull(eventName#56)) AND isnotnull(eventTimestamp#58)) AND isnotnull(userId#62)) AND isnotnull(sessionId#61))
+- LocalRelation [appId#54, eventId#55, eventName#56, eventParams#57, eventTimestamp#58, pageUrl#59, platform#60, sessionId#61, userId#62]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:258)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:211)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:4201)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1712)
	at com.example.clickstream.validation.EventValidator.validateEvents(EventValidator.java:27)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:39)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-13 22:19:15.736 [http-nio-8080-exec-6 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:19:15.743 [http-nio-8080-exec-6 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `event` cannot be resolved. Did you mean one of the following? [`eventId`, `eventName`, `appId`, `userId`, `eventParams`].;
'Filter ((((isnotnull('event) AND isnotnull(eventName#74)) AND isnotnull(eventTimestamp#76)) AND isnotnull(userId#80)) AND isnotnull(sessionId#79))
+- LocalRelation [appId#72, eventId#73, eventName#74, eventParams#75, eventTimestamp#76, pageUrl#77, platform#78, sessionId#79, userId#80]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:258)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:211)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:4201)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1712)
	at com.example.clickstream.validation.EventValidator.validateEvents(EventValidator.java:27)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:39)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-13 22:19:39.805 [http-nio-8080-exec-7 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:19:39.815 [http-nio-8080-exec-7 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `event` cannot be resolved. Did you mean one of the following? [`eventId`, `eventName`, `appId`, `userId`, `eventParams`].;
'Filter ((((isnotnull('event) AND isnotnull(eventName#92)) AND isnotnull(eventTimestamp#94)) AND isnotnull(userId#98)) AND isnotnull(sessionId#97))
+- LocalRelation [appId#90, eventId#91, eventName#92, eventParams#93, eventTimestamp#94, pageUrl#95, platform#96, sessionId#97, userId#98]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:258)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:211)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:4201)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1712)
	at com.example.clickstream.validation.EventValidator.validateEvents(EventValidator.java:27)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:39)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-13 22:19:49.463 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-13 22:19:49.463 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:19:49.469 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@a579e4a{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:19:49.471 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:19:49.471 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-13 22:19:49.471 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext already stopped.
2025-06-13 22:19:51.403 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 38592 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-13 22:19:51.405 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-13 22:19:51.926 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-13 22:19:51.928 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-13 22:19:51.929 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-13 22:19:51.929 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-13 22:19:51.969 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-13 22:19:51.969 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 546 ms
2025-06-13 22:19:52.336 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-13 22:19:52.379 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-13 22:19:52.407 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:19:52.407 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-13 22:19:52.407 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:19:52.407 [main INFO ] org.apache.spark.SparkContext - Submitted application: ClickstreamProcessor
2025-06-13 22:19:52.415 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-13 22:19:52.420 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-13 22:19:52.420 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-13 22:19:52.437 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-13 22:19:52.437 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-13 22:19:52.438 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-13 22:19:52.438 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-13 22:19:52.438 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-13 22:19:52.515 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 39751.
2025-06-13 22:19:52.524 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-13 22:19:52.535 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-13 22:19:52.542 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-13 22:19:52.542 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-13 22:19:52.544 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-13 22:19:52.549 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-3044ff4a-c369-4213-89c0-f44f6cc30382
2025-06-13 22:19:52.564 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-13 22:19:52.571 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-13 22:19:52.581 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1622ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-13 22:19:52.620 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-13 22:19:52.624 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-13 22:19:52.630 [main INFO ] org.sparkproject.jetty.server.Server - Started @1671ms
2025-06-13 22:19:52.641 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@6b066bc1{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:19:52.641 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-13 22:19:52.649 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45e69324{/,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.678 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-13 22:19:52.681 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-13 22:19:52.689 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35597.
2025-06-13 22:19:52.689 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:35597
2025-06-13 22:19:52.690 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-13 22:19:52.692 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 35597, None)
2025-06-13 22:19:52.694 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:35597 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 35597, None)
2025-06-13 22:19:52.695 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 35597, None)
2025-06-13 22:19:52.696 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 35597, None)
2025-06-13 22:19:52.710 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@45e69324{/,null,STOPPED,@Spark}
2025-06-13 22:19:52.711 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@183ef89a{/jobs,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.711 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6fa7ce4{/jobs/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.711 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44106e25{/jobs/job,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.712 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5649f55{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.712 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12270a01{/stages,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.712 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@646d58cd{/stages/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.713 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fb2c2f3{/stages/stage,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.713 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d2a8819{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.714 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b64bf61{/stages/pool,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.714 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7846913f{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.714 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60b553f{/storage,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.715 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66abb2fa{/storage/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.715 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133b712{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.715 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70f91ae3{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.716 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c2a3f0c{/environment,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.716 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d93ff21{/environment/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.716 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ca4c88a{/executors,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.717 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55397d15{/executors/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.717 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24ac6fef{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.718 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@227b9277{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.720 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b56d8a7{/static,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.720 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7323c38c{/,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.721 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63a72cc6{/api,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.721 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25f14e93{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.722 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c02899{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.724 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@575d48db{/metrics/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.812 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-13 22:19:52.813 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-13 22:19:52.816 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-13 22:19:52.820 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54ae1240{/SQL,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.821 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@390a7532{/SQL/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.821 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@560d6d2{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.822 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@520ee6b3{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:52.822 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70700b8a{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:19:53.098 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-13 22:19:53.252 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-13 22:19:53.269 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-13 22:19:53.288 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-13 22:19:53.288 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-13 22:19:53.288 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749827993288
2025-06-13 22:19:53.421 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-13 22:19:53.423 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-13 22:19:53.423 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-13 22:19:53.423 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-13 22:19:53.428 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-13 22:19:53.431 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-13 22:19:53.440 [main INFO ] com.example.Application - Started Application in 2.171 seconds (JVM running for 2.481)
2025-06-13 22:19:53.585 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-13 22:19:53.585 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-13 22:19:53.586 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-13 22:19:53.629 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-13 22:19:53.636 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@342f0ffb{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-13 22:19:53.637 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22790275{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:53.638 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3af2ab97{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-13 22:19:53.638 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a0dbb21{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-13 22:19:53.639 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@285f8344{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:19:59.501 [http-nio-8080-exec-1 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:20:00.273 [http-nio-8080-exec-1 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_id` cannot be resolved. Did you mean one of the following? [`userId`, `eventId`, `appId`, `sessionId`, `pageUrl`].;
'Project [appId#0, eventId#1, eventName#2, eventParams#3, eventTimestamp#4, pageUrl#5, platform#6, sessionId#7, userId#8, concat_ws(-, 'user_id, sum(CASE WHEN (cast((cast('event_time as bigint) - lag('event_time, -1, null) windowspecdefinition('user_id, cast('event_time as bigint) ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1))) as bigint) > 1800) THEN 1 ELSE 0 END) windowspecdefinition('user_id, cast('event_time as bigint) ASC NULLS FIRST, unspecifiedframe$())) AS session_id#19]
+- Filter validTimestamp(eventTimestamp#4)
   +- Filter validUrl(pageUrl#5)
      +- Filter (((isnotnull(eventName#2) AND isnotnull(eventTimestamp#4)) AND isnotnull(userId#8)) AND isnotnull(sessionId#7))
         +- LocalRelation [appId#0, eventId#1, eventName#2, eventParams#3, eventTimestamp#4, pageUrl#5, platform#6, sessionId#7, userId#8]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:258)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)
	at org.apache.spark.sql.Dataset.withPlan(Dataset.scala:4196)
	at org.apache.spark.sql.Dataset.select(Dataset.scala:1578)
	at org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2776)
	at org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2715)
	at com.example.clickstream.service.SessionService.sessionizeEvents(SessionService.java:26)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:40)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-13 22:20:00.317 [http-nio-8080-exec-2 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:20:00.340 [http-nio-8080-exec-2 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_id` cannot be resolved. Did you mean one of the following? [`userId`, `eventId`, `appId`, `sessionId`, `pageUrl`].;
'Project [appId#20, eventId#21, eventName#22, eventParams#23, eventTimestamp#24, pageUrl#25, platform#26, sessionId#27, userId#28, concat_ws(-, 'user_id, sum(CASE WHEN (cast((cast('event_time as bigint) - lag('event_time, -1, null) windowspecdefinition('user_id, cast('event_time as bigint) ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1))) as bigint) > 1800) THEN 1 ELSE 0 END) windowspecdefinition('user_id, cast('event_time as bigint) ASC NULLS FIRST, unspecifiedframe$())) AS session_id#39]
+- Filter validTimestamp(eventTimestamp#24)
   +- Filter validUrl(pageUrl#25)
      +- Filter (((isnotnull(eventName#22) AND isnotnull(eventTimestamp#24)) AND isnotnull(userId#28)) AND isnotnull(sessionId#27))
         +- LocalRelation [appId#20, eventId#21, eventName#22, eventParams#23, eventTimestamp#24, pageUrl#25, platform#26, sessionId#27, userId#28]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:258)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)
	at org.apache.spark.sql.Dataset.withPlan(Dataset.scala:4196)
	at org.apache.spark.sql.Dataset.select(Dataset.scala:1578)
	at org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2776)
	at org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2715)
	at com.example.clickstream.service.SessionService.sessionizeEvents(SessionService.java:26)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:40)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-13 22:22:06.350 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-13 22:22:06.351 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:22:06.358 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@6b066bc1{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:22:06.361 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-13 22:22:06.362 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:22:06.362 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext already stopped.
2025-06-13 22:22:08.411 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 40456 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-13 22:22:08.412 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-13 22:22:08.947 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-13 22:22:08.954 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-13 22:22:08.955 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-13 22:22:08.955 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-13 22:22:08.993 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-13 22:22:08.994 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 563 ms
2025-06-13 22:22:09.351 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-13 22:22:09.397 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-13 22:22:09.426 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:22:09.426 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-13 22:22:09.427 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:22:09.427 [main INFO ] org.apache.spark.SparkContext - Submitted application: ClickstreamProcessor
2025-06-13 22:22:09.435 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-13 22:22:09.440 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-13 22:22:09.440 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-13 22:22:09.457 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-13 22:22:09.457 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-13 22:22:09.457 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-13 22:22:09.457 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-13 22:22:09.457 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-13 22:22:09.539 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 32963.
2025-06-13 22:22:09.546 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-13 22:22:09.559 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-13 22:22:09.566 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-13 22:22:09.567 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-13 22:22:09.568 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-13 22:22:09.573 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-5e27583a-c558-4ba3-93a7-9b5522c1d411
2025-06-13 22:22:09.589 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-13 22:22:09.596 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-13 22:22:09.614 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1696ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-13 22:22:09.651 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-13 22:22:09.655 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-13 22:22:09.662 [main INFO ] org.sparkproject.jetty.server.Server - Started @1744ms
2025-06-13 22:22:09.674 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@39f30b6a{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:22:09.674 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-13 22:22:09.682 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d0b447b{/,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.713 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-13 22:22:09.717 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-13 22:22:09.725 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46423.
2025-06-13 22:22:09.725 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:46423
2025-06-13 22:22:09.726 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-13 22:22:09.729 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 46423, None)
2025-06-13 22:22:09.731 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:46423 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 46423, None)
2025-06-13 22:22:09.733 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 46423, None)
2025-06-13 22:22:09.734 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 46423, None)
2025-06-13 22:22:09.749 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1d0b447b{/,null,STOPPED,@Spark}
2025-06-13 22:22:09.750 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48da64f2{/jobs,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.751 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2aa811f9{/jobs/json,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.751 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b7e4d14{/jobs/job,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.752 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@601d9f3a{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.752 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6585df70{/stages,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.752 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51fb5fe6{/stages/json,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.753 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3cb49121{/stages/stage,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.753 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c4215d7{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.754 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13f36d75{/stages/pool,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.754 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3155f190{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.754 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ebd8d2{/storage,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.755 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a63fa71{/storage/json,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.755 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5018b56b{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.755 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@737ff5c4{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.756 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ff64d{/environment,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.756 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79777da7{/environment/json,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.756 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e05a706{/executors,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.757 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a69014e{/executors/json,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.758 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@543ac221{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.758 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50e1f3fc{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.761 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56da8847{/static,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.761 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@545f0b6{/,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.762 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4888425d{/api,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.763 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@499c4d61{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.763 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59b3f754{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.765 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74eab077{/metrics/json,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.856 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-13 22:22:09.857 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-13 22:22:09.860 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-13 22:22:09.865 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58b03029{/SQL,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.865 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3428420d{/SQL/json,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.866 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e79e9e{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.866 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b99648a{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-13 22:22:09.867 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@bff764c{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:22:10.148 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-13 22:22:10.296 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-13 22:22:10.314 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-13 22:22:10.332 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-13 22:22:10.332 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-13 22:22:10.332 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749828130331
2025-06-13 22:22:10.463 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-13 22:22:10.465 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-13 22:22:10.465 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-13 22:22:10.465 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-13 22:22:10.470 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-13 22:22:10.474 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-13 22:22:10.483 [main INFO ] com.example.Application - Started Application in 2.231 seconds (JVM running for 2.566)
2025-06-13 22:22:10.538 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-13 22:22:10.538 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-13 22:22:10.539 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-13 22:22:10.579 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-13 22:22:10.586 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52d51811{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-13 22:22:10.587 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c10a201{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-13 22:22:10.587 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64d9274e{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-13 22:22:10.588 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e86ea5{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-13 22:22:10.588 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d64eb51{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:22:15.234 [http-nio-8080-exec-1 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:22:16.011 [http-nio-8080-exec-1 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.sql.AnalysisException: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "(CAST(eventTimestamp AS BIGINT) - lag(eventTimestamp, 1, NULL) OVER (PARTITION BY userId ORDER BY CAST(eventTimestamp AS BIGINT) ASC NULLS FIRST ROWS BETWEEN -1 FOLLOWING AND -1 FOLLOWING))" due to data type mismatch: Parameter 1 requires the "(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)" type, however "CAST(eventTimestamp AS BIGINT)" has the type "BIGINT".;
'Project [appId#0, eventId#1, eventName#2, eventParams#3, eventTimestamp#4, pageUrl#5, platform#6, concat_ws(-, userId#8, sum(CASE WHEN (cast((cast(eventTimestamp#4 as bigint) - lag(eventTimestamp#4, -1, null) windowspecdefinition(userId#8, cast(eventTimestamp#4 as bigint) ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1))) as bigint) > 1800) THEN 1 ELSE 0 END) windowspecdefinition(userId#8, cast(eventTimestamp#4 as bigint) ASC NULLS FIRST, unspecifiedframe$())) AS sessionId#19, userId#8]
+- Filter validTimestamp(eventTimestamp#4)
   +- Filter validUrl(pageUrl#5)
      +- Filter (((isnotnull(eventName#2) AND isnotnull(eventTimestamp#4)) AND isnotnull(userId#8)) AND isnotnull(sessionId#7))
         +- LocalRelation [appId#0, eventId#1, eventName#2, eventParams#3, eventTimestamp#4, pageUrl#5, platform#6, sessionId#7, userId#8]

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:73)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:269)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)
	at org.apache.spark.sql.Dataset.withPlan(Dataset.scala:4196)
	at org.apache.spark.sql.Dataset.select(Dataset.scala:1578)
	at org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2776)
	at org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2715)
	at com.example.clickstream.service.SessionService.sessionizeEvents(SessionService.java:26)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:40)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-13 22:22:40.804 [http-nio-8080-exec-2 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:22:40.829 [http-nio-8080-exec-2 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.sql.AnalysisException: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "(CAST(eventTimestamp AS BIGINT) - lag(eventTimestamp, 1, NULL) OVER (PARTITION BY userId ORDER BY CAST(eventTimestamp AS BIGINT) ASC NULLS FIRST ROWS BETWEEN -1 FOLLOWING AND -1 FOLLOWING))" due to data type mismatch: Parameter 1 requires the "(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)" type, however "CAST(eventTimestamp AS BIGINT)" has the type "BIGINT".;
'Project [appId#20, eventId#21, eventName#22, eventParams#23, eventTimestamp#24, pageUrl#25, platform#26, concat_ws(-, userId#28, sum(CASE WHEN (cast((cast(eventTimestamp#24 as bigint) - lag(eventTimestamp#24, -1, null) windowspecdefinition(userId#28, cast(eventTimestamp#24 as bigint) ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1))) as bigint) > 1800) THEN 1 ELSE 0 END) windowspecdefinition(userId#28, cast(eventTimestamp#24 as bigint) ASC NULLS FIRST, unspecifiedframe$())) AS sessionId#39, userId#28]
+- Filter validTimestamp(eventTimestamp#24)
   +- Filter validUrl(pageUrl#25)
      +- Filter (((isnotnull(eventName#22) AND isnotnull(eventTimestamp#24)) AND isnotnull(userId#28)) AND isnotnull(sessionId#27))
         +- LocalRelation [appId#20, eventId#21, eventName#22, eventParams#23, eventTimestamp#24, pageUrl#25, platform#26, sessionId#27, userId#28]

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:73)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:269)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)
	at org.apache.spark.sql.Dataset.withPlan(Dataset.scala:4196)
	at org.apache.spark.sql.Dataset.select(Dataset.scala:1578)
	at org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2776)
	at org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2715)
	at com.example.clickstream.service.SessionService.sessionizeEvents(SessionService.java:26)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:40)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-13 22:23:51.994 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-13 22:23:51.994 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:23:51.998 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@39f30b6a{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:23:52.002 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-13 22:23:52.003 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:23:52.003 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext already stopped.
2025-06-13 22:23:53.928 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 41944 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-13 22:23:53.929 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-13 22:23:54.458 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-13 22:23:54.461 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-13 22:23:54.461 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-13 22:23:54.461 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-13 22:23:54.501 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-13 22:23:54.501 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 553 ms
2025-06-13 22:23:54.922 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-13 22:23:54.963 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-13 22:23:54.991 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:23:54.991 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-13 22:23:54.991 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:23:54.991 [main INFO ] org.apache.spark.SparkContext - Submitted application: ClickstreamProcessor
2025-06-13 22:23:54.999 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-13 22:23:55.005 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-13 22:23:55.005 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-13 22:23:55.022 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-13 22:23:55.022 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-13 22:23:55.023 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-13 22:23:55.023 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-13 22:23:55.023 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-13 22:23:55.102 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 41537.
2025-06-13 22:23:55.110 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-13 22:23:55.122 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-13 22:23:55.129 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-13 22:23:55.129 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-13 22:23:55.131 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-13 22:23:55.135 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-d080f072-ff86-4605-b0c2-075a48faf7cc
2025-06-13 22:23:55.149 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-13 22:23:55.156 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-13 22:23:55.166 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1711ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-13 22:23:55.202 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-13 22:23:55.206 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-13 22:23:55.212 [main INFO ] org.sparkproject.jetty.server.Server - Started @1758ms
2025-06-13 22:23:55.223 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@47407e31{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:23:55.223 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-13 22:23:55.231 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d978ab9{/,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.260 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-13 22:23:55.263 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-13 22:23:55.270 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37743.
2025-06-13 22:23:55.270 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:37743
2025-06-13 22:23:55.271 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-13 22:23:55.274 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 37743, None)
2025-06-13 22:23:55.275 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:37743 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 37743, None)
2025-06-13 22:23:55.277 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 37743, None)
2025-06-13 22:23:55.277 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 37743, None)
2025-06-13 22:23:55.291 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@d978ab9{/,null,STOPPED,@Spark}
2025-06-13 22:23:55.292 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133b712{/jobs,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.292 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70f91ae3{/jobs/json,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.293 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d93ff21{/jobs/job,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.293 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ca4c88a{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.293 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55397d15{/stages,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.294 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24ac6fef{/stages/json,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.294 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6de5ad56{/stages/stage,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.295 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cfb94fd{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.295 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44a44a04{/stages/pool,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.295 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a6fc1bc{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.296 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@360a3106{/storage,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.296 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e9a836{/storage/json,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.296 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75aa7703{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.297 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3395c2a7{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.297 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7831d1aa{/environment,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.297 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27746c5e{/environment/json,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.298 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2270f58d{/executors,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.298 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54737322{/executors/json,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.299 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7323c38c{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.299 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63a72cc6{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.301 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cef885d{/static,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.302 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5da3f32a{/,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.302 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51f4439e{/api,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.303 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8315e4a{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.303 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22ff11ef{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.305 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@215a0264{/metrics/json,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.388 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-13 22:23:55.388 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-13 22:23:55.392 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-13 22:23:55.396 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3664c596{/SQL,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.397 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e79e9e{/SQL/json,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.397 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74450c9b{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.398 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ad50b02{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.398 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@142918a0{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:23:55.670 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-13 22:23:55.822 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-13 22:23:55.840 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-13 22:23:55.859 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-13 22:23:55.859 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-13 22:23:55.859 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749828235858
2025-06-13 22:23:55.992 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-13 22:23:55.994 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-13 22:23:55.994 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-13 22:23:55.994 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-13 22:23:55.999 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-13 22:23:56.003 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-13 22:23:56.013 [main INFO ] com.example.Application - Started Application in 2.232 seconds (JVM running for 2.558)
2025-06-13 22:23:56.063 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-13 22:23:56.063 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-13 22:23:56.064 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-13 22:23:56.108 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-13 22:23:56.114 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@32b65550{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-13 22:23:56.115 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ccae188{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-13 22:23:56.115 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39517fdb{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-13 22:23:56.116 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@159b25fe{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-13 22:23:56.116 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c2d03c{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:24:01.392 [http-nio-8080-exec-1 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:24:02.349 [http-nio-8080-exec-1 INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 138.281356 ms
2025-06-13 22:24:02.696 [http-nio-8080-exec-1 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.SparkException: Task not serializable
	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:444)
	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:416)
	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)
	at org.apache.spark.SparkContext.clean(SparkContext.scala:2526)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$1(RDD.scala:1000)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
	at org.apache.spark.rdd.RDD.foreach(RDD.scala:999)
	at org.apache.spark.sql.Dataset.$anonfun$foreach$1(Dataset.scala:3340)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4154)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4152)
	at org.apache.spark.sql.Dataset.foreach(Dataset.scala:3340)
	at org.apache.spark.sql.Dataset.foreach(Dataset.scala:3350)
	at com.example.clickstream.producer.ClickstreamProducer.sendEvents(ClickstreamProducer.java:69)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:42)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.io.NotSerializableException: com.example.clickstream.producer.ClickstreamProducer
Serialization stack:
	- object not serializable (class: com.example.clickstream.producer.ClickstreamProducer, value: com.example.clickstream.producer.ClickstreamProducer@12aaa85d)
	- element of array (index: 0)
	- array (class [Ljava.lang.Object;, size 1)
	- field (class: java.lang.invoke.SerializedLambda, name: capturedArgs, type: class [Ljava.lang.Object;)
	- object (class java.lang.invoke.SerializedLambda, SerializedLambda[capturingClass=class com.example.clickstream.producer.ClickstreamProducer, functionalInterfaceMethod=org/apache/spark/api/java/function/ForeachFunction.call:(Ljava/lang/Object;)V, implementation=invokeVirtual com/example/clickstream/producer/ClickstreamProducer.sendEvent:(Lorg/apache/spark/sql/Row;)V, instantiatedMethodType=(Lorg/apache/spark/sql/Row;)V, numCaptured=1])
	- writeReplace data (class: java.lang.invoke.SerializedLambda)
	- object (class com.example.clickstream.producer.ClickstreamProducer$$Lambda$2772/0x0000000841019840, com.example.clickstream.producer.ClickstreamProducer$$Lambda$2772/0x0000000841019840@256b4613)
	- element of array (index: 0)
	- array (class [Ljava.lang.Object;, size 1)
	- field (class: java.lang.invoke.SerializedLambda, name: capturedArgs, type: class [Ljava.lang.Object;)
	- object (class java.lang.invoke.SerializedLambda, SerializedLambda[capturingClass=class org.apache.spark.sql.Dataset, functionalInterfaceMethod=scala/Function1.apply:(Ljava/lang/Object;)Ljava/lang/Object;, implementation=invokeStatic org/apache/spark/sql/Dataset.$anonfun$foreach$2$adapted:(Lorg/apache/spark/api/java/function/ForeachFunction;Ljava/lang/Object;)Ljava/lang/Object;, instantiatedMethodType=(Ljava/lang/Object;)Ljava/lang/Object;, numCaptured=1])
	- writeReplace data (class: java.lang.invoke.SerializedLambda)
	- object (class org.apache.spark.sql.Dataset$$Lambda$2773/0x000000084101a040, org.apache.spark.sql.Dataset$$Lambda$2773/0x000000084101a040@40c5a7ea)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:49)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)
	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:441)
	... 76 common frames omitted
2025-06-13 22:26:06.723 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-13 22:26:06.724 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:26:06.731 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@47407e31{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:26:06.732 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:26:06.732 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext already stopped.
2025-06-13 22:26:06.733 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-13 22:26:08.641 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 43763 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-13 22:26:08.642 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-13 22:26:09.212 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-13 22:26:09.219 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-13 22:26:09.219 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-13 22:26:09.219 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-13 22:26:09.270 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-13 22:26:09.270 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 607 ms
2025-06-13 22:26:09.684 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-13 22:26:09.733 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-13 22:26:09.761 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:26:09.761 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-13 22:26:09.761 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:26:09.762 [main INFO ] org.apache.spark.SparkContext - Submitted application: ClickstreamProcessor
2025-06-13 22:26:09.769 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-13 22:26:09.778 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-13 22:26:09.778 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-13 22:26:09.798 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-13 22:26:09.798 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-13 22:26:09.798 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-13 22:26:09.798 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-13 22:26:09.798 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-13 22:26:09.881 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 45571.
2025-06-13 22:26:09.890 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-13 22:26:09.903 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-13 22:26:09.909 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-13 22:26:09.909 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-13 22:26:09.911 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-13 22:26:09.915 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-6186c0b6-3518-478d-88e4-d1e0e7055d4c
2025-06-13 22:26:09.930 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-13 22:26:09.937 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-13 22:26:09.947 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1774ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-13 22:26:09.985 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-13 22:26:09.989 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-13 22:26:09.996 [main INFO ] org.sparkproject.jetty.server.Server - Started @1823ms
2025-06-13 22:26:10.007 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@19711f1{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:26:10.007 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-13 22:26:10.015 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a8640f7{/,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.048 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-13 22:26:10.051 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-13 22:26:10.060 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43703.
2025-06-13 22:26:10.061 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:43703
2025-06-13 22:26:10.061 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-13 22:26:10.064 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 43703, None)
2025-06-13 22:26:10.066 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:43703 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 43703, None)
2025-06-13 22:26:10.068 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 43703, None)
2025-06-13 22:26:10.069 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 43703, None)
2025-06-13 22:26:10.085 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@3a8640f7{/,null,STOPPED,@Spark}
2025-06-13 22:26:10.086 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4bb9f7d4{/jobs,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.087 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@183ef89a{/jobs/json,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.087 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a8b42a3{/jobs/job,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.088 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44106e25{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.088 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5649f55{/stages,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.088 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12270a01{/stages/json,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.089 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e3ee457{/stages/stage,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.089 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fb2c2f3{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.090 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d2a8819{/stages/pool,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.090 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b64bf61{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.090 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7846913f{/storage,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.091 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60b553f{/storage/json,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.091 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66abb2fa{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.091 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133b712{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.092 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70f91ae3{/environment,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.092 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c2a3f0c{/environment/json,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.093 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d93ff21{/executors,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.093 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ca4c88a{/executors/json,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.094 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55397d15{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.094 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24ac6fef{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.096 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@227b9277{/static,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.097 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54737322{/,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.098 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7323c38c{/api,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.098 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64688978{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.098 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25f14e93{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.100 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51468039{/metrics/json,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.191 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-13 22:26:10.192 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-13 22:26:10.195 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-13 22:26:10.200 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ebf776c{/SQL,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.200 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54ae1240{/SQL/json,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.201 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a0baec0{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.201 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@560d6d2{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.202 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a12f3e7{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:26:10.502 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-13 22:26:10.668 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-13 22:26:10.688 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-13 22:26:10.707 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-13 22:26:10.707 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-13 22:26:10.707 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749828370706
2025-06-13 22:26:10.843 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-13 22:26:10.845 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-13 22:26:10.845 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-13 22:26:10.845 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-13 22:26:10.850 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-13 22:26:10.854 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-13 22:26:10.863 [main INFO ] com.example.Application - Started Application in 2.383 seconds (JVM running for 2.691)
2025-06-13 22:26:11.348 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-13 22:26:11.348 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-13 22:26:11.348 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 0 ms
2025-06-13 22:26:11.391 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-13 22:26:11.398 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42156914{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-13 22:26:11.398 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e36d181{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-13 22:26:11.399 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3da86211{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-13 22:26:11.399 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@293cb405{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-13 22:26:11.400 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28946baf{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:26:20.351 [http-nio-8080-exec-2 WARN ] o.s.w.s.m.s.DefaultHandlerExceptionResolver - Resolved [org.springframework.web.HttpMediaTypeNotSupportedException: Content type 'text/plain;charset=UTF-8' not supported]
2025-06-13 22:26:25.319 [http-nio-8080-exec-3 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:26:26.185 [http-nio-8080-exec-3 INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 91.007236 ms
2025-06-13 22:26:30.364 [http-nio-8080-exec-4 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:26:31.709 [http-nio-8080-exec-5 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:26:33.105 [http-nio-8080-exec-7 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:26:37.645 [http-nio-8080-exec-1 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:27:25.617 [http-nio-8080-exec-8 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:27:29.618 [http-nio-8080-exec-9 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:32:05.731 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-13 22:32:05.731 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:32:05.738 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@19711f1{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:32:05.740 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-13 22:32:05.740 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:32:05.740 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext already stopped.
2025-06-13 22:32:07.715 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 48941 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-13 22:32:07.716 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-13 22:32:08.259 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-13 22:32:08.262 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-13 22:32:08.262 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-13 22:32:08.262 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-13 22:32:08.303 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-13 22:32:08.303 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 567 ms
2025-06-13 22:32:08.700 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-13 22:32:08.741 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-13 22:32:08.769 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:32:08.770 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-13 22:32:08.770 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:32:08.770 [main INFO ] org.apache.spark.SparkContext - Submitted application: ClickstreamProcessor
2025-06-13 22:32:08.777 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-13 22:32:08.782 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-13 22:32:08.783 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-13 22:32:08.799 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-13 22:32:08.799 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-13 22:32:08.799 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-13 22:32:08.799 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-13 22:32:08.799 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-13 22:32:08.876 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 44309.
2025-06-13 22:32:08.884 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-13 22:32:08.896 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-13 22:32:08.902 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-13 22:32:08.903 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-13 22:32:08.904 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-13 22:32:08.910 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-ec4a7243-620d-43fa-a4e8-856c2131adb1
2025-06-13 22:32:08.926 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-13 22:32:08.933 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-13 22:32:08.943 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1683ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-13 22:32:08.980 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-13 22:32:08.984 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-13 22:32:08.992 [main INFO ] org.sparkproject.jetty.server.Server - Started @1732ms
2025-06-13 22:32:09.004 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@33163b39{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:32:09.004 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-13 22:32:09.012 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f65a203{/,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.044 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-13 22:32:09.048 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-13 22:32:09.056 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34671.
2025-06-13 22:32:09.056 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:34671
2025-06-13 22:32:09.057 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-13 22:32:09.060 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 34671, None)
2025-06-13 22:32:09.061 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:34671 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 34671, None)
2025-06-13 22:32:09.063 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 34671, None)
2025-06-13 22:32:09.064 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 34671, None)
2025-06-13 22:32:09.078 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6f65a203{/,null,STOPPED,@Spark}
2025-06-13 22:32:09.079 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7846913f{/jobs,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.079 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60b553f{/jobs/json,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.080 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133b712{/jobs/job,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.080 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70f91ae3{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.080 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c2a3f0c{/stages,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.081 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d93ff21{/stages/json,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.081 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24ac6fef{/stages/stage,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.082 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@227b9277{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.082 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b56d8a7{/stages/pool,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.082 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6de5ad56{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.083 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cfb94fd{/storage,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.083 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44a44a04{/storage/json,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.083 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a6fc1bc{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.084 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@360a3106{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.084 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e9a836{/environment,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.084 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75aa7703{/environment/json,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.085 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3395c2a7{/executors,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.085 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7831d1aa{/executors/json,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.086 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27746c5e{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.086 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2270f58d{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.088 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54737322{/static,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.089 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3278d065{/,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.089 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c9ef37b{/api,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.090 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e94de5f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.090 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74badf19{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.092 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50e8ed74{/metrics/json,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.181 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-13 22:32:09.182 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-13 22:32:09.185 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-13 22:32:09.189 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62808e9e{/SQL,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.190 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@539dd2d0{/SQL/json,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.190 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40aad17d{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.191 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bc14211{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.191 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@570299e3{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.474 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-13 22:32:09.632 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-13 22:32:09.651 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-13 22:32:09.668 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-13 22:32:09.668 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-13 22:32:09.668 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749828729668
2025-06-13 22:32:09.809 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-13 22:32:09.811 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-13 22:32:09.811 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-13 22:32:09.811 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-13 22:32:09.816 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-13 22:32:09.820 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-13 22:32:09.829 [main INFO ] com.example.Application - Started Application in 2.253 seconds (JVM running for 2.569)
2025-06-13 22:32:09.854 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-13 22:32:09.854 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-13 22:32:09.855 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-13 22:32:09.895 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-13 22:32:09.902 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8716585{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.903 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34653136{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.903 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68001441{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.904 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46c32490{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-13 22:32:09.904 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f266a8b{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:32:15.442 [http-nio-8080-exec-1 WARN ] o.s.w.s.m.s.DefaultHandlerExceptionResolver - Resolved [org.springframework.web.HttpMediaTypeNotSupportedException: Content type 'text/plain;charset=UTF-8' not supported]
2025-06-13 22:32:17.383 [http-nio-8080-exec-2 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:32:18.321 [http-nio-8080-exec-2 INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 91.266931 ms
2025-06-13 22:32:22.535 [http-nio-8080-exec-3 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:32:26.269 [http-nio-8080-exec-4 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:32:28.993 [http-nio-8080-exec-6 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:33:19.804 [http-nio-8080-exec-7 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:38:07.346 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-13 22:38:07.346 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:38:07.352 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@33163b39{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:38:07.354 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:38:07.354 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext already stopped.
2025-06-13 22:38:07.354 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-13 22:38:09.408 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 53573 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-13 22:38:09.409 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-13 22:38:09.939 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-13 22:38:09.944 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-13 22:38:09.945 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-13 22:38:09.945 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-13 22:38:09.985 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-13 22:38:09.985 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 557 ms
2025-06-13 22:38:10.387 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-13 22:38:10.432 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-13 22:38:10.458 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:38:10.459 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-13 22:38:10.459 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:38:10.459 [main INFO ] org.apache.spark.SparkContext - Submitted application: ClickstreamProcessor
2025-06-13 22:38:10.467 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-13 22:38:10.472 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-13 22:38:10.472 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-13 22:38:10.491 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-13 22:38:10.491 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-13 22:38:10.491 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-13 22:38:10.492 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-13 22:38:10.492 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-13 22:38:10.569 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 38575.
2025-06-13 22:38:10.577 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-13 22:38:10.589 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-13 22:38:10.595 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-13 22:38:10.595 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-13 22:38:10.596 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-13 22:38:10.600 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-e9d49e92-6602-43db-8e2b-f7c475e0fa35
2025-06-13 22:38:10.615 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-13 22:38:10.621 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-13 22:38:10.631 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1670ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-13 22:38:10.666 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-13 22:38:10.670 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-13 22:38:10.677 [main INFO ] org.sparkproject.jetty.server.Server - Started @1716ms
2025-06-13 22:38:10.688 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@febd667{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:38:10.688 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-13 22:38:10.695 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d978ab9{/,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.724 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-13 22:38:10.727 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-13 22:38:10.735 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45009.
2025-06-13 22:38:10.735 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:45009
2025-06-13 22:38:10.736 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-13 22:38:10.738 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 45009, None)
2025-06-13 22:38:10.740 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:45009 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 45009, None)
2025-06-13 22:38:10.741 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 45009, None)
2025-06-13 22:38:10.742 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 45009, None)
2025-06-13 22:38:10.756 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@d978ab9{/,null,STOPPED,@Spark}
2025-06-13 22:38:10.756 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133b712{/jobs,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.757 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70f91ae3{/jobs/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.757 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d93ff21{/jobs/job,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.757 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ca4c88a{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.758 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55397d15{/stages,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.758 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24ac6fef{/stages/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.758 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6de5ad56{/stages/stage,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.759 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cfb94fd{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.759 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44a44a04{/stages/pool,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.760 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a6fc1bc{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.760 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@360a3106{/storage,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.760 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e9a836{/storage/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.761 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75aa7703{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.761 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3395c2a7{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.761 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7831d1aa{/environment,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.762 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27746c5e{/environment/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.762 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2270f58d{/executors,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.762 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54737322{/executors/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.763 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7323c38c{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.763 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63a72cc6{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.765 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cef885d{/static,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.766 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5da3f32a{/,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.766 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51f4439e{/api,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.767 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8315e4a{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.767 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22ff11ef{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.769 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@215a0264{/metrics/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.860 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-13 22:38:10.861 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-13 22:38:10.864 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-13 22:38:10.868 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3664c596{/SQL,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.868 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e79e9e{/SQL/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.869 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74450c9b{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.869 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ad50b02{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:10.870 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@142918a0{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:38:11.134 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-13 22:38:11.288 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-13 22:38:11.307 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-13 22:38:11.327 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-13 22:38:11.327 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-13 22:38:11.327 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749829091326
2025-06-13 22:38:11.460 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-13 22:38:11.462 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-13 22:38:11.462 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-13 22:38:11.463 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-13 22:38:11.467 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-13 22:38:11.471 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-13 22:38:11.480 [main INFO ] com.example.Application - Started Application in 2.212 seconds (JVM running for 2.52)
2025-06-13 22:38:11.565 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-13 22:38:11.565 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-13 22:38:11.566 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-13 22:38:11.603 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-13 22:38:11.610 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c94c996{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-13 22:38:11.611 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33b97145{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:11.612 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76a8c8df{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-13 22:38:11.612 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d577db4{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:11.613 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16d5ad9c{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:38:19.912 [http-nio-8080-exec-1 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:38:20.774 [http-nio-8080-exec-1 INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 94.459214 ms
2025-06-13 22:38:32.255 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-13 22:38:32.255 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:38:32.260 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@febd667{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:38:32.262 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-13 22:38:32.262 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:38:32.263 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext already stopped.
2025-06-13 22:38:39.618 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 54282 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-13 22:38:39.619 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-13 22:38:40.179 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-13 22:38:40.182 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-13 22:38:40.182 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-13 22:38:40.182 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-13 22:38:40.229 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-13 22:38:40.229 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 589 ms
2025-06-13 22:38:40.595 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-13 22:38:40.641 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-13 22:38:40.671 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:38:40.671 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-13 22:38:40.671 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:38:40.671 [main INFO ] org.apache.spark.SparkContext - Submitted application: ClickstreamProcessor
2025-06-13 22:38:40.679 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-13 22:38:40.684 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-13 22:38:40.684 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-13 22:38:40.702 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-13 22:38:40.702 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-13 22:38:40.702 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-13 22:38:40.702 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-13 22:38:40.702 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-13 22:38:40.780 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 40737.
2025-06-13 22:38:40.788 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-13 22:38:40.799 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-13 22:38:40.805 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-13 22:38:40.805 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-13 22:38:40.806 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-13 22:38:40.811 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-d9345e49-bdd6-4d32-9914-9ec4bc22e193
2025-06-13 22:38:40.825 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-13 22:38:40.832 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-13 22:38:40.841 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1703ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-13 22:38:40.876 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-13 22:38:40.880 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-13 22:38:40.886 [main INFO ] org.sparkproject.jetty.server.Server - Started @1749ms
2025-06-13 22:38:40.897 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@4644258c{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:38:40.897 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-13 22:38:40.904 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a8640f7{/,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.935 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-13 22:38:40.938 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-13 22:38:40.946 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36283.
2025-06-13 22:38:40.946 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:36283
2025-06-13 22:38:40.946 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-13 22:38:40.949 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 36283, None)
2025-06-13 22:38:40.951 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:36283 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 36283, None)
2025-06-13 22:38:40.953 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 36283, None)
2025-06-13 22:38:40.953 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 36283, None)
2025-06-13 22:38:40.968 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@3a8640f7{/,null,STOPPED,@Spark}
2025-06-13 22:38:40.968 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4bb9f7d4{/jobs,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.969 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@183ef89a{/jobs/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.969 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a8b42a3{/jobs/job,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.970 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44106e25{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.970 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5649f55{/stages,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.970 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12270a01{/stages/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.971 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e3ee457{/stages/stage,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.971 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fb2c2f3{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.971 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d2a8819{/stages/pool,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.972 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b64bf61{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.972 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7846913f{/storage,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.972 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60b553f{/storage/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.973 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66abb2fa{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.973 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133b712{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.973 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70f91ae3{/environment,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.974 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c2a3f0c{/environment/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.974 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d93ff21{/executors,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.974 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ca4c88a{/executors/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.975 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55397d15{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.975 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24ac6fef{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.977 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@227b9277{/static,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.978 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54737322{/,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.979 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7323c38c{/api,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.979 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64688978{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.979 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25f14e93{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-13 22:38:40.981 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51468039{/metrics/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:41.066 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-13 22:38:41.066 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-13 22:38:41.069 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-13 22:38:41.074 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ebf776c{/SQL,null,AVAILABLE,@Spark}
2025-06-13 22:38:41.074 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54ae1240{/SQL/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:41.074 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a0baec0{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-13 22:38:41.075 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@560d6d2{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:41.075 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a12f3e7{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:38:41.351 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-13 22:38:41.498 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-13 22:38:41.518 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-13 22:38:41.535 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-13 22:38:41.535 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-13 22:38:41.535 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749829121534
2025-06-13 22:38:41.665 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-13 22:38:41.668 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-13 22:38:41.668 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-13 22:38:41.668 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-13 22:38:41.674 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-13 22:38:41.678 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-13 22:38:41.688 [main INFO ] com.example.Application - Started Application in 2.23 seconds (JVM running for 2.55)
2025-06-13 22:38:41.721 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-13 22:38:41.721 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-13 22:38:41.722 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-13 22:38:41.760 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-13 22:38:41.767 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@782a28fd{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-13 22:38:41.767 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c089e95{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:41.767 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@597de4bb{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-13 22:38:41.768 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77d222e7{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-13 22:38:41.768 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a8b6a0d{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:38:53.994 [http-nio-8080-exec-1 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:38:54.855 [http-nio-8080-exec-1 INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 90.683281 ms
2025-06-13 22:38:57.759 [http-nio-8080-exec-2 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:38:59.971 [http-nio-8080-exec-3 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:39:02.008 [http-nio-8080-exec-4 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:39:06.308 [http-nio-8080-exec-5 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:41:11.088 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-13 22:41:11.088 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:41:11.093 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@4644258c{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:41:11.096 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-13 22:41:11.098 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:41:11.099 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext already stopped.
2025-06-13 22:41:13.046 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 56397 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-13 22:41:13.047 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-13 22:41:13.571 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-13 22:41:13.573 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-13 22:41:13.574 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-13 22:41:13.574 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-13 22:41:13.610 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-13 22:41:13.610 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 545 ms
2025-06-13 22:41:13.987 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-13 22:41:14.030 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-13 22:41:14.060 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:41:14.060 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-13 22:41:14.060 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:41:14.060 [main INFO ] org.apache.spark.SparkContext - Submitted application: ClickstreamProcessor
2025-06-13 22:41:14.069 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-13 22:41:14.074 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-13 22:41:14.074 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-13 22:41:14.091 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-13 22:41:14.091 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-13 22:41:14.091 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-13 22:41:14.091 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-13 22:41:14.091 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-13 22:41:14.167 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 41885.
2025-06-13 22:41:14.175 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-13 22:41:14.187 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-13 22:41:14.193 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-13 22:41:14.193 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-13 22:41:14.195 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-13 22:41:14.199 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-9d737b4a-65fa-4d33-89f4-4a0c22f32981
2025-06-13 22:41:14.213 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-13 22:41:14.220 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-13 22:41:14.230 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1639ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-13 22:41:14.265 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-13 22:41:14.269 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-13 22:41:14.275 [main INFO ] org.sparkproject.jetty.server.Server - Started @1685ms
2025-06-13 22:41:14.286 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@47a91eb{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:41:14.286 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-13 22:41:14.294 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f65a203{/,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.323 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-13 22:41:14.326 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-13 22:41:14.333 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42385.
2025-06-13 22:41:14.333 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:42385
2025-06-13 22:41:14.334 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-13 22:41:14.336 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 42385, None)
2025-06-13 22:41:14.339 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:42385 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 42385, None)
2025-06-13 22:41:14.341 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 42385, None)
2025-06-13 22:41:14.341 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 42385, None)
2025-06-13 22:41:14.355 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6f65a203{/,null,STOPPED,@Spark}
2025-06-13 22:41:14.356 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7846913f{/jobs,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.356 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60b553f{/jobs/json,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.357 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133b712{/jobs/job,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.357 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70f91ae3{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.358 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c2a3f0c{/stages,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.358 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d93ff21{/stages/json,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.358 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24ac6fef{/stages/stage,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.359 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@227b9277{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.359 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b56d8a7{/stages/pool,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.359 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6de5ad56{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.360 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cfb94fd{/storage,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.360 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44a44a04{/storage/json,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.360 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a6fc1bc{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.361 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@360a3106{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.361 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e9a836{/environment,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.361 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75aa7703{/environment/json,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.362 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3395c2a7{/executors,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.362 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7831d1aa{/executors/json,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.363 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27746c5e{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.363 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2270f58d{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.365 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54737322{/static,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.366 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3278d065{/,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.366 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c9ef37b{/api,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.367 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e94de5f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.367 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74badf19{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.369 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50e8ed74{/metrics/json,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.454 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-13 22:41:14.454 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-13 22:41:14.457 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-13 22:41:14.461 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62808e9e{/SQL,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.462 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@539dd2d0{/SQL/json,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.462 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40aad17d{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.463 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bc14211{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.463 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@570299e3{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:41:14.738 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-13 22:41:14.886 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-13 22:41:14.903 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-13 22:41:14.920 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-13 22:41:14.920 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-13 22:41:14.920 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749829274919
2025-06-13 22:41:15.050 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-13 22:41:15.052 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-13 22:41:15.052 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-13 22:41:15.052 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-13 22:41:15.057 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-13 22:41:15.061 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-13 22:41:15.069 [main INFO ] com.example.Application - Started Application in 2.16 seconds (JVM running for 2.479)
2025-06-13 22:41:15.191 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-13 22:41:15.191 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-13 22:41:15.191 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 0 ms
2025-06-13 22:41:15.229 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-13 22:41:15.235 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6eecc1b4{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-13 22:41:15.236 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@440d4802{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-13 22:41:15.236 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e2e1d02{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-13 22:41:15.237 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36deb216{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-13 22:41:15.237 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a0644b6{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:41:18.304 [http-nio-8080-exec-1 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:41:19.171 [http-nio-8080-exec-1 INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 93.242519 ms
2025-06-13 22:43:52.421 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-13 22:43:52.421 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:43:52.427 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@47a91eb{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:43:52.430 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:43:52.430 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-13 22:43:52.430 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext already stopped.
2025-06-13 22:43:53.675 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 58551 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-13 22:43:53.676 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-13 22:43:54.342 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-13 22:43:54.344 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-13 22:43:54.345 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-13 22:43:54.345 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-13 22:43:54.390 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-13 22:43:54.390 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 694 ms
2025-06-13 22:43:54.755 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-13 22:43:54.800 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-13 22:43:54.829 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:43:54.829 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-13 22:43:54.829 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:43:54.829 [main INFO ] org.apache.spark.SparkContext - Submitted application: ClickstreamProcessor
2025-06-13 22:43:54.837 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-13 22:43:54.842 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-13 22:43:54.842 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-13 22:43:54.859 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-13 22:43:54.860 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-13 22:43:54.860 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-13 22:43:54.860 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-13 22:43:54.860 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-13 22:43:54.939 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 34913.
2025-06-13 22:43:54.947 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-13 22:43:54.959 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-13 22:43:54.966 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-13 22:43:54.966 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-13 22:43:54.968 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-13 22:43:54.972 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-bde1b00a-5266-4b0d-93fd-d9b2571c6ec7
2025-06-13 22:43:54.987 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-13 22:43:54.994 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-13 22:43:55.004 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1806ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-13 22:43:55.044 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-13 22:43:55.048 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-13 22:43:55.055 [main INFO ] org.sparkproject.jetty.server.Server - Started @1857ms
2025-06-13 22:43:55.066 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@7afcb2f0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:43:55.066 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-13 22:43:55.073 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d978ab9{/,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.103 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-13 22:43:55.107 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-13 22:43:55.116 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42077.
2025-06-13 22:43:55.116 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:42077
2025-06-13 22:43:55.117 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-13 22:43:55.119 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 42077, None)
2025-06-13 22:43:55.121 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:42077 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 42077, None)
2025-06-13 22:43:55.123 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 42077, None)
2025-06-13 22:43:55.123 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 42077, None)
2025-06-13 22:43:55.138 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@d978ab9{/,null,STOPPED,@Spark}
2025-06-13 22:43:55.138 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133b712{/jobs,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.139 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70f91ae3{/jobs/json,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.139 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d93ff21{/jobs/job,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.139 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ca4c88a{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.140 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55397d15{/stages,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.140 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24ac6fef{/stages/json,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.141 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6de5ad56{/stages/stage,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.141 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cfb94fd{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.141 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44a44a04{/stages/pool,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.142 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a6fc1bc{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.142 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@360a3106{/storage,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.142 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e9a836{/storage/json,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.143 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75aa7703{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.143 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3395c2a7{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.143 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7831d1aa{/environment,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.144 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27746c5e{/environment/json,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.144 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2270f58d{/executors,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.144 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54737322{/executors/json,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.145 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7323c38c{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.145 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63a72cc6{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.148 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cef885d{/static,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.148 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5da3f32a{/,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.149 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51f4439e{/api,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.149 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8315e4a{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.150 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22ff11ef{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.151 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@215a0264{/metrics/json,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.242 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-13 22:43:55.242 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-13 22:43:55.245 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-13 22:43:55.250 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3664c596{/SQL,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.250 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e79e9e{/SQL/json,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.250 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74450c9b{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.251 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ad50b02{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.251 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@142918a0{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:43:55.536 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-13 22:43:55.689 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-13 22:43:55.706 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-13 22:43:55.723 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-13 22:43:55.723 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-13 22:43:55.723 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749829435722
2025-06-13 22:43:55.860 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-13 22:43:55.863 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-13 22:43:55.863 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-13 22:43:55.863 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-13 22:43:55.869 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-13 22:43:55.873 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-13 22:43:55.883 [main INFO ] com.example.Application - Started Application in 2.356 seconds (JVM running for 2.684)
2025-06-13 22:43:56.298 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-13 22:43:56.298 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-13 22:43:56.299 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-13 22:43:56.342 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-13 22:43:56.351 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@354b4ddf{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-13 22:43:56.352 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e5d8c0a{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-13 22:43:56.353 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@627446d4{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-13 22:43:56.353 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@527f9b1{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-13 22:43:56.354 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76ecd9bb{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:44:05.190 [http-nio-8080-exec-1 WARN ] o.s.w.s.m.s.DefaultHandlerExceptionResolver - Resolved [org.springframework.web.HttpMediaTypeNotSupportedException: Content type 'text/plain;charset=UTF-8' not supported]
2025-06-13 22:44:06.364 [http-nio-8080-exec-2 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:44:07.274 [http-nio-8080-exec-2 INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 95.928884 ms
2025-06-13 22:46:05.550 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-13 22:46:05.550 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:46:05.557 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@7afcb2f0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:46:05.558 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:46:05.558 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext already stopped.
2025-06-13 22:46:05.558 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-13 22:46:07.481 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 60391 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-13 22:46:07.482 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-13 22:46:08.030 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-13 22:46:08.033 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-13 22:46:08.033 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-13 22:46:08.034 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-13 22:46:08.080 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-13 22:46:08.080 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 580 ms
2025-06-13 22:46:08.504 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-13 22:46:08.551 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-13 22:46:08.580 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:46:08.581 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-13 22:46:08.581 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:46:08.581 [main INFO ] org.apache.spark.SparkContext - Submitted application: ClickstreamProcessor
2025-06-13 22:46:08.589 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-13 22:46:08.595 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-13 22:46:08.595 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-13 22:46:08.613 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-13 22:46:08.613 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-13 22:46:08.614 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-13 22:46:08.614 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-13 22:46:08.614 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-13 22:46:08.689 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 32965.
2025-06-13 22:46:08.697 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-13 22:46:08.709 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-13 22:46:08.715 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-13 22:46:08.715 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-13 22:46:08.716 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-13 22:46:08.720 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-ac1ed782-c041-49dc-9c1f-88e8aa01d942
2025-06-13 22:46:08.735 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-13 22:46:08.741 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-13 22:46:08.751 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1731ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-13 22:46:08.786 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-13 22:46:08.790 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-13 22:46:08.796 [main INFO ] org.sparkproject.jetty.server.Server - Started @1777ms
2025-06-13 22:46:08.807 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@488db032{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:46:08.807 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-13 22:46:08.815 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48d31d25{/,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.844 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-13 22:46:08.847 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-13 22:46:08.855 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40705.
2025-06-13 22:46:08.855 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:40705
2025-06-13 22:46:08.855 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-13 22:46:08.858 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 40705, None)
2025-06-13 22:46:08.859 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:40705 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 40705, None)
2025-06-13 22:46:08.861 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 40705, None)
2025-06-13 22:46:08.861 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 40705, None)
2025-06-13 22:46:08.875 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@48d31d25{/,null,STOPPED,@Spark}
2025-06-13 22:46:08.876 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b64bf61{/jobs,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.876 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7846913f{/jobs/json,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.877 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66abb2fa{/jobs/job,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.877 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133b712{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.877 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70f91ae3{/stages,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.878 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c2a3f0c{/stages/json,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.878 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55397d15{/stages/stage,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.878 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24ac6fef{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.879 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@227b9277{/stages/pool,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.879 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b56d8a7{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.879 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6de5ad56{/storage,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.880 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cfb94fd{/storage/json,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.880 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44a44a04{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.880 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a6fc1bc{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.881 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@360a3106{/environment,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.881 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e9a836{/environment/json,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.881 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75aa7703{/executors,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.882 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3395c2a7{/executors/json,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.882 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7831d1aa{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.883 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27746c5e{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.885 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2270f58d{/static,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.885 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4feec184{/,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.886 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3278d065{/api,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.886 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@575d48db{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.887 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e94de5f{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.888 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5de335cf{/metrics/json,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.976 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-13 22:46:08.977 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-13 22:46:08.980 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-13 22:46:08.984 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71df5f30{/SQL,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.985 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62808e9e{/SQL/json,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.985 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b529d7e{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.985 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40aad17d{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-13 22:46:08.986 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74f89bad{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:46:09.267 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-13 22:46:09.422 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-13 22:46:09.441 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-13 22:46:09.458 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-13 22:46:09.458 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-13 22:46:09.459 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749829569458
2025-06-13 22:46:09.588 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-13 22:46:09.590 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-13 22:46:09.590 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-13 22:46:09.590 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-13 22:46:09.595 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-13 22:46:09.598 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-13 22:46:09.617 [main INFO ] com.example.Application - Started Application in 2.278 seconds (JVM running for 2.598)
2025-06-13 22:46:10.122 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-13 22:46:10.122 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-13 22:46:10.122 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 0 ms
2025-06-13 22:46:10.167 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-13 22:46:10.172 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@714b131b{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-13 22:46:10.173 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27eb380b{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-13 22:46:10.173 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@550ae801{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-13 22:46:10.174 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77507770{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-13 22:46:10.174 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5e43bf{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:46:13.598 [http-nio-8080-exec-1 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:46:14.490 [http-nio-8080-exec-1 INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 110.26848 ms
2025-06-13 22:46:14.519 [http-nio-8080-exec-1 INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 20.034529 ms
2025-06-13 22:46:14.534 [http-nio-8080-exec-1 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {}
2025-06-13 22:46:14.536 [http-nio-8080-exec-1 ERROR] c.e.c.producer.ClickstreamProducer - Unexpected error sending event
java.lang.IllegalArgumentException: event_name does not exist. Available: appId, eventId, eventName, eventParams, eventTimestamp, pageUrl, platform, sessionId, userId
	at org.apache.spark.sql.types.StructType.$anonfun$fieldIndex$1(StructType.scala:313)
	at scala.collection.immutable.HashMap$HashTrieMap.getOrElse0(HashMap.scala:596)
	at scala.collection.immutable.HashMap.getOrElse(HashMap.scala:73)
	at org.apache.spark.sql.types.StructType.fieldIndex(StructType.scala:312)
	at org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema.fieldIndex(rows.scala:187)
	at com.example.clickstream.producer.ClickstreamProducer.sendEvent(ClickstreamProducer.java:57)
	at com.example.clickstream.producer.ClickstreamProducer.sendEvents(ClickstreamProducer.java:82)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:42)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-13 22:46:14.537 [http-nio-8080-exec-1 ERROR] c.e.c.c.ClickstreamController - Error processing events
com.example.clickstream.exception.ProducerException: Failed to send event
	at com.example.clickstream.producer.ClickstreamProducer.sendEvent(ClickstreamProducer.java:74)
	at com.example.clickstream.producer.ClickstreamProducer.sendEvents(ClickstreamProducer.java:82)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:42)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: event_name does not exist. Available: appId, eventId, eventName, eventParams, eventTimestamp, pageUrl, platform, sessionId, userId
	at org.apache.spark.sql.types.StructType.$anonfun$fieldIndex$1(StructType.scala:313)
	at scala.collection.immutable.HashMap$HashTrieMap.getOrElse0(HashMap.scala:596)
	at scala.collection.immutable.HashMap.getOrElse(HashMap.scala:73)
	at org.apache.spark.sql.types.StructType.fieldIndex(StructType.scala:312)
	at org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema.fieldIndex(rows.scala:187)
	at com.example.clickstream.producer.ClickstreamProducer.sendEvent(ClickstreamProducer.java:57)
	... 57 common frames omitted
2025-06-13 22:46:17.618 [http-nio-8080-exec-2 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:46:17.640 [http-nio-8080-exec-2 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {}
2025-06-13 22:46:17.640 [http-nio-8080-exec-2 ERROR] c.e.c.producer.ClickstreamProducer - Unexpected error sending event
java.lang.IllegalArgumentException: event_name does not exist. Available: appId, eventId, eventName, eventParams, eventTimestamp, pageUrl, platform, sessionId, userId
	at org.apache.spark.sql.types.StructType.$anonfun$fieldIndex$1(StructType.scala:313)
	at scala.collection.immutable.HashMap$HashTrieMap.getOrElse0(HashMap.scala:596)
	at scala.collection.immutable.HashMap.getOrElse(HashMap.scala:73)
	at org.apache.spark.sql.types.StructType.fieldIndex(StructType.scala:312)
	at org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema.fieldIndex(rows.scala:187)
	at com.example.clickstream.producer.ClickstreamProducer.sendEvent(ClickstreamProducer.java:57)
	at com.example.clickstream.producer.ClickstreamProducer.sendEvents(ClickstreamProducer.java:82)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:42)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-13 22:46:17.640 [http-nio-8080-exec-2 ERROR] c.e.c.c.ClickstreamController - Error processing events
com.example.clickstream.exception.ProducerException: Failed to send event
	at com.example.clickstream.producer.ClickstreamProducer.sendEvent(ClickstreamProducer.java:74)
	at com.example.clickstream.producer.ClickstreamProducer.sendEvents(ClickstreamProducer.java:82)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:42)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: event_name does not exist. Available: appId, eventId, eventName, eventParams, eventTimestamp, pageUrl, platform, sessionId, userId
	at org.apache.spark.sql.types.StructType.$anonfun$fieldIndex$1(StructType.scala:313)
	at scala.collection.immutable.HashMap$HashTrieMap.getOrElse0(HashMap.scala:596)
	at scala.collection.immutable.HashMap.getOrElse(HashMap.scala:73)
	at org.apache.spark.sql.types.StructType.fieldIndex(StructType.scala:312)
	at org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema.fieldIndex(rows.scala:187)
	at com.example.clickstream.producer.ClickstreamProducer.sendEvent(ClickstreamProducer.java:57)
	... 57 common frames omitted
2025-06-13 22:46:21.062 [http-nio-8080-exec-3 WARN ] o.s.w.s.m.s.DefaultHandlerExceptionResolver - Resolved [org.springframework.web.HttpRequestMethodNotSupportedException: Request method 'GET' not supported]
2025-06-13 22:48:12.446 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-13 22:48:12.446 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:48:12.452 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@488db032{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:48:12.454 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-13 22:48:12.454 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:48:12.455 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext already stopped.
2025-06-13 22:48:14.449 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 62213 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-13 22:48:14.450 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-13 22:48:14.981 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-13 22:48:14.984 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-13 22:48:14.984 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-13 22:48:14.985 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-13 22:48:15.026 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-13 22:48:15.026 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 554 ms
2025-06-13 22:48:15.470 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-13 22:48:15.513 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-13 22:48:15.542 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:48:15.542 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-13 22:48:15.542 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:48:15.542 [main INFO ] org.apache.spark.SparkContext - Submitted application: ClickstreamProcessor
2025-06-13 22:48:15.550 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-13 22:48:15.555 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-13 22:48:15.556 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-13 22:48:15.574 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-13 22:48:15.575 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-13 22:48:15.575 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-13 22:48:15.575 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-13 22:48:15.575 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-13 22:48:15.657 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 40989.
2025-06-13 22:48:15.665 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-13 22:48:15.677 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-13 22:48:15.683 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-13 22:48:15.684 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-13 22:48:15.685 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-13 22:48:15.690 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-85bfa6a7-d887-46b5-99b4-08680cfa7294
2025-06-13 22:48:15.706 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-13 22:48:15.713 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-13 22:48:15.723 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1750ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-13 22:48:15.761 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-13 22:48:15.765 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-13 22:48:15.771 [main INFO ] org.sparkproject.jetty.server.Server - Started @1799ms
2025-06-13 22:48:15.782 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@3c27481e{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:48:15.782 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-13 22:48:15.789 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d978ab9{/,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.819 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-13 22:48:15.823 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-13 22:48:15.830 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44363.
2025-06-13 22:48:15.831 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:44363
2025-06-13 22:48:15.831 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-13 22:48:15.834 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 44363, None)
2025-06-13 22:48:15.836 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:44363 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 44363, None)
2025-06-13 22:48:15.838 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 44363, None)
2025-06-13 22:48:15.839 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 44363, None)
2025-06-13 22:48:15.853 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@d978ab9{/,null,STOPPED,@Spark}
2025-06-13 22:48:15.853 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133b712{/jobs,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.854 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70f91ae3{/jobs/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.854 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d93ff21{/jobs/job,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.855 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ca4c88a{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.855 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55397d15{/stages,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.855 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24ac6fef{/stages/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.856 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6de5ad56{/stages/stage,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.856 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cfb94fd{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.857 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44a44a04{/stages/pool,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.857 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a6fc1bc{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.857 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@360a3106{/storage,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.858 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e9a836{/storage/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.858 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75aa7703{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.858 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3395c2a7{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.859 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7831d1aa{/environment,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.859 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27746c5e{/environment/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.859 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2270f58d{/executors,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.860 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54737322{/executors/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.860 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7323c38c{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.860 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63a72cc6{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.863 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cef885d{/static,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.863 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5da3f32a{/,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.864 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51f4439e{/api,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.864 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8315e4a{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.865 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22ff11ef{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.866 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@215a0264{/metrics/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.956 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-13 22:48:15.956 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-13 22:48:15.960 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-13 22:48:15.964 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3664c596{/SQL,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.965 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e79e9e{/SQL/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.965 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74450c9b{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.965 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ad50b02{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:15.966 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@142918a0{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:48:16.251 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-13 22:48:16.402 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-13 22:48:16.421 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-13 22:48:16.441 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-13 22:48:16.441 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-13 22:48:16.441 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749829696440
2025-06-13 22:48:16.577 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-13 22:48:16.580 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-13 22:48:16.580 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-13 22:48:16.580 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-13 22:48:16.585 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-13 22:48:16.589 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-13 22:48:16.597 [main INFO ] com.example.Application - Started Application in 2.291 seconds (JVM running for 2.625)
2025-06-13 22:48:17.086 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-13 22:48:17.086 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-13 22:48:17.086 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 0 ms
2025-06-13 22:48:17.127 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-13 22:48:17.133 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@168e94d6{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-13 22:48:17.134 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4700d76a{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:17.134 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@675c9134{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-13 22:48:17.134 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37a2572e{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:17.135 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b5bda23{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:48:23.646 [http-nio-8080-exec-1 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:48:24.349 [http-nio-8080-exec-1 INFO ] c.e.c.service.ClickstreamService - Input raw events size: 5
2025-06-13 22:48:24.503 [http-nio-8080-exec-1 INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 88.458254 ms
2025-06-13 22:48:24.774 [http-nio-8080-exec-1 INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 11.375859 ms
2025-06-13 22:48:24.830 [http-nio-8080-exec-1 INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.306989 ms
2025-06-13 22:48:24.891 [http-nio-8080-exec-1 INFO ] org.apache.spark.SparkContext - Starting job: count at ClickstreamService.java:42
2025-06-13 22:48:24.899 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 2 (count at ClickstreamService.java:42) as input to shuffle 0
2025-06-13 22:48:24.902 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (count at ClickstreamService.java:42) with 1 output partitions
2025-06-13 22:48:24.902 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at ClickstreamService.java:42)
2025-06-13 22:48:24.902 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-13 22:48:24.903 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-13 22:48:24.904 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at count at ClickstreamService.java:42), which has no missing parents
2025-06-13 22:48:24.933 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-13 22:48:25.031 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-13 22:48:25.033 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:44363 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 22:48:25.035 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-13 22:48:25.039 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at count at ClickstreamService.java:42) (first 15 tasks are for partitions Vector(0))
2025-06-13 22:48:25.039 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-13 22:48:25.060 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-13 22:48:25.064 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
2025-06-13 22:48:25.109 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-13 22:48:25.110 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
2025-06-13 22:48:25.123 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 3800 bytes result sent to driver
2025-06-13 22:48:25.133 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 78 ms on phamviethoa (executor driver) (1/1)
2025-06-13 22:48:25.134 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-13 22:48:25.136 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (count at ClickstreamService.java:42) finished in 0.227 s
2025-06-13 22:48:25.137 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-13 22:48:25.138 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-13 22:48:25.138 [http-nio-8080-exec-1 INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: count at ClickstreamService.java:42, took 0.246747 s
2025-06-13 22:48:25.142 [http-nio-8080-exec-1 INFO ] c.e.c.service.ClickstreamService - Validated event count: 0
2025-06-13 22:48:25.168 [http-nio-8080-exec-1 INFO ] org.apache.spark.SparkContext - Starting job: count at ClickstreamService.java:43
2025-06-13 22:48:25.168 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 8 (count at ClickstreamService.java:43) as input to shuffle 1
2025-06-13 22:48:25.169 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 1 (count at ClickstreamService.java:43) with 1 output partitions
2025-06-13 22:48:25.169 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (count at ClickstreamService.java:43)
2025-06-13 22:48:25.169 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
2025-06-13 22:48:25.169 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-13 22:48:25.169 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[11] at count at ClickstreamService.java:43), which has no missing parents
2025-06-13 22:48:25.170 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-13 22:48:25.175 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-13 22:48:25.175 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:44363 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 22:48:25.175 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-13 22:48:25.175 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at count at ClickstreamService.java:43) (first 15 tasks are for partitions Vector(0))
2025-06-13 22:48:25.175 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks resource profile 0
2025-06-13 22:48:25.176 [dispatcher-event-loop-10 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 1) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-13 22:48:25.177 [Executor task launch worker for task 0.0 in stage 3.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 1)
2025-06-13 22:48:25.179 [Executor task launch worker for task 0.0 in stage 3.0 (TID 1) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-13 22:48:25.179 [Executor task launch worker for task 0.0 in stage 3.0 (TID 1) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
2025-06-13 22:48:25.180 [Executor task launch worker for task 0.0 in stage 3.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 1). 3800 bytes result sent to driver
2025-06-13 22:48:25.182 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 1) in 6 ms on phamviethoa (executor driver) (1/1)
2025-06-13 22:48:25.182 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2025-06-13 22:48:25.183 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 3 (count at ClickstreamService.java:43) finished in 0.012 s
2025-06-13 22:48:25.183 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-13 22:48:25.183 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 3: Stage finished
2025-06-13 22:48:25.183 [http-nio-8080-exec-1 INFO ] o.a.spark.scheduler.DAGScheduler - Job 1 finished: count at ClickstreamService.java:43, took 0.014841 s
2025-06-13 22:48:25.183 [http-nio-8080-exec-1 INFO ] c.e.c.service.ClickstreamService - Enriched event count: 0
2025-06-13 22:48:25.214 [http-nio-8080-exec-1 INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 13.504435 ms
2025-06-13 22:48:25.236 [http-nio-8080-exec-1 INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 14.509248 ms
2025-06-13 22:48:25.244 [http-nio-8080-exec-1 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {}
2025-06-13 22:48:25.247 [http-nio-8080-exec-1 ERROR] c.e.c.producer.ClickstreamProducer - Unexpected error sending event
java.lang.IllegalArgumentException: event_name does not exist. Available: appId, eventId, eventName, eventParams, eventTimestamp, pageUrl, platform, sessionId, userId
	at org.apache.spark.sql.types.StructType.$anonfun$fieldIndex$1(StructType.scala:313)
	at scala.collection.immutable.HashMap$HashTrieMap.getOrElse0(HashMap.scala:596)
	at scala.collection.immutable.HashMap.getOrElse(HashMap.scala:73)
	at org.apache.spark.sql.types.StructType.fieldIndex(StructType.scala:312)
	at org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema.fieldIndex(rows.scala:187)
	at com.example.clickstream.producer.ClickstreamProducer.sendEvent(ClickstreamProducer.java:57)
	at com.example.clickstream.producer.ClickstreamProducer.sendEvents(ClickstreamProducer.java:82)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:45)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-13 22:48:25.247 [http-nio-8080-exec-1 ERROR] c.e.c.c.ClickstreamController - Error processing events
com.example.clickstream.exception.ProducerException: Failed to send event
	at com.example.clickstream.producer.ClickstreamProducer.sendEvent(ClickstreamProducer.java:74)
	at com.example.clickstream.producer.ClickstreamProducer.sendEvents(ClickstreamProducer.java:82)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:45)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: event_name does not exist. Available: appId, eventId, eventName, eventParams, eventTimestamp, pageUrl, platform, sessionId, userId
	at org.apache.spark.sql.types.StructType.$anonfun$fieldIndex$1(StructType.scala:313)
	at scala.collection.immutable.HashMap$HashTrieMap.getOrElse0(HashMap.scala:596)
	at scala.collection.immutable.HashMap.getOrElse(HashMap.scala:73)
	at org.apache.spark.sql.types.StructType.fieldIndex(StructType.scala:312)
	at org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema.fieldIndex(rows.scala:187)
	at com.example.clickstream.producer.ClickstreamProducer.sendEvent(ClickstreamProducer.java:57)
	... 57 common frames omitted
2025-06-13 22:48:40.572 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-13 22:48:40.572 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:48:40.578 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@3c27481e{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:48:40.582 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-13 22:48:40.583 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:48:40.583 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext already stopped.
2025-06-13 22:48:42.548 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 62887 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-13 22:48:42.549 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-13 22:48:43.093 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-13 22:48:43.099 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-13 22:48:43.100 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-13 22:48:43.100 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-13 22:48:43.145 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-13 22:48:43.145 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 575 ms
2025-06-13 22:48:43.523 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-13 22:48:43.568 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-13 22:48:43.596 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:48:43.596 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-13 22:48:43.596 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:48:43.596 [main INFO ] org.apache.spark.SparkContext - Submitted application: ClickstreamProcessor
2025-06-13 22:48:43.604 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-13 22:48:43.609 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-13 22:48:43.609 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-13 22:48:43.625 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-13 22:48:43.625 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-13 22:48:43.625 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-13 22:48:43.625 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-13 22:48:43.625 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-13 22:48:43.704 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 43971.
2025-06-13 22:48:43.712 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-13 22:48:43.724 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-13 22:48:43.730 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-13 22:48:43.730 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-13 22:48:43.731 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-13 22:48:43.736 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-ed7b0f6a-3d87-4fca-a5a8-ee93537dd134
2025-06-13 22:48:43.751 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-13 22:48:43.758 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-13 22:48:43.768 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1730ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-13 22:48:43.804 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-13 22:48:43.808 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-13 22:48:43.814 [main INFO ] org.sparkproject.jetty.server.Server - Started @1777ms
2025-06-13 22:48:43.825 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@26e05c2c{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:48:43.825 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-13 22:48:43.832 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e7cfd49{/,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.861 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-13 22:48:43.864 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-13 22:48:43.871 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39289.
2025-06-13 22:48:43.871 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:39289
2025-06-13 22:48:43.872 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-13 22:48:43.875 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 39289, None)
2025-06-13 22:48:43.877 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:39289 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 39289, None)
2025-06-13 22:48:43.879 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 39289, None)
2025-06-13 22:48:43.879 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 39289, None)
2025-06-13 22:48:43.895 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1e7cfd49{/,null,STOPPED,@Spark}
2025-06-13 22:48:43.895 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59c862af{/jobs,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.896 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@673a9db4{/jobs/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.896 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@351e86b2{/jobs/job,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.896 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11582db6{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.897 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57a5b03{/stages,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.897 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@558127d2{/stages/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.898 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@401b67a9{/stages/stage,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.898 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41b66d1{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.898 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@148fca83{/stages/pool,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.899 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72585e83{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.899 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d5bb5c{/storage,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.899 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48da64f2{/storage/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.900 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2aa811f9{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.900 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d4da729{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.900 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b7e4d14{/environment,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.901 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@601d9f3a{/environment/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.901 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6585df70{/executors,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.901 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51fb5fe6{/executors/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.902 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1791e231{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.902 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e360c3b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.905 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3cb49121{/static,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.905 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50e1f3fc{/,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.906 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56da8847{/api,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.906 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70ce2fb2{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.906 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d525897{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.908 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@225ddf5f{/metrics/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:43.995 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-13 22:48:43.996 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-13 22:48:43.999 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-13 22:48:44.005 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f08f8a9{/SQL,null,AVAILABLE,@Spark}
2025-06-13 22:48:44.005 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7fd3fd06{/SQL/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:44.005 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d49a1a0{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-13 22:48:44.006 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50c2ef56{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:44.006 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27210a3b{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:48:44.281 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-13 22:48:44.432 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-13 22:48:44.448 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-13 22:48:44.465 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-13 22:48:44.465 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-13 22:48:44.465 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749829724464
2025-06-13 22:48:44.596 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-13 22:48:44.607 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-13 22:48:44.608 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-13 22:48:44.608 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-13 22:48:44.612 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-13 22:48:44.616 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-13 22:48:44.625 [main INFO ] com.example.Application - Started Application in 2.229 seconds (JVM running for 2.588)
2025-06-13 22:48:44.670 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-13 22:48:44.670 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-13 22:48:44.670 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 0 ms
2025-06-13 22:48:44.706 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-13 22:48:44.713 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a21be51{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-13 22:48:44.713 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56a71add{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:44.714 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a1c02aa{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-13 22:48:44.714 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67b1b671{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-13 22:48:44.715 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16104f30{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:48:49.481 [http-nio-8080-exec-1 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:48:50.333 [http-nio-8080-exec-1 INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 91.400696 ms
2025-06-13 22:49:17.043 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-13 22:49:17.044 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:49:17.049 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@26e05c2c{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:49:17.053 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-13 22:49:17.056 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 22:49:17.056 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext already stopped.
2025-06-13 22:49:18.997 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 63561 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-13 22:49:18.999 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-13 22:49:19.522 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-13 22:49:19.528 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-13 22:49:19.528 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-13 22:49:19.528 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-13 22:49:19.569 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-13 22:49:19.569 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 551 ms
2025-06-13 22:49:19.979 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-13 22:49:20.022 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-13 22:49:20.050 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:49:20.051 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-13 22:49:20.051 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 22:49:20.051 [main INFO ] org.apache.spark.SparkContext - Submitted application: ClickstreamProcessor
2025-06-13 22:49:20.059 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-13 22:49:20.065 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-13 22:49:20.065 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-13 22:49:20.083 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-13 22:49:20.083 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-13 22:49:20.083 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-13 22:49:20.083 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-13 22:49:20.083 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-13 22:49:20.166 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 36583.
2025-06-13 22:49:20.174 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-13 22:49:20.186 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-13 22:49:20.192 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-13 22:49:20.192 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-13 22:49:20.193 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-13 22:49:20.198 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-85c2602f-11d8-45ad-8db6-0abb931578e9
2025-06-13 22:49:20.212 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-13 22:49:20.219 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-13 22:49:20.229 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1691ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-13 22:49:20.265 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-13 22:49:20.269 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-13 22:49:20.275 [main INFO ] org.sparkproject.jetty.server.Server - Started @1738ms
2025-06-13 22:49:20.286 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@67c7b396{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 22:49:20.286 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-13 22:49:20.293 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c89b864{/,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.323 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-13 22:49:20.326 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-13 22:49:20.334 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35755.
2025-06-13 22:49:20.334 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:35755
2025-06-13 22:49:20.334 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-13 22:49:20.337 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 35755, None)
2025-06-13 22:49:20.339 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:35755 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 35755, None)
2025-06-13 22:49:20.341 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 35755, None)
2025-06-13 22:49:20.341 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 35755, None)
2025-06-13 22:49:20.355 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@3c89b864{/,null,STOPPED,@Spark}
2025-06-13 22:49:20.356 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2792c28{/jobs,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.356 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@351e86b2{/jobs/json,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.357 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57a5b03{/jobs/job,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.357 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@558127d2{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.357 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4552f905{/stages,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.358 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@388e4c25{/stages/json,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.358 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@148fca83{/stages/stage,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.359 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72585e83{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.359 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d5bb5c{/stages/pool,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.359 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48da64f2{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.360 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2aa811f9{/storage,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.360 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d4da729{/storage/json,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.360 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b7e4d14{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.361 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@601d9f3a{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.361 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6585df70{/environment,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.361 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51fb5fe6{/environment/json,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.362 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1791e231{/executors,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.362 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e360c3b{/executors/json,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.363 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3cb49121{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.363 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c4215d7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.365 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13f36d75{/static,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.365 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c02a007{/,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.366 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61bd0845{/api,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.367 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24435620{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.367 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b895e76{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.369 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b36d248{/metrics/json,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.457 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-13 22:49:20.458 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-13 22:49:20.461 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-13 22:49:20.466 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c2b65cc{/SQL,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.466 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@595184d8{/SQL/json,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.466 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3514237f{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.467 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15f11bfb{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.467 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5fafa76d{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:49:20.747 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-13 22:49:20.896 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-13 22:49:20.914 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-13 22:49:20.932 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-13 22:49:20.932 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-13 22:49:20.932 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749829760931
2025-06-13 22:49:21.060 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-13 22:49:21.062 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-13 22:49:21.062 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-13 22:49:21.062 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-13 22:49:21.067 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-13 22:49:21.071 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-13 22:49:21.079 [main INFO ] com.example.Application - Started Application in 2.224 seconds (JVM running for 2.542)
2025-06-13 22:49:21.140 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-13 22:49:21.140 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-13 22:49:21.141 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 0 ms
2025-06-13 22:49:21.178 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-13 22:49:21.186 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b7b44c5{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-13 22:49:21.186 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@474d79cd{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-13 22:49:21.187 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@361fb538{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-13 22:49:21.187 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e5b97e3{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-13 22:49:21.188 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22bc533c{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 22:49:26.258 [http-nio-8080-exec-1 WARN ] o.s.w.s.m.s.DefaultHandlerExceptionResolver - Resolved [org.springframework.web.HttpMediaTypeNotSupportedException: Content type 'text/plain;charset=UTF-8' not supported]
2025-06-13 22:49:27.402 [http-nio-8080-exec-2 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:49:28.121 [http-nio-8080-exec-2 INFO ] c.e.c.service.ClickstreamService - Input raw events size: 3
2025-06-13 22:49:28.280 [http-nio-8080-exec-2 INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 92.341612 ms
2025-06-13 22:49:28.539 [http-nio-8080-exec-2 INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 11.121417 ms
2025-06-13 22:49:28.587 [http-nio-8080-exec-2 INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 5.562714 ms
2025-06-13 22:49:28.636 [http-nio-8080-exec-2 INFO ] org.apache.spark.SparkContext - Starting job: count at ClickstreamService.java:43
2025-06-13 22:49:28.642 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 2 (count at ClickstreamService.java:43) as input to shuffle 0
2025-06-13 22:49:28.644 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (count at ClickstreamService.java:43) with 1 output partitions
2025-06-13 22:49:28.644 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at ClickstreamService.java:43)
2025-06-13 22:49:28.644 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-13 22:49:28.645 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-13 22:49:28.646 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at count at ClickstreamService.java:43), which has no missing parents
2025-06-13 22:49:28.668 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-13 22:49:28.733 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-13 22:49:28.735 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:35755 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 22:49:28.737 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-13 22:49:28.741 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at count at ClickstreamService.java:43) (first 15 tasks are for partitions Vector(0))
2025-06-13 22:49:28.742 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-13 22:49:28.764 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-13 22:49:28.769 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
2025-06-13 22:49:28.811 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-13 22:49:28.812 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
2025-06-13 22:49:28.822 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 3800 bytes result sent to driver
2025-06-13 22:49:28.831 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 71 ms on phamviethoa (executor driver) (1/1)
2025-06-13 22:49:28.831 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-13 22:49:28.834 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (count at ClickstreamService.java:43) finished in 0.183 s
2025-06-13 22:49:28.835 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-13 22:49:28.836 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-13 22:49:28.836 [http-nio-8080-exec-2 INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: count at ClickstreamService.java:43, took 0.200685 s
2025-06-13 22:49:28.840 [http-nio-8080-exec-2 INFO ] c.e.c.service.ClickstreamService - Validated event count: 0
2025-06-13 22:49:28.866 [http-nio-8080-exec-2 INFO ] org.apache.spark.SparkContext - Starting job: count at ClickstreamService.java:44
2025-06-13 22:49:28.867 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 8 (count at ClickstreamService.java:44) as input to shuffle 1
2025-06-13 22:49:28.867 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 1 (count at ClickstreamService.java:44) with 1 output partitions
2025-06-13 22:49:28.867 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (count at ClickstreamService.java:44)
2025-06-13 22:49:28.867 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
2025-06-13 22:49:28.867 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-13 22:49:28.867 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[11] at count at ClickstreamService.java:44), which has no missing parents
2025-06-13 22:49:28.869 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-13 22:49:28.873 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-13 22:49:28.873 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:35755 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 22:49:28.873 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-13 22:49:28.873 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at count at ClickstreamService.java:44) (first 15 tasks are for partitions Vector(0))
2025-06-13 22:49:28.873 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks resource profile 0
2025-06-13 22:49:28.874 [dispatcher-event-loop-10 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 1) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-13 22:49:28.874 [Executor task launch worker for task 0.0 in stage 3.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 1)
2025-06-13 22:49:28.876 [Executor task launch worker for task 0.0 in stage 3.0 (TID 1) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-13 22:49:28.876 [Executor task launch worker for task 0.0 in stage 3.0 (TID 1) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
2025-06-13 22:49:28.877 [Executor task launch worker for task 0.0 in stage 3.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 1). 3800 bytes result sent to driver
2025-06-13 22:49:28.878 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 1) in 4 ms on phamviethoa (executor driver) (1/1)
2025-06-13 22:49:28.878 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2025-06-13 22:49:28.879 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 3 (count at ClickstreamService.java:44) finished in 0.011 s
2025-06-13 22:49:28.879 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-13 22:49:28.879 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 3: Stage finished
2025-06-13 22:49:28.879 [http-nio-8080-exec-2 INFO ] o.a.spark.scheduler.DAGScheduler - Job 1 finished: count at ClickstreamService.java:44, took 0.012625 s
2025-06-13 22:49:28.880 [http-nio-8080-exec-2 INFO ] c.e.c.service.ClickstreamService - Enriched event count: 0
2025-06-13 22:49:33.163 [http-nio-8080-exec-3 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 22:49:33.190 [http-nio-8080-exec-3 INFO ] c.e.c.service.ClickstreamService - Input raw events size: 3
2025-06-13 22:49:33.213 [http-nio-8080-exec-3 INFO ] org.apache.spark.SparkContext - Starting job: count at ClickstreamService.java:43
2025-06-13 22:49:33.213 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 14 (count at ClickstreamService.java:43) as input to shuffle 2
2025-06-13 22:49:33.214 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 2 (count at ClickstreamService.java:43) with 1 output partitions
2025-06-13 22:49:33.214 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (count at ClickstreamService.java:43)
2025-06-13 22:49:33.214 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
2025-06-13 22:49:33.214 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-13 22:49:33.214 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[17] at count at ClickstreamService.java:43), which has no missing parents
2025-06-13 22:49:33.215 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-13 22:49:33.219 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-13 22:49:33.219 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on phamviethoa:35755 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 22:49:33.219 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1535
2025-06-13 22:49:33.219 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at count at ClickstreamService.java:43) (first 15 tasks are for partitions Vector(0))
2025-06-13 22:49:33.219 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks resource profile 0
2025-06-13 22:49:33.220 [dispatcher-event-loop-13 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 2) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-13 22:49:33.220 [Executor task launch worker for task 0.0 in stage 5.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 2)
2025-06-13 22:49:33.222 [Executor task launch worker for task 0.0 in stage 5.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-13 22:49:33.222 [Executor task launch worker for task 0.0 in stage 5.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
2025-06-13 22:49:33.223 [Executor task launch worker for task 0.0 in stage 5.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 2). 3800 bytes result sent to driver
2025-06-13 22:49:33.225 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 2) in 5 ms on phamviethoa (executor driver) (1/1)
2025-06-13 22:49:33.225 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2025-06-13 22:49:33.225 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 5 (count at ClickstreamService.java:43) finished in 0.011 s
2025-06-13 22:49:33.225 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-13 22:49:33.225 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 5: Stage finished
2025-06-13 22:49:33.225 [http-nio-8080-exec-3 INFO ] o.a.spark.scheduler.DAGScheduler - Job 2 finished: count at ClickstreamService.java:43, took 0.012285 s
2025-06-13 22:49:33.226 [http-nio-8080-exec-3 INFO ] c.e.c.service.ClickstreamService - Validated event count: 0
2025-06-13 22:49:33.248 [http-nio-8080-exec-3 INFO ] org.apache.spark.SparkContext - Starting job: count at ClickstreamService.java:44
2025-06-13 22:49:33.249 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 20 (count at ClickstreamService.java:44) as input to shuffle 3
2025-06-13 22:49:33.249 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 3 (count at ClickstreamService.java:44) with 1 output partitions
2025-06-13 22:49:33.249 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (count at ClickstreamService.java:44)
2025-06-13 22:49:33.249 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6)
2025-06-13 22:49:33.249 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-13 22:49:33.249 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (MapPartitionsRDD[23] at count at ClickstreamService.java:44), which has no missing parents
2025-06-13 22:49:33.250 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-13 22:49:33.253 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-13 22:49:33.254 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on phamviethoa:35755 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 22:49:33.254 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1535
2025-06-13 22:49:33.254 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[23] at count at ClickstreamService.java:44) (first 15 tasks are for partitions Vector(0))
2025-06-13 22:49:33.254 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 1 tasks resource profile 0
2025-06-13 22:49:33.255 [dispatcher-event-loop-0 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 3) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-13 22:49:33.255 [Executor task launch worker for task 0.0 in stage 7.0 (TID 3) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 3)
2025-06-13 22:49:33.257 [Executor task launch worker for task 0.0 in stage 7.0 (TID 3) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-13 22:49:33.257 [Executor task launch worker for task 0.0 in stage 7.0 (TID 3) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
2025-06-13 22:49:33.258 [Executor task launch worker for task 0.0 in stage 7.0 (TID 3) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 3). 3757 bytes result sent to driver
2025-06-13 22:49:33.259 [task-result-getter-3 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 3) in 5 ms on phamviethoa (executor driver) (1/1)
2025-06-13 22:49:33.259 [task-result-getter-3 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2025-06-13 22:49:33.259 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 7 (count at ClickstreamService.java:44) finished in 0.010 s
2025-06-13 22:49:33.259 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-13 22:49:33.259 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 7: Stage finished
2025-06-13 22:49:33.259 [http-nio-8080-exec-3 INFO ] o.a.spark.scheduler.DAGScheduler - Job 3 finished: count at ClickstreamService.java:44, took 0.011156 s
2025-06-13 22:49:33.260 [http-nio-8080-exec-3 INFO ] c.e.c.service.ClickstreamService - Enriched event count: 0
2025-06-13 23:09:59.155 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-13 23:09:59.155 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 23:09:59.162 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@67c7b396{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 23:09:59.163 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 23:09:59.164 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext already stopped.
2025-06-13 23:09:59.164 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-13 23:10:01.291 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 84175 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-13 23:10:01.292 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-13 23:10:01.818 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-13 23:10:01.824 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-13 23:10:01.824 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-13 23:10:01.824 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-13 23:10:01.865 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-13 23:10:01.865 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 555 ms
2025-06-13 23:10:02.203 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-13 23:10:02.244 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-13 23:10:02.271 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 23:10:02.271 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-13 23:10:02.271 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 23:10:02.272 [main INFO ] org.apache.spark.SparkContext - Submitted application: ClickstreamProcessor
2025-06-13 23:10:02.279 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-13 23:10:02.283 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-13 23:10:02.284 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-13 23:10:02.299 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-13 23:10:02.299 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-13 23:10:02.299 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-13 23:10:02.299 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-13 23:10:02.299 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-13 23:10:02.374 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 34531.
2025-06-13 23:10:02.382 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-13 23:10:02.393 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-13 23:10:02.400 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-13 23:10:02.400 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-13 23:10:02.402 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-13 23:10:02.406 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-fa1296f6-a8f4-4bf1-ab90-81291e4fb912
2025-06-13 23:10:02.421 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-13 23:10:02.428 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-13 23:10:02.438 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1599ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-13 23:10:02.474 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-13 23:10:02.478 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-13 23:10:02.485 [main INFO ] org.sparkproject.jetty.server.Server - Started @1647ms
2025-06-13 23:10:02.495 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@5204e7bd{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 23:10:02.495 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-13 23:10:02.503 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d978ab9{/,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.532 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-13 23:10:02.535 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-13 23:10:02.543 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34399.
2025-06-13 23:10:02.543 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:34399
2025-06-13 23:10:02.544 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-13 23:10:02.546 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 34399, None)
2025-06-13 23:10:02.548 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:34399 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 34399, None)
2025-06-13 23:10:02.550 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 34399, None)
2025-06-13 23:10:02.551 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 34399, None)
2025-06-13 23:10:02.567 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@d978ab9{/,null,STOPPED,@Spark}
2025-06-13 23:10:02.568 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133b712{/jobs,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.568 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70f91ae3{/jobs/json,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.569 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d93ff21{/jobs/job,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.569 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ca4c88a{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.570 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55397d15{/stages,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.570 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24ac6fef{/stages/json,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.571 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6de5ad56{/stages/stage,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.571 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cfb94fd{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.572 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44a44a04{/stages/pool,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.572 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a6fc1bc{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.572 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@360a3106{/storage,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.573 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e9a836{/storage/json,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.573 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75aa7703{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.573 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3395c2a7{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.574 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7831d1aa{/environment,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.574 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27746c5e{/environment/json,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.574 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2270f58d{/executors,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.575 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54737322{/executors/json,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.575 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7323c38c{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.575 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63a72cc6{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.578 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cef885d{/static,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.578 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5da3f32a{/,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.579 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51f4439e{/api,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.579 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8315e4a{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.579 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22ff11ef{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.581 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@215a0264{/metrics/json,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.670 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-13 23:10:02.670 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-13 23:10:02.673 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-13 23:10:02.678 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3664c596{/SQL,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.678 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e79e9e{/SQL/json,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.679 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74450c9b{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.679 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ad50b02{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.680 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@142918a0{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 23:10:02.955 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-13 23:10:03.103 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-13 23:10:03.120 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-13 23:10:03.137 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-13 23:10:03.137 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-13 23:10:03.138 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749831003137
2025-06-13 23:10:03.269 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-13 23:10:03.271 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-13 23:10:03.271 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-13 23:10:03.271 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-13 23:10:03.276 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-13 23:10:03.280 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-13 23:10:03.288 [main INFO ] com.example.Application - Started Application in 2.143 seconds (JVM running for 2.45)
2025-06-13 23:10:03.434 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-13 23:10:03.435 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-13 23:10:03.436 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-13 23:10:03.475 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-13 23:10:03.483 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@423b677b{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-13 23:10:03.483 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1768b218{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-13 23:10:03.484 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1588dc7{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-13 23:10:03.485 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@400f1367{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-13 23:10:03.486 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1930cc26{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 23:10:08.128 [http-nio-8080-exec-1 WARN ] o.s.w.s.m.s.DefaultHandlerExceptionResolver - Resolved [org.springframework.web.HttpMediaTypeNotSupportedException: Content type 'text/plain;charset=UTF-8' not supported]
2025-06-13 23:10:09.281 [http-nio-8080-exec-2 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 23:10:10.011 [http-nio-8080-exec-2 INFO ] c.e.c.service.ClickstreamService - Input raw events size: 3
2025-06-13 23:10:10.171 [http-nio-8080-exec-2 INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 92.733356 ms
2025-06-13 23:10:10.430 [http-nio-8080-exec-2 INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 11.676756 ms
2025-06-13 23:10:10.489 [http-nio-8080-exec-2 INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.79404 ms
2025-06-13 23:10:10.581 [http-nio-8080-exec-2 INFO ] org.apache.spark.SparkContext - Starting job: count at ClickstreamService.java:43
2025-06-13 23:10:10.589 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 2 (count at ClickstreamService.java:43) as input to shuffle 0
2025-06-13 23:10:10.592 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (count at ClickstreamService.java:43) with 1 output partitions
2025-06-13 23:10:10.592 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at ClickstreamService.java:43)
2025-06-13 23:10:10.592 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-13 23:10:10.593 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-13 23:10:10.594 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at count at ClickstreamService.java:43), which has no missing parents
2025-06-13 23:10:10.622 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-13 23:10:10.699 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-13 23:10:10.701 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:34399 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 23:10:10.702 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-13 23:10:10.708 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at count at ClickstreamService.java:43) (first 15 tasks are for partitions Vector(0))
2025-06-13 23:10:10.709 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-13 23:10:10.732 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-13 23:10:10.738 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
2025-06-13 23:10:10.780 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-13 23:10:10.781 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
2025-06-13 23:10:10.790 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 3800 bytes result sent to driver
2025-06-13 23:10:10.800 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 72 ms on phamviethoa (executor driver) (1/1)
2025-06-13 23:10:10.800 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-13 23:10:10.803 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (count at ClickstreamService.java:43) finished in 0.203 s
2025-06-13 23:10:10.804 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-13 23:10:10.804 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-13 23:10:10.805 [http-nio-8080-exec-2 INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: count at ClickstreamService.java:43, took 0.223571 s
2025-06-13 23:10:10.809 [http-nio-8080-exec-2 INFO ] c.e.c.service.ClickstreamService - Validated event count: 0
2025-06-13 23:10:10.834 [http-nio-8080-exec-2 INFO ] org.apache.spark.SparkContext - Starting job: count at ClickstreamService.java:44
2025-06-13 23:10:10.835 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 8 (count at ClickstreamService.java:44) as input to shuffle 1
2025-06-13 23:10:10.835 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 1 (count at ClickstreamService.java:44) with 1 output partitions
2025-06-13 23:10:10.835 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (count at ClickstreamService.java:44)
2025-06-13 23:10:10.835 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
2025-06-13 23:10:10.835 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-13 23:10:10.836 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[11] at count at ClickstreamService.java:44), which has no missing parents
2025-06-13 23:10:10.837 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-13 23:10:10.841 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-13 23:10:10.842 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:34399 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 23:10:10.842 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-13 23:10:10.842 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at count at ClickstreamService.java:44) (first 15 tasks are for partitions Vector(0))
2025-06-13 23:10:10.842 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks resource profile 0
2025-06-13 23:10:10.843 [dispatcher-event-loop-10 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 1) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-13 23:10:10.843 [Executor task launch worker for task 0.0 in stage 3.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 1)
2025-06-13 23:10:10.845 [Executor task launch worker for task 0.0 in stage 3.0 (TID 1) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-13 23:10:10.845 [Executor task launch worker for task 0.0 in stage 3.0 (TID 1) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
2025-06-13 23:10:10.846 [Executor task launch worker for task 0.0 in stage 3.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 1). 3800 bytes result sent to driver
2025-06-13 23:10:10.847 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 1) in 4 ms on phamviethoa (executor driver) (1/1)
2025-06-13 23:10:10.848 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2025-06-13 23:10:10.848 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 3 (count at ClickstreamService.java:44) finished in 0.012 s
2025-06-13 23:10:10.848 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-13 23:10:10.848 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 3: Stage finished
2025-06-13 23:10:10.848 [http-nio-8080-exec-2 INFO ] o.a.spark.scheduler.DAGScheduler - Job 1 finished: count at ClickstreamService.java:44, took 0.013695 s
2025-06-13 23:10:10.849 [http-nio-8080-exec-2 INFO ] c.e.c.service.ClickstreamService - Enriched event count: 0
2025-06-13 23:10:14.347 [http-nio-8080-exec-3 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 23:10:14.372 [http-nio-8080-exec-3 INFO ] c.e.c.service.ClickstreamService - Input raw events size: 3
2025-06-13 23:10:14.395 [http-nio-8080-exec-3 INFO ] org.apache.spark.SparkContext - Starting job: count at ClickstreamService.java:43
2025-06-13 23:10:14.396 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 14 (count at ClickstreamService.java:43) as input to shuffle 2
2025-06-13 23:10:14.396 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 2 (count at ClickstreamService.java:43) with 1 output partitions
2025-06-13 23:10:14.396 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (count at ClickstreamService.java:43)
2025-06-13 23:10:14.396 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
2025-06-13 23:10:14.396 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-13 23:10:14.396 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[17] at count at ClickstreamService.java:43), which has no missing parents
2025-06-13 23:10:14.397 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-13 23:10:14.402 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-13 23:10:14.402 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on phamviethoa:34399 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 23:10:14.402 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1535
2025-06-13 23:10:14.403 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at count at ClickstreamService.java:43) (first 15 tasks are for partitions Vector(0))
2025-06-13 23:10:14.403 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks resource profile 0
2025-06-13 23:10:14.403 [dispatcher-event-loop-13 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 2) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-13 23:10:14.404 [Executor task launch worker for task 0.0 in stage 5.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 2)
2025-06-13 23:10:14.405 [Executor task launch worker for task 0.0 in stage 5.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-13 23:10:14.405 [Executor task launch worker for task 0.0 in stage 5.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
2025-06-13 23:10:14.406 [Executor task launch worker for task 0.0 in stage 5.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 2). 3800 bytes result sent to driver
2025-06-13 23:10:14.408 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 2) in 5 ms on phamviethoa (executor driver) (1/1)
2025-06-13 23:10:14.408 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2025-06-13 23:10:14.408 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 5 (count at ClickstreamService.java:43) finished in 0.011 s
2025-06-13 23:10:14.409 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-13 23:10:14.409 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 5: Stage finished
2025-06-13 23:10:14.409 [http-nio-8080-exec-3 INFO ] o.a.spark.scheduler.DAGScheduler - Job 2 finished: count at ClickstreamService.java:43, took 0.013608 s
2025-06-13 23:10:14.409 [http-nio-8080-exec-3 INFO ] c.e.c.service.ClickstreamService - Validated event count: 0
2025-06-13 23:10:14.432 [http-nio-8080-exec-3 INFO ] org.apache.spark.SparkContext - Starting job: count at ClickstreamService.java:44
2025-06-13 23:10:14.432 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 20 (count at ClickstreamService.java:44) as input to shuffle 3
2025-06-13 23:10:14.433 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 3 (count at ClickstreamService.java:44) with 1 output partitions
2025-06-13 23:10:14.433 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (count at ClickstreamService.java:44)
2025-06-13 23:10:14.433 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6)
2025-06-13 23:10:14.433 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-13 23:10:14.433 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (MapPartitionsRDD[23] at count at ClickstreamService.java:44), which has no missing parents
2025-06-13 23:10:14.434 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-13 23:10:14.438 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-13 23:10:14.438 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on phamviethoa:34399 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 23:10:14.438 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1535
2025-06-13 23:10:14.438 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[23] at count at ClickstreamService.java:44) (first 15 tasks are for partitions Vector(0))
2025-06-13 23:10:14.438 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 1 tasks resource profile 0
2025-06-13 23:10:14.439 [dispatcher-event-loop-0 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 3) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-13 23:10:14.439 [Executor task launch worker for task 0.0 in stage 7.0 (TID 3) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 3)
2025-06-13 23:10:14.441 [Executor task launch worker for task 0.0 in stage 7.0 (TID 3) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-13 23:10:14.441 [Executor task launch worker for task 0.0 in stage 7.0 (TID 3) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
2025-06-13 23:10:14.442 [Executor task launch worker for task 0.0 in stage 7.0 (TID 3) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 3). 3757 bytes result sent to driver
2025-06-13 23:10:14.443 [task-result-getter-3 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 3) in 4 ms on phamviethoa (executor driver) (1/1)
2025-06-13 23:10:14.443 [task-result-getter-3 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2025-06-13 23:10:14.444 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 7 (count at ClickstreamService.java:44) finished in 0.010 s
2025-06-13 23:10:14.444 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-13 23:10:14.444 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 7: Stage finished
2025-06-13 23:10:14.444 [http-nio-8080-exec-3 INFO ] o.a.spark.scheduler.DAGScheduler - Job 3 finished: count at ClickstreamService.java:44, took 0.012003 s
2025-06-13 23:10:14.445 [http-nio-8080-exec-3 INFO ] c.e.c.service.ClickstreamService - Enriched event count: 0
2025-06-13 23:10:21.819 [http-nio-8080-exec-4 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 23:10:21.843 [http-nio-8080-exec-4 INFO ] c.e.c.service.ClickstreamService - Input raw events size: 3
2025-06-13 23:10:21.862 [http-nio-8080-exec-4 INFO ] org.apache.spark.SparkContext - Starting job: count at ClickstreamService.java:43
2025-06-13 23:10:21.863 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 26 (count at ClickstreamService.java:43) as input to shuffle 4
2025-06-13 23:10:21.863 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 4 (count at ClickstreamService.java:43) with 1 output partitions
2025-06-13 23:10:21.863 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 9 (count at ClickstreamService.java:43)
2025-06-13 23:10:21.863 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 8)
2025-06-13 23:10:21.863 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-13 23:10:21.863 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 9 (MapPartitionsRDD[29] at count at ClickstreamService.java:43), which has no missing parents
2025-06-13 23:10:21.864 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-13 23:10:21.867 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-13 23:10:21.867 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on phamviethoa:34399 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 23:10:21.867 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1535
2025-06-13 23:10:21.868 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[29] at count at ClickstreamService.java:43) (first 15 tasks are for partitions Vector(0))
2025-06-13 23:10:21.868 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 1 tasks resource profile 0
2025-06-13 23:10:21.868 [dispatcher-event-loop-4 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 4) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-13 23:10:21.869 [Executor task launch worker for task 0.0 in stage 9.0 (TID 4) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 4)
2025-06-13 23:10:21.870 [Executor task launch worker for task 0.0 in stage 9.0 (TID 4) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-13 23:10:21.870 [Executor task launch worker for task 0.0 in stage 9.0 (TID 4) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
2025-06-13 23:10:21.871 [Executor task launch worker for task 0.0 in stage 9.0 (TID 4) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 4). 3714 bytes result sent to driver
2025-06-13 23:10:21.872 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 4) in 4 ms on phamviethoa (executor driver) (1/1)
2025-06-13 23:10:21.872 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
2025-06-13 23:10:21.872 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 9 (count at ClickstreamService.java:43) finished in 0.009 s
2025-06-13 23:10:21.872 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-13 23:10:21.872 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 9: Stage finished
2025-06-13 23:10:21.872 [http-nio-8080-exec-4 INFO ] o.a.spark.scheduler.DAGScheduler - Job 4 finished: count at ClickstreamService.java:43, took 0.010205 s
2025-06-13 23:10:21.873 [http-nio-8080-exec-4 INFO ] c.e.c.service.ClickstreamService - Validated event count: 0
2025-06-13 23:10:21.893 [http-nio-8080-exec-4 INFO ] org.apache.spark.SparkContext - Starting job: count at ClickstreamService.java:44
2025-06-13 23:10:21.893 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 32 (count at ClickstreamService.java:44) as input to shuffle 5
2025-06-13 23:10:21.893 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 5 (count at ClickstreamService.java:44) with 1 output partitions
2025-06-13 23:10:21.893 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (count at ClickstreamService.java:44)
2025-06-13 23:10:21.893 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 10)
2025-06-13 23:10:21.893 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-13 23:10:21.894 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[35] at count at ClickstreamService.java:44), which has no missing parents
2025-06-13 23:10:21.894 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-13 23:10:21.897 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-13 23:10:21.898 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on phamviethoa:34399 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 23:10:21.898 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1535
2025-06-13 23:10:21.898 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[35] at count at ClickstreamService.java:44) (first 15 tasks are for partitions Vector(0))
2025-06-13 23:10:21.898 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 1 tasks resource profile 0
2025-06-13 23:10:21.899 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 5) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-13 23:10:21.899 [Executor task launch worker for task 0.0 in stage 11.0 (TID 5) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 5)
2025-06-13 23:10:21.900 [Executor task launch worker for task 0.0 in stage 11.0 (TID 5) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-13 23:10:21.901 [Executor task launch worker for task 0.0 in stage 11.0 (TID 5) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
2025-06-13 23:10:21.901 [Executor task launch worker for task 0.0 in stage 11.0 (TID 5) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 5). 3757 bytes result sent to driver
2025-06-13 23:10:21.902 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 5) in 4 ms on phamviethoa (executor driver) (1/1)
2025-06-13 23:10:21.902 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool 
2025-06-13 23:10:21.903 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 11 (count at ClickstreamService.java:44) finished in 0.008 s
2025-06-13 23:10:21.903 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-13 23:10:21.903 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 11: Stage finished
2025-06-13 23:10:21.903 [http-nio-8080-exec-4 INFO ] o.a.spark.scheduler.DAGScheduler - Job 5 finished: count at ClickstreamService.java:44, took 0.009979 s
2025-06-13 23:10:21.903 [http-nio-8080-exec-4 INFO ] c.e.c.service.ClickstreamService - Enriched event count: 0
2025-06-13 23:10:25.805 [http-nio-8080-exec-6 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 23:10:25.824 [http-nio-8080-exec-6 INFO ] c.e.c.service.ClickstreamService - Input raw events size: 3
2025-06-13 23:10:25.845 [http-nio-8080-exec-6 INFO ] org.apache.spark.SparkContext - Starting job: count at ClickstreamService.java:43
2025-06-13 23:10:25.845 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 38 (count at ClickstreamService.java:43) as input to shuffle 6
2025-06-13 23:10:25.845 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 6 (count at ClickstreamService.java:43) with 1 output partitions
2025-06-13 23:10:25.845 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 13 (count at ClickstreamService.java:43)
2025-06-13 23:10:25.845 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 12)
2025-06-13 23:10:25.845 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-13 23:10:25.846 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 13 (MapPartitionsRDD[41] at count at ClickstreamService.java:43), which has no missing parents
2025-06-13 23:10:25.846 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-13 23:10:25.849 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-13 23:10:25.850 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on phamviethoa:34399 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 23:10:25.850 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1535
2025-06-13 23:10:25.850 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[41] at count at ClickstreamService.java:43) (first 15 tasks are for partitions Vector(0))
2025-06-13 23:10:25.850 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 13.0 with 1 tasks resource profile 0
2025-06-13 23:10:25.850 [dispatcher-event-loop-10 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 13.0 (TID 6) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-13 23:10:25.851 [Executor task launch worker for task 0.0 in stage 13.0 (TID 6) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 13.0 (TID 6)
2025-06-13 23:10:25.852 [Executor task launch worker for task 0.0 in stage 13.0 (TID 6) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-13 23:10:25.852 [Executor task launch worker for task 0.0 in stage 13.0 (TID 6) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
2025-06-13 23:10:25.854 [Executor task launch worker for task 0.0 in stage 13.0 (TID 6) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 13.0 (TID 6). 3757 bytes result sent to driver
2025-06-13 23:10:25.855 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 13.0 (TID 6) in 5 ms on phamviethoa (executor driver) (1/1)
2025-06-13 23:10:25.855 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 13.0, whose tasks have all completed, from pool 
2025-06-13 23:10:25.855 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 13 (count at ClickstreamService.java:43) finished in 0.009 s
2025-06-13 23:10:25.855 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-13 23:10:25.855 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 13: Stage finished
2025-06-13 23:10:25.855 [http-nio-8080-exec-6 INFO ] o.a.spark.scheduler.DAGScheduler - Job 6 finished: count at ClickstreamService.java:43, took 0.010578 s
2025-06-13 23:10:25.856 [http-nio-8080-exec-6 INFO ] c.e.c.service.ClickstreamService - Validated event count: 0
2025-06-13 23:10:25.877 [http-nio-8080-exec-6 INFO ] org.apache.spark.SparkContext - Starting job: count at ClickstreamService.java:44
2025-06-13 23:10:25.877 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 44 (count at ClickstreamService.java:44) as input to shuffle 7
2025-06-13 23:10:25.877 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 7 (count at ClickstreamService.java:44) with 1 output partitions
2025-06-13 23:10:25.877 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 15 (count at ClickstreamService.java:44)
2025-06-13 23:10:25.877 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 14)
2025-06-13 23:10:25.877 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-13 23:10:25.878 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 15 (MapPartitionsRDD[47] at count at ClickstreamService.java:44), which has no missing parents
2025-06-13 23:10:25.878 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-13 23:10:25.881 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-13 23:10:25.882 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on phamviethoa:34399 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 23:10:25.882 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1535
2025-06-13 23:10:25.882 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[47] at count at ClickstreamService.java:44) (first 15 tasks are for partitions Vector(0))
2025-06-13 23:10:25.882 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 15.0 with 1 tasks resource profile 0
2025-06-13 23:10:25.882 [dispatcher-event-loop-13 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 15.0 (TID 7) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-13 23:10:25.883 [Executor task launch worker for task 0.0 in stage 15.0 (TID 7) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 15.0 (TID 7)
2025-06-13 23:10:25.884 [Executor task launch worker for task 0.0 in stage 15.0 (TID 7) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-13 23:10:25.884 [Executor task launch worker for task 0.0 in stage 15.0 (TID 7) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
2025-06-13 23:10:25.885 [Executor task launch worker for task 0.0 in stage 15.0 (TID 7) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 15.0 (TID 7). 3757 bytes result sent to driver
2025-06-13 23:10:25.887 [task-result-getter-3 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 15.0 (TID 7) in 5 ms on phamviethoa (executor driver) (1/1)
2025-06-13 23:10:25.887 [task-result-getter-3 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 15.0, whose tasks have all completed, from pool 
2025-06-13 23:10:25.887 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 15 (count at ClickstreamService.java:44) finished in 0.009 s
2025-06-13 23:10:25.887 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-13 23:10:25.887 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 15: Stage finished
2025-06-13 23:10:25.887 [http-nio-8080-exec-6 INFO ] o.a.spark.scheduler.DAGScheduler - Job 7 finished: count at ClickstreamService.java:44, took 0.010227 s
2025-06-13 23:10:25.887 [http-nio-8080-exec-6 INFO ] c.e.c.service.ClickstreamService - Enriched event count: 0
2025-06-13 23:12:29.804 [http-nio-8080-exec-7 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-13 23:12:29.822 [http-nio-8080-exec-7 INFO ] c.e.c.service.ClickstreamService - Input raw events size: 3
2025-06-13 23:12:29.847 [http-nio-8080-exec-7 INFO ] org.apache.spark.SparkContext - Starting job: count at ClickstreamService.java:43
2025-06-13 23:12:29.848 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 50 (count at ClickstreamService.java:43) as input to shuffle 8
2025-06-13 23:12:29.848 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 8 (count at ClickstreamService.java:43) with 1 output partitions
2025-06-13 23:12:29.848 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 17 (count at ClickstreamService.java:43)
2025-06-13 23:12:29.848 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 16)
2025-06-13 23:12:29.848 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-13 23:12:29.849 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 17 (MapPartitionsRDD[53] at count at ClickstreamService.java:43), which has no missing parents
2025-06-13 23:12:29.850 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-13 23:12:29.854 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-13 23:12:29.855 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on phamviethoa:34399 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 23:12:29.855 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1535
2025-06-13 23:12:29.855 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[53] at count at ClickstreamService.java:43) (first 15 tasks are for partitions Vector(0))
2025-06-13 23:12:29.855 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 17.0 with 1 tasks resource profile 0
2025-06-13 23:12:29.856 [dispatcher-event-loop-13 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 17.0 (TID 8) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-13 23:12:29.857 [Executor task launch worker for task 0.0 in stage 17.0 (TID 8) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 17.0 (TID 8)
2025-06-13 23:12:29.859 [Executor task launch worker for task 0.0 in stage 17.0 (TID 8) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-13 23:12:29.859 [Executor task launch worker for task 0.0 in stage 17.0 (TID 8) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
2025-06-13 23:12:29.861 [Executor task launch worker for task 0.0 in stage 17.0 (TID 8) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 17.0 (TID 8). 3800 bytes result sent to driver
2025-06-13 23:12:29.863 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 17.0 (TID 8) in 7 ms on phamviethoa (executor driver) (1/1)
2025-06-13 23:12:29.863 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 17.0, whose tasks have all completed, from pool 
2025-06-13 23:12:29.863 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 17 (count at ClickstreamService.java:43) finished in 0.014 s
2025-06-13 23:12:29.864 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-13 23:12:29.864 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 17: Stage finished
2025-06-13 23:12:29.864 [http-nio-8080-exec-7 INFO ] o.a.spark.scheduler.DAGScheduler - Job 8 finished: count at ClickstreamService.java:43, took 0.016221 s
2025-06-13 23:12:29.864 [http-nio-8080-exec-7 INFO ] c.e.c.service.ClickstreamService - Validated event count: 0
2025-06-13 23:12:29.888 [http-nio-8080-exec-7 INFO ] org.apache.spark.SparkContext - Starting job: count at ClickstreamService.java:44
2025-06-13 23:12:29.888 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 56 (count at ClickstreamService.java:44) as input to shuffle 9
2025-06-13 23:12:29.888 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 9 (count at ClickstreamService.java:44) with 1 output partitions
2025-06-13 23:12:29.888 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 19 (count at ClickstreamService.java:44)
2025-06-13 23:12:29.888 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 18)
2025-06-13 23:12:29.889 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-13 23:12:29.889 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 19 (MapPartitionsRDD[59] at count at ClickstreamService.java:44), which has no missing parents
2025-06-13 23:12:29.890 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-13 23:12:29.894 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-13 23:12:29.894 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on phamviethoa:34399 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 23:12:29.894 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1535
2025-06-13 23:12:29.894 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[59] at count at ClickstreamService.java:44) (first 15 tasks are for partitions Vector(0))
2025-06-13 23:12:29.894 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 19.0 with 1 tasks resource profile 0
2025-06-13 23:12:29.895 [dispatcher-event-loop-0 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 19.0 (TID 9) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-13 23:12:29.895 [Executor task launch worker for task 0.0 in stage 19.0 (TID 9) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 19.0 (TID 9)
2025-06-13 23:12:29.897 [Executor task launch worker for task 0.0 in stage 19.0 (TID 9) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-13 23:12:29.898 [Executor task launch worker for task 0.0 in stage 19.0 (TID 9) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
2025-06-13 23:12:29.899 [Executor task launch worker for task 0.0 in stage 19.0 (TID 9) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 19.0 (TID 9). 3800 bytes result sent to driver
2025-06-13 23:12:29.900 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 19.0 (TID 9) in 5 ms on phamviethoa (executor driver) (1/1)
2025-06-13 23:12:29.900 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 19.0, whose tasks have all completed, from pool 
2025-06-13 23:12:29.901 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 19 (count at ClickstreamService.java:44) finished in 0.012 s
2025-06-13 23:12:29.901 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-13 23:12:29.901 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 19: Stage finished
2025-06-13 23:12:29.901 [http-nio-8080-exec-7 INFO ] o.a.spark.scheduler.DAGScheduler - Job 9 finished: count at ClickstreamService.java:44, took 0.013124 s
2025-06-13 23:12:29.901 [http-nio-8080-exec-7 INFO ] c.e.c.service.ClickstreamService - Enriched event count: 0
2025-06-13 23:19:00.619 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on phamviethoa:34399 in memory (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 23:19:00.623 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on phamviethoa:34399 in memory (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 23:19:00.624 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on phamviethoa:34399 in memory (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 23:19:00.625 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on phamviethoa:34399 in memory (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 23:19:00.627 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on phamviethoa:34399 in memory (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 23:19:00.628 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on phamviethoa:34399 in memory (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 23:19:00.630 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on phamviethoa:34399 in memory (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 23:19:00.631 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_9_piece0 on phamviethoa:34399 in memory (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 23:19:00.633 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on phamviethoa:34399 in memory (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 23:19:00.634 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on phamviethoa:34399 in memory (size: 5.8 KiB, free: 9.2 GiB)
2025-06-13 23:19:39.607 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-13 23:19:39.608 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 23:19:39.615 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@5204e7bd{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 23:19:39.619 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-13 23:19:39.620 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-13 23:19:39.620 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext already stopped.
2025-06-13 23:19:41.698 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 92058 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-13 23:19:41.700 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-13 23:19:42.222 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-13 23:19:42.225 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-13 23:19:42.225 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-13 23:19:42.225 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-13 23:19:42.267 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-13 23:19:42.268 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 549 ms
2025-06-13 23:19:42.612 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-13 23:19:42.655 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-13 23:19:42.685 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 23:19:42.685 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-13 23:19:42.685 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-13 23:19:42.685 [main INFO ] org.apache.spark.SparkContext - Submitted application: ClickstreamProcessor
2025-06-13 23:19:42.694 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-13 23:19:42.699 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-13 23:19:42.699 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-13 23:19:42.716 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-13 23:19:42.716 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-13 23:19:42.716 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-13 23:19:42.716 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-13 23:19:42.716 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-13 23:19:42.794 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 38027.
2025-06-13 23:19:42.801 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-13 23:19:42.814 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-13 23:19:42.820 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-13 23:19:42.820 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-13 23:19:42.821 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-13 23:19:42.825 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-c6479651-edf5-424e-b106-d5a166f9b0e3
2025-06-13 23:19:42.840 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-13 23:19:42.846 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-13 23:19:42.856 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1632ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-13 23:19:42.892 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-13 23:19:42.896 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-13 23:19:42.902 [main INFO ] org.sparkproject.jetty.server.Server - Started @1679ms
2025-06-13 23:19:42.913 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@5457ba34{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-13 23:19:42.914 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-13 23:19:42.921 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b0ba697{/,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.950 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-13 23:19:42.953 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-13 23:19:42.961 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39917.
2025-06-13 23:19:42.961 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:39917
2025-06-13 23:19:42.961 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-13 23:19:42.964 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 39917, None)
2025-06-13 23:19:42.966 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:39917 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 39917, None)
2025-06-13 23:19:42.968 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 39917, None)
2025-06-13 23:19:42.968 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 39917, None)
2025-06-13 23:19:42.982 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6b0ba697{/,null,STOPPED,@Spark}
2025-06-13 23:19:42.983 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@146833a2{/jobs,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.983 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@424a152f{/jobs/json,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.984 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4bb9f7d4{/jobs/job,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.984 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@183ef89a{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.985 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6fa7ce4{/stages,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.985 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a8b42a3{/stages/json,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.985 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12270a01{/stages/stage,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.986 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@646d58cd{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.986 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12532e37{/stages/pool,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.986 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e3ee457{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.987 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fb2c2f3{/storage,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.987 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d2a8819{/storage/json,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.988 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b64bf61{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.988 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7846913f{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.988 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60b553f{/environment,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.989 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66abb2fa{/environment/json,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.989 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133b712{/executors,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.989 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70f91ae3{/executors/json,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.990 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c2a3f0c{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.990 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d93ff21{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.992 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ca4c88a{/static,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.993 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7831d1aa{/,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.993 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27746c5e{/api,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.994 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16e4db59{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.994 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12a0d249{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-13 23:19:42.996 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5da3f32a{/metrics/json,null,AVAILABLE,@Spark}
2025-06-13 23:19:43.087 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-13 23:19:43.088 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-13 23:19:43.091 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-13 23:19:43.095 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57eed461{/SQL,null,AVAILABLE,@Spark}
2025-06-13 23:19:43.096 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f08f8a9{/SQL/json,null,AVAILABLE,@Spark}
2025-06-13 23:19:43.096 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@390a7532{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-13 23:19:43.097 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d49a1a0{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-13 23:19:43.097 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16a499d1{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 23:19:43.369 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-13 23:19:43.519 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-13 23:19:43.536 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-13 23:19:43.554 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-13 23:19:43.554 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-13 23:19:43.554 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749831583553
2025-06-13 23:19:43.682 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-13 23:19:43.684 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-13 23:19:43.685 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-13 23:19:43.685 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-13 23:19:43.689 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-13 23:19:43.693 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-13 23:19:43.702 [main INFO ] com.example.Application - Started Application in 2.152 seconds (JVM running for 2.478)
2025-06-13 23:19:43.833 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-13 23:19:43.833 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-13 23:19:43.833 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 0 ms
2025-06-13 23:19:43.874 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-13 23:19:43.880 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241ffa7{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-13 23:19:43.880 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24be92db{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-13 23:19:43.881 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e1e5d0{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-13 23:19:43.881 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6035ffa4{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-13 23:19:43.882 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1cd4b8f9{/static/sql,null,AVAILABLE,@Spark}
2025-06-13 23:19:50.891 [http-nio-8080-exec-1 WARN ] o.s.w.s.m.s.DefaultHandlerExceptionResolver - Resolved [org.springframework.web.HttpMediaTypeNotSupportedException: Content type 'text/plain;charset=UTF-8' not supported]
2025-06-13 23:19:52.078 [http-nio-8080-exec-2 INFO ] c.e.c.c.ClickstreamController - Received payload: {events=[{event_id=06c61f36-66a0-48c7-83ac-461b4eb0a807, event_name=page_view, event_time=2025-06-13T16:19:50.878Z, user_id=user_pej5ild, session_id=session_m9xzvy2, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/}, {event_id=eb45323b-b2a6-42c9-a150-a58a39bf50fd, event_name=scroll, event_time=2025-06-13T16:19:51.068Z, user_id=user_pej5ild, session_id=session_m9xzvy2, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=9}, {event_id=e5079f38-26a8-4338-b7b8-6dd27a1912ef, event_name=scroll, event_time=2025-06-13T16:19:52.068Z, user_id=user_pej5ild, session_id=session_m9xzvy2, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=64}]}
2025-06-13 23:19:52.078 [http-nio-8080-exec-2 INFO ] c.e.c.c.ClickstreamController - Received 3 events
2025-06-13 23:19:52.087 [http-nio-8080-exec-2 INFO ] o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 1000
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-06-13 23:19:52.112 [http-nio-8080-exec-2 INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-13 23:19:52.117 [http-nio-8080-exec-2 INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-06-13 23:19:52.130 [http-nio-8080-exec-2 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-13 23:19:52.130 [http-nio-8080-exec-2 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-13 23:19:52.130 [http-nio-8080-exec-2 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749831592130
2025-06-13 23:19:52.137 [kafka-producer-network-thread | producer-1 INFO ] org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: HhNeO4joR2-K_kbMfLUuyQ
2025-06-13 23:19:52.146 [http-nio-8080-exec-2 INFO ] c.e.c.c.ClickstreamController - Successfully processed 3 events
2025-06-13 23:19:52.147 [kafka-producer-network-thread | producer-1 INFO ] o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 0 with epoch 0
2025-06-13 23:19:55.073 [http-nio-8080-exec-3 INFO ] c.e.c.c.ClickstreamController - Received payload: {events=[{event_id=9d1e2225-b138-4430-987a-d6655cb4f748, event_name=scroll, event_time=2025-06-13T16:19:53.069Z, user_id=user_pej5ild, session_id=session_m9xzvy2, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=53}, {event_id=2e9f052b-c9c4-4e1a-8cbf-7e6d8585ee25, event_name=scroll, event_time=2025-06-13T16:19:54.069Z, user_id=user_pej5ild, session_id=session_m9xzvy2, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=73}, {event_id=a4c859e3-d98c-4070-8d8d-21158dd2cad3, event_name=scroll, event_time=2025-06-13T16:19:55.070Z, user_id=user_pej5ild, session_id=session_m9xzvy2, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=93}]}
2025-06-13 23:19:55.073 [http-nio-8080-exec-3 INFO ] c.e.c.c.ClickstreamController - Received 3 events
2025-06-13 23:19:55.073 [http-nio-8080-exec-3 INFO ] c.e.c.c.ClickstreamController - Successfully processed 3 events
2025-06-13 23:19:59.197 [http-nio-8080-exec-4 INFO ] c.e.c.c.ClickstreamController - Received payload: {events=[{event_id=5bc49303-4906-4abd-b16d-d1bad71f7992, event_name=scroll, event_time=2025-06-13T16:19:56.071Z, user_id=user_pej5ild, session_id=session_m9xzvy2, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=47}, {event_id=7465edb7-5c31-444a-895c-18c86a8ef75c, event_name=scroll, event_time=2025-06-13T16:19:58.192Z, user_id=user_pej5ild, session_id=session_m9xzvy2, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=5}, {event_id=1aad6add-3280-49d0-b078-7259d98977a9, event_name=scroll, event_time=2025-06-13T16:19:59.193Z, user_id=user_pej5ild, session_id=session_m9xzvy2, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=32}]}
2025-06-13 23:19:59.197 [http-nio-8080-exec-4 INFO ] c.e.c.c.ClickstreamController - Received 3 events
2025-06-13 23:19:59.198 [http-nio-8080-exec-4 INFO ] c.e.c.c.ClickstreamController - Successfully processed 3 events
2025-06-13 23:20:00.691 [http-nio-8080-exec-5 INFO ] c.e.c.c.ClickstreamController - Received payload: {events=[{event_id=6d89736d-45aa-4838-9ccd-f34fe76777a1, event_name=click, event_time=2025-06-13T16:19:59.886Z, user_id=user_pej5ild, session_id=session_m9xzvy2, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 1
                        High-quality ite, element_type=div, element_name=null, track=product_click, productId=1}, {event_id=b9aab3d9-16e9-46ff-9e6f-da9c09df6b2e, event_name=click, event_time=2025-06-13T16:20:00.275Z, user_id=user_pej5ild, session_id=session_m9xzvy2, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 2
                        Durable and styl, element_type=div, element_name=null, track=product_click, productId=2}, {event_id=ab382cf2-dce1-4ecc-9f1a-a315ffffbd7e, event_name=click, event_time=2025-06-13T16:20:00.688Z, user_id=user_pej5ild, session_id=session_m9xzvy2, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 3
                        Customer favorit, element_type=div, element_name=null, track=product_click, productId=3}]}
2025-06-13 23:20:00.691 [http-nio-8080-exec-5 INFO ] c.e.c.c.ClickstreamController - Received 3 events
2025-06-13 23:20:00.692 [http-nio-8080-exec-5 INFO ] c.e.c.c.ClickstreamController - Successfully processed 3 events
2025-06-13 23:20:03.565 [http-nio-8080-exec-6 INFO ] c.e.c.c.ClickstreamController - Received payload: {events=[{event_id=15d22e04-c63e-44e9-8a05-f294817562d6, event_name=click, event_time=2025-06-13T16:20:01.985Z, user_id=user_pej5ild, session_id=session_m9xzvy2, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 5
                        Bestseller with , element_type=div, element_name=null, track=product_click, productId=5}, {event_id=7904d796-5658-4d21-923c-1f799f7ff8e9, event_name=scroll, event_time=2025-06-13T16:20:03.043Z, user_id=user_pej5ild, session_id=session_m9xzvy2, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=85}, {event_id=5b022c00-a4e5-41b6-a0a0-ce2e10236c97, event_name=click, event_time=2025-06-13T16:20:03.561Z, user_id=user_pej5ild, session_id=session_m9xzvy2, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 6
                        Sleek design and, element_type=div, element_name=null, track=product_click, productId=6}]}
2025-06-13 23:20:03.565 [http-nio-8080-exec-6 INFO ] c.e.c.c.ClickstreamController - Received 3 events
2025-06-13 23:20:03.566 [http-nio-8080-exec-6 INFO ] c.e.c.c.ClickstreamController - Successfully processed 3 events
2025-06-13 23:28:52.367 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node -1 disconnected.
2025-06-13 23:38:52.420 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node -1 disconnected.
