2025-06-14 00:58:30.513 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-14 00:58:30.515 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-14 00:58:30.519 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-14 00:58:30.519 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext already stopped.
2025-06-14 00:58:30.520 [SpringApplicationShutdownHook INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-06-14 00:58:30.520 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@5457ba34{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-14 00:58:30.521 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-14 00:58:30.522 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-14 00:58:30.522 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-14 00:58:30.522 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-14 00:58:30.522 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-14 00:58:30.523 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-1 unregistered
2025-06-14 08:32:18.148 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 205622 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-14 08:32:18.149 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-14 08:32:18.707 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-14 08:32:18.709 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-14 08:32:18.709 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-14 08:32:18.710 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-14 08:32:18.759 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-14 08:32:18.760 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 590 ms
2025-06-14 08:32:19.143 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-14 08:32:19.192 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-14 08:32:19.221 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-14 08:32:19.221 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-14 08:32:19.222 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-14 08:32:19.222 [main INFO ] org.apache.spark.SparkContext - Submitted application: ClickstreamProcessor
2025-06-14 08:32:19.230 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-14 08:32:19.236 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-14 08:32:19.236 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-14 08:32:19.254 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-14 08:32:19.255 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-14 08:32:19.255 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-14 08:32:19.255 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-14 08:32:19.255 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-14 08:32:19.335 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 40661.
2025-06-14 08:32:19.343 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-14 08:32:19.356 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-14 08:32:19.363 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-14 08:32:19.364 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-14 08:32:19.366 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-14 08:32:19.371 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-fd7cdc10-7ec7-4537-ac03-c35ebe71ab55
2025-06-14 08:32:19.388 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-14 08:32:19.395 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-14 08:32:19.405 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1730ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-14 08:32:19.445 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-14 08:32:19.449 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-14 08:32:19.455 [main INFO ] org.sparkproject.jetty.server.Server - Started @1780ms
2025-06-14 08:32:19.465 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@5b737f6a{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-14 08:32:19.465 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-14 08:32:19.472 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d0b447b{/,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.502 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-14 08:32:19.505 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-14 08:32:19.512 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43349.
2025-06-14 08:32:19.513 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:43349
2025-06-14 08:32:19.513 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-14 08:32:19.516 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 43349, None)
2025-06-14 08:32:19.517 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:43349 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 43349, None)
2025-06-14 08:32:19.519 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 43349, None)
2025-06-14 08:32:19.519 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 43349, None)
2025-06-14 08:32:19.533 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1d0b447b{/,null,STOPPED,@Spark}
2025-06-14 08:32:19.534 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48da64f2{/jobs,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.534 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2aa811f9{/jobs/json,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.534 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b7e4d14{/jobs/job,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.535 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@601d9f3a{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.535 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6585df70{/stages,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.535 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51fb5fe6{/stages/json,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.536 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3cb49121{/stages/stage,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.536 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c4215d7{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.537 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13f36d75{/stages/pool,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.537 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3155f190{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.537 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ebd8d2{/storage,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.538 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a63fa71{/storage/json,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.538 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5018b56b{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.538 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@737ff5c4{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.539 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ff64d{/environment,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.539 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79777da7{/environment/json,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.539 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e05a706{/executors,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.540 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a69014e{/executors/json,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.540 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@543ac221{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.540 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50e1f3fc{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.543 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56da8847{/static,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.543 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@545f0b6{/,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.544 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4888425d{/api,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.544 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@499c4d61{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.544 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59b3f754{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.546 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74eab077{/metrics/json,null,AVAILABLE,@Spark}
2025-06-14 08:32:19.684 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-14 08:32:19.830 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-14 08:32:19.846 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-14 08:32:19.862 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-14 08:32:19.862 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-14 08:32:19.862 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749864739862
2025-06-14 08:32:19.996 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-14 08:32:19.998 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-14 08:32:19.999 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-14 08:32:19.999 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-14 08:32:20.003 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-14 08:32:20.007 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-14 08:32:20.016 [main INFO ] com.example.Application - Started Application in 2.016 seconds (JVM running for 2.341)
2025-06-14 08:32:20.053 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-14 08:32:20.054 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-14 08:32:20.057 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-14 08:32:20.061 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@556843a5{/SQL,null,AVAILABLE,@Spark}
2025-06-14 08:32:20.061 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3db5195{/SQL/json,null,AVAILABLE,@Spark}
2025-06-14 08:32:20.062 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33bd9ac3{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-14 08:32:20.062 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42028589{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-14 08:32:20.063 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d99df7a{/static/sql,null,AVAILABLE,@Spark}
2025-06-14 08:32:20.293 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-14 08:32:20.293 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-14 08:32:20.294 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-14 08:32:20.401 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-14 08:32:20.408 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33d53f97{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-14 08:32:20.409 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21bf955e{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-14 08:32:20.410 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37edee6d{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-14 08:32:20.410 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44c2df71{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-14 08:32:20.411 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7820f30d{/static/sql,null,AVAILABLE,@Spark}
2025-06-14 08:32:21.072 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d2df20a7-cfbf-4a27-a986-a9fbe8d4a6e6. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-14 08:32:21.088 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-d2df20a7-cfbf-4a27-a986-a9fbe8d4a6e6 resolved to file:/tmp/temporary-d2df20a7-cfbf-4a27-a986-a9fbe8d4a6e6.
2025-06-14 08:32:21.088 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-14 08:32:21.145 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-d2df20a7-cfbf-4a27-a986-a9fbe8d4a6e6/metadata using temp file file:/tmp/temporary-d2df20a7-cfbf-4a27-a986-a9fbe8d4a6e6/.metadata.cfb5b71d-73af-4325-806e-d0365f0c8827.tmp
2025-06-14 08:32:21.189 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-d2df20a7-cfbf-4a27-a986-a9fbe8d4a6e6/.metadata.cfb5b71d-73af-4325-806e-d0365f0c8827.tmp to file:/tmp/temporary-d2df20a7-cfbf-4a27-a986-a9fbe8d4a6e6/metadata
2025-06-14 08:32:21.207 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466]. Use file:/tmp/temporary-d2df20a7-cfbf-4a27-a986-a9fbe8d4a6e6 to store the query checkpoint.
2025-06-14 08:32:21.213 [stream execution thread for [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@535e2f63] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@7a6a1c7c]
2025-06-14 08:32:21.231 [stream execution thread for [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-14 08:32:21.231 [stream execution thread for [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-14 08:32:21.231 [stream execution thread for [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-14 08:32:21.233 [stream execution thread for [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-14 08:32:21.349 [stream execution thread for [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-14 08:32:21.352 [stream execution thread for [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-14 08:32:21.352 [stream execution thread for [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-14 08:32:21.352 [stream execution thread for [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-14 08:32:21.352 [stream execution thread for [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749864741352
2025-06-14 08:32:21.385 [stream execution thread for [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-d2df20a7-cfbf-4a27-a986-a9fbe8d4a6e6/sources/0/0 using temp file file:/tmp/temporary-d2df20a7-cfbf-4a27-a986-a9fbe8d4a6e6/sources/0/.0.22df476b-d214-44c9-8e17-b2c1a25afc1e.tmp
2025-06-14 08:32:21.399 [stream execution thread for [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-d2df20a7-cfbf-4a27-a986-a9fbe8d4a6e6/sources/0/.0.22df476b-d214-44c9-8e17-b2c1a25afc1e.tmp to file:/tmp/temporary-d2df20a7-cfbf-4a27-a986-a9fbe8d4a6e6/sources/0/0
2025-06-14 08:32:21.399 [stream execution thread for [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-14 08:32:21.411 [stream execution thread for [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-d2df20a7-cfbf-4a27-a986-a9fbe8d4a6e6/offsets/0 using temp file file:/tmp/temporary-d2df20a7-cfbf-4a27-a986-a9fbe8d4a6e6/offsets/.0.17954509-2ed7-4465-998c-2e0b2c61203b.tmp
2025-06-14 08:32:21.429 [stream execution thread for [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-d2df20a7-cfbf-4a27-a986-a9fbe8d4a6e6/offsets/.0.17954509-2ed7-4465-998c-2e0b2c61203b.tmp to file:/tmp/temporary-d2df20a7-cfbf-4a27-a986-a9fbe8d4a6e6/offsets/0
2025-06-14 08:32:21.429 [stream execution thread for [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749864741405,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:32:21.592 [stream execution thread for [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:32:21.619 [stream execution thread for [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:32:21.654 [stream execution thread for [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:32:21.655 [stream execution thread for [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:32:21.836 [stream execution thread for [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 108.925532 ms
2025-06-14 08:32:21.954 [stream execution thread for [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] terminated with error
java.sql.SQLException: Code: 62. DB::Exception: Syntax error: failed at position 153 (end of query): . Expected one of: storage definition, ENGINE, AS. (SYNTAX_ERROR) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream]@-172704139
	at com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:85)
	at com.clickhouse.jdbc.SqlExceptionUtils.create(SqlExceptionUtils.java:31)
	at com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:90)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:122)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeLargeUpdate(ClickHouseStatementImpl.java:489)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeUpdate(ClickHouseStatementImpl.java:498)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.executeStatement(JdbcUtils.scala:1083)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:913)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.ClickstreamProcessor.lambda$startProcessing$8292e96f$1(ClickstreamProcessor.java:63)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
Caused by: java.io.IOException: Code: 62. DB::Exception: Syntax error: failed at position 153 (end of query): . Expected one of: storage definition, ENGINE, AS. (SYNTAX_ERROR) (version 22.1.3.7 (official build))

	at com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)
	at com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)
	at com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)
	at com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)
	at com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)
	at com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:120)
	... 62 common frames omitted
2025-06-14 08:32:21.955 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-2 unregistered
2025-06-14 08:32:21.956 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-14 08:32:21.956 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-14 08:32:21.956 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-14 08:32:21.957 [stream execution thread for [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = 80925b2e-5fca-4e30-9c53-846365f92f6e, runId = 69a409ad-edac-438b-866e-9bf1bca1a466] has been shutdown
2025-06-14 08:32:21.965 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Pausing ProtocolHandler ["http-nio-8080"]
2025-06-14 08:32:21.965 [main INFO ] o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-06-14 08:32:21.966 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Destroying Spring FrameworkServlet 'dispatcherServlet'
2025-06-14 08:32:21.967 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-06-14 08:32:21.968 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Destroying ProtocolHandler ["http-nio-8080"]
2025-06-14 08:32:21.970 [main INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-14 08:32:21.973 [main INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@5b737f6a{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-14 08:32:21.974 [main INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-14 08:32:21.980 [dispatcher-event-loop-9 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-14 08:32:21.984 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-14 08:32:21.984 [main INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-14 08:32:21.987 [main INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-14 08:32:21.988 [dispatcher-event-loop-13 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-14 08:32:21.990 [main INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-14 08:33:44.715 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 207171 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-14 08:33:44.716 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-14 08:33:45.229 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-14 08:33:45.232 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-14 08:33:45.232 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-14 08:33:45.232 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-14 08:33:45.274 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-14 08:33:45.274 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 539 ms
2025-06-14 08:33:45.665 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-14 08:33:45.708 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-14 08:33:45.736 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-14 08:33:45.736 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-14 08:33:45.736 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-14 08:33:45.737 [main INFO ] org.apache.spark.SparkContext - Submitted application: ClickstreamProcessor
2025-06-14 08:33:45.745 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-14 08:33:45.750 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-14 08:33:45.750 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-14 08:33:45.769 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-14 08:33:45.769 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-14 08:33:45.769 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-14 08:33:45.769 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-14 08:33:45.769 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-14 08:33:45.852 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 46465.
2025-06-14 08:33:45.859 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-14 08:33:45.871 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-14 08:33:45.877 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-14 08:33:45.877 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-14 08:33:45.879 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-14 08:33:45.883 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-bfa6c72e-e5ad-429f-919c-e8cfbd82d809
2025-06-14 08:33:45.898 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-14 08:33:45.905 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-14 08:33:45.915 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1677ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-14 08:33:45.953 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-14 08:33:45.957 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-14 08:33:45.963 [main INFO ] org.sparkproject.jetty.server.Server - Started @1725ms
2025-06-14 08:33:45.974 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@221672d7{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-14 08:33:45.974 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-14 08:33:45.982 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30ef32eb{/,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.012 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-14 08:33:46.015 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-14 08:33:46.022 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35589.
2025-06-14 08:33:46.022 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:35589
2025-06-14 08:33:46.023 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-14 08:33:46.026 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 35589, None)
2025-06-14 08:33:46.027 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:35589 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 35589, None)
2025-06-14 08:33:46.029 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 35589, None)
2025-06-14 08:33:46.029 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 35589, None)
2025-06-14 08:33:46.043 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@30ef32eb{/,null,STOPPED,@Spark}
2025-06-14 08:33:46.043 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b56d8a7{/jobs,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.044 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6de5ad56{/jobs/json,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.044 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44a44a04{/jobs/job,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.045 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a6fc1bc{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.045 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@360a3106{/stages,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.045 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e9a836{/stages/json,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.046 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7831d1aa{/stages/stage,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.046 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27746c5e{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.046 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2270f58d{/stages/pool,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.047 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54737322{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.047 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7323c38c{/storage,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.047 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63a72cc6{/storage/json,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.048 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cef885d{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.048 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f4fc83f{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.048 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16e4db59{/environment,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.049 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12a0d249{/environment/json,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.049 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c6c84fa{/executors,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.049 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64688978{/executors/json,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.050 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25f14e93{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.050 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c02899{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.053 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74231642{/static,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.053 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8315e4a{/,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.054 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22ff11ef{/api,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.054 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50e8ed74{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.054 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3063be68{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.056 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@df6f19b{/metrics/json,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.191 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-14 08:33:46.346 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-14 08:33:46.364 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-14 08:33:46.383 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-14 08:33:46.383 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-14 08:33:46.383 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749864826382
2025-06-14 08:33:46.516 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-14 08:33:46.519 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-14 08:33:46.519 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-14 08:33:46.519 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-14 08:33:46.523 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-14 08:33:46.527 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-14 08:33:46.535 [main INFO ] com.example.Application - Started Application in 1.957 seconds (JVM running for 2.298)
2025-06-14 08:33:46.572 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-14 08:33:46.572 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-14 08:33:46.575 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-14 08:33:46.579 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4536a09a{/SQL,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.579 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16cbba0f{/SQL/json,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.580 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d117280{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.580 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dddcd91{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.581 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39a2e77d{/static/sql,null,AVAILABLE,@Spark}
2025-06-14 08:33:46.952 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-14 08:33:46.952 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-14 08:33:46.954 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 2 ms
2025-06-14 08:33:46.996 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-14 08:33:47.006 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad25055{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-14 08:33:47.007 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8b8bb33{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-14 08:33:47.007 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@533cba9{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-14 08:33:47.007 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61dec7cc{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-14 08:33:47.008 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28d6b652{/static/sql,null,AVAILABLE,@Spark}
2025-06-14 08:33:47.519 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-0c395398-cbfe-432b-9a3e-c2b9ee2b1388. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-14 08:33:47.527 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-0c395398-cbfe-432b-9a3e-c2b9ee2b1388 resolved to file:/tmp/temporary-0c395398-cbfe-432b-9a3e-c2b9ee2b1388.
2025-06-14 08:33:47.527 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-14 08:33:47.561 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-0c395398-cbfe-432b-9a3e-c2b9ee2b1388/metadata using temp file file:/tmp/temporary-0c395398-cbfe-432b-9a3e-c2b9ee2b1388/.metadata.cf526f61-773f-4279-87a5-3fc64dafc450.tmp
2025-06-14 08:33:47.597 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-0c395398-cbfe-432b-9a3e-c2b9ee2b1388/.metadata.cf526f61-773f-4279-87a5-3fc64dafc450.tmp to file:/tmp/temporary-0c395398-cbfe-432b-9a3e-c2b9ee2b1388/metadata
2025-06-14 08:33:47.612 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e]. Use file:/tmp/temporary-0c395398-cbfe-432b-9a3e-c2b9ee2b1388 to store the query checkpoint.
2025-06-14 08:33:47.618 [stream execution thread for [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@6124313d] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@36b06671]
2025-06-14 08:33:47.630 [stream execution thread for [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-14 08:33:47.631 [stream execution thread for [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-14 08:33:47.631 [stream execution thread for [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-14 08:33:47.632 [stream execution thread for [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-14 08:33:47.745 [stream execution thread for [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-14 08:33:47.748 [stream execution thread for [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-14 08:33:47.748 [stream execution thread for [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-14 08:33:47.748 [stream execution thread for [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-14 08:33:47.748 [stream execution thread for [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749864827748
2025-06-14 08:33:47.772 [stream execution thread for [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-0c395398-cbfe-432b-9a3e-c2b9ee2b1388/sources/0/0 using temp file file:/tmp/temporary-0c395398-cbfe-432b-9a3e-c2b9ee2b1388/sources/0/.0.4b5c0dbe-dec2-4696-81bf-d3d0642bead6.tmp
2025-06-14 08:33:47.786 [stream execution thread for [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-0c395398-cbfe-432b-9a3e-c2b9ee2b1388/sources/0/.0.4b5c0dbe-dec2-4696-81bf-d3d0642bead6.tmp to file:/tmp/temporary-0c395398-cbfe-432b-9a3e-c2b9ee2b1388/sources/0/0
2025-06-14 08:33:47.787 [stream execution thread for [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-14 08:33:47.798 [stream execution thread for [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-0c395398-cbfe-432b-9a3e-c2b9ee2b1388/offsets/0 using temp file file:/tmp/temporary-0c395398-cbfe-432b-9a3e-c2b9ee2b1388/offsets/.0.7afb2d7c-7036-43e5-9fde-d15867ae7d4e.tmp
2025-06-14 08:33:47.814 [stream execution thread for [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-0c395398-cbfe-432b-9a3e-c2b9ee2b1388/offsets/.0.7afb2d7c-7036-43e5-9fde-d15867ae7d4e.tmp to file:/tmp/temporary-0c395398-cbfe-432b-9a3e-c2b9ee2b1388/offsets/0
2025-06-14 08:33:47.814 [stream execution thread for [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749864827792,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:33:47.959 [stream execution thread for [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:33:47.981 [stream execution thread for [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:33:48.016 [stream execution thread for [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:33:48.017 [stream execution thread for [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:33:48.184 [stream execution thread for [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 101.617264 ms
2025-06-14 08:33:48.278 [stream execution thread for [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] terminated with error
java.sql.SQLException: Code: 62. DB::Exception: Syntax error: failed at position 153 (end of query): . Expected one of: storage definition, ENGINE, AS. (SYNTAX_ERROR) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream]@173363209
	at com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:85)
	at com.clickhouse.jdbc.SqlExceptionUtils.create(SqlExceptionUtils.java:31)
	at com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:90)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:122)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeLargeUpdate(ClickHouseStatementImpl.java:489)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeUpdate(ClickHouseStatementImpl.java:498)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.executeStatement(JdbcUtils.scala:1083)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:913)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.ClickstreamProcessor.lambda$startProcessing$8292e96f$1(ClickstreamProcessor.java:63)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
Caused by: java.io.IOException: Code: 62. DB::Exception: Syntax error: failed at position 153 (end of query): . Expected one of: storage definition, ENGINE, AS. (SYNTAX_ERROR) (version 22.1.3.7 (official build))

	at com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)
	at com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)
	at com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)
	at com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)
	at com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)
	at com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:120)
	... 62 common frames omitted
2025-06-14 08:33:48.279 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-2 unregistered
2025-06-14 08:33:48.280 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-14 08:33:48.281 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-14 08:33:48.281 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-14 08:33:48.281 [stream execution thread for [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = 5a514f5c-f836-431a-805c-69f1c1b1015f, runId = c58ffaac-6571-48d9-84af-39900625d68e] has been shutdown
2025-06-14 08:33:48.287 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Pausing ProtocolHandler ["http-nio-8080"]
2025-06-14 08:33:48.287 [main INFO ] o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-06-14 08:33:48.287 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Destroying Spring FrameworkServlet 'dispatcherServlet'
2025-06-14 08:33:48.288 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-06-14 08:33:48.290 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Destroying ProtocolHandler ["http-nio-8080"]
2025-06-14 08:33:48.291 [main INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-14 08:33:48.294 [main INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@221672d7{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-14 08:33:48.295 [main INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-14 08:33:48.302 [dispatcher-event-loop-9 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-14 08:33:48.306 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-14 08:33:48.306 [main INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-14 08:33:48.309 [main INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-14 08:33:48.310 [dispatcher-event-loop-13 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-14 08:33:48.313 [main INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-14 08:34:25.315 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 208008 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-14 08:34:25.316 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-14 08:34:25.848 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-14 08:34:25.851 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-14 08:34:25.852 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-14 08:34:25.852 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-14 08:34:25.900 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-14 08:34:25.901 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 566 ms
2025-06-14 08:34:26.316 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-14 08:34:26.361 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-14 08:34:26.389 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-14 08:34:26.389 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-14 08:34:26.389 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-14 08:34:26.390 [main INFO ] org.apache.spark.SparkContext - Submitted application: ClickstreamProcessor
2025-06-14 08:34:26.398 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-14 08:34:26.404 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-14 08:34:26.404 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-14 08:34:26.422 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-14 08:34:26.422 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-14 08:34:26.422 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-14 08:34:26.423 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-14 08:34:26.423 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-14 08:34:26.503 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 46413.
2025-06-14 08:34:26.511 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-14 08:34:26.522 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-14 08:34:26.528 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-14 08:34:26.529 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-14 08:34:26.530 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-14 08:34:26.534 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-b7e0a455-2bbc-4dad-b74b-934665b1401d
2025-06-14 08:34:26.549 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-14 08:34:26.558 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-14 08:34:26.568 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1709ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-14 08:34:26.602 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-14 08:34:26.607 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-14 08:34:26.613 [main INFO ] org.sparkproject.jetty.server.Server - Started @1755ms
2025-06-14 08:34:26.624 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@b218ff8{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-14 08:34:26.624 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-14 08:34:26.631 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30ef32eb{/,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.660 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-14 08:34:26.663 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-14 08:34:26.671 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41975.
2025-06-14 08:34:26.671 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:41975
2025-06-14 08:34:26.672 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-14 08:34:26.674 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 41975, None)
2025-06-14 08:34:26.676 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:41975 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 41975, None)
2025-06-14 08:34:26.677 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 41975, None)
2025-06-14 08:34:26.678 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 41975, None)
2025-06-14 08:34:26.694 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@30ef32eb{/,null,STOPPED,@Spark}
2025-06-14 08:34:26.695 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b56d8a7{/jobs,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.695 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6de5ad56{/jobs/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.695 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44a44a04{/jobs/job,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.696 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a6fc1bc{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.696 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@360a3106{/stages,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.696 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e9a836{/stages/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.697 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7831d1aa{/stages/stage,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.697 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27746c5e{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.698 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2270f58d{/stages/pool,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.698 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54737322{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.698 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7323c38c{/storage,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.699 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63a72cc6{/storage/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.699 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cef885d{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.699 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f4fc83f{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.700 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16e4db59{/environment,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.700 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12a0d249{/environment/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.700 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c6c84fa{/executors,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.701 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64688978{/executors/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.701 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25f14e93{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.702 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c02899{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.704 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74231642{/static,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.704 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8315e4a{/,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.705 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22ff11ef{/api,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.705 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50e8ed74{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.706 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3063be68{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.707 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@df6f19b{/metrics/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:26.837 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-14 08:34:26.995 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-14 08:34:27.014 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-14 08:34:27.031 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-14 08:34:27.032 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-14 08:34:27.032 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749864867031
2025-06-14 08:34:27.158 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-14 08:34:27.160 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-14 08:34:27.160 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-14 08:34:27.160 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-14 08:34:27.165 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-14 08:34:27.169 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-14 08:34:27.177 [main INFO ] com.example.Application - Started Application in 2.006 seconds (JVM running for 2.319)
2025-06-14 08:34:27.213 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-14 08:34:27.214 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-14 08:34:27.217 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-14 08:34:27.221 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4536a09a{/SQL,null,AVAILABLE,@Spark}
2025-06-14 08:34:27.221 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16cbba0f{/SQL/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:27.222 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d117280{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-14 08:34:27.222 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dddcd91{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:27.222 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39a2e77d{/static/sql,null,AVAILABLE,@Spark}
2025-06-14 08:34:27.439 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-14 08:34:27.440 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-14 08:34:27.440 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 0 ms
2025-06-14 08:34:27.546 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-14 08:34:27.552 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6099b747{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-14 08:34:27.553 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e977138{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:27.553 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73fca2bc{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-14 08:34:27.554 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5532e913{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:27.554 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f63ea4f{/static/sql,null,AVAILABLE,@Spark}
2025-06-14 08:34:28.237 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-e1e1b5d4-45e1-44ef-bce6-505a082ed929. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-14 08:34:28.244 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-e1e1b5d4-45e1-44ef-bce6-505a082ed929 resolved to file:/tmp/temporary-e1e1b5d4-45e1-44ef-bce6-505a082ed929.
2025-06-14 08:34:28.244 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-14 08:34:28.280 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-e1e1b5d4-45e1-44ef-bce6-505a082ed929/metadata using temp file file:/tmp/temporary-e1e1b5d4-45e1-44ef-bce6-505a082ed929/.metadata.e256e47d-7829-4872-af00-52e1b6663e04.tmp
2025-06-14 08:34:28.315 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-e1e1b5d4-45e1-44ef-bce6-505a082ed929/.metadata.e256e47d-7829-4872-af00-52e1b6663e04.tmp to file:/tmp/temporary-e1e1b5d4-45e1-44ef-bce6-505a082ed929/metadata
2025-06-14 08:34:28.329 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68]. Use file:/tmp/temporary-e1e1b5d4-45e1-44ef-bce6-505a082ed929 to store the query checkpoint.
2025-06-14 08:34:28.334 [stream execution thread for [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@16cd1b2d] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@5b0a4385]
2025-06-14 08:34:28.349 [stream execution thread for [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-14 08:34:28.350 [stream execution thread for [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-14 08:34:28.350 [stream execution thread for [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-14 08:34:28.351 [stream execution thread for [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-14 08:34:28.462 [stream execution thread for [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-14 08:34:28.464 [stream execution thread for [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-14 08:34:28.464 [stream execution thread for [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-14 08:34:28.465 [stream execution thread for [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-14 08:34:28.465 [stream execution thread for [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749864868464
2025-06-14 08:34:28.492 [stream execution thread for [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-e1e1b5d4-45e1-44ef-bce6-505a082ed929/sources/0/0 using temp file file:/tmp/temporary-e1e1b5d4-45e1-44ef-bce6-505a082ed929/sources/0/.0.9652fbb9-16f6-4b8e-b396-9cab6a4aa1ec.tmp
2025-06-14 08:34:28.504 [stream execution thread for [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-e1e1b5d4-45e1-44ef-bce6-505a082ed929/sources/0/.0.9652fbb9-16f6-4b8e-b396-9cab6a4aa1ec.tmp to file:/tmp/temporary-e1e1b5d4-45e1-44ef-bce6-505a082ed929/sources/0/0
2025-06-14 08:34:28.504 [stream execution thread for [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-14 08:34:28.514 [stream execution thread for [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-e1e1b5d4-45e1-44ef-bce6-505a082ed929/offsets/0 using temp file file:/tmp/temporary-e1e1b5d4-45e1-44ef-bce6-505a082ed929/offsets/.0.4eb6099a-3ef5-4944-9580-8f7740fc5bcb.tmp
2025-06-14 08:34:28.531 [stream execution thread for [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-e1e1b5d4-45e1-44ef-bce6-505a082ed929/offsets/.0.4eb6099a-3ef5-4944-9580-8f7740fc5bcb.tmp to file:/tmp/temporary-e1e1b5d4-45e1-44ef-bce6-505a082ed929/offsets/0
2025-06-14 08:34:28.531 [stream execution thread for [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749864868510,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:34:28.681 [stream execution thread for [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:34:28.703 [stream execution thread for [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:34:28.741 [stream execution thread for [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:34:28.742 [stream execution thread for [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:34:28.917 [stream execution thread for [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 104.908084 ms
2025-06-14 08:34:29.006 [stream execution thread for [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] terminated with error
java.sql.SQLException: Code: 62. DB::Exception: Syntax error: failed at position 153 (end of query): . Expected one of: storage definition, ENGINE, AS. (SYNTAX_ERROR) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream]@250111399
	at com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:85)
	at com.clickhouse.jdbc.SqlExceptionUtils.create(SqlExceptionUtils.java:31)
	at com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:90)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:122)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeLargeUpdate(ClickHouseStatementImpl.java:489)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeUpdate(ClickHouseStatementImpl.java:498)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.executeStatement(JdbcUtils.scala:1083)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:913)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.ClickstreamProcessor.lambda$startProcessing$8292e96f$1(ClickstreamProcessor.java:63)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
Caused by: java.io.IOException: Code: 62. DB::Exception: Syntax error: failed at position 153 (end of query): . Expected one of: storage definition, ENGINE, AS. (SYNTAX_ERROR) (version 22.1.3.7 (official build))

	at com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)
	at com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)
	at com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)
	at com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)
	at com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)
	at com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:120)
	... 62 common frames omitted
2025-06-14 08:34:29.006 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-2 unregistered
2025-06-14 08:34:29.008 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-14 08:34:29.008 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-14 08:34:29.008 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-14 08:34:29.008 [stream execution thread for [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = 218cba7b-76bf-435c-83d2-4619419be2c4, runId = 6b38e8fd-b262-42d6-b5b4-22c63e2cbe68] has been shutdown
2025-06-14 08:34:29.014 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Pausing ProtocolHandler ["http-nio-8080"]
2025-06-14 08:34:29.014 [main INFO ] o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-06-14 08:34:29.014 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Destroying Spring FrameworkServlet 'dispatcherServlet'
2025-06-14 08:34:29.015 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-06-14 08:34:29.017 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Destroying ProtocolHandler ["http-nio-8080"]
2025-06-14 08:34:29.018 [main INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-14 08:34:29.021 [main INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@b218ff8{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-14 08:34:29.022 [main INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-14 08:34:29.028 [dispatcher-event-loop-9 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-14 08:34:29.032 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-14 08:34:29.032 [main INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-14 08:34:29.034 [main INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-14 08:34:29.036 [dispatcher-event-loop-13 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-14 08:34:29.038 [main INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-14 08:34:33.449 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 208397 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-14 08:34:33.450 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-14 08:34:33.968 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-14 08:34:33.970 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-14 08:34:33.971 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-14 08:34:33.971 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-14 08:34:34.008 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-14 08:34:34.008 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 539 ms
2025-06-14 08:34:34.409 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-14 08:34:34.458 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-14 08:34:34.490 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-14 08:34:34.490 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-14 08:34:34.490 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-14 08:34:34.490 [main INFO ] org.apache.spark.SparkContext - Submitted application: ClickstreamProcessor
2025-06-14 08:34:34.500 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-14 08:34:34.505 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-14 08:34:34.506 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-14 08:34:34.524 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-14 08:34:34.524 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-14 08:34:34.525 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-14 08:34:34.525 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-14 08:34:34.525 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-14 08:34:34.604 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 43783.
2025-06-14 08:34:34.612 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-14 08:34:34.625 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-14 08:34:34.632 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-14 08:34:34.632 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-14 08:34:34.634 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-14 08:34:34.639 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-e2337037-1553-4aff-80eb-0935f95b3b3b
2025-06-14 08:34:34.655 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-14 08:34:34.661 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-14 08:34:34.671 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1677ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-14 08:34:34.707 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-14 08:34:34.711 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-14 08:34:34.717 [main INFO ] org.sparkproject.jetty.server.Server - Started @1723ms
2025-06-14 08:34:34.728 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@26935b6c{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-14 08:34:34.728 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-14 08:34:34.736 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@bb5f9d{/,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.764 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-14 08:34:34.767 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-14 08:34:34.775 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37883.
2025-06-14 08:34:34.775 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:37883
2025-06-14 08:34:34.776 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-14 08:34:34.778 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 37883, None)
2025-06-14 08:34:34.780 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:37883 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 37883, None)
2025-06-14 08:34:34.782 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 37883, None)
2025-06-14 08:34:34.782 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 37883, None)
2025-06-14 08:34:34.796 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@bb5f9d{/,null,STOPPED,@Spark}
2025-06-14 08:34:34.797 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6de5ad56{/jobs,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.797 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cfb94fd{/jobs/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.798 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a6fc1bc{/jobs/job,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.798 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@360a3106{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.798 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e9a836{/stages,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.799 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75aa7703{/stages/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.799 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27746c5e{/stages/stage,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.799 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2270f58d{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.800 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54737322{/stages/pool,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.800 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7323c38c{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.800 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63a72cc6{/storage,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.801 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cef885d{/storage/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.801 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f4fc83f{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.801 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16e4db59{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.802 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12a0d249{/environment,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.802 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c6c84fa{/environment/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.802 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64688978{/executors,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.803 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25f14e93{/executors/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.803 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c02899{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.804 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74231642{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.806 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4feec184{/static,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.806 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22ff11ef{/,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.807 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23d978b{/api,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.807 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3063be68{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.808 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d2f09a4{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.809 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@614cbec4{/metrics/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:34.943 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-14 08:34:35.085 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-14 08:34:35.103 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-14 08:34:35.120 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-14 08:34:35.120 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-14 08:34:35.120 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749864875119
2025-06-14 08:34:35.254 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-14 08:34:35.256 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-14 08:34:35.256 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-14 08:34:35.256 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-14 08:34:35.260 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-14 08:34:35.264 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-14 08:34:35.273 [main INFO ] com.example.Application - Started Application in 1.961 seconds (JVM running for 2.279)
2025-06-14 08:34:35.309 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-14 08:34:35.309 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-14 08:34:35.313 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-14 08:34:35.317 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16cbba0f{/SQL,null,AVAILABLE,@Spark}
2025-06-14 08:34:35.317 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58932d08{/SQL/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:35.318 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dddcd91{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-14 08:34:35.318 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79e9c14{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:35.319 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3adeca1f{/static/sql,null,AVAILABLE,@Spark}
2025-06-14 08:34:35.709 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-14 08:34:35.709 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-14 08:34:35.710 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-14 08:34:35.748 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-14 08:34:35.762 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@559ea76{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-14 08:34:35.762 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4249fdb4{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:35.763 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1048f0d1{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-14 08:34:35.764 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12bba2af{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-14 08:34:35.764 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e0f44f{/static/sql,null,AVAILABLE,@Spark}
2025-06-14 08:34:36.268 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-906a5b51-e875-4789-8734-54c6339c3cbe. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-14 08:34:36.275 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-906a5b51-e875-4789-8734-54c6339c3cbe resolved to file:/tmp/temporary-906a5b51-e875-4789-8734-54c6339c3cbe.
2025-06-14 08:34:36.275 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-14 08:34:36.309 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-906a5b51-e875-4789-8734-54c6339c3cbe/metadata using temp file file:/tmp/temporary-906a5b51-e875-4789-8734-54c6339c3cbe/.metadata.53b9d0ee-d839-46cc-a41b-3bac6004bf96.tmp
2025-06-14 08:34:36.345 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-906a5b51-e875-4789-8734-54c6339c3cbe/.metadata.53b9d0ee-d839-46cc-a41b-3bac6004bf96.tmp to file:/tmp/temporary-906a5b51-e875-4789-8734-54c6339c3cbe/metadata
2025-06-14 08:34:36.358 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b]. Use file:/tmp/temporary-906a5b51-e875-4789-8734-54c6339c3cbe to store the query checkpoint.
2025-06-14 08:34:36.363 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@6901d842] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@ed00072]
2025-06-14 08:34:36.375 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-14 08:34:36.376 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-14 08:34:36.376 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-14 08:34:36.377 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-14 08:34:36.487 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-14 08:34:36.489 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-14 08:34:36.489 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-14 08:34:36.489 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-14 08:34:36.489 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749864876489
2025-06-14 08:34:36.514 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-906a5b51-e875-4789-8734-54c6339c3cbe/sources/0/0 using temp file file:/tmp/temporary-906a5b51-e875-4789-8734-54c6339c3cbe/sources/0/.0.3074538b-4e75-4708-bd9c-eff1fe583bd7.tmp
2025-06-14 08:34:36.529 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-906a5b51-e875-4789-8734-54c6339c3cbe/sources/0/.0.3074538b-4e75-4708-bd9c-eff1fe583bd7.tmp to file:/tmp/temporary-906a5b51-e875-4789-8734-54c6339c3cbe/sources/0/0
2025-06-14 08:34:36.530 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-14 08:34:36.541 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-906a5b51-e875-4789-8734-54c6339c3cbe/offsets/0 using temp file file:/tmp/temporary-906a5b51-e875-4789-8734-54c6339c3cbe/offsets/.0.b7ce4505-4eff-4e7d-8f06-398eeb2db567.tmp
2025-06-14 08:34:36.557 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-906a5b51-e875-4789-8734-54c6339c3cbe/offsets/.0.b7ce4505-4eff-4e7d-8f06-398eeb2db567.tmp to file:/tmp/temporary-906a5b51-e875-4789-8734-54c6339c3cbe/offsets/0
2025-06-14 08:34:36.558 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749864876536,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:34:36.704 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:34:36.725 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:34:36.759 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:34:36.760 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:34:36.936 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 110.408547 ms
2025-06-14 08:34:37.049 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.624846 ms
2025-06-14 08:34:37.093 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:34:37.101 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at ClickstreamProcessor.java:66) with 2 output partitions
2025-06-14 08:34:37.101 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (start at ClickstreamProcessor.java:66)
2025-06-14 08:34:37.101 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:34:37.101 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:34:37.103 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[8] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:34:37.165 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 39.4 KiB, free 9.2 GiB)
2025-06-14 08:34:37.231 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 9.2 GiB)
2025-06-14 08:34:37.232 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:37883 (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:34:37.233 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:34:37.238 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0, 1))
2025-06-14 08:34:37.238 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-14 08:34:37.257 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:34:37.258 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:34:37.261 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-14 08:34:37.261 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-14 08:34:37.347 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 15.175189 ms
2025-06-14 08:34:37.362 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.800086 ms
2025-06-14 08:34:37.386 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 5.498551 ms
2025-06-14 08:34:37.396 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.982574 ms
2025-06-14 08:34:37.406 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-14 08:34:37.406 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-14 08:34:37.427 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-14 08:34:37.427 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-14 08:34:37.451 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-14 08:34:37.451 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-14 08:34:37.452 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749864877451
2025-06-14 08:34:37.452 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-14 08:34:37.452 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-14 08:34:37.452 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749864877451
2025-06-14 08:34:37.452 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor-1, groupId=spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor] Assigned to partition(s): clickstream-events-0
2025-06-14 08:34:37.452 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor-2, groupId=spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor] Assigned to partition(s): clickstream-events-1
2025-06-14 08:34:37.457 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor-2, groupId=spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-14 08:34:37.457 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor-1, groupId=spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-14 08:34:37.462 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor-2, groupId=spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor] Cluster ID: HhNeO4joR2-K_kbMfLUuyQ
2025-06-14 08:34:37.462 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor-1, groupId=spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor] Cluster ID: HhNeO4joR2-K_kbMfLUuyQ
2025-06-14 08:34:37.506 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor-1, groupId=spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-14 08:34:37.506 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor-2, groupId=spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-14 08:34:38.015 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor-2, groupId=spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:34:38.015 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor-1, groupId=spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:34:38.015 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor-2, groupId=spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-14 08:34:38.015 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor-1, groupId=spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-14 08:34:38.016 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor-1, groupId=spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=11, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:34:38.016 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor-2, groupId=spark-kafka-source-e063dd9a-243c-42e4-98c5-4d1ecdc096ab-1937638236-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=4, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:34:38.068 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:34:38.068 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:34:38.085 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:34:38.086 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [cb2217cb-9d07-47a4-a075-412cc8e408fd] (2 queries & 0 savepoints) is committed.
2025-06-14 08:34:38.086 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:34:38.086 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [d0a4c69b-e6aa-4a04-accc-1afc47cf0d9a] (0 queries & 0 savepoints) is committed.
2025-06-14 08:34:38.086 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [2e255ae2-3954-46dd-a416-6f5364e79f05] (2 queries & 0 savepoints) is committed.
2025-06-14 08:34:38.086 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [6d784089-48ed-4beb-b2fc-9086ecfe0545] (0 queries & 0 savepoints) is committed.
2025-06-14 08:34:38.093 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1646 bytes result sent to driver
2025-06-14 08:34:38.094 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1689 bytes result sent to driver
2025-06-14 08:34:38.098 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 839 ms on phamviethoa (executor driver) (1/2)
2025-06-14 08:34:38.098 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 845 ms on phamviethoa (executor driver) (2/2)
2025-06-14 08:34:38.099 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-14 08:34:38.101 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (start at ClickstreamProcessor.java:66) finished in 0.993 s
2025-06-14 08:34:38.102 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:34:38.102 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-14 08:34:38.103 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at ClickstreamProcessor.java:66, took 1.009633 s
2025-06-14 08:34:38.124 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] terminated with error
org.apache.spark.SparkSQLException: Unsupported type TIMESTAMP_WITH_TIMEZONE.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedJdbcTypeError(QueryExecutionErrors.scala:1019)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getCatalystType(JdbcUtils.scala:239)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getSchema$1(JdbcUtils.scala:321)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:321)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:71)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:88)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.ClickstreamProcessor.lambda$startProcessing$8292e96f$1(ClickstreamProcessor.java:63)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
2025-06-14 08:34:38.124 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-2 unregistered
2025-06-14 08:34:38.126 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-14 08:34:38.126 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-14 08:34:38.126 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-14 08:34:38.126 [stream execution thread for [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = ba84efbb-803a-46ca-ae56-7d4f0d76d284, runId = a634eb0e-34f3-4da6-a851-32d3a29f0f4b] has been shutdown
2025-06-14 08:34:38.133 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Pausing ProtocolHandler ["http-nio-8080"]
2025-06-14 08:34:38.133 [main INFO ] o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-06-14 08:34:38.133 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Destroying Spring FrameworkServlet 'dispatcherServlet'
2025-06-14 08:34:38.134 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-06-14 08:34:38.135 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Destroying ProtocolHandler ["http-nio-8080"]
2025-06-14 08:34:38.137 [main INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-14 08:34:38.140 [main INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@26935b6c{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-14 08:34:38.141 [main INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-14 08:34:38.171 [dispatcher-event-loop-14 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-14 08:34:38.182 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-14 08:34:38.182 [main INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-14 08:34:38.185 [main INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-14 08:34:38.186 [dispatcher-event-loop-2 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-14 08:34:38.188 [main INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-14 08:35:11.270 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 209073 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-14 08:35:11.271 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-14 08:35:11.832 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-14 08:35:11.835 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-14 08:35:11.835 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-14 08:35:11.835 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-14 08:35:11.882 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-14 08:35:11.883 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 593 ms
2025-06-14 08:35:12.297 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-14 08:35:12.342 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-14 08:35:12.372 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-14 08:35:12.372 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-14 08:35:12.372 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-14 08:35:12.372 [main INFO ] org.apache.spark.SparkContext - Submitted application: ClickstreamProcessor
2025-06-14 08:35:12.380 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-14 08:35:12.386 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-14 08:35:12.386 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-14 08:35:12.405 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-14 08:35:12.405 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-14 08:35:12.405 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-14 08:35:12.405 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-14 08:35:12.405 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-14 08:35:12.486 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 45365.
2025-06-14 08:35:12.493 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-14 08:35:12.505 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-14 08:35:12.511 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-14 08:35:12.512 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-14 08:35:12.513 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-14 08:35:12.517 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-a290724d-ce79-423b-8be5-973008e4f255
2025-06-14 08:35:12.532 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-14 08:35:12.538 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-14 08:35:12.548 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1749ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-14 08:35:12.583 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-14 08:35:12.587 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-14 08:35:12.593 [main INFO ] org.sparkproject.jetty.server.Server - Started @1795ms
2025-06-14 08:35:12.604 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@8e09dcb{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-14 08:35:12.604 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-14 08:35:12.611 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af7ac25{/,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.640 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-14 08:35:12.643 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-14 08:35:12.650 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45719.
2025-06-14 08:35:12.651 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:45719
2025-06-14 08:35:12.651 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-14 08:35:12.654 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 45719, None)
2025-06-14 08:35:12.655 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:45719 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 45719, None)
2025-06-14 08:35:12.657 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 45719, None)
2025-06-14 08:35:12.657 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 45719, None)
2025-06-14 08:35:12.671 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4af7ac25{/,null,STOPPED,@Spark}
2025-06-14 08:35:12.672 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d4da729{/jobs,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.672 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b7e4d14{/jobs/json,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.673 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6585df70{/jobs/job,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.673 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51fb5fe6{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.673 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1791e231{/stages,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.674 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e360c3b{/stages/json,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.674 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13f36d75{/stages/stage,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.675 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3155f190{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.675 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ebd8d2{/stages/pool,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.675 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a63fa71{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.676 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5018b56b{/storage,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.676 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@737ff5c4{/storage/json,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.676 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ff64d{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.677 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79777da7{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.677 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e05a706{/environment,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.677 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a69014e{/environment/json,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.678 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@543ac221{/executors,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.678 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50e1f3fc{/executors/json,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.678 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56da8847{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.679 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c02a007{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.681 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61bd0845{/static,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.681 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@166ddfb7{/,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.682 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57b3d869{/api,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.682 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a510e0e{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.683 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4efe014f{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.684 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c677d7e{/metrics/json,null,AVAILABLE,@Spark}
2025-06-14 08:35:12.815 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-14 08:35:12.960 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-14 08:35:12.979 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-14 08:35:12.996 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-14 08:35:12.996 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-14 08:35:12.996 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749864912995
2025-06-14 08:35:13.124 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-14 08:35:13.126 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-14 08:35:13.126 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-14 08:35:13.126 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-14 08:35:13.130 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-14 08:35:13.134 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-14 08:35:13.142 [main INFO ] com.example.Application - Started Application in 2.017 seconds (JVM running for 2.344)
2025-06-14 08:35:13.179 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-14 08:35:13.179 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-14 08:35:13.182 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-14 08:35:13.186 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1cda75be{/SQL,null,AVAILABLE,@Spark}
2025-06-14 08:35:13.186 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b1d2f65{/SQL/json,null,AVAILABLE,@Spark}
2025-06-14 08:35:13.187 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fc21ff4{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-14 08:35:13.187 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e1c91cd{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-14 08:35:13.188 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f9236d8{/static/sql,null,AVAILABLE,@Spark}
2025-06-14 08:35:13.478 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-14 08:35:13.478 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-14 08:35:13.479 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-14 08:35:13.519 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-14 08:35:13.527 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@157df8a2{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-14 08:35:13.527 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39861f6a{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-14 08:35:13.528 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67f5c44e{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-14 08:35:13.528 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f908d57{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-14 08:35:13.529 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c1df7e6{/static/sql,null,AVAILABLE,@Spark}
2025-06-14 08:35:14.124 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-14 08:35:14.131 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c resolved to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c.
2025-06-14 08:35:14.131 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-14 08:35:14.166 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/metadata using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/.metadata.2c4dd07d-cbc3-4a0e-9a44-96508cb28cdb.tmp
2025-06-14 08:35:14.203 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/.metadata.2c4dd07d-cbc3-4a0e-9a44-96508cb28cdb.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/metadata
2025-06-14 08:35:14.216 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f]. Use file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c to store the query checkpoint.
2025-06-14 08:35:14.221 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@212f7891] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@2ff532dc]
2025-06-14 08:35:14.234 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-14 08:35:14.235 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-14 08:35:14.235 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-14 08:35:14.236 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-14 08:35:14.349 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-14 08:35:14.351 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-14 08:35:14.352 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-14 08:35:14.352 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-14 08:35:14.352 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749864914352
2025-06-14 08:35:14.375 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/sources/0/0 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/sources/0/.0.5a24aaac-78e1-4fa3-83e4-18f91de546af.tmp
2025-06-14 08:35:14.387 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/sources/0/.0.5a24aaac-78e1-4fa3-83e4-18f91de546af.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/sources/0/0
2025-06-14 08:35:14.387 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-14 08:35:14.398 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/0 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.0.5573be05-7d8b-4808-8c25-bde441370267.tmp
2025-06-14 08:35:14.413 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.0.5573be05-7d8b-4808-8c25-bde441370267.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/0
2025-06-14 08:35:14.413 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749864914393,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:35:14.561 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:14.583 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:14.618 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:14.619 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:14.799 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 107.826843 ms
2025-06-14 08:35:14.908 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.354042 ms
2025-06-14 08:35:14.951 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:35:14.960 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at ClickstreamProcessor.java:66) with 2 output partitions
2025-06-14 08:35:14.960 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (start at ClickstreamProcessor.java:66)
2025-06-14 08:35:14.960 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:35:14.961 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:35:14.962 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[8] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:35:15.021 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 39.4 KiB, free 9.2 GiB)
2025-06-14 08:35:15.087 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 9.2 GiB)
2025-06-14 08:35:15.089 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:45719 (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:35:15.090 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:35:15.094 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0, 1))
2025-06-14 08:35:15.095 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-14 08:35:15.114 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:35:15.115 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:35:15.119 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-14 08:35:15.119 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-14 08:35:15.207 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 15.158873 ms
2025-06-14 08:35:15.222 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.761841 ms
2025-06-14 08:35:15.242 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 5.432541 ms
2025-06-14 08:35:15.253 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.180319 ms
2025-06-14 08:35:15.261 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-14 08:35:15.261 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-14 08:35:15.277 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-14 08:35:15.277 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-14 08:35:15.300 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-14 08:35:15.300 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-14 08:35:15.300 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749864915300
2025-06-14 08:35:15.300 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-14 08:35:15.300 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-14 08:35:15.300 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749864915300
2025-06-14 08:35:15.301 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Assigned to partition(s): clickstream-events-1
2025-06-14 08:35:15.301 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Assigned to partition(s): clickstream-events-0
2025-06-14 08:35:15.306 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-14 08:35:15.306 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-14 08:35:15.310 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Cluster ID: HhNeO4joR2-K_kbMfLUuyQ
2025-06-14 08:35:15.310 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Cluster ID: HhNeO4joR2-K_kbMfLUuyQ
2025-06-14 08:35:15.333 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-14 08:35:15.333 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-14 08:35:15.836 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:15.836 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-14 08:35:15.836 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:15.836 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-14 08:35:15.837 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=4, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:15.837 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=11, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:15.882 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:15.882 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:15.895 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:15.895 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [1747fcd9-067d-4b56-8dc3-ae3ce01a9393] (2 queries & 0 savepoints) is committed.
2025-06-14 08:35:15.895 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [c0aa9a25-d22c-4e43-a41a-c09643b45f11] (0 queries & 0 savepoints) is committed.
2025-06-14 08:35:15.895 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:15.895 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [a7608c65-f5aa-41e7-9f5b-014629f15070] (2 queries & 0 savepoints) is committed.
2025-06-14 08:35:15.895 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [a83ed938-9f11-480f-9e1a-7c8b988c63f5] (0 queries & 0 savepoints) is committed.
2025-06-14 08:35:15.903 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1646 bytes result sent to driver
2025-06-14 08:35:15.903 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1689 bytes result sent to driver
2025-06-14 08:35:15.909 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 798 ms on phamviethoa (executor driver) (1/2)
2025-06-14 08:35:15.909 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 794 ms on phamviethoa (executor driver) (2/2)
2025-06-14 08:35:15.910 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-14 08:35:15.912 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (start at ClickstreamProcessor.java:66) finished in 0.944 s
2025-06-14 08:35:15.913 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:35:15.913 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-14 08:35:15.914 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at ClickstreamProcessor.java:66, took 0.962265 s
2025-06-14 08:35:15.934 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/0 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.0.65a12d25-c502-4978-8baf-8dd9f552323f.tmp
2025-06-14 08:35:15.947 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.0.65a12d25-c502-4978-8baf-8dd9f552323f.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/0
2025-06-14 08:35:15.965 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:35:14.232Z",
  "batchId" : 0,
  "numInputRows" : 15,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 8.746355685131196,
  "durationMs" : {
    "addBatch" : 1323,
    "commitOffsets" : 17,
    "getBatch" : 11,
    "latestOffset" : 156,
    "queryPlanning" : 177,
    "triggerExecution" : 1715,
    "walCommit" : 19
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : null,
    "endOffset" : {
      "clickstream-events" : {
        "1" : 4,
        "0" : 11
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 4,
        "0" : 11
      }
    },
    "numInputRows" : 15,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 8.746355685131196,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:35:19.065 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on phamviethoa:45719 in memory (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:35:25.966 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:35:25.965Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 0,
    "triggerExecution" : 0
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 4,
        "0" : 11
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 4,
        "0" : 11
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 4,
        "0" : 11
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:35:34.006 [http-nio-8080-exec-1 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-14 08:35:34.006 [http-nio-8080-exec-1 INFO ] c.e.c.c.ClickstreamController - Received payload: {events=[{event_id=26979215-08df-40c3-8f6d-1253fa64eed1, event_name=page_view, event_time=2025-06-14T01:35:31.704Z, user_id=user_pej5ild, session_id=session_sv7246k, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/}, {event_id=e610598d-ac15-4dd0-9772-bdf78a0458a9, event_name=scroll, event_time=2025-06-14T01:35:33.106Z, user_id=user_pej5ild, session_id=session_sv7246k, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=1}, {event_id=f12e128a-7ddb-4171-b3ac-7887feaee7ec, event_name=click, event_time=2025-06-14T01:35:33.959Z, user_id=user_pej5ild, session_id=session_sv7246k, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 1
                        High-quality ite, element_type=div, element_name=null, track=product_click, productId=1}]}
2025-06-14 08:35:34.006 [http-nio-8080-exec-1 INFO ] c.e.c.c.ClickstreamController - Received 3 events
2025-06-14 08:35:34.008 [http-nio-8080-exec-1 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"26979215-08df-40c3-8f6d-1253fa64eed1","eventName":"page_view","eventTimestamp":"2025-06-14T01:35:31.704Z","userId":"user_pej5ild","sessionId":"session_sv7246k","appId":null,"platform":"web","pageUrl":"http://localhost/","eventParams":{"page_url":"http://localhost/","viewport_size":"1838x935","page_title":"E-Commerce Store","session_id":"session_sv7246k","language":"vi","screen_resolution":"1920x1080","platform":"web","event_id":"26979215-08df-40c3-8f6d-1253fa64eed1","user_id":"user_pej5ild","event_name":"page_view","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:35:31.704Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:35:34.012 [http-nio-8080-exec-1 INFO ] o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 1000
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-06-14 08:35:34.012 [http-nio-8080-exec-1 INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-14 08:35:34.016 [http-nio-8080-exec-1 INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-06-14 08:35:34.023 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-14 08:35:34.023 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-14 08:35:34.023 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749864934023
2025-06-14 08:35:34.027 [kafka-producer-network-thread | producer-1 INFO ] org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: HhNeO4joR2-K_kbMfLUuyQ
2025-06-14 08:35:34.027 [kafka-producer-network-thread | producer-1 INFO ] o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 1 with epoch 0
2025-06-14 08:35:34.038 [http-nio-8080-exec-1 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 1, offset: 5
2025-06-14 08:35:34.038 [http-nio-8080-exec-1 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"e610598d-ac15-4dd0-9772-bdf78a0458a9","eventName":"scroll","eventTimestamp":"2025-06-14T01:35:33.106Z","userId":"user_pej5ild","sessionId":"session_sv7246k","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","viewport_size":"1838x935","page_title":"E-Commerce Store","session_id":"session_sv7246k","language":"vi","screen_resolution":"1920x1080","platform":"web","scrollDepth":"1","event_id":"e610598d-ac15-4dd0-9772-bdf78a0458a9","user_id":"user_pej5ild","event_name":"scroll","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:35:33.106Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:35:34.040 [http-nio-8080-exec-1 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 0, offset: 12
2025-06-14 08:35:34.041 [http-nio-8080-exec-1 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"f12e128a-7ddb-4171-b3ac-7887feaee7ec","eventName":"click","eventTimestamp":"2025-06-14T01:35:33.959Z","userId":"user_pej5ild","sessionId":"session_sv7246k","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","productId":"1","element_text":"Product 1\n                        High-quality ite","viewport_size":"1838x935","page_title":"E-Commerce Store","session_id":"session_sv7246k","language":"vi","element_type":"div","screen_resolution":"1920x1080","element_class":"card product_card","platform":"web","event_id":"f12e128a-7ddb-4171-b3ac-7887feaee7ec","user_id":"user_pej5ild","event_name":"click","page_path":"/","page_referrer":"direct","track":"product_click","event_time":"2025-06-14T01:35:33.959Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:35:34.043 [http-nio-8080-exec-1 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 1, offset: 7
2025-06-14 08:35:34.043 [http-nio-8080-exec-1 INFO ] c.e.c.c.ClickstreamController - Successfully processed 3 events
2025-06-14 08:35:34.045 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/1 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.1.2d5dc372-7f63-4bc5-91ab-0d3f402539c4.tmp
2025-06-14 08:35:34.057 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.1.2d5dc372-7f63-4bc5-91ab-0d3f402539c4.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/1
2025-06-14 08:35:34.057 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1749864934042,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:35:34.071 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:34.072 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:34.079 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:34.080 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:34.115 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:35:34.116 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 1 (start at ClickstreamProcessor.java:66) with 2 output partitions
2025-06-14 08:35:34.116 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at ClickstreamProcessor.java:66)
2025-06-14 08:35:34.116 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:35:34.116 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:35:34.116 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[17] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:35:34.120 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 39.4 KiB, free 9.2 GiB)
2025-06-14 08:35:34.123 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 9.2 GiB)
2025-06-14 08:35:34.123 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:45719 (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:35:34.124 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:35:34.124 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[17] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0, 1))
2025-06-14 08:35:34.124 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 2 tasks resource profile 0
2025-06-14 08:35:34.125 [dispatcher-event-loop-14 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:35:34.125 [dispatcher-event-loop-14 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 1.0 (TID 3) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:35:34.125 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-14 08:35:34.125 [Executor task launch worker for task 1.0 in stage 1.0 (TID 3) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 1.0 (TID 3)
2025-06-14 08:35:34.134 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 4 for partition clickstream-events-1
2025-06-14 08:35:34.134 [Executor task launch worker for task 1.0 in stage 1.0 (TID 3) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 11 for partition clickstream-events-0
2025-06-14 08:35:34.137 [Executor task launch worker for task 1.0 in stage 1.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-14 08:35:34.137 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-14 08:35:34.638 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:34.638 [Executor task launch worker for task 1.0 in stage 1.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:34.639 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-14 08:35:34.639 [Executor task launch worker for task 1.0 in stage 1.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-14 08:35:34.639 [Executor task launch worker for task 1.0 in stage 1.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=13, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:34.639 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:34.648 [Executor task launch worker for task 1.0 in stage 1.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:34.648 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:34.654 [Executor task launch worker for task 1.0 in stage 1.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:34.654 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:34.654 [Executor task launch worker for task 1.0 in stage 1.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [a53ae43f-fe96-4963-a473-c297135d39c3] (2 queries & 0 savepoints) is committed.
2025-06-14 08:35:34.654 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [aea0407c-e280-418d-b67b-b50824bc2a23] (2 queries & 0 savepoints) is committed.
2025-06-14 08:35:34.654 [Executor task launch worker for task 1.0 in stage 1.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [d47f81de-aa3f-4bf7-9ba6-44e7a8d69b8a] (0 queries & 0 savepoints) is committed.
2025-06-14 08:35:34.654 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [21654719-6b89-40da-97d8-3bbc58352f28] (0 queries & 0 savepoints) is committed.
2025-06-14 08:35:34.655 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 1646 bytes result sent to driver
2025-06-14 08:35:34.655 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 531 ms on phamviethoa (executor driver) (1/2)
2025-06-14 08:35:34.656 [Executor task launch worker for task 1.0 in stage 1.0 (TID 3) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 1.0 (TID 3). 1689 bytes result sent to driver
2025-06-14 08:35:34.657 [task-result-getter-3 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 1.0 (TID 3) in 532 ms on phamviethoa (executor driver) (2/2)
2025-06-14 08:35:34.657 [task-result-getter-3 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-14 08:35:34.657 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at ClickstreamProcessor.java:66) finished in 0.540 s
2025-06-14 08:35:34.658 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:35:34.658 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-14 08:35:34.658 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.spark.scheduler.DAGScheduler - Job 1 finished: start at ClickstreamProcessor.java:66, took 0.542535 s
2025-06-14 08:35:34.668 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/1 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.1.19346ef2-5d50-43f7-98be-e06f31406664.tmp
2025-06-14 08:35:34.678 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.1.19346ef2-5d50-43f7-98be-e06f31406664.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/1
2025-06-14 08:35:34.678 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:35:34.041Z",
  "batchId" : 1,
  "numInputRows" : 5,
  "inputRowsPerSecond" : 416.6666666666667,
  "processedRowsPerSecond" : 7.849293563579278,
  "durationMs" : {
    "addBatch" : 592,
    "commitOffsets" : 13,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 14,
    "triggerExecution" : 637,
    "walCommit" : 16
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 4,
        "0" : 11
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 7,
        "0" : 13
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 7,
        "0" : 13
      }
    },
    "numInputRows" : 5,
    "inputRowsPerSecond" : 416.6666666666667,
    "processedRowsPerSecond" : 7.849293563579278,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:35:34.682 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/2 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.2.32dd28c3-e57c-4229-9eea-9e31d3a0ab70.tmp
2025-06-14 08:35:34.691 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.2.32dd28c3-e57c-4229-9eea-9e31d3a0ab70.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/2
2025-06-14 08:35:34.691 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1749864934679,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:35:34.700 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:34.700 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:34.707 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:34.708 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:34.749 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:35:34.750 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 2 (start at ClickstreamProcessor.java:66) with 1 output partitions
2025-06-14 08:35:34.750 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (start at ClickstreamProcessor.java:66)
2025-06-14 08:35:34.750 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:35:34.750 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:35:34.751 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[26] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:35:34.755 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 39.3 KiB, free 9.2 GiB)
2025-06-14 08:35:34.758 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 17.5 KiB, free 9.2 GiB)
2025-06-14 08:35:34.759 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on phamviethoa:45719 (size: 17.5 KiB, free: 9.2 GiB)
2025-06-14 08:35:34.759 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:35:34.759 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[26] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0))
2025-06-14 08:35:34.759 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks resource profile 0
2025-06-14 08:35:34.760 [dispatcher-event-loop-3 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 4) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:35:34.760 [Executor task launch worker for task 0.0 in stage 2.0 (TID 4) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 4)
2025-06-14 08:35:34.776 [Executor task launch worker for task 0.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:34.780 [Executor task launch worker for task 0.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:34.780 [Executor task launch worker for task 0.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [2ee8298c-0a02-4068-a964-6217e847afb0] (2 queries & 0 savepoints) is committed.
2025-06-14 08:35:34.780 [Executor task launch worker for task 0.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [97d77fa0-45f0-4036-ac28-29e39ed12993] (0 queries & 0 savepoints) is committed.
2025-06-14 08:35:34.781 [Executor task launch worker for task 0.0 in stage 2.0 (TID 4) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 4). 1646 bytes result sent to driver
2025-06-14 08:35:34.782 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 4) in 22 ms on phamviethoa (executor driver) (1/1)
2025-06-14 08:35:34.782 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2025-06-14 08:35:34.782 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 2 (start at ClickstreamProcessor.java:66) finished in 0.031 s
2025-06-14 08:35:34.782 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:35:34.782 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage finished
2025-06-14 08:35:34.782 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.spark.scheduler.DAGScheduler - Job 2 finished: start at ClickstreamProcessor.java:66, took 0.032570 s
2025-06-14 08:35:34.793 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/2 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.2.92b7a255-73dc-42c2-8b11-ec3081b4117d.tmp
2025-06-14 08:35:34.802 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.2.92b7a255-73dc-42c2-8b11-ec3081b4117d.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/2
2025-06-14 08:35:34.802 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:35:34.678Z",
  "batchId" : 2,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 1.5698587127158556,
  "processedRowsPerSecond" : 8.064516129032258,
  "durationMs" : {
    "addBatch" : 88,
    "commitOffsets" : 12,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 10,
    "triggerExecution" : 124,
    "walCommit" : 12
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 7,
        "0" : 13
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 8,
        "0" : 13
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 8,
        "0" : 13
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 1.5698587127158556,
    "processedRowsPerSecond" : 8.064516129032258,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:35:35.046 [http-nio-8080-exec-2 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-14 08:35:35.046 [http-nio-8080-exec-2 INFO ] c.e.c.c.ClickstreamController - Received payload: {events=[{event_id=86058ecb-10f1-471a-b117-9fa2067ab951, event_name=scroll, event_time=2025-06-14T01:35:34.106Z, user_id=user_pej5ild, session_id=session_sv7246k, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=100}, {event_id=e5f8ce4c-f072-45f2-ac7f-0b705719cd6f, event_name=click, event_time=2025-06-14T01:35:34.717Z, user_id=user_pej5ild, session_id=session_sv7246k, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 3
                        Customer favorit, element_type=div, element_name=null, track=product_click, productId=3}, {event_id=a6661486-b127-498e-a160-6015a91b23b0, event_name=click, event_time=2025-06-14T01:35:35.043Z, user_id=user_pej5ild, session_id=session_sv7246k, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 2
                        Durable and styl, element_type=div, element_name=null, track=product_click, productId=2}]}
2025-06-14 08:35:35.047 [http-nio-8080-exec-2 INFO ] c.e.c.c.ClickstreamController - Received 3 events
2025-06-14 08:35:35.047 [http-nio-8080-exec-2 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"86058ecb-10f1-471a-b117-9fa2067ab951","eventName":"scroll","eventTimestamp":"2025-06-14T01:35:34.106Z","userId":"user_pej5ild","sessionId":"session_sv7246k","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","viewport_size":"1838x935","page_title":"E-Commerce Store","session_id":"session_sv7246k","language":"vi","screen_resolution":"1920x1080","platform":"web","scrollDepth":"100","event_id":"86058ecb-10f1-471a-b117-9fa2067ab951","user_id":"user_pej5ild","event_name":"scroll","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:35:34.106Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:35:35.049 [http-nio-8080-exec-2 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 0, offset: 14
2025-06-14 08:35:35.050 [http-nio-8080-exec-2 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"e5f8ce4c-f072-45f2-ac7f-0b705719cd6f","eventName":"click","eventTimestamp":"2025-06-14T01:35:34.717Z","userId":"user_pej5ild","sessionId":"session_sv7246k","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","productId":"3","element_text":"Product 3\n                        Customer favorit","viewport_size":"1838x935","page_title":"E-Commerce Store","session_id":"session_sv7246k","language":"vi","element_type":"div","screen_resolution":"1920x1080","element_class":"card product_card","platform":"web","event_id":"e5f8ce4c-f072-45f2-ac7f-0b705719cd6f","user_id":"user_pej5ild","event_name":"click","page_path":"/","page_referrer":"direct","track":"product_click","event_time":"2025-06-14T01:35:34.717Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:35:35.052 [http-nio-8080-exec-2 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 0, offset: 16
2025-06-14 08:35:35.052 [http-nio-8080-exec-2 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"a6661486-b127-498e-a160-6015a91b23b0","eventName":"click","eventTimestamp":"2025-06-14T01:35:35.043Z","userId":"user_pej5ild","sessionId":"session_sv7246k","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","productId":"2","element_text":"Product 2\n                        Durable and styl","viewport_size":"1838x935","page_title":"E-Commerce Store","session_id":"session_sv7246k","language":"vi","element_type":"div","screen_resolution":"1920x1080","element_class":"card product_card","platform":"web","event_id":"a6661486-b127-498e-a160-6015a91b23b0","user_id":"user_pej5ild","event_name":"click","page_path":"/","page_referrer":"direct","track":"product_click","event_time":"2025-06-14T01:35:35.043Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:35:35.053 [http-nio-8080-exec-2 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 0, offset: 18
2025-06-14 08:35:35.053 [http-nio-8080-exec-2 INFO ] c.e.c.c.ClickstreamController - Successfully processed 3 events
2025-06-14 08:35:35.057 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/3 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.3.e79b2536-b19a-46bf-a082-90cb84911cbe.tmp
2025-06-14 08:35:35.066 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.3.e79b2536-b19a-46bf-a082-90cb84911cbe.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/3
2025-06-14 08:35:35.066 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1749864935054,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:35:35.075 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:35.075 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:35.082 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:35.082 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:35.116 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:35:35.116 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 3 (start at ClickstreamProcessor.java:66) with 1 output partitions
2025-06-14 08:35:35.116 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (start at ClickstreamProcessor.java:66)
2025-06-14 08:35:35.116 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:35:35.116 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:35:35.117 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[35] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:35:35.120 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 39.3 KiB, free 9.2 GiB)
2025-06-14 08:35:35.124 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 17.5 KiB, free 9.2 GiB)
2025-06-14 08:35:35.124 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on phamviethoa:45719 (size: 17.5 KiB, free: 9.2 GiB)
2025-06-14 08:35:35.124 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:35:35.124 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[35] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0))
2025-06-14 08:35:35.124 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks resource profile 0
2025-06-14 08:35:35.125 [dispatcher-event-loop-6 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 5) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:35:35.125 [Executor task launch worker for task 0.0 in stage 3.0 (TID 5) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 5)
2025-06-14 08:35:35.133 [Executor task launch worker for task 0.0 in stage 3.0 (TID 5) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 13 for partition clickstream-events-0
2025-06-14 08:35:35.134 [Executor task launch worker for task 0.0 in stage 3.0 (TID 5) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-14 08:35:35.636 [Executor task launch worker for task 0.0 in stage 3.0 (TID 5) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:35.636 [Executor task launch worker for task 0.0 in stage 3.0 (TID 5) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-14 08:35:35.637 [Executor task launch worker for task 0.0 in stage 3.0 (TID 5) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=19, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:35.644 [Executor task launch worker for task 0.0 in stage 3.0 (TID 5) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:35.650 [Executor task launch worker for task 0.0 in stage 3.0 (TID 5) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:35.650 [Executor task launch worker for task 0.0 in stage 3.0 (TID 5) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [b1212b3d-c271-4aa7-bd61-4658da9954e8] (2 queries & 0 savepoints) is committed.
2025-06-14 08:35:35.650 [Executor task launch worker for task 0.0 in stage 3.0 (TID 5) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [1c71944c-ced1-4fd5-b1d2-4d1f0f0d19e5] (0 queries & 0 savepoints) is committed.
2025-06-14 08:35:35.651 [Executor task launch worker for task 0.0 in stage 3.0 (TID 5) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 5). 1646 bytes result sent to driver
2025-06-14 08:35:35.651 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 5) in 526 ms on phamviethoa (executor driver) (1/1)
2025-06-14 08:35:35.651 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2025-06-14 08:35:35.651 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 3 (start at ClickstreamProcessor.java:66) finished in 0.534 s
2025-06-14 08:35:35.651 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:35:35.652 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 3: Stage finished
2025-06-14 08:35:35.652 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.spark.scheduler.DAGScheduler - Job 3 finished: start at ClickstreamProcessor.java:66, took 0.535802 s
2025-06-14 08:35:35.662 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/3 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.3.140d956e-f753-4cdc-962c-9e33c3e38589.tmp
2025-06-14 08:35:35.671 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.3.140d956e-f753-4cdc-962c-9e33c3e38589.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/3
2025-06-14 08:35:35.672 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:35:35.053Z",
  "batchId" : 3,
  "numInputRows" : 6,
  "inputRowsPerSecond" : 545.4545454545455,
  "processedRowsPerSecond" : 9.693053311793214,
  "durationMs" : {
    "addBatch" : 583,
    "commitOffsets" : 13,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 10,
    "triggerExecution" : 619,
    "walCommit" : 12
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 8,
        "0" : 13
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 8,
        "0" : 19
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 8,
        "0" : 19
      }
    },
    "numInputRows" : 6,
    "inputRowsPerSecond" : 545.4545454545455,
    "processedRowsPerSecond" : 9.693053311793214,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:35:36.041 [http-nio-8080-exec-3 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-14 08:35:36.041 [http-nio-8080-exec-3 INFO ] c.e.c.c.ClickstreamController - Received payload: {events=[{event_id=bd90333d-a986-4805-83a8-2bb0361eda46, event_name=click, event_time=2025-06-14T01:35:35.417Z, user_id=user_pej5ild, session_id=session_sv7246k, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 4
                        Reliable and aff, element_type=div, element_name=null, track=product_click, productId=4}, {event_id=28fde37a-4567-4e51-b3c7-42e923f27974, event_name=click, event_time=2025-06-14T01:35:35.716Z, user_id=user_pej5ild, session_id=session_sv7246k, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 5
                        Bestseller with , element_type=div, element_name=null, track=product_click, productId=5}, {event_id=65a73a6b-050b-4ea7-bb1f-497f3766557f, event_name=click, event_time=2025-06-14T01:35:36.039Z, user_id=user_pej5ild, session_id=session_sv7246k, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 6
                        Sleek design and, element_type=div, element_name=null, track=product_click, productId=6}]}
2025-06-14 08:35:36.041 [http-nio-8080-exec-3 INFO ] c.e.c.c.ClickstreamController - Received 3 events
2025-06-14 08:35:36.041 [http-nio-8080-exec-3 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"bd90333d-a986-4805-83a8-2bb0361eda46","eventName":"click","eventTimestamp":"2025-06-14T01:35:35.417Z","userId":"user_pej5ild","sessionId":"session_sv7246k","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","productId":"4","element_text":"Product 4\n                        Reliable and aff","viewport_size":"1838x935","page_title":"E-Commerce Store","session_id":"session_sv7246k","language":"vi","element_type":"div","screen_resolution":"1920x1080","element_class":"card product_card","platform":"web","event_id":"bd90333d-a986-4805-83a8-2bb0361eda46","user_id":"user_pej5ild","event_name":"click","page_path":"/","page_referrer":"direct","track":"product_click","event_time":"2025-06-14T01:35:35.417Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:35:36.043 [http-nio-8080-exec-3 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 1, offset: 9
2025-06-14 08:35:36.044 [http-nio-8080-exec-3 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"28fde37a-4567-4e51-b3c7-42e923f27974","eventName":"click","eventTimestamp":"2025-06-14T01:35:35.716Z","userId":"user_pej5ild","sessionId":"session_sv7246k","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","productId":"5","element_text":"Product 5\n                        Bestseller with ","viewport_size":"1838x935","page_title":"E-Commerce Store","session_id":"session_sv7246k","language":"vi","element_type":"div","screen_resolution":"1920x1080","element_class":"card product_card","platform":"web","event_id":"28fde37a-4567-4e51-b3c7-42e923f27974","user_id":"user_pej5ild","event_name":"click","page_path":"/","page_referrer":"direct","track":"product_click","event_time":"2025-06-14T01:35:35.716Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:35:36.045 [http-nio-8080-exec-3 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 1, offset: 11
2025-06-14 08:35:36.045 [http-nio-8080-exec-3 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"65a73a6b-050b-4ea7-bb1f-497f3766557f","eventName":"click","eventTimestamp":"2025-06-14T01:35:36.039Z","userId":"user_pej5ild","sessionId":"session_sv7246k","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","productId":"6","element_text":"Product 6\n                        Sleek design and","viewport_size":"1838x935","page_title":"E-Commerce Store","session_id":"session_sv7246k","language":"vi","element_type":"div","screen_resolution":"1920x1080","element_class":"card product_card","platform":"web","event_id":"65a73a6b-050b-4ea7-bb1f-497f3766557f","user_id":"user_pej5ild","event_name":"click","page_path":"/","page_referrer":"direct","track":"product_click","event_time":"2025-06-14T01:35:36.039Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:35:36.046 [http-nio-8080-exec-3 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 0, offset: 20
2025-06-14 08:35:36.046 [http-nio-8080-exec-3 INFO ] c.e.c.c.ClickstreamController - Successfully processed 3 events
2025-06-14 08:35:36.055 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/4 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.4.0c830454-5e34-447d-bb38-8a32d87c0a98.tmp
2025-06-14 08:35:36.064 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.4.0c830454-5e34-447d-bb38-8a32d87c0a98.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/4
2025-06-14 08:35:36.065 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1749864936052,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:35:36.072 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:36.072 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:36.078 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:36.078 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:36.109 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:35:36.110 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 4 (start at ClickstreamProcessor.java:66) with 2 output partitions
2025-06-14 08:35:36.110 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 4 (start at ClickstreamProcessor.java:66)
2025-06-14 08:35:36.110 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:35:36.110 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:35:36.110 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 4 (MapPartitionsRDD[44] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:35:36.114 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 39.4 KiB, free 9.2 GiB)
2025-06-14 08:35:36.117 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 9.2 GiB)
2025-06-14 08:35:36.117 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on phamviethoa:45719 (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:35:36.117 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:35:36.118 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[44] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0, 1))
2025-06-14 08:35:36.118 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 2 tasks resource profile 0
2025-06-14 08:35:36.118 [dispatcher-event-loop-8 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 6) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:35:36.118 [dispatcher-event-loop-8 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 4.0 (TID 7) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:35:36.119 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 6)
2025-06-14 08:35:36.119 [Executor task launch worker for task 1.0 in stage 4.0 (TID 7) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 4.0 (TID 7)
2025-06-14 08:35:36.126 [Executor task launch worker for task 1.0 in stage 4.0 (TID 7) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 19 for partition clickstream-events-0
2025-06-14 08:35:36.126 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 8 for partition clickstream-events-1
2025-06-14 08:35:36.128 [Executor task launch worker for task 1.0 in stage 4.0 (TID 7) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-14 08:35:36.128 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-14 08:35:36.629 [Executor task launch worker for task 1.0 in stage 4.0 (TID 7) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:36.629 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:36.629 [Executor task launch worker for task 1.0 in stage 4.0 (TID 7) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-14 08:35:36.629 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-14 08:35:36.630 [Executor task launch worker for task 1.0 in stage 4.0 (TID 7) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=21, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:36.630 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=12, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:36.637 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:36.638 [Executor task launch worker for task 1.0 in stage 4.0 (TID 7) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:36.644 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:36.644 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [252360e4-7879-4f36-91c3-2506db68e17c] (2 queries & 0 savepoints) is committed.
2025-06-14 08:35:36.644 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [e137eb32-e33f-4b73-95bb-42cb9f4aaa23] (0 queries & 0 savepoints) is committed.
2025-06-14 08:35:36.644 [Executor task launch worker for task 1.0 in stage 4.0 (TID 7) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:36.644 [Executor task launch worker for task 1.0 in stage 4.0 (TID 7) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [26c63eb0-5e0b-4915-ab77-6c82d7639061] (2 queries & 0 savepoints) is committed.
2025-06-14 08:35:36.644 [Executor task launch worker for task 1.0 in stage 4.0 (TID 7) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [b178c393-4441-428f-af13-5ebf566aa94f] (0 queries & 0 savepoints) is committed.
2025-06-14 08:35:36.645 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 6). 1646 bytes result sent to driver
2025-06-14 08:35:36.645 [Executor task launch worker for task 1.0 in stage 4.0 (TID 7) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 4.0 (TID 7). 1646 bytes result sent to driver
2025-06-14 08:35:36.645 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 6) in 527 ms on phamviethoa (executor driver) (1/2)
2025-06-14 08:35:36.647 [task-result-getter-3 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 4.0 (TID 7) in 529 ms on phamviethoa (executor driver) (2/2)
2025-06-14 08:35:36.647 [task-result-getter-3 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2025-06-14 08:35:36.647 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 4 (start at ClickstreamProcessor.java:66) finished in 0.536 s
2025-06-14 08:35:36.647 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:35:36.647 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 4: Stage finished
2025-06-14 08:35:36.648 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.spark.scheduler.DAGScheduler - Job 4 finished: start at ClickstreamProcessor.java:66, took 0.538210 s
2025-06-14 08:35:36.660 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/4 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.4.994bcb20-0096-4be8-acaa-efd553e6d82a.tmp
2025-06-14 08:35:36.671 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.4.994bcb20-0096-4be8-acaa-efd553e6d82a.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/4
2025-06-14 08:35:36.671 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:35:36.051Z",
  "batchId" : 4,
  "numInputRows" : 6,
  "inputRowsPerSecond" : 545.4545454545455,
  "processedRowsPerSecond" : 9.67741935483871,
  "durationMs" : {
    "addBatch" : 584,
    "commitOffsets" : 14,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 8,
    "triggerExecution" : 620,
    "walCommit" : 13
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 8,
        "0" : 19
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 12,
        "0" : 21
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 12,
        "0" : 21
      }
    },
    "numInputRows" : 6,
    "inputRowsPerSecond" : 545.4545454545455,
    "processedRowsPerSecond" : 9.67741935483871,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:35:37.747 [http-nio-8080-exec-4 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-14 08:35:37.747 [http-nio-8080-exec-4 INFO ] c.e.c.c.ClickstreamController - Received payload: {events=[{event_id=1ca83c96-2605-4360-81b5-8fe7985b7123, event_name=click, event_time=2025-06-14T01:35:36.304Z, user_id=user_pej5ild, session_id=session_sv7246k, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 9
                        Top-rated with e, element_type=div, element_name=null, track=product_click, productId=9}, {event_id=80a93629-22e8-45b3-9e31-bc2395c4d6b8, event_name=click, event_time=2025-06-14T01:35:36.567Z, user_id=user_pej5ild, session_id=session_sv7246k, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 8
                        Eco-friendly and, element_type=div, element_name=null, track=product_click, productId=8}, {event_id=c5d4961f-100a-4af4-ac7d-88a0b65d6e7f, event_name=scroll, event_time=2025-06-14T01:35:37.745Z, user_id=user_pej5ild, session_id=session_sv7246k, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=99}]}
2025-06-14 08:35:37.747 [http-nio-8080-exec-4 INFO ] c.e.c.c.ClickstreamController - Received 3 events
2025-06-14 08:35:37.748 [http-nio-8080-exec-4 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"1ca83c96-2605-4360-81b5-8fe7985b7123","eventName":"click","eventTimestamp":"2025-06-14T01:35:36.304Z","userId":"user_pej5ild","sessionId":"session_sv7246k","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","productId":"9","element_text":"Product 9\n                        Top-rated with e","viewport_size":"1838x935","page_title":"E-Commerce Store","session_id":"session_sv7246k","language":"vi","element_type":"div","screen_resolution":"1920x1080","element_class":"card product_card","platform":"web","event_id":"1ca83c96-2605-4360-81b5-8fe7985b7123","user_id":"user_pej5ild","event_name":"click","page_path":"/","page_referrer":"direct","track":"product_click","event_time":"2025-06-14T01:35:36.304Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:35:37.750 [http-nio-8080-exec-4 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 0, offset: 22
2025-06-14 08:35:37.750 [http-nio-8080-exec-4 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"80a93629-22e8-45b3-9e31-bc2395c4d6b8","eventName":"click","eventTimestamp":"2025-06-14T01:35:36.567Z","userId":"user_pej5ild","sessionId":"session_sv7246k","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","productId":"8","element_text":"Product 8\n                        Eco-friendly and","viewport_size":"1838x935","page_title":"E-Commerce Store","session_id":"session_sv7246k","language":"vi","element_type":"div","screen_resolution":"1920x1080","element_class":"card product_card","platform":"web","event_id":"80a93629-22e8-45b3-9e31-bc2395c4d6b8","user_id":"user_pej5ild","event_name":"click","page_path":"/","page_referrer":"direct","track":"product_click","event_time":"2025-06-14T01:35:36.567Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:35:37.752 [http-nio-8080-exec-4 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 1, offset: 13
2025-06-14 08:35:37.752 [http-nio-8080-exec-4 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"c5d4961f-100a-4af4-ac7d-88a0b65d6e7f","eventName":"scroll","eventTimestamp":"2025-06-14T01:35:37.745Z","userId":"user_pej5ild","sessionId":"session_sv7246k","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","viewport_size":"1838x935","page_title":"E-Commerce Store","session_id":"session_sv7246k","language":"vi","screen_resolution":"1920x1080","platform":"web","scrollDepth":"99","event_id":"c5d4961f-100a-4af4-ac7d-88a0b65d6e7f","user_id":"user_pej5ild","event_name":"scroll","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:35:37.745Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:35:37.754 [http-nio-8080-exec-4 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 0, offset: 24
2025-06-14 08:35:37.754 [http-nio-8080-exec-4 INFO ] c.e.c.c.ClickstreamController - Successfully processed 3 events
2025-06-14 08:35:37.754 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/5 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.5.9f32bc67-d7b3-4eee-82b2-0f0fda9ceab1.tmp
2025-06-14 08:35:37.764 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.5.9f32bc67-d7b3-4eee-82b2-0f0fda9ceab1.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/5
2025-06-14 08:35:37.764 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1749864937752,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:35:37.771 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:37.772 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:37.777 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:37.777 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:37.807 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:35:37.808 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 5 (start at ClickstreamProcessor.java:66) with 2 output partitions
2025-06-14 08:35:37.808 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (start at ClickstreamProcessor.java:66)
2025-06-14 08:35:37.808 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:35:37.808 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:35:37.809 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[53] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:35:37.812 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 39.4 KiB, free 9.2 GiB)
2025-06-14 08:35:37.815 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 9.2 GiB)
2025-06-14 08:35:37.816 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on phamviethoa:45719 (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:35:37.816 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:35:37.816 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[53] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0, 1))
2025-06-14 08:35:37.816 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 2 tasks resource profile 0
2025-06-14 08:35:37.816 [dispatcher-event-loop-14 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 8) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:35:37.817 [dispatcher-event-loop-14 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 5.0 (TID 9) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:35:37.817 [Executor task launch worker for task 0.0 in stage 5.0 (TID 8) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 8)
2025-06-14 08:35:37.817 [Executor task launch worker for task 1.0 in stage 5.0 (TID 9) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 5.0 (TID 9)
2025-06-14 08:35:37.824 [Executor task launch worker for task 0.0 in stage 5.0 (TID 8) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 12 for partition clickstream-events-1
2025-06-14 08:35:37.826 [Executor task launch worker for task 0.0 in stage 5.0 (TID 8) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-14 08:35:37.826 [Executor task launch worker for task 1.0 in stage 5.0 (TID 9) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 21 for partition clickstream-events-0
2025-06-14 08:35:37.827 [Executor task launch worker for task 1.0 in stage 5.0 (TID 9) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-14 08:35:38.327 [Executor task launch worker for task 0.0 in stage 5.0 (TID 8) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:38.327 [Executor task launch worker for task 0.0 in stage 5.0 (TID 8) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-14 08:35:38.327 [Executor task launch worker for task 0.0 in stage 5.0 (TID 8) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=14, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:38.328 [Executor task launch worker for task 1.0 in stage 5.0 (TID 9) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:38.328 [Executor task launch worker for task 1.0 in stage 5.0 (TID 9) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-14 08:35:38.328 [Executor task launch worker for task 1.0 in stage 5.0 (TID 9) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=25, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:38.334 [Executor task launch worker for task 0.0 in stage 5.0 (TID 8) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:38.336 [Executor task launch worker for task 1.0 in stage 5.0 (TID 9) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:38.339 [Executor task launch worker for task 0.0 in stage 5.0 (TID 8) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:38.339 [Executor task launch worker for task 0.0 in stage 5.0 (TID 8) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [5700382b-e5a3-45d1-8c04-82bee3144c11] (2 queries & 0 savepoints) is committed.
2025-06-14 08:35:38.339 [Executor task launch worker for task 0.0 in stage 5.0 (TID 8) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [9652314d-89af-43d1-bd10-f3bd1e179525] (0 queries & 0 savepoints) is committed.
2025-06-14 08:35:38.340 [Executor task launch worker for task 0.0 in stage 5.0 (TID 8) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 8). 1646 bytes result sent to driver
2025-06-14 08:35:38.340 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 8) in 524 ms on phamviethoa (executor driver) (1/2)
2025-06-14 08:35:38.342 [Executor task launch worker for task 1.0 in stage 5.0 (TID 9) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:38.342 [Executor task launch worker for task 1.0 in stage 5.0 (TID 9) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [ec9f311a-d411-4c28-98f3-951e4d36d7d5] (2 queries & 0 savepoints) is committed.
2025-06-14 08:35:38.342 [Executor task launch worker for task 1.0 in stage 5.0 (TID 9) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [60f67f64-29b8-49cb-a129-18839e623b45] (0 queries & 0 savepoints) is committed.
2025-06-14 08:35:38.344 [Executor task launch worker for task 1.0 in stage 5.0 (TID 9) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 5.0 (TID 9). 1689 bytes result sent to driver
2025-06-14 08:35:38.345 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 5.0 (TID 9) in 529 ms on phamviethoa (executor driver) (2/2)
2025-06-14 08:35:38.345 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2025-06-14 08:35:38.345 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 5 (start at ClickstreamProcessor.java:66) finished in 0.536 s
2025-06-14 08:35:38.345 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:35:38.345 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 5: Stage finished
2025-06-14 08:35:38.345 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.spark.scheduler.DAGScheduler - Job 5 finished: start at ClickstreamProcessor.java:66, took 0.537733 s
2025-06-14 08:35:38.356 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/5 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.5.21045b4e-7270-4702-9ced-f4639f183f3c.tmp
2025-06-14 08:35:38.366 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.5.21045b4e-7270-4702-9ced-f4639f183f3c.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/5
2025-06-14 08:35:38.367 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:35:37.751Z",
  "batchId" : 5,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 272.72727272727275,
  "processedRowsPerSecond" : 4.878048780487805,
  "durationMs" : {
    "addBatch" : 580,
    "commitOffsets" : 13,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 7,
    "triggerExecution" : 615,
    "walCommit" : 12
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 12,
        "0" : 21
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 13,
        "0" : 23
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 13,
        "0" : 23
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 272.72727272727275,
    "processedRowsPerSecond" : 4.878048780487805,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:35:38.370 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/6 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.6.d1a0b580-2c11-48c7-a42e-836d059cf194.tmp
2025-06-14 08:35:38.379 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.6.d1a0b580-2c11-48c7-a42e-836d059cf194.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/6
2025-06-14 08:35:38.379 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1749864938368,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:35:38.386 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:38.386 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:38.392 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:38.392 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:38.423 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:35:38.423 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 6 (start at ClickstreamProcessor.java:66) with 2 output partitions
2025-06-14 08:35:38.423 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (start at ClickstreamProcessor.java:66)
2025-06-14 08:35:38.423 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:35:38.423 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:35:38.424 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (MapPartitionsRDD[62] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:35:38.426 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 39.4 KiB, free 9.2 GiB)
2025-06-14 08:35:38.429 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 9.2 GiB)
2025-06-14 08:35:38.429 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on phamviethoa:45719 (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:35:38.430 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:35:38.430 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 6 (MapPartitionsRDD[62] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0, 1))
2025-06-14 08:35:38.430 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 2 tasks resource profile 0
2025-06-14 08:35:38.430 [dispatcher-event-loop-3 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 10) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:35:38.430 [dispatcher-event-loop-3 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 6.0 (TID 11) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:35:38.431 [Executor task launch worker for task 0.0 in stage 6.0 (TID 10) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 10)
2025-06-14 08:35:38.431 [Executor task launch worker for task 1.0 in stage 6.0 (TID 11) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 6.0 (TID 11)
2025-06-14 08:35:38.444 [Executor task launch worker for task 0.0 in stage 6.0 (TID 10) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:38.444 [Executor task launch worker for task 1.0 in stage 6.0 (TID 11) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:38.449 [Executor task launch worker for task 0.0 in stage 6.0 (TID 10) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:38.449 [Executor task launch worker for task 0.0 in stage 6.0 (TID 10) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [588b7d82-670a-4d82-ac02-0ab08676c8ab] (2 queries & 0 savepoints) is committed.
2025-06-14 08:35:38.449 [Executor task launch worker for task 0.0 in stage 6.0 (TID 10) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [ccd83b4c-cad0-4adb-9c9f-332beaf27b6c] (0 queries & 0 savepoints) is committed.
2025-06-14 08:35:38.449 [Executor task launch worker for task 1.0 in stage 6.0 (TID 11) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:38.449 [Executor task launch worker for task 1.0 in stage 6.0 (TID 11) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [87690d88-42ae-41b1-8d54-f0a1ba8955f5] (2 queries & 0 savepoints) is committed.
2025-06-14 08:35:38.449 [Executor task launch worker for task 1.0 in stage 6.0 (TID 11) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [fc16ddf7-4614-442f-a931-2e86b73b8540] (0 queries & 0 savepoints) is committed.
2025-06-14 08:35:38.450 [Executor task launch worker for task 1.0 in stage 6.0 (TID 11) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 6.0 (TID 11). 1646 bytes result sent to driver
2025-06-14 08:35:38.450 [Executor task launch worker for task 0.0 in stage 6.0 (TID 10) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 10). 1646 bytes result sent to driver
2025-06-14 08:35:38.450 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 6.0 (TID 11) in 20 ms on phamviethoa (executor driver) (1/2)
2025-06-14 08:35:38.452 [task-result-getter-3 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 10) in 22 ms on phamviethoa (executor driver) (2/2)
2025-06-14 08:35:38.452 [task-result-getter-3 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
2025-06-14 08:35:38.452 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 6 (start at ClickstreamProcessor.java:66) finished in 0.028 s
2025-06-14 08:35:38.452 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:35:38.452 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 6: Stage finished
2025-06-14 08:35:38.452 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.spark.scheduler.DAGScheduler - Job 6 finished: start at ClickstreamProcessor.java:66, took 0.029137 s
2025-06-14 08:35:38.464 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/6 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.6.7d6b0df5-f56c-496a-bc2f-99d5fb614877.tmp
2025-06-14 08:35:38.473 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.6.7d6b0df5-f56c-496a-bc2f-99d5fb614877.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/6
2025-06-14 08:35:38.474 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:35:38.367Z",
  "batchId" : 6,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 4.87012987012987,
  "processedRowsPerSecond" : 28.30188679245283,
  "durationMs" : {
    "addBatch" : 74,
    "commitOffsets" : 12,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 7,
    "triggerExecution" : 106,
    "walCommit" : 12
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 13,
        "0" : 23
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 14,
        "0" : 25
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 14,
        "0" : 25
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 4.87012987012987,
    "processedRowsPerSecond" : 28.30188679245283,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:35:40.748 [http-nio-8080-exec-5 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-14 08:35:40.748 [http-nio-8080-exec-5 INFO ] c.e.c.c.ClickstreamController - Received payload: {events=[{event_id=1068a751-fc4f-42fc-bcab-3957ec7b58b7, event_name=scroll, event_time=2025-06-14T01:35:38.745Z, user_id=user_pej5ild, session_id=session_sv7246k, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=23}, {event_id=42f6a8e3-2228-416c-96f9-e783083e169c, event_name=scroll, event_time=2025-06-14T01:35:39.745Z, user_id=user_pej5ild, session_id=session_sv7246k, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=9}, {event_id=21cfe978-7926-4b75-b03a-9c652f4b5075, event_name=scroll, event_time=2025-06-14T01:35:40.745Z, user_id=user_pej5ild, session_id=session_sv7246k, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=77}]}
2025-06-14 08:35:40.748 [http-nio-8080-exec-5 INFO ] c.e.c.c.ClickstreamController - Received 3 events
2025-06-14 08:35:40.748 [http-nio-8080-exec-5 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"1068a751-fc4f-42fc-bcab-3957ec7b58b7","eventName":"scroll","eventTimestamp":"2025-06-14T01:35:38.745Z","userId":"user_pej5ild","sessionId":"session_sv7246k","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","viewport_size":"1838x935","page_title":"E-Commerce Store","session_id":"session_sv7246k","language":"vi","screen_resolution":"1920x1080","platform":"web","scrollDepth":"23","event_id":"1068a751-fc4f-42fc-bcab-3957ec7b58b7","user_id":"user_pej5ild","event_name":"scroll","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:35:38.745Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:35:40.750 [http-nio-8080-exec-5 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 0, offset: 26
2025-06-14 08:35:40.750 [http-nio-8080-exec-5 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"42f6a8e3-2228-416c-96f9-e783083e169c","eventName":"scroll","eventTimestamp":"2025-06-14T01:35:39.745Z","userId":"user_pej5ild","sessionId":"session_sv7246k","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","viewport_size":"1838x935","page_title":"E-Commerce Store","session_id":"session_sv7246k","language":"vi","screen_resolution":"1920x1080","platform":"web","scrollDepth":"9","event_id":"42f6a8e3-2228-416c-96f9-e783083e169c","user_id":"user_pej5ild","event_name":"scroll","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:35:39.745Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:35:40.752 [http-nio-8080-exec-5 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 0, offset: 28
2025-06-14 08:35:40.752 [http-nio-8080-exec-5 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"21cfe978-7926-4b75-b03a-9c652f4b5075","eventName":"scroll","eventTimestamp":"2025-06-14T01:35:40.745Z","userId":"user_pej5ild","sessionId":"session_sv7246k","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","viewport_size":"1838x935","page_title":"E-Commerce Store","session_id":"session_sv7246k","language":"vi","screen_resolution":"1920x1080","platform":"web","scrollDepth":"77","event_id":"21cfe978-7926-4b75-b03a-9c652f4b5075","user_id":"user_pej5ild","event_name":"scroll","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:35:40.745Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:35:40.753 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/7 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.7.6f05f1a5-3923-4f2b-96b6-b262cf633f28.tmp
2025-06-14 08:35:40.754 [http-nio-8080-exec-5 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 1, offset: 15
2025-06-14 08:35:40.754 [http-nio-8080-exec-5 INFO ] c.e.c.c.ClickstreamController - Successfully processed 3 events
2025-06-14 08:35:40.763 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.7.6f05f1a5-3923-4f2b-96b6-b262cf633f28.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/7
2025-06-14 08:35:40.763 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 7. Metadata OffsetSeqMetadata(0,1749864940750,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:35:40.770 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:40.770 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:40.775 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:40.776 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:40.805 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:35:40.805 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 7 (start at ClickstreamProcessor.java:66) with 1 output partitions
2025-06-14 08:35:40.805 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (start at ClickstreamProcessor.java:66)
2025-06-14 08:35:40.805 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:35:40.805 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:35:40.806 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (MapPartitionsRDD[71] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:35:40.808 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 39.3 KiB, free 9.2 GiB)
2025-06-14 08:35:40.811 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 17.5 KiB, free 9.2 GiB)
2025-06-14 08:35:40.811 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on phamviethoa:45719 (size: 17.5 KiB, free: 9.2 GiB)
2025-06-14 08:35:40.812 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:35:40.812 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[71] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0))
2025-06-14 08:35:40.812 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 1 tasks resource profile 0
2025-06-14 08:35:40.812 [dispatcher-event-loop-9 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 12) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:35:40.813 [Executor task launch worker for task 0.0 in stage 7.0 (TID 12) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 12)
2025-06-14 08:35:40.820 [Executor task launch worker for task 0.0 in stage 7.0 (TID 12) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 25 for partition clickstream-events-0
2025-06-14 08:35:40.821 [Executor task launch worker for task 0.0 in stage 7.0 (TID 12) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-14 08:35:41.322 [Executor task launch worker for task 0.0 in stage 7.0 (TID 12) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:41.322 [Executor task launch worker for task 0.0 in stage 7.0 (TID 12) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-14 08:35:41.322 [Executor task launch worker for task 0.0 in stage 7.0 (TID 12) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=29, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:41.329 [Executor task launch worker for task 0.0 in stage 7.0 (TID 12) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:41.334 [Executor task launch worker for task 0.0 in stage 7.0 (TID 12) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:41.334 [Executor task launch worker for task 0.0 in stage 7.0 (TID 12) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [1eb772f4-a487-4471-8b58-2de0b2e98020] (2 queries & 0 savepoints) is committed.
2025-06-14 08:35:41.334 [Executor task launch worker for task 0.0 in stage 7.0 (TID 12) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [b40279ef-ab5d-453e-8b3c-e02f2dee4452] (0 queries & 0 savepoints) is committed.
2025-06-14 08:35:41.335 [Executor task launch worker for task 0.0 in stage 7.0 (TID 12) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 12). 1646 bytes result sent to driver
2025-06-14 08:35:41.335 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 12) in 523 ms on phamviethoa (executor driver) (1/1)
2025-06-14 08:35:41.335 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2025-06-14 08:35:41.335 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 7 (start at ClickstreamProcessor.java:66) finished in 0.529 s
2025-06-14 08:35:41.335 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:35:41.335 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 7: Stage finished
2025-06-14 08:35:41.335 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.spark.scheduler.DAGScheduler - Job 7 finished: start at ClickstreamProcessor.java:66, took 0.530600 s
2025-06-14 08:35:41.347 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/7 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.7.aa1bd750-ac8c-4a9e-bf9e-ba0f19c36b06.tmp
2025-06-14 08:35:41.356 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.7.aa1bd750-ac8c-4a9e-bf9e-ba0f19c36b06.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/7
2025-06-14 08:35:41.357 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:35:40.750Z",
  "batchId" : 7,
  "numInputRows" : 2,
  "inputRowsPerSecond" : 181.81818181818184,
  "processedRowsPerSecond" : 3.3003300330033003,
  "durationMs" : {
    "addBatch" : 573,
    "commitOffsets" : 12,
    "getBatch" : 0,
    "latestOffset" : 0,
    "queryPlanning" : 7,
    "triggerExecution" : 606,
    "walCommit" : 14
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 14,
        "0" : 25
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 14,
        "0" : 27
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 14,
        "0" : 27
      }
    },
    "numInputRows" : 2,
    "inputRowsPerSecond" : 181.81818181818184,
    "processedRowsPerSecond" : 3.3003300330033003,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:35:41.360 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/8 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.8.d2fa831f-d7bd-49fb-a321-dfc2fb68077e.tmp
2025-06-14 08:35:41.370 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.8.d2fa831f-d7bd-49fb-a321-dfc2fb68077e.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/8
2025-06-14 08:35:41.370 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 8. Metadata OffsetSeqMetadata(0,1749864941358,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:35:41.377 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:41.378 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:41.382 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:41.382 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:41.411 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:35:41.411 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 8 (start at ClickstreamProcessor.java:66) with 2 output partitions
2025-06-14 08:35:41.411 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 (start at ClickstreamProcessor.java:66)
2025-06-14 08:35:41.411 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:35:41.411 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:35:41.412 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 8 (MapPartitionsRDD[80] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:35:41.414 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 39.4 KiB, free 9.2 GiB)
2025-06-14 08:35:41.417 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 9.2 GiB)
2025-06-14 08:35:41.417 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on phamviethoa:45719 (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:35:41.417 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:35:41.417 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 8 (MapPartitionsRDD[80] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0, 1))
2025-06-14 08:35:41.417 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 2 tasks resource profile 0
2025-06-14 08:35:41.418 [dispatcher-event-loop-11 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 13) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:35:41.418 [dispatcher-event-loop-11 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 8.0 (TID 14) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:35:41.418 [Executor task launch worker for task 1.0 in stage 8.0 (TID 14) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 8.0 (TID 14)
2025-06-14 08:35:41.418 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 13)
2025-06-14 08:35:41.426 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 14 for partition clickstream-events-1
2025-06-14 08:35:41.427 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-14 08:35:41.433 [Executor task launch worker for task 1.0 in stage 8.0 (TID 14) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:41.438 [Executor task launch worker for task 1.0 in stage 8.0 (TID 14) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:41.438 [Executor task launch worker for task 1.0 in stage 8.0 (TID 14) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [a38781b7-ead6-4d90-96e0-6c76ed7d574e] (2 queries & 0 savepoints) is committed.
2025-06-14 08:35:41.438 [Executor task launch worker for task 1.0 in stage 8.0 (TID 14) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [217be1ab-e206-4272-ac99-0ccaacaf9f91] (0 queries & 0 savepoints) is committed.
2025-06-14 08:35:41.439 [Executor task launch worker for task 1.0 in stage 8.0 (TID 14) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 8.0 (TID 14). 1646 bytes result sent to driver
2025-06-14 08:35:41.439 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 8.0 (TID 14) in 21 ms on phamviethoa (executor driver) (1/2)
2025-06-14 08:35:41.928 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:41.928 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-14 08:35:41.929 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=16, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:41.935 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:41.940 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:41.940 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [e1924180-e05c-44f3-b5ef-a6a738caceb6] (2 queries & 0 savepoints) is committed.
2025-06-14 08:35:41.940 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [7b67029a-8234-40c5-94cc-5aa3cfa364ea] (0 queries & 0 savepoints) is committed.
2025-06-14 08:35:41.942 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 13). 1689 bytes result sent to driver
2025-06-14 08:35:41.942 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 13) in 524 ms on phamviethoa (executor driver) (2/2)
2025-06-14 08:35:41.942 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool 
2025-06-14 08:35:41.942 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 8 (start at ClickstreamProcessor.java:66) finished in 0.530 s
2025-06-14 08:35:41.942 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:35:41.942 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 8: Stage finished
2025-06-14 08:35:41.943 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.spark.scheduler.DAGScheduler - Job 8 finished: start at ClickstreamProcessor.java:66, took 0.531526 s
2025-06-14 08:35:41.953 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/8 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.8.894b1037-0914-4160-854f-d00a66a069a6.tmp
2025-06-14 08:35:41.962 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.8.894b1037-0914-4160-854f-d00a66a069a6.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/8
2025-06-14 08:35:41.963 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:35:41.357Z",
  "batchId" : 8,
  "numInputRows" : 4,
  "inputRowsPerSecond" : 6.589785831960461,
  "processedRowsPerSecond" : 6.6115702479338845,
  "durationMs" : {
    "addBatch" : 572,
    "commitOffsets" : 12,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 7,
    "triggerExecution" : 605,
    "walCommit" : 12
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 14,
        "0" : 27
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 16,
        "0" : 29
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 16,
        "0" : 29
      }
    },
    "numInputRows" : 4,
    "inputRowsPerSecond" : 6.589785831960461,
    "processedRowsPerSecond" : 6.6115702479338845,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:35:46.751 [http-nio-8080-exec-6 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-14 08:35:46.751 [http-nio-8080-exec-6 INFO ] c.e.c.c.ClickstreamController - Received payload: {events=[{event_id=18d5c576-0167-46a4-8a6e-9aa6514fa5cf, event_name=scroll, event_time=2025-06-14T01:35:42.746Z, user_id=user_pej5ild, session_id=session_sv7246k, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=15}, {event_id=8644fe2d-b412-4acc-a8b5-2e2ce13c57d6, event_name=scroll, event_time=2025-06-14T01:35:43.747Z, user_id=user_pej5ild, session_id=session_sv7246k, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=27}, {event_id=5fb21d9c-c7b8-4124-92e0-bf83f5f12e23, event_name=scroll, event_time=2025-06-14T01:35:46.748Z, user_id=user_pej5ild, session_id=session_sv7246k, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=26}]}
2025-06-14 08:35:46.751 [http-nio-8080-exec-6 INFO ] c.e.c.c.ClickstreamController - Received 3 events
2025-06-14 08:35:46.751 [http-nio-8080-exec-6 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"18d5c576-0167-46a4-8a6e-9aa6514fa5cf","eventName":"scroll","eventTimestamp":"2025-06-14T01:35:42.746Z","userId":"user_pej5ild","sessionId":"session_sv7246k","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","viewport_size":"1838x935","page_title":"E-Commerce Store","session_id":"session_sv7246k","language":"vi","screen_resolution":"1920x1080","platform":"web","scrollDepth":"15","event_id":"18d5c576-0167-46a4-8a6e-9aa6514fa5cf","user_id":"user_pej5ild","event_name":"scroll","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:35:42.746Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:35:46.754 [http-nio-8080-exec-6 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 0, offset: 30
2025-06-14 08:35:46.754 [http-nio-8080-exec-6 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"8644fe2d-b412-4acc-a8b5-2e2ce13c57d6","eventName":"scroll","eventTimestamp":"2025-06-14T01:35:43.747Z","userId":"user_pej5ild","sessionId":"session_sv7246k","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","viewport_size":"1838x935","page_title":"E-Commerce Store","session_id":"session_sv7246k","language":"vi","screen_resolution":"1920x1080","platform":"web","scrollDepth":"27","event_id":"8644fe2d-b412-4acc-a8b5-2e2ce13c57d6","user_id":"user_pej5ild","event_name":"scroll","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:35:43.747Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:35:46.755 [http-nio-8080-exec-6 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 0, offset: 32
2025-06-14 08:35:46.755 [http-nio-8080-exec-6 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"5fb21d9c-c7b8-4124-92e0-bf83f5f12e23","eventName":"scroll","eventTimestamp":"2025-06-14T01:35:46.748Z","userId":"user_pej5ild","sessionId":"session_sv7246k","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","viewport_size":"1838x935","page_title":"E-Commerce Store","session_id":"session_sv7246k","language":"vi","screen_resolution":"1920x1080","platform":"web","scrollDepth":"26","event_id":"5fb21d9c-c7b8-4124-92e0-bf83f5f12e23","user_id":"user_pej5ild","event_name":"scroll","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:35:46.748Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:35:46.757 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/9 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.9.453d61ea-7e8f-4196-9ded-84d1a4c580c7.tmp
2025-06-14 08:35:46.757 [http-nio-8080-exec-6 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 1, offset: 17
2025-06-14 08:35:46.757 [http-nio-8080-exec-6 INFO ] c.e.c.c.ClickstreamController - Successfully processed 3 events
2025-06-14 08:35:46.767 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.9.453d61ea-7e8f-4196-9ded-84d1a4c580c7.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/9
2025-06-14 08:35:46.767 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 9. Metadata OffsetSeqMetadata(0,1749864946753,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:35:46.773 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:46.774 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:46.779 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:46.780 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:46.817 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:35:46.817 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 9 (start at ClickstreamProcessor.java:66) with 1 output partitions
2025-06-14 08:35:46.817 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 9 (start at ClickstreamProcessor.java:66)
2025-06-14 08:35:46.817 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:35:46.817 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:35:46.818 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 9 (MapPartitionsRDD[89] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:35:46.820 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 39.3 KiB, free 9.2 GiB)
2025-06-14 08:35:46.823 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 17.5 KiB, free 9.2 GiB)
2025-06-14 08:35:46.823 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on phamviethoa:45719 (size: 17.5 KiB, free: 9.2 GiB)
2025-06-14 08:35:46.823 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:35:46.824 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[89] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0))
2025-06-14 08:35:46.824 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 1 tasks resource profile 0
2025-06-14 08:35:46.824 [dispatcher-event-loop-1 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 15) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:35:46.825 [Executor task launch worker for task 0.0 in stage 9.0 (TID 15) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 15)
2025-06-14 08:35:46.833 [Executor task launch worker for task 0.0 in stage 9.0 (TID 15) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 29 for partition clickstream-events-0
2025-06-14 08:35:46.834 [Executor task launch worker for task 0.0 in stage 9.0 (TID 15) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-14 08:35:47.334 [Executor task launch worker for task 0.0 in stage 9.0 (TID 15) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:47.334 [Executor task launch worker for task 0.0 in stage 9.0 (TID 15) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-14 08:35:47.335 [Executor task launch worker for task 0.0 in stage 9.0 (TID 15) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=33, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:47.342 [Executor task launch worker for task 0.0 in stage 9.0 (TID 15) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:47.347 [Executor task launch worker for task 0.0 in stage 9.0 (TID 15) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:47.347 [Executor task launch worker for task 0.0 in stage 9.0 (TID 15) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [ab0851ac-72a9-4414-838e-6384eeeda4a5] (2 queries & 0 savepoints) is committed.
2025-06-14 08:35:47.347 [Executor task launch worker for task 0.0 in stage 9.0 (TID 15) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [b25ba133-f0a6-4124-807d-51272c98167d] (0 queries & 0 savepoints) is committed.
2025-06-14 08:35:47.348 [Executor task launch worker for task 0.0 in stage 9.0 (TID 15) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 15). 1646 bytes result sent to driver
2025-06-14 08:35:47.348 [task-result-getter-3 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 15) in 524 ms on phamviethoa (executor driver) (1/1)
2025-06-14 08:35:47.348 [task-result-getter-3 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
2025-06-14 08:35:47.348 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 9 (start at ClickstreamProcessor.java:66) finished in 0.530 s
2025-06-14 08:35:47.349 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:35:47.349 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 9: Stage finished
2025-06-14 08:35:47.349 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.spark.scheduler.DAGScheduler - Job 9 finished: start at ClickstreamProcessor.java:66, took 0.531692 s
2025-06-14 08:35:47.359 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/9 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.9.a3d4b057-dc25-451a-a417-e0c002b287b8.tmp
2025-06-14 08:35:47.368 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.9.a3d4b057-dc25-451a-a417-e0c002b287b8.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/9
2025-06-14 08:35:47.369 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:35:46.752Z",
  "batchId" : 9,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 90.90909090909092,
  "processedRowsPerSecond" : 1.6233766233766234,
  "durationMs" : {
    "addBatch" : 581,
    "commitOffsets" : 12,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 7,
    "triggerExecution" : 616,
    "walCommit" : 14
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 16,
        "0" : 29
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 16,
        "0" : 30
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 16,
        "0" : 30
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 90.90909090909092,
    "processedRowsPerSecond" : 1.6233766233766234,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:35:47.373 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/10 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.10.7a3009bc-6098-48c8-b6a5-34393786481e.tmp
2025-06-14 08:35:47.381 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.10.7a3009bc-6098-48c8-b6a5-34393786481e.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/10
2025-06-14 08:35:47.381 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 10. Metadata OffsetSeqMetadata(0,1749864947370,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:35:47.388 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:47.388 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:47.393 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:47.394 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:47.426 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:35:47.427 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 10 (start at ClickstreamProcessor.java:66) with 2 output partitions
2025-06-14 08:35:47.427 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 10 (start at ClickstreamProcessor.java:66)
2025-06-14 08:35:47.427 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:35:47.427 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:35:47.427 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 10 (MapPartitionsRDD[98] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:35:47.429 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 39.4 KiB, free 9.2 GiB)
2025-06-14 08:35:47.432 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 9.2 GiB)
2025-06-14 08:35:47.433 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on phamviethoa:45719 (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:35:47.433 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 10 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:35:47.433 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 10 (MapPartitionsRDD[98] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0, 1))
2025-06-14 08:35:47.433 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 10.0 with 2 tasks resource profile 0
2025-06-14 08:35:47.434 [dispatcher-event-loop-5 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 10.0 (TID 16) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:35:47.434 [dispatcher-event-loop-5 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 10.0 (TID 17) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:35:47.434 [Executor task launch worker for task 1.0 in stage 10.0 (TID 17) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 10.0 (TID 17)
2025-06-14 08:35:47.434 [Executor task launch worker for task 0.0 in stage 10.0 (TID 16) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 10.0 (TID 16)
2025-06-14 08:35:47.443 [Executor task launch worker for task 0.0 in stage 10.0 (TID 16) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 16 for partition clickstream-events-1
2025-06-14 08:35:47.445 [Executor task launch worker for task 0.0 in stage 10.0 (TID 16) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-14 08:35:47.450 [Executor task launch worker for task 1.0 in stage 10.0 (TID 17) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:47.454 [Executor task launch worker for task 1.0 in stage 10.0 (TID 17) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:47.454 [Executor task launch worker for task 1.0 in stage 10.0 (TID 17) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [a62c93b7-c998-4ef5-8cd6-89dbfba90308] (2 queries & 0 savepoints) is committed.
2025-06-14 08:35:47.454 [Executor task launch worker for task 1.0 in stage 10.0 (TID 17) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [47bc9bcf-659a-46ce-9adc-ad2cc3ec47bb] (0 queries & 0 savepoints) is committed.
2025-06-14 08:35:47.455 [Executor task launch worker for task 1.0 in stage 10.0 (TID 17) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 10.0 (TID 17). 1646 bytes result sent to driver
2025-06-14 08:35:47.456 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 10.0 (TID 17) in 22 ms on phamviethoa (executor driver) (1/2)
2025-06-14 08:35:47.947 [Executor task launch worker for task 0.0 in stage 10.0 (TID 16) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:47.947 [Executor task launch worker for task 0.0 in stage 10.0 (TID 16) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-14 08:35:47.948 [Executor task launch worker for task 0.0 in stage 10.0 (TID 16) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=18, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:47.955 [Executor task launch worker for task 0.0 in stage 10.0 (TID 16) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:47.959 [Executor task launch worker for task 0.0 in stage 10.0 (TID 16) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:47.959 [Executor task launch worker for task 0.0 in stage 10.0 (TID 16) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [df54bfb8-ba69-4510-ad74-7bc5934dd554] (2 queries & 0 savepoints) is committed.
2025-06-14 08:35:47.960 [Executor task launch worker for task 0.0 in stage 10.0 (TID 16) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [4575d21e-005a-4586-b7d2-78135f191cc2] (0 queries & 0 savepoints) is committed.
2025-06-14 08:35:47.962 [Executor task launch worker for task 0.0 in stage 10.0 (TID 16) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 10.0 (TID 16). 1689 bytes result sent to driver
2025-06-14 08:35:47.962 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 10.0 (TID 16) in 529 ms on phamviethoa (executor driver) (2/2)
2025-06-14 08:35:47.962 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 10.0, whose tasks have all completed, from pool 
2025-06-14 08:35:47.962 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 10 (start at ClickstreamProcessor.java:66) finished in 0.534 s
2025-06-14 08:35:47.962 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:35:47.962 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 10: Stage finished
2025-06-14 08:35:47.963 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.spark.scheduler.DAGScheduler - Job 10 finished: start at ClickstreamProcessor.java:66, took 0.535900 s
2025-06-14 08:35:47.972 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/10 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.10.7ad2075f-c169-4e08-96ef-c28a29092b22.tmp
2025-06-14 08:35:47.981 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.10.7ad2075f-c169-4e08-96ef-c28a29092b22.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/10
2025-06-14 08:35:47.982 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:35:47.369Z",
  "batchId" : 10,
  "numInputRows" : 5,
  "inputRowsPerSecond" : 8.103727714748784,
  "processedRowsPerSecond" : 8.169934640522875,
  "durationMs" : {
    "addBatch" : 581,
    "commitOffsets" : 11,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 7,
    "triggerExecution" : 612,
    "walCommit" : 12
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 16,
        "0" : 30
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 18,
        "0" : 33
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 18,
        "0" : 33
      }
    },
    "numInputRows" : 5,
    "inputRowsPerSecond" : 8.103727714748784,
    "processedRowsPerSecond" : 8.169934640522875,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:35:55.124 [http-nio-8080-exec-7 WARN ] o.s.w.s.m.s.DefaultHandlerExceptionResolver - Resolved [org.springframework.web.HttpMediaTypeNotSupportedException: Content type 'text/plain;charset=UTF-8' not supported]
2025-06-14 08:35:56.280 [http-nio-8080-exec-8 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-14 08:35:56.280 [http-nio-8080-exec-8 INFO ] c.e.c.c.ClickstreamController - Received payload: {events=[{event_id=92e7132d-88dd-4a2d-910a-3f9e68c19e6b, event_name=page_view, event_time=2025-06-14T01:35:55.141Z, user_id=user_pej5ild, session_id=session_mltrxqm, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/}, {event_id=cc00ab91-e9ae-452d-92d9-ea393bb40911, event_name=scroll, event_time=2025-06-14T01:35:55.274Z, user_id=user_pej5ild, session_id=session_mltrxqm, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=0}, {event_id=ce20bcde-c35d-4b7f-8337-1ecbf6578fc8, event_name=scroll, event_time=2025-06-14T01:35:56.276Z, user_id=user_pej5ild, session_id=session_mltrxqm, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=64}]}
2025-06-14 08:35:56.280 [http-nio-8080-exec-8 INFO ] c.e.c.c.ClickstreamController - Received 3 events
2025-06-14 08:35:56.280 [http-nio-8080-exec-8 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"92e7132d-88dd-4a2d-910a-3f9e68c19e6b","eventName":"page_view","eventTimestamp":"2025-06-14T01:35:55.141Z","userId":"user_pej5ild","sessionId":"session_mltrxqm","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","viewport_size":"659x935","page_title":"E-Commerce Store","session_id":"session_mltrxqm","language":"vi","screen_resolution":"1920x1080","platform":"web","event_id":"92e7132d-88dd-4a2d-910a-3f9e68c19e6b","user_id":"user_pej5ild","event_name":"page_view","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:35:55.141Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:35:56.283 [http-nio-8080-exec-8 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 1, offset: 19
2025-06-14 08:35:56.283 [http-nio-8080-exec-8 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"cc00ab91-e9ae-452d-92d9-ea393bb40911","eventName":"scroll","eventTimestamp":"2025-06-14T01:35:55.274Z","userId":"user_pej5ild","sessionId":"session_mltrxqm","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","viewport_size":"659x935","page_title":"E-Commerce Store","session_id":"session_mltrxqm","language":"vi","screen_resolution":"1920x1080","platform":"web","scrollDepth":"0","event_id":"cc00ab91-e9ae-452d-92d9-ea393bb40911","user_id":"user_pej5ild","event_name":"scroll","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:35:55.274Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:35:56.285 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/11 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.11.29253cd9-2afe-4715-89a2-b634a2ee6b0d.tmp
2025-06-14 08:35:56.286 [http-nio-8080-exec-8 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 1, offset: 21
2025-06-14 08:35:56.286 [http-nio-8080-exec-8 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"ce20bcde-c35d-4b7f-8337-1ecbf6578fc8","eventName":"scroll","eventTimestamp":"2025-06-14T01:35:56.276Z","userId":"user_pej5ild","sessionId":"session_mltrxqm","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","viewport_size":"659x935","page_title":"E-Commerce Store","session_id":"session_mltrxqm","language":"vi","screen_resolution":"1920x1080","platform":"web","scrollDepth":"64","event_id":"ce20bcde-c35d-4b7f-8337-1ecbf6578fc8","user_id":"user_pej5ild","event_name":"scroll","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:35:56.276Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:35:56.288 [http-nio-8080-exec-8 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 1, offset: 23
2025-06-14 08:35:56.288 [http-nio-8080-exec-8 INFO ] c.e.c.c.ClickstreamController - Successfully processed 3 events
2025-06-14 08:35:56.296 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.11.29253cd9-2afe-4715-89a2-b634a2ee6b0d.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/11
2025-06-14 08:35:56.296 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 11. Metadata OffsetSeqMetadata(0,1749864956281,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:35:56.303 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:56.304 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:56.310 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:56.310 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:56.342 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:35:56.343 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 11 (start at ClickstreamProcessor.java:66) with 1 output partitions
2025-06-14 08:35:56.343 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (start at ClickstreamProcessor.java:66)
2025-06-14 08:35:56.343 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:35:56.343 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:35:56.344 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[107] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:35:56.346 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 9.2 GiB)
2025-06-14 08:35:56.350 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_11_piece0 stored as bytes in memory (estimated size 17.5 KiB, free 9.2 GiB)
2025-06-14 08:35:56.350 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on phamviethoa:45719 (size: 17.5 KiB, free: 9.2 GiB)
2025-06-14 08:35:56.351 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 11 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:35:56.351 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[107] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0))
2025-06-14 08:35:56.351 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 1 tasks resource profile 0
2025-06-14 08:35:56.351 [dispatcher-event-loop-10 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 18) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:35:56.352 [Executor task launch worker for task 0.0 in stage 11.0 (TID 18) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 18)
2025-06-14 08:35:56.360 [Executor task launch worker for task 0.0 in stage 11.0 (TID 18) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 18 for partition clickstream-events-1
2025-06-14 08:35:56.361 [Executor task launch worker for task 0.0 in stage 11.0 (TID 18) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-14 08:35:56.863 [Executor task launch worker for task 0.0 in stage 11.0 (TID 18) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:56.863 [Executor task launch worker for task 0.0 in stage 11.0 (TID 18) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-14 08:35:56.863 [Executor task launch worker for task 0.0 in stage 11.0 (TID 18) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=24, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:56.871 [Executor task launch worker for task 0.0 in stage 11.0 (TID 18) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:56.876 [Executor task launch worker for task 0.0 in stage 11.0 (TID 18) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:56.876 [Executor task launch worker for task 0.0 in stage 11.0 (TID 18) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [b31d7258-214e-4a62-90b1-604980cc3631] (2 queries & 0 savepoints) is committed.
2025-06-14 08:35:56.876 [Executor task launch worker for task 0.0 in stage 11.0 (TID 18) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [2c533e2b-81eb-4c12-a2e9-888be602ee82] (0 queries & 0 savepoints) is committed.
2025-06-14 08:35:56.877 [Executor task launch worker for task 0.0 in stage 11.0 (TID 18) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 18). 1646 bytes result sent to driver
2025-06-14 08:35:56.877 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 18) in 526 ms on phamviethoa (executor driver) (1/1)
2025-06-14 08:35:56.877 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool 
2025-06-14 08:35:56.878 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 11 (start at ClickstreamProcessor.java:66) finished in 0.534 s
2025-06-14 08:35:56.878 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:35:56.878 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 11: Stage finished
2025-06-14 08:35:56.878 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.spark.scheduler.DAGScheduler - Job 11 finished: start at ClickstreamProcessor.java:66, took 0.535215 s
2025-06-14 08:35:56.889 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/11 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.11.d833da9a-cf46-4768-a7c8-f65cd290ed6a.tmp
2025-06-14 08:35:56.899 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.11.d833da9a-cf46-4768-a7c8-f65cd290ed6a.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/11
2025-06-14 08:35:56.900 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:35:56.280Z",
  "batchId" : 11,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 90.90909090909092,
  "processedRowsPerSecond" : 1.6155088852988693,
  "durationMs" : {
    "addBatch" : 580,
    "commitOffsets" : 14,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 8,
    "triggerExecution" : 619,
    "walCommit" : 15
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 18,
        "0" : 33
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 19,
        "0" : 33
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 19,
        "0" : 33
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 90.90909090909092,
    "processedRowsPerSecond" : 1.6155088852988693,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:35:56.903 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/12 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.12.67ee5b27-8730-4d7e-98c8-d2cb971af174.tmp
2025-06-14 08:35:56.912 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.12.67ee5b27-8730-4d7e-98c8-d2cb971af174.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/12
2025-06-14 08:35:56.912 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 12. Metadata OffsetSeqMetadata(0,1749864956900,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:35:56.919 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:56.920 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:56.927 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:56.927 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:56.963 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:35:56.963 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 12 (start at ClickstreamProcessor.java:66) with 1 output partitions
2025-06-14 08:35:56.964 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 12 (start at ClickstreamProcessor.java:66)
2025-06-14 08:35:56.964 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:35:56.964 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:35:56.964 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 12 (MapPartitionsRDD[116] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:35:56.966 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_12 stored as values in memory (estimated size 39.3 KiB, free 9.2 GiB)
2025-06-14 08:35:56.969 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_12_piece0 stored as bytes in memory (estimated size 17.5 KiB, free 9.2 GiB)
2025-06-14 08:35:56.969 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on phamviethoa:45719 (size: 17.5 KiB, free: 9.2 GiB)
2025-06-14 08:35:56.969 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 12 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:35:56.969 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[116] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0))
2025-06-14 08:35:56.969 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 12.0 with 1 tasks resource profile 0
2025-06-14 08:35:56.970 [dispatcher-event-loop-13 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 12.0 (TID 19) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:35:56.970 [Executor task launch worker for task 0.0 in stage 12.0 (TID 19) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 12.0 (TID 19)
2025-06-14 08:35:56.983 [Executor task launch worker for task 0.0 in stage 12.0 (TID 19) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:56.989 [Executor task launch worker for task 0.0 in stage 12.0 (TID 19) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:56.989 [Executor task launch worker for task 0.0 in stage 12.0 (TID 19) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [f0e3cd97-e0ea-438f-bc5d-c10b3ae2da66] (2 queries & 0 savepoints) is committed.
2025-06-14 08:35:56.989 [Executor task launch worker for task 0.0 in stage 12.0 (TID 19) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [a888d314-c1f1-4926-9cc5-568b7cf7ae32] (0 queries & 0 savepoints) is committed.
2025-06-14 08:35:56.989 [Executor task launch worker for task 0.0 in stage 12.0 (TID 19) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 12.0 (TID 19). 1646 bytes result sent to driver
2025-06-14 08:35:56.990 [task-result-getter-3 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 12.0 (TID 19) in 20 ms on phamviethoa (executor driver) (1/1)
2025-06-14 08:35:56.990 [task-result-getter-3 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 12.0, whose tasks have all completed, from pool 
2025-06-14 08:35:56.990 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 12 (start at ClickstreamProcessor.java:66) finished in 0.026 s
2025-06-14 08:35:56.991 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:35:56.991 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 12: Stage finished
2025-06-14 08:35:56.991 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.spark.scheduler.DAGScheduler - Job 12 finished: start at ClickstreamProcessor.java:66, took 0.027476 s
2025-06-14 08:35:57.001 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/12 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.12.33a5594f-c95b-411e-81cc-f4f31afc0619.tmp
2025-06-14 08:35:57.010 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.12.33a5594f-c95b-411e-81cc-f4f31afc0619.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/12
2025-06-14 08:35:57.011 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:35:56.900Z",
  "batchId" : 12,
  "numInputRows" : 5,
  "inputRowsPerSecond" : 8.064516129032258,
  "processedRowsPerSecond" : 45.04504504504504,
  "durationMs" : {
    "addBatch" : 77,
    "commitOffsets" : 12,
    "getBatch" : 0,
    "latestOffset" : 0,
    "queryPlanning" : 7,
    "triggerExecution" : 111,
    "walCommit" : 13
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 19,
        "0" : 33
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 24,
        "0" : 33
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 24,
        "0" : 33
      }
    },
    "numInputRows" : 5,
    "inputRowsPerSecond" : 8.064516129032258,
    "processedRowsPerSecond" : 45.04504504504504,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:35:58.510 [http-nio-8080-exec-9 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-14 08:35:58.511 [http-nio-8080-exec-9 INFO ] c.e.c.c.ClickstreamController - Received payload: {events=[{event_id=5371360c-2480-4c02-b123-e550c30409e0, event_name=click, event_time=2025-06-14T01:35:56.870Z, user_id=user_pej5ild, session_id=session_mltrxqm, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 3
                        Customer favorit, element_type=div, element_name=null, track=product_click, productId=3}, {event_id=9898c9f5-4c89-479f-a70f-8ecf4889d952, event_name=click, event_time=2025-06-14T01:35:57.690Z, user_id=user_pej5ild, session_id=session_mltrxqm, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 4
                        Reliable and aff, element_type=div, element_name=null, track=product_click, productId=4}, {event_id=9dc3a926-99e2-4f9c-9275-6f70d4d53e4d, event_name=click, event_time=2025-06-14T01:35:58.508Z, user_id=user_pej5ild, session_id=session_mltrxqm, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 5
                        Bestseller with , element_type=div, element_name=null, track=product_click, productId=5}]}
2025-06-14 08:35:58.511 [http-nio-8080-exec-9 INFO ] c.e.c.c.ClickstreamController - Received 3 events
2025-06-14 08:35:58.511 [http-nio-8080-exec-9 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"5371360c-2480-4c02-b123-e550c30409e0","eventName":"click","eventTimestamp":"2025-06-14T01:35:56.870Z","userId":"user_pej5ild","sessionId":"session_mltrxqm","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","productId":"3","element_text":"Product 3\n                        Customer favorit","viewport_size":"659x935","page_title":"E-Commerce Store","session_id":"session_mltrxqm","language":"vi","element_type":"div","screen_resolution":"1920x1080","element_class":"card product_card","platform":"web","event_id":"5371360c-2480-4c02-b123-e550c30409e0","user_id":"user_pej5ild","event_name":"click","page_path":"/","page_referrer":"direct","track":"product_click","event_time":"2025-06-14T01:35:56.870Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:35:58.513 [http-nio-8080-exec-9 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 1, offset: 25
2025-06-14 08:35:58.513 [http-nio-8080-exec-9 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"9898c9f5-4c89-479f-a70f-8ecf4889d952","eventName":"click","eventTimestamp":"2025-06-14T01:35:57.690Z","userId":"user_pej5ild","sessionId":"session_mltrxqm","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","productId":"4","element_text":"Product 4\n                        Reliable and aff","viewport_size":"659x935","page_title":"E-Commerce Store","session_id":"session_mltrxqm","language":"vi","element_type":"div","screen_resolution":"1920x1080","element_class":"card product_card","platform":"web","event_id":"9898c9f5-4c89-479f-a70f-8ecf4889d952","user_id":"user_pej5ild","event_name":"click","page_path":"/","page_referrer":"direct","track":"product_click","event_time":"2025-06-14T01:35:57.690Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:35:58.515 [http-nio-8080-exec-9 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 1, offset: 27
2025-06-14 08:35:58.515 [http-nio-8080-exec-9 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"9dc3a926-99e2-4f9c-9275-6f70d4d53e4d","eventName":"click","eventTimestamp":"2025-06-14T01:35:58.508Z","userId":"user_pej5ild","sessionId":"session_mltrxqm","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","productId":"5","element_text":"Product 5\n                        Bestseller with ","viewport_size":"659x935","page_title":"E-Commerce Store","session_id":"session_mltrxqm","language":"vi","element_type":"div","screen_resolution":"1920x1080","element_class":"card product_card","platform":"web","event_id":"9dc3a926-99e2-4f9c-9275-6f70d4d53e4d","user_id":"user_pej5ild","event_name":"click","page_path":"/","page_referrer":"direct","track":"product_click","event_time":"2025-06-14T01:35:58.508Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:35:58.517 [http-nio-8080-exec-9 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 0, offset: 34
2025-06-14 08:35:58.517 [http-nio-8080-exec-9 INFO ] c.e.c.c.ClickstreamController - Successfully processed 3 events
2025-06-14 08:35:58.524 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/13 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.13.13372448-2bd1-4c2c-9223-6a5d254d7d77.tmp
2025-06-14 08:35:58.534 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.13.13372448-2bd1-4c2c-9223-6a5d254d7d77.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/13
2025-06-14 08:35:58.534 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 13. Metadata OffsetSeqMetadata(0,1749864958521,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:35:58.540 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:58.541 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:58.547 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:58.547 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:35:58.566 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on phamviethoa:45719 in memory (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:35:58.568 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on phamviethoa:45719 in memory (size: 17.5 KiB, free: 9.2 GiB)
2025-06-14 08:35:58.569 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_11_piece0 on phamviethoa:45719 in memory (size: 17.5 KiB, free: 9.2 GiB)
2025-06-14 08:35:58.570 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_10_piece0 on phamviethoa:45719 in memory (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:35:58.572 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_12_piece0 on phamviethoa:45719 in memory (size: 17.5 KiB, free: 9.2 GiB)
2025-06-14 08:35:58.573 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on phamviethoa:45719 in memory (size: 17.5 KiB, free: 9.2 GiB)
2025-06-14 08:35:58.574 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on phamviethoa:45719 in memory (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:35:58.576 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on phamviethoa:45719 in memory (size: 17.5 KiB, free: 9.2 GiB)
2025-06-14 08:35:58.577 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on phamviethoa:45719 in memory (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:35:58.578 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_9_piece0 on phamviethoa:45719 in memory (size: 17.5 KiB, free: 9.2 GiB)
2025-06-14 08:35:58.580 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on phamviethoa:45719 in memory (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:35:58.581 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on phamviethoa:45719 in memory (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:35:58.599 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:35:58.599 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 13 (start at ClickstreamProcessor.java:66) with 2 output partitions
2025-06-14 08:35:58.599 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 13 (start at ClickstreamProcessor.java:66)
2025-06-14 08:35:58.599 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:35:58.599 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:35:58.599 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 13 (MapPartitionsRDD[125] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:35:58.602 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_13 stored as values in memory (estimated size 39.4 KiB, free 9.2 GiB)
2025-06-14 08:35:58.604 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_13_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 9.2 GiB)
2025-06-14 08:35:58.604 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_13_piece0 in memory on phamviethoa:45719 (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:35:58.604 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 13 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:35:58.604 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 13 (MapPartitionsRDD[125] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0, 1))
2025-06-14 08:35:58.604 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 13.0 with 2 tasks resource profile 0
2025-06-14 08:35:58.605 [dispatcher-event-loop-15 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 13.0 (TID 20) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:35:58.605 [dispatcher-event-loop-15 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 13.0 (TID 21) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:35:58.605 [Executor task launch worker for task 1.0 in stage 13.0 (TID 21) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 13.0 (TID 21)
2025-06-14 08:35:58.605 [Executor task launch worker for task 0.0 in stage 13.0 (TID 20) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 13.0 (TID 20)
2025-06-14 08:35:58.613 [Executor task launch worker for task 0.0 in stage 13.0 (TID 20) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 24 for partition clickstream-events-1
2025-06-14 08:35:58.614 [Executor task launch worker for task 0.0 in stage 13.0 (TID 20) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-14 08:35:58.616 [Executor task launch worker for task 1.0 in stage 13.0 (TID 21) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 33 for partition clickstream-events-0
2025-06-14 08:35:58.617 [Executor task launch worker for task 1.0 in stage 13.0 (TID 21) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-14 08:35:59.116 [Executor task launch worker for task 0.0 in stage 13.0 (TID 20) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:59.116 [Executor task launch worker for task 0.0 in stage 13.0 (TID 20) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-14 08:35:59.117 [Executor task launch worker for task 0.0 in stage 13.0 (TID 20) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=28, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:59.118 [Executor task launch worker for task 1.0 in stage 13.0 (TID 21) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:59.118 [Executor task launch worker for task 1.0 in stage 13.0 (TID 21) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-14 08:35:59.118 [Executor task launch worker for task 1.0 in stage 13.0 (TID 21) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=35, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:35:59.124 [Executor task launch worker for task 0.0 in stage 13.0 (TID 20) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:59.124 [Executor task launch worker for task 1.0 in stage 13.0 (TID 21) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:59.129 [Executor task launch worker for task 0.0 in stage 13.0 (TID 20) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:59.129 [Executor task launch worker for task 0.0 in stage 13.0 (TID 20) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [24a572ca-9fac-4f41-95e3-fe6c96c53155] (2 queries & 0 savepoints) is committed.
2025-06-14 08:35:59.129 [Executor task launch worker for task 1.0 in stage 13.0 (TID 21) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:35:59.129 [Executor task launch worker for task 0.0 in stage 13.0 (TID 20) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [5e6e9587-d1fa-4e9e-b431-a6804796158e] (0 queries & 0 savepoints) is committed.
2025-06-14 08:35:59.129 [Executor task launch worker for task 1.0 in stage 13.0 (TID 21) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [d0fe7d4b-fad6-4944-b847-d9e8e2195fbf] (2 queries & 0 savepoints) is committed.
2025-06-14 08:35:59.129 [Executor task launch worker for task 1.0 in stage 13.0 (TID 21) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [240f2575-cd0c-420b-9cc7-f5b8cbe0a747] (0 queries & 0 savepoints) is committed.
2025-06-14 08:35:59.130 [Executor task launch worker for task 0.0 in stage 13.0 (TID 20) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 13.0 (TID 20). 1646 bytes result sent to driver
2025-06-14 08:35:59.130 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 13.0 (TID 20) in 525 ms on phamviethoa (executor driver) (1/2)
2025-06-14 08:35:59.131 [Executor task launch worker for task 1.0 in stage 13.0 (TID 21) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 13.0 (TID 21). 1689 bytes result sent to driver
2025-06-14 08:35:59.132 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 13.0 (TID 21) in 527 ms on phamviethoa (executor driver) (2/2)
2025-06-14 08:35:59.132 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 13.0, whose tasks have all completed, from pool 
2025-06-14 08:35:59.132 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 13 (start at ClickstreamProcessor.java:66) finished in 0.532 s
2025-06-14 08:35:59.132 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:35:59.132 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 13: Stage finished
2025-06-14 08:35:59.132 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.spark.scheduler.DAGScheduler - Job 13 finished: start at ClickstreamProcessor.java:66, took 0.533195 s
2025-06-14 08:35:59.142 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/13 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.13.48b0dd16-ee18-4ed5-a83e-1bfcc5437571.tmp
2025-06-14 08:35:59.152 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.13.48b0dd16-ee18-4ed5-a83e-1bfcc5437571.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/13
2025-06-14 08:35:59.153 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:35:58.520Z",
  "batchId" : 13,
  "numInputRows" : 6,
  "inputRowsPerSecond" : 545.4545454545455,
  "processedRowsPerSecond" : 9.493670886075948,
  "durationMs" : {
    "addBatch" : 597,
    "commitOffsets" : 13,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 7,
    "triggerExecution" : 632,
    "walCommit" : 13
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 24,
        "0" : 33
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 28,
        "0" : 35
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 28,
        "0" : 35
      }
    },
    "numInputRows" : 6,
    "inputRowsPerSecond" : 545.4545454545455,
    "processedRowsPerSecond" : 9.493670886075948,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:36:02.087 [http-nio-8080-exec-10 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-14 08:36:02.087 [http-nio-8080-exec-10 INFO ] c.e.c.c.ClickstreamController - Received payload: {events=[{event_id=2cada865-6d96-44fd-8ad2-b8e2ca109fdb, event_name=scroll, event_time=2025-06-14T01:36:00.083Z, user_id=user_pej5ild, session_id=session_mltrxqm, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=74}, {event_id=270934c6-eb1d-4882-96d2-00f88acf38d8, event_name=scroll, event_time=2025-06-14T01:36:01.084Z, user_id=user_pej5ild, session_id=session_mltrxqm, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=85}, {event_id=9b23e3c0-8da2-43f6-b585-c4daadce65ff, event_name=scroll, event_time=2025-06-14T01:36:02.084Z, user_id=user_pej5ild, session_id=session_mltrxqm, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=100}]}
2025-06-14 08:36:02.087 [http-nio-8080-exec-10 INFO ] c.e.c.c.ClickstreamController - Received 3 events
2025-06-14 08:36:02.088 [http-nio-8080-exec-10 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"2cada865-6d96-44fd-8ad2-b8e2ca109fdb","eventName":"scroll","eventTimestamp":"2025-06-14T01:36:00.083Z","userId":"user_pej5ild","sessionId":"session_mltrxqm","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","viewport_size":"659x935","page_title":"E-Commerce Store","session_id":"session_mltrxqm","language":"vi","screen_resolution":"1920x1080","platform":"web","scrollDepth":"74","event_id":"2cada865-6d96-44fd-8ad2-b8e2ca109fdb","user_id":"user_pej5ild","event_name":"scroll","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:36:00.083Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:36:02.090 [http-nio-8080-exec-10 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 1, offset: 29
2025-06-14 08:36:02.090 [http-nio-8080-exec-10 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"270934c6-eb1d-4882-96d2-00f88acf38d8","eventName":"scroll","eventTimestamp":"2025-06-14T01:36:01.084Z","userId":"user_pej5ild","sessionId":"session_mltrxqm","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","viewport_size":"659x935","page_title":"E-Commerce Store","session_id":"session_mltrxqm","language":"vi","screen_resolution":"1920x1080","platform":"web","scrollDepth":"85","event_id":"270934c6-eb1d-4882-96d2-00f88acf38d8","user_id":"user_pej5ild","event_name":"scroll","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:36:01.084Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:36:02.092 [http-nio-8080-exec-10 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 1, offset: 31
2025-06-14 08:36:02.092 [http-nio-8080-exec-10 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"9b23e3c0-8da2-43f6-b585-c4daadce65ff","eventName":"scroll","eventTimestamp":"2025-06-14T01:36:02.084Z","userId":"user_pej5ild","sessionId":"session_mltrxqm","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","viewport_size":"659x935","page_title":"E-Commerce Store","session_id":"session_mltrxqm","language":"vi","screen_resolution":"1920x1080","platform":"web","scrollDepth":"100","event_id":"9b23e3c0-8da2-43f6-b585-c4daadce65ff","user_id":"user_pej5ild","event_name":"scroll","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:36:02.084Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:36:02.094 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/14 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.14.8c60a761-f6c0-48eb-bd88-46ca255aa90f.tmp
2025-06-14 08:36:02.094 [http-nio-8080-exec-10 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 0, offset: 36
2025-06-14 08:36:02.094 [http-nio-8080-exec-10 INFO ] c.e.c.c.ClickstreamController - Successfully processed 3 events
2025-06-14 08:36:02.105 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.14.8c60a761-f6c0-48eb-bd88-46ca255aa90f.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/14
2025-06-14 08:36:02.105 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 14. Metadata OffsetSeqMetadata(0,1749864962091,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:36:02.112 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:02.113 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:02.118 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:02.118 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:02.150 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:36:02.150 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 14 (start at ClickstreamProcessor.java:66) with 1 output partitions
2025-06-14 08:36:02.150 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 14 (start at ClickstreamProcessor.java:66)
2025-06-14 08:36:02.150 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:36:02.150 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:36:02.150 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 14 (MapPartitionsRDD[134] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:36:02.154 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_14 stored as values in memory (estimated size 39.3 KiB, free 9.2 GiB)
2025-06-14 08:36:02.157 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_14_piece0 stored as bytes in memory (estimated size 17.5 KiB, free 9.2 GiB)
2025-06-14 08:36:02.157 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_14_piece0 in memory on phamviethoa:45719 (size: 17.5 KiB, free: 9.2 GiB)
2025-06-14 08:36:02.157 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 14 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:36:02.157 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[134] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0))
2025-06-14 08:36:02.157 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 14.0 with 1 tasks resource profile 0
2025-06-14 08:36:02.158 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 14.0 (TID 22) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:36:02.158 [Executor task launch worker for task 0.0 in stage 14.0 (TID 22) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 14.0 (TID 22)
2025-06-14 08:36:02.166 [Executor task launch worker for task 0.0 in stage 14.0 (TID 22) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 28 for partition clickstream-events-1
2025-06-14 08:36:02.167 [Executor task launch worker for task 0.0 in stage 14.0 (TID 22) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-14 08:36:02.668 [Executor task launch worker for task 0.0 in stage 14.0 (TID 22) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:36:02.668 [Executor task launch worker for task 0.0 in stage 14.0 (TID 22) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-14 08:36:02.669 [Executor task launch worker for task 0.0 in stage 14.0 (TID 22) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=32, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:36:02.675 [Executor task launch worker for task 0.0 in stage 14.0 (TID 22) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:02.679 [Executor task launch worker for task 0.0 in stage 14.0 (TID 22) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:02.679 [Executor task launch worker for task 0.0 in stage 14.0 (TID 22) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [9468c4b1-05e1-4cc2-be2e-a69d68b3056a] (2 queries & 0 savepoints) is committed.
2025-06-14 08:36:02.679 [Executor task launch worker for task 0.0 in stage 14.0 (TID 22) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [ad4ebf0c-6446-44e7-b56f-0d6de6d9cc70] (0 queries & 0 savepoints) is committed.
2025-06-14 08:36:02.680 [Executor task launch worker for task 0.0 in stage 14.0 (TID 22) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 14.0 (TID 22). 1646 bytes result sent to driver
2025-06-14 08:36:02.681 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 14.0 (TID 22) in 523 ms on phamviethoa (executor driver) (1/1)
2025-06-14 08:36:02.681 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 14.0, whose tasks have all completed, from pool 
2025-06-14 08:36:02.681 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 14 (start at ClickstreamProcessor.java:66) finished in 0.530 s
2025-06-14 08:36:02.681 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:36:02.681 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 14: Stage finished
2025-06-14 08:36:02.681 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.spark.scheduler.DAGScheduler - Job 14 finished: start at ClickstreamProcessor.java:66, took 0.531234 s
2025-06-14 08:36:02.690 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/14 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.14.7d1fb26c-e8a6-4ef7-aaac-dfcf97d1f5c2.tmp
2025-06-14 08:36:02.700 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.14.7d1fb26c-e8a6-4ef7-aaac-dfcf97d1f5c2.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/14
2025-06-14 08:36:02.701 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:36:02.090Z",
  "batchId" : 14,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 272.72727272727275,
  "processedRowsPerSecond" : 4.918032786885246,
  "durationMs" : {
    "addBatch" : 573,
    "commitOffsets" : 13,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 8,
    "triggerExecution" : 610,
    "walCommit" : 14
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 28,
        "0" : 35
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 31,
        "0" : 35
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 31,
        "0" : 35
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 272.72727272727275,
    "processedRowsPerSecond" : 4.918032786885246,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:36:02.704 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/15 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.15.f113e152-b3e7-4ab8-861b-e28696d896be.tmp
2025-06-14 08:36:02.713 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.15.f113e152-b3e7-4ab8-861b-e28696d896be.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/15
2025-06-14 08:36:02.713 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 15. Metadata OffsetSeqMetadata(0,1749864962702,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:36:02.720 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:02.720 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:02.725 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:02.725 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:02.757 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:36:02.757 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 15 (start at ClickstreamProcessor.java:66) with 2 output partitions
2025-06-14 08:36:02.757 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 15 (start at ClickstreamProcessor.java:66)
2025-06-14 08:36:02.757 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:36:02.757 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:36:02.758 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 15 (MapPartitionsRDD[143] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:36:02.761 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_15 stored as values in memory (estimated size 39.4 KiB, free 9.2 GiB)
2025-06-14 08:36:02.762 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_15_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 9.2 GiB)
2025-06-14 08:36:02.763 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_15_piece0 in memory on phamviethoa:45719 (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:36:02.763 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 15 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:36:02.763 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 15 (MapPartitionsRDD[143] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0, 1))
2025-06-14 08:36:02.763 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 15.0 with 2 tasks resource profile 0
2025-06-14 08:36:02.764 [dispatcher-event-loop-9 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 15.0 (TID 23) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:36:02.764 [dispatcher-event-loop-9 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 15.0 (TID 24) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:36:02.764 [Executor task launch worker for task 0.0 in stage 15.0 (TID 23) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 15.0 (TID 23)
2025-06-14 08:36:02.764 [Executor task launch worker for task 1.0 in stage 15.0 (TID 24) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 15.0 (TID 24)
2025-06-14 08:36:02.771 [Executor task launch worker for task 1.0 in stage 15.0 (TID 24) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 35 for partition clickstream-events-0
2025-06-14 08:36:02.773 [Executor task launch worker for task 1.0 in stage 15.0 (TID 24) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-14 08:36:02.777 [Executor task launch worker for task 0.0 in stage 15.0 (TID 23) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:02.781 [Executor task launch worker for task 0.0 in stage 15.0 (TID 23) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:02.781 [Executor task launch worker for task 0.0 in stage 15.0 (TID 23) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [aea33e90-f552-4896-b9f1-64956db65302] (2 queries & 0 savepoints) is committed.
2025-06-14 08:36:02.781 [Executor task launch worker for task 0.0 in stage 15.0 (TID 23) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [9a43f7f2-ef7d-4a8f-a925-72269096d031] (0 queries & 0 savepoints) is committed.
2025-06-14 08:36:02.782 [Executor task launch worker for task 0.0 in stage 15.0 (TID 23) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 15.0 (TID 23). 1646 bytes result sent to driver
2025-06-14 08:36:02.783 [task-result-getter-3 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 15.0 (TID 23) in 20 ms on phamviethoa (executor driver) (1/2)
2025-06-14 08:36:03.274 [Executor task launch worker for task 1.0 in stage 15.0 (TID 24) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:36:03.274 [Executor task launch worker for task 1.0 in stage 15.0 (TID 24) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-14 08:36:03.275 [Executor task launch worker for task 1.0 in stage 15.0 (TID 24) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=37, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:36:03.281 [Executor task launch worker for task 1.0 in stage 15.0 (TID 24) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:03.285 [Executor task launch worker for task 1.0 in stage 15.0 (TID 24) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:03.285 [Executor task launch worker for task 1.0 in stage 15.0 (TID 24) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [6af05057-bed8-49c0-9c2b-fb54292917e8] (2 queries & 0 savepoints) is committed.
2025-06-14 08:36:03.285 [Executor task launch worker for task 1.0 in stage 15.0 (TID 24) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [c4b4f90f-1626-411f-bdb9-3d41533880cd] (0 queries & 0 savepoints) is committed.
2025-06-14 08:36:03.287 [Executor task launch worker for task 1.0 in stage 15.0 (TID 24) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 15.0 (TID 24). 1689 bytes result sent to driver
2025-06-14 08:36:03.288 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 15.0 (TID 24) in 524 ms on phamviethoa (executor driver) (2/2)
2025-06-14 08:36:03.288 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 15.0, whose tasks have all completed, from pool 
2025-06-14 08:36:03.288 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 15 (start at ClickstreamProcessor.java:66) finished in 0.530 s
2025-06-14 08:36:03.288 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:36:03.288 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 15: Stage finished
2025-06-14 08:36:03.288 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.spark.scheduler.DAGScheduler - Job 15 finished: start at ClickstreamProcessor.java:66, took 0.531185 s
2025-06-14 08:36:03.298 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/15 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.15.72059bdc-fd89-4eed-81f9-c27af55f9594.tmp
2025-06-14 08:36:03.307 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.15.72059bdc-fd89-4eed-81f9-c27af55f9594.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/15
2025-06-14 08:36:03.308 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:36:02.701Z",
  "batchId" : 15,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 4.909983633387889,
  "processedRowsPerSecond" : 4.9504950495049505,
  "durationMs" : {
    "addBatch" : 574,
    "commitOffsets" : 12,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 7,
    "triggerExecution" : 606,
    "walCommit" : 12
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 31,
        "0" : 35
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 32,
        "0" : 37
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 32,
        "0" : 37
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 4.909983633387889,
    "processedRowsPerSecond" : 4.9504950495049505,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:36:05.088 [http-nio-8080-exec-2 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-14 08:36:05.088 [http-nio-8080-exec-2 INFO ] c.e.c.c.ClickstreamController - Received payload: {events=[{event_id=5d030086-adb6-4a3c-9ff7-cd5cc5ac10f8, event_name=scroll, event_time=2025-06-14T01:36:03.084Z, user_id=user_pej5ild, session_id=session_mltrxqm, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=79}, {event_id=607f8326-78fc-4a20-bc00-507546157984, event_name=scroll, event_time=2025-06-14T01:36:04.085Z, user_id=user_pej5ild, session_id=session_mltrxqm, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=68}, {event_id=568054b5-98fc-418c-ab93-9d5e1dcaf43a, event_name=scroll, event_time=2025-06-14T01:36:05.085Z, user_id=user_pej5ild, session_id=session_mltrxqm, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=58}]}
2025-06-14 08:36:05.088 [http-nio-8080-exec-2 INFO ] c.e.c.c.ClickstreamController - Received 3 events
2025-06-14 08:36:05.088 [http-nio-8080-exec-2 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"5d030086-adb6-4a3c-9ff7-cd5cc5ac10f8","eventName":"scroll","eventTimestamp":"2025-06-14T01:36:03.084Z","userId":"user_pej5ild","sessionId":"session_mltrxqm","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","viewport_size":"659x935","page_title":"E-Commerce Store","session_id":"session_mltrxqm","language":"vi","screen_resolution":"1920x1080","platform":"web","scrollDepth":"79","event_id":"5d030086-adb6-4a3c-9ff7-cd5cc5ac10f8","user_id":"user_pej5ild","event_name":"scroll","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:36:03.084Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:36:05.090 [http-nio-8080-exec-2 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 0, offset: 38
2025-06-14 08:36:05.091 [http-nio-8080-exec-2 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"607f8326-78fc-4a20-bc00-507546157984","eventName":"scroll","eventTimestamp":"2025-06-14T01:36:04.085Z","userId":"user_pej5ild","sessionId":"session_mltrxqm","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","viewport_size":"659x935","page_title":"E-Commerce Store","session_id":"session_mltrxqm","language":"vi","screen_resolution":"1920x1080","platform":"web","scrollDepth":"68","event_id":"607f8326-78fc-4a20-bc00-507546157984","user_id":"user_pej5ild","event_name":"scroll","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:36:04.085Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:36:05.092 [http-nio-8080-exec-2 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 0, offset: 40
2025-06-14 08:36:05.092 [http-nio-8080-exec-2 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"568054b5-98fc-418c-ab93-9d5e1dcaf43a","eventName":"scroll","eventTimestamp":"2025-06-14T01:36:05.085Z","userId":"user_pej5ild","sessionId":"session_mltrxqm","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","viewport_size":"659x935","page_title":"E-Commerce Store","session_id":"session_mltrxqm","language":"vi","screen_resolution":"1920x1080","platform":"web","scrollDepth":"58","event_id":"568054b5-98fc-418c-ab93-9d5e1dcaf43a","user_id":"user_pej5ild","event_name":"scroll","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:36:05.085Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:36:05.094 [http-nio-8080-exec-2 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 1, offset: 33
2025-06-14 08:36:05.094 [http-nio-8080-exec-2 INFO ] c.e.c.c.ClickstreamController - Successfully processed 3 events
2025-06-14 08:36:05.095 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/16 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.16.ddca2b22-dc05-4bc6-ba42-2f84111f3e08.tmp
2025-06-14 08:36:05.105 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.16.ddca2b22-dc05-4bc6-ba42-2f84111f3e08.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/16
2025-06-14 08:36:05.105 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 16. Metadata OffsetSeqMetadata(0,1749864965092,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:36:05.112 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:05.112 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:05.117 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:05.117 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:05.150 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:36:05.151 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 16 (start at ClickstreamProcessor.java:66) with 1 output partitions
2025-06-14 08:36:05.151 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 16 (start at ClickstreamProcessor.java:66)
2025-06-14 08:36:05.151 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:36:05.151 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:36:05.151 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 16 (MapPartitionsRDD[152] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:36:05.153 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_16 stored as values in memory (estimated size 39.3 KiB, free 9.2 GiB)
2025-06-14 08:36:05.155 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_16_piece0 stored as bytes in memory (estimated size 17.5 KiB, free 9.2 GiB)
2025-06-14 08:36:05.156 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_16_piece0 in memory on phamviethoa:45719 (size: 17.5 KiB, free: 9.2 GiB)
2025-06-14 08:36:05.156 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 16 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:36:05.156 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[152] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0))
2025-06-14 08:36:05.156 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 16.0 with 1 tasks resource profile 0
2025-06-14 08:36:05.157 [dispatcher-event-loop-14 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 16.0 (TID 25) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:36:05.157 [Executor task launch worker for task 0.0 in stage 16.0 (TID 25) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 16.0 (TID 25)
2025-06-14 08:36:05.165 [Executor task launch worker for task 0.0 in stage 16.0 (TID 25) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 37 for partition clickstream-events-0
2025-06-14 08:36:05.166 [Executor task launch worker for task 0.0 in stage 16.0 (TID 25) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-14 08:36:05.668 [Executor task launch worker for task 0.0 in stage 16.0 (TID 25) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:36:05.668 [Executor task launch worker for task 0.0 in stage 16.0 (TID 25) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-14 08:36:05.669 [Executor task launch worker for task 0.0 in stage 16.0 (TID 25) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=41, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:36:05.677 [Executor task launch worker for task 0.0 in stage 16.0 (TID 25) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:05.681 [Executor task launch worker for task 0.0 in stage 16.0 (TID 25) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:05.681 [Executor task launch worker for task 0.0 in stage 16.0 (TID 25) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [e01c972e-cb03-4a7e-9b88-292193c41174] (2 queries & 0 savepoints) is committed.
2025-06-14 08:36:05.681 [Executor task launch worker for task 0.0 in stage 16.0 (TID 25) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [878f9c88-b033-4bd7-9a3e-d6fd615dd2fc] (0 queries & 0 savepoints) is committed.
2025-06-14 08:36:05.682 [Executor task launch worker for task 0.0 in stage 16.0 (TID 25) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 16.0 (TID 25). 1646 bytes result sent to driver
2025-06-14 08:36:05.683 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 16.0 (TID 25) in 526 ms on phamviethoa (executor driver) (1/1)
2025-06-14 08:36:05.683 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 16.0, whose tasks have all completed, from pool 
2025-06-14 08:36:05.683 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 16 (start at ClickstreamProcessor.java:66) finished in 0.532 s
2025-06-14 08:36:05.683 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:36:05.683 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 16: Stage finished
2025-06-14 08:36:05.683 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.spark.scheduler.DAGScheduler - Job 16 finished: start at ClickstreamProcessor.java:66, took 0.532801 s
2025-06-14 08:36:05.693 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/16 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.16.bb519d8e-278c-4155-a657-bc16ec569352.tmp
2025-06-14 08:36:05.702 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.16.bb519d8e-278c-4155-a657-bc16ec569352.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/16
2025-06-14 08:36:05.703 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:36:05.091Z",
  "batchId" : 16,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 272.72727272727275,
  "processedRowsPerSecond" : 4.909983633387889,
  "durationMs" : {
    "addBatch" : 577,
    "commitOffsets" : 12,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 8,
    "triggerExecution" : 611,
    "walCommit" : 13
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 32,
        "0" : 37
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 32,
        "0" : 40
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 32,
        "0" : 40
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 272.72727272727275,
    "processedRowsPerSecond" : 4.909983633387889,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:36:05.706 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/17 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.17.da196526-0e2e-454a-855c-9abd04d3a8b0.tmp
2025-06-14 08:36:05.715 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.17.da196526-0e2e-454a-855c-9abd04d3a8b0.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/17
2025-06-14 08:36:05.716 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 17. Metadata OffsetSeqMetadata(0,1749864965704,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:36:05.722 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:05.722 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:05.727 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:05.727 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:05.759 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:36:05.759 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 17 (start at ClickstreamProcessor.java:66) with 2 output partitions
2025-06-14 08:36:05.759 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 17 (start at ClickstreamProcessor.java:66)
2025-06-14 08:36:05.759 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:36:05.759 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:36:05.759 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 17 (MapPartitionsRDD[161] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:36:05.762 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_17 stored as values in memory (estimated size 39.4 KiB, free 9.2 GiB)
2025-06-14 08:36:05.764 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_17_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 9.2 GiB)
2025-06-14 08:36:05.764 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_17_piece0 in memory on phamviethoa:45719 (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:36:05.764 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 17 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:36:05.764 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 17 (MapPartitionsRDD[161] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0, 1))
2025-06-14 08:36:05.764 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 17.0 with 2 tasks resource profile 0
2025-06-14 08:36:05.765 [dispatcher-event-loop-2 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 17.0 (TID 26) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:36:05.765 [dispatcher-event-loop-2 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 17.0 (TID 27) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:36:05.766 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 17.0 (TID 26)
2025-06-14 08:36:05.766 [Executor task launch worker for task 1.0 in stage 17.0 (TID 27) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 17.0 (TID 27)
2025-06-14 08:36:05.773 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 32 for partition clickstream-events-1
2025-06-14 08:36:05.775 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-14 08:36:05.779 [Executor task launch worker for task 1.0 in stage 17.0 (TID 27) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:05.784 [Executor task launch worker for task 1.0 in stage 17.0 (TID 27) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:05.784 [Executor task launch worker for task 1.0 in stage 17.0 (TID 27) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [1aa9c32f-f63e-4be5-941e-0a3d884f1c2b] (2 queries & 0 savepoints) is committed.
2025-06-14 08:36:05.784 [Executor task launch worker for task 1.0 in stage 17.0 (TID 27) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [c9334570-0c3c-479e-9d6b-b21e2a571d57] (0 queries & 0 savepoints) is committed.
2025-06-14 08:36:05.784 [Executor task launch worker for task 1.0 in stage 17.0 (TID 27) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 17.0 (TID 27). 1646 bytes result sent to driver
2025-06-14 08:36:05.785 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 17.0 (TID 27) in 20 ms on phamviethoa (executor driver) (1/2)
2025-06-14 08:36:06.276 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:36:06.276 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-14 08:36:06.276 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=34, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:36:06.282 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:06.287 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:06.287 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [06d048ad-2934-4339-87a0-8301c336d849] (2 queries & 0 savepoints) is committed.
2025-06-14 08:36:06.287 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [f78eb0de-b797-4cc5-aece-2afb7ecfc321] (0 queries & 0 savepoints) is committed.
2025-06-14 08:36:06.289 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 17.0 (TID 26). 1689 bytes result sent to driver
2025-06-14 08:36:06.289 [task-result-getter-3 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 17.0 (TID 26) in 524 ms on phamviethoa (executor driver) (2/2)
2025-06-14 08:36:06.289 [task-result-getter-3 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 17.0, whose tasks have all completed, from pool 
2025-06-14 08:36:06.289 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 17 (start at ClickstreamProcessor.java:66) finished in 0.530 s
2025-06-14 08:36:06.289 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:36:06.290 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 17: Stage finished
2025-06-14 08:36:06.290 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.spark.scheduler.DAGScheduler - Job 17 finished: start at ClickstreamProcessor.java:66, took 0.530948 s
2025-06-14 08:36:06.299 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/17 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.17.0e98b824-2b6d-4943-815c-b92ffe72d9a5.tmp
2025-06-14 08:36:06.309 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.17.0e98b824-2b6d-4943-815c-b92ffe72d9a5.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/17
2025-06-14 08:36:06.309 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:36:05.703Z",
  "batchId" : 17,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 4.901960784313726,
  "processedRowsPerSecond" : 4.9504950495049505,
  "durationMs" : {
    "addBatch" : 573,
    "commitOffsets" : 13,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 7,
    "triggerExecution" : 606,
    "walCommit" : 12
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 32,
        "0" : 40
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 34,
        "0" : 41
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 34,
        "0" : 41
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 4.901960784313726,
    "processedRowsPerSecond" : 4.9504950495049505,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:36:07.307 [http-nio-8080-exec-3 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-14 08:36:07.307 [http-nio-8080-exec-3 INFO ] c.e.c.c.ClickstreamController - Received payload: {events=[{event_id=4f22524a-779b-4ee4-95a8-89073b3ab24a, event_name=click, event_time=2025-06-14T01:36:06.275Z, user_id=user_pej5ild, session_id=session_mltrxqm, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 2
                        Durable and styl, element_type=div, element_name=null, track=product_click, productId=2}, {event_id=35710aa1-3e68-4304-9cd8-4c1f1a338b10, event_name=scroll, event_time=2025-06-14T01:36:06.716Z, user_id=user_pej5ild, session_id=session_mltrxqm, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=57}, {event_id=8ef230c2-2571-4a79-8f66-f08d8c97fcde, event_name=click, event_time=2025-06-14T01:36:07.305Z, user_id=user_pej5ild, session_id=session_mltrxqm, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 1
                        High-quality ite, element_type=div, element_name=null, track=product_click, productId=1}]}
2025-06-14 08:36:07.307 [http-nio-8080-exec-3 INFO ] c.e.c.c.ClickstreamController - Received 3 events
2025-06-14 08:36:07.308 [http-nio-8080-exec-3 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"4f22524a-779b-4ee4-95a8-89073b3ab24a","eventName":"click","eventTimestamp":"2025-06-14T01:36:06.275Z","userId":"user_pej5ild","sessionId":"session_mltrxqm","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","productId":"2","element_text":"Product 2\n                        Durable and styl","viewport_size":"659x935","page_title":"E-Commerce Store","session_id":"session_mltrxqm","language":"vi","element_type":"div","screen_resolution":"1920x1080","element_class":"card product_card","platform":"web","event_id":"4f22524a-779b-4ee4-95a8-89073b3ab24a","user_id":"user_pej5ild","event_name":"click","page_path":"/","page_referrer":"direct","track":"product_click","event_time":"2025-06-14T01:36:06.275Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:36:07.309 [http-nio-8080-exec-3 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 0, offset: 42
2025-06-14 08:36:07.310 [http-nio-8080-exec-3 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"35710aa1-3e68-4304-9cd8-4c1f1a338b10","eventName":"scroll","eventTimestamp":"2025-06-14T01:36:06.716Z","userId":"user_pej5ild","sessionId":"session_mltrxqm","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","viewport_size":"659x935","page_title":"E-Commerce Store","session_id":"session_mltrxqm","language":"vi","screen_resolution":"1920x1080","platform":"web","scrollDepth":"57","event_id":"35710aa1-3e68-4304-9cd8-4c1f1a338b10","user_id":"user_pej5ild","event_name":"scroll","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:36:06.716Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:36:07.312 [http-nio-8080-exec-3 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 1, offset: 35
2025-06-14 08:36:07.312 [http-nio-8080-exec-3 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"8ef230c2-2571-4a79-8f66-f08d8c97fcde","eventName":"click","eventTimestamp":"2025-06-14T01:36:07.305Z","userId":"user_pej5ild","sessionId":"session_mltrxqm","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","productId":"1","element_text":"Product 1\n                        High-quality ite","viewport_size":"659x935","page_title":"E-Commerce Store","session_id":"session_mltrxqm","language":"vi","element_type":"div","screen_resolution":"1920x1080","element_class":"card product_card","platform":"web","event_id":"8ef230c2-2571-4a79-8f66-f08d8c97fcde","user_id":"user_pej5ild","event_name":"click","page_path":"/","page_referrer":"direct","track":"product_click","event_time":"2025-06-14T01:36:07.305Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:36:07.314 [http-nio-8080-exec-3 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 0, offset: 44
2025-06-14 08:36:07.314 [http-nio-8080-exec-3 INFO ] c.e.c.c.ClickstreamController - Successfully processed 3 events
2025-06-14 08:36:07.314 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/18 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.18.7ff4bf7d-497d-4a4b-b864-826cc4b343d5.tmp
2025-06-14 08:36:07.324 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.18.7ff4bf7d-497d-4a4b-b864-826cc4b343d5.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/18
2025-06-14 08:36:07.324 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 18. Metadata OffsetSeqMetadata(0,1749864967311,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:36:07.330 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:07.331 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:07.336 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:07.336 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:07.369 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:36:07.369 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 18 (start at ClickstreamProcessor.java:66) with 2 output partitions
2025-06-14 08:36:07.369 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 18 (start at ClickstreamProcessor.java:66)
2025-06-14 08:36:07.369 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:36:07.369 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:36:07.369 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 18 (MapPartitionsRDD[170] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:36:07.371 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_18 stored as values in memory (estimated size 39.4 KiB, free 9.2 GiB)
2025-06-14 08:36:07.373 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_18_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 9.2 GiB)
2025-06-14 08:36:07.374 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_18_piece0 in memory on phamviethoa:45719 (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:36:07.374 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 18 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:36:07.374 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 18 (MapPartitionsRDD[170] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0, 1))
2025-06-14 08:36:07.374 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 18.0 with 2 tasks resource profile 0
2025-06-14 08:36:07.374 [dispatcher-event-loop-4 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 18.0 (TID 28) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:36:07.375 [dispatcher-event-loop-4 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 18.0 (TID 29) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:36:07.375 [Executor task launch worker for task 1.0 in stage 18.0 (TID 29) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 18.0 (TID 29)
2025-06-14 08:36:07.375 [Executor task launch worker for task 0.0 in stage 18.0 (TID 28) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 18.0 (TID 28)
2025-06-14 08:36:07.382 [Executor task launch worker for task 0.0 in stage 18.0 (TID 28) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 34 for partition clickstream-events-1
2025-06-14 08:36:07.382 [Executor task launch worker for task 1.0 in stage 18.0 (TID 29) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 41 for partition clickstream-events-0
2025-06-14 08:36:07.384 [Executor task launch worker for task 0.0 in stage 18.0 (TID 28) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-14 08:36:07.384 [Executor task launch worker for task 1.0 in stage 18.0 (TID 29) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-14 08:36:07.885 [Executor task launch worker for task 0.0 in stage 18.0 (TID 28) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:36:07.885 [Executor task launch worker for task 1.0 in stage 18.0 (TID 29) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:36:07.885 [Executor task launch worker for task 0.0 in stage 18.0 (TID 28) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-14 08:36:07.885 [Executor task launch worker for task 1.0 in stage 18.0 (TID 29) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-14 08:36:07.885 [Executor task launch worker for task 0.0 in stage 18.0 (TID 28) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=36, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:36:07.885 [Executor task launch worker for task 1.0 in stage 18.0 (TID 29) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:36:07.891 [Executor task launch worker for task 1.0 in stage 18.0 (TID 29) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:07.892 [Executor task launch worker for task 0.0 in stage 18.0 (TID 28) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:07.896 [Executor task launch worker for task 1.0 in stage 18.0 (TID 29) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:07.896 [Executor task launch worker for task 1.0 in stage 18.0 (TID 29) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [816bdc50-a72a-46b0-a1d5-75645b2be976] (2 queries & 0 savepoints) is committed.
2025-06-14 08:36:07.896 [Executor task launch worker for task 1.0 in stage 18.0 (TID 29) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [2b0f7ac7-7abe-4190-b028-e3554a898393] (0 queries & 0 savepoints) is committed.
2025-06-14 08:36:07.897 [Executor task launch worker for task 0.0 in stage 18.0 (TID 28) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:07.897 [Executor task launch worker for task 0.0 in stage 18.0 (TID 28) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [b1ea824f-8579-4a9a-8b46-98f9690dcf81] (2 queries & 0 savepoints) is committed.
2025-06-14 08:36:07.897 [Executor task launch worker for task 0.0 in stage 18.0 (TID 28) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [d969ef93-e2d4-4833-82ed-bdfa3162cba3] (0 queries & 0 savepoints) is committed.
2025-06-14 08:36:07.897 [Executor task launch worker for task 1.0 in stage 18.0 (TID 29) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 18.0 (TID 29). 1646 bytes result sent to driver
2025-06-14 08:36:07.898 [Executor task launch worker for task 0.0 in stage 18.0 (TID 28) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 18.0 (TID 28). 1646 bytes result sent to driver
2025-06-14 08:36:07.898 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 18.0 (TID 28) in 524 ms on phamviethoa (executor driver) (1/2)
2025-06-14 08:36:07.899 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 18.0 (TID 29) in 525 ms on phamviethoa (executor driver) (2/2)
2025-06-14 08:36:07.899 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 18.0, whose tasks have all completed, from pool 
2025-06-14 08:36:07.899 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 18 (start at ClickstreamProcessor.java:66) finished in 0.529 s
2025-06-14 08:36:07.899 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:36:07.899 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 18: Stage finished
2025-06-14 08:36:07.899 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.spark.scheduler.DAGScheduler - Job 18 finished: start at ClickstreamProcessor.java:66, took 0.530140 s
2025-06-14 08:36:07.909 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/18 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.18.7dc3bdcf-34d5-41ad-a7ec-84b41fb8f46d.tmp
2025-06-14 08:36:07.918 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.18.7dc3bdcf-34d5-41ad-a7ec-84b41fb8f46d.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/18
2025-06-14 08:36:07.919 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:36:07.310Z",
  "batchId" : 18,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 272.72727272727275,
  "processedRowsPerSecond" : 4.926108374384237,
  "durationMs" : {
    "addBatch" : 574,
    "commitOffsets" : 13,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 7,
    "triggerExecution" : 609,
    "walCommit" : 13
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 34,
        "0" : 41
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 35,
        "0" : 43
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 35,
        "0" : 43
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 272.72727272727275,
    "processedRowsPerSecond" : 4.926108374384237,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:36:07.922 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/19 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.19.2c7351ae-a860-4e18-a81e-20b7fa11905d.tmp
2025-06-14 08:36:07.932 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.19.2c7351ae-a860-4e18-a81e-20b7fa11905d.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/19
2025-06-14 08:36:07.932 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 19. Metadata OffsetSeqMetadata(0,1749864967920,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:36:07.938 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:07.938 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:07.943 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:07.943 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:07.976 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:36:07.976 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 19 (start at ClickstreamProcessor.java:66) with 2 output partitions
2025-06-14 08:36:07.976 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 19 (start at ClickstreamProcessor.java:66)
2025-06-14 08:36:07.976 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:36:07.976 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:36:07.977 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 19 (MapPartitionsRDD[179] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:36:07.981 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_19 stored as values in memory (estimated size 39.4 KiB, free 9.2 GiB)
2025-06-14 08:36:07.983 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_19_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 9.2 GiB)
2025-06-14 08:36:07.983 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_19_piece0 in memory on phamviethoa:45719 (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:36:07.983 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 19 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:36:07.984 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 19 (MapPartitionsRDD[179] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0, 1))
2025-06-14 08:36:07.984 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 19.0 with 2 tasks resource profile 0
2025-06-14 08:36:07.985 [dispatcher-event-loop-11 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 19.0 (TID 30) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:36:07.985 [dispatcher-event-loop-11 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 19.0 (TID 31) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:36:07.985 [Executor task launch worker for task 1.0 in stage 19.0 (TID 31) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 19.0 (TID 31)
2025-06-14 08:36:07.985 [Executor task launch worker for task 0.0 in stage 19.0 (TID 30) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 19.0 (TID 30)
2025-06-14 08:36:07.999 [Executor task launch worker for task 0.0 in stage 19.0 (TID 30) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:08.000 [Executor task launch worker for task 1.0 in stage 19.0 (TID 31) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:08.004 [Executor task launch worker for task 0.0 in stage 19.0 (TID 30) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:08.004 [Executor task launch worker for task 0.0 in stage 19.0 (TID 30) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [a3381a7f-4725-4e66-84bd-e27ed7cccfdb] (2 queries & 0 savepoints) is committed.
2025-06-14 08:36:08.004 [Executor task launch worker for task 0.0 in stage 19.0 (TID 30) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [e8e6fcbe-72a4-40a3-9666-5082ff1546f3] (0 queries & 0 savepoints) is committed.
2025-06-14 08:36:08.005 [Executor task launch worker for task 1.0 in stage 19.0 (TID 31) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:08.005 [Executor task launch worker for task 1.0 in stage 19.0 (TID 31) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [b728dcbc-8435-4521-9c69-7a36e6e0e67b] (2 queries & 0 savepoints) is committed.
2025-06-14 08:36:08.005 [Executor task launch worker for task 1.0 in stage 19.0 (TID 31) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [c4999829-f386-48b5-b883-972b437058f9] (0 queries & 0 savepoints) is committed.
2025-06-14 08:36:08.005 [Executor task launch worker for task 0.0 in stage 19.0 (TID 30) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 19.0 (TID 30). 1646 bytes result sent to driver
2025-06-14 08:36:08.006 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 19.0 (TID 30) in 21 ms on phamviethoa (executor driver) (1/2)
2025-06-14 08:36:08.006 [Executor task launch worker for task 1.0 in stage 19.0 (TID 31) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 19.0 (TID 31). 1646 bytes result sent to driver
2025-06-14 08:36:08.007 [task-result-getter-3 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 19.0 (TID 31) in 22 ms on phamviethoa (executor driver) (2/2)
2025-06-14 08:36:08.007 [task-result-getter-3 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 19.0, whose tasks have all completed, from pool 
2025-06-14 08:36:08.008 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 19 (start at ClickstreamProcessor.java:66) finished in 0.031 s
2025-06-14 08:36:08.008 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:36:08.008 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 19: Stage finished
2025-06-14 08:36:08.008 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.spark.scheduler.DAGScheduler - Job 19 finished: start at ClickstreamProcessor.java:66, took 0.031579 s
2025-06-14 08:36:08.020 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/19 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.19.7e27f071-7d3d-4f50-9408-8924f028855f.tmp
2025-06-14 08:36:08.030 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.19.7e27f071-7d3d-4f50-9408-8924f028855f.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/19
2025-06-14 08:36:08.030 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:36:07.919Z",
  "batchId" : 19,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 4.926108374384237,
  "processedRowsPerSecond" : 27.027027027027028,
  "durationMs" : {
    "addBatch" : 78,
    "commitOffsets" : 13,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 7,
    "triggerExecution" : 111,
    "walCommit" : 12
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 35,
        "0" : 43
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 36,
        "0" : 45
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 36,
        "0" : 45
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 4.926108374384237,
    "processedRowsPerSecond" : 27.027027027027028,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:36:10.420 [http-nio-8080-exec-4 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-14 08:36:10.420 [http-nio-8080-exec-4 INFO ] c.e.c.c.ClickstreamController - Received payload: {events=[{event_id=f2d0a425-0564-4e43-b4ad-8f38ceeb8bff, event_name=scroll, event_time=2025-06-14T01:36:07.717Z, user_id=user_pej5ild, session_id=session_mltrxqm, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=46}, {event_id=9a2f6af3-47bb-455f-ab34-b7a16d167593, event_name=scroll, event_time=2025-06-14T01:36:08.718Z, user_id=user_pej5ild, session_id=session_mltrxqm, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=32}, {event_id=75d200c5-65f6-4b3b-b55f-07cdcd709a6d, event_name=click, event_time=2025-06-14T01:36:10.417Z, user_id=user_pej5ild, session_id=session_mltrxqm, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 6
                        Sleek design and, element_type=div, element_name=null, track=product_click, productId=6}]}
2025-06-14 08:36:10.420 [http-nio-8080-exec-4 INFO ] c.e.c.c.ClickstreamController - Received 3 events
2025-06-14 08:36:10.421 [http-nio-8080-exec-4 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"f2d0a425-0564-4e43-b4ad-8f38ceeb8bff","eventName":"scroll","eventTimestamp":"2025-06-14T01:36:07.717Z","userId":"user_pej5ild","sessionId":"session_mltrxqm","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","viewport_size":"659x935","page_title":"E-Commerce Store","session_id":"session_mltrxqm","language":"vi","screen_resolution":"1920x1080","platform":"web","scrollDepth":"46","event_id":"f2d0a425-0564-4e43-b4ad-8f38ceeb8bff","user_id":"user_pej5ild","event_name":"scroll","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:36:07.717Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:36:10.423 [http-nio-8080-exec-4 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 1, offset: 37
2025-06-14 08:36:10.423 [http-nio-8080-exec-4 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"9a2f6af3-47bb-455f-ab34-b7a16d167593","eventName":"scroll","eventTimestamp":"2025-06-14T01:36:08.718Z","userId":"user_pej5ild","sessionId":"session_mltrxqm","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","viewport_size":"659x935","page_title":"E-Commerce Store","session_id":"session_mltrxqm","language":"vi","screen_resolution":"1920x1080","platform":"web","scrollDepth":"32","event_id":"9a2f6af3-47bb-455f-ab34-b7a16d167593","user_id":"user_pej5ild","event_name":"scroll","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:36:08.718Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:36:10.424 [http-nio-8080-exec-4 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 1, offset: 39
2025-06-14 08:36:10.424 [http-nio-8080-exec-4 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"75d200c5-65f6-4b3b-b55f-07cdcd709a6d","eventName":"click","eventTimestamp":"2025-06-14T01:36:10.417Z","userId":"user_pej5ild","sessionId":"session_mltrxqm","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","productId":"6","element_text":"Product 6\n                        Sleek design and","viewport_size":"659x935","page_title":"E-Commerce Store","session_id":"session_mltrxqm","language":"vi","element_type":"div","screen_resolution":"1920x1080","element_class":"card product_card","platform":"web","event_id":"75d200c5-65f6-4b3b-b55f-07cdcd709a6d","user_id":"user_pej5ild","event_name":"click","page_path":"/","page_referrer":"direct","track":"product_click","event_time":"2025-06-14T01:36:10.417Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:36:10.426 [http-nio-8080-exec-4 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 0, offset: 46
2025-06-14 08:36:10.426 [http-nio-8080-exec-4 INFO ] c.e.c.c.ClickstreamController - Successfully processed 3 events
2025-06-14 08:36:10.430 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/20 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.20.6dbc79f1-f595-4594-8c93-1606cea94c1f.tmp
2025-06-14 08:36:10.440 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.20.6dbc79f1-f595-4594-8c93-1606cea94c1f.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/20
2025-06-14 08:36:10.440 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 20. Metadata OffsetSeqMetadata(0,1749864970427,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:36:10.447 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:10.447 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:10.452 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:10.452 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:10.482 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:36:10.483 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 20 (start at ClickstreamProcessor.java:66) with 2 output partitions
2025-06-14 08:36:10.483 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 20 (start at ClickstreamProcessor.java:66)
2025-06-14 08:36:10.483 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:36:10.483 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:36:10.483 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 20 (MapPartitionsRDD[188] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:36:10.485 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_20 stored as values in memory (estimated size 39.4 KiB, free 9.2 GiB)
2025-06-14 08:36:10.487 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_20_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 9.2 GiB)
2025-06-14 08:36:10.487 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_20_piece0 in memory on phamviethoa:45719 (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:36:10.487 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 20 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:36:10.487 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 20 (MapPartitionsRDD[188] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0, 1))
2025-06-14 08:36:10.487 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 20.0 with 2 tasks resource profile 0
2025-06-14 08:36:10.488 [dispatcher-event-loop-15 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 20.0 (TID 32) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:36:10.488 [dispatcher-event-loop-15 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 20.0 (TID 33) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:36:10.488 [Executor task launch worker for task 1.0 in stage 20.0 (TID 33) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 20.0 (TID 33)
2025-06-14 08:36:10.488 [Executor task launch worker for task 0.0 in stage 20.0 (TID 32) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 20.0 (TID 32)
2025-06-14 08:36:10.496 [Executor task launch worker for task 1.0 in stage 20.0 (TID 33) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 45 for partition clickstream-events-0
2025-06-14 08:36:10.497 [Executor task launch worker for task 0.0 in stage 20.0 (TID 32) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 36 for partition clickstream-events-1
2025-06-14 08:36:10.497 [Executor task launch worker for task 1.0 in stage 20.0 (TID 33) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-14 08:36:10.498 [Executor task launch worker for task 0.0 in stage 20.0 (TID 32) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-14 08:36:11.000 [Executor task launch worker for task 1.0 in stage 20.0 (TID 33) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:36:11.000 [Executor task launch worker for task 0.0 in stage 20.0 (TID 32) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:36:11.000 [Executor task launch worker for task 1.0 in stage 20.0 (TID 33) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-14 08:36:11.000 [Executor task launch worker for task 0.0 in stage 20.0 (TID 32) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-14 08:36:11.000 [Executor task launch worker for task 1.0 in stage 20.0 (TID 33) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=47, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:36:11.000 [Executor task launch worker for task 0.0 in stage 20.0 (TID 32) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=40, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:36:11.007 [Executor task launch worker for task 1.0 in stage 20.0 (TID 33) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:11.008 [Executor task launch worker for task 0.0 in stage 20.0 (TID 32) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:11.012 [Executor task launch worker for task 1.0 in stage 20.0 (TID 33) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:11.012 [Executor task launch worker for task 1.0 in stage 20.0 (TID 33) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [2606a1fc-cb93-4b9f-9572-0e77fa3a57e7] (2 queries & 0 savepoints) is committed.
2025-06-14 08:36:11.012 [Executor task launch worker for task 1.0 in stage 20.0 (TID 33) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [472f43d6-7f59-4c73-bfe7-be40f5c19216] (0 queries & 0 savepoints) is committed.
2025-06-14 08:36:11.013 [Executor task launch worker for task 0.0 in stage 20.0 (TID 32) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:11.013 [Executor task launch worker for task 0.0 in stage 20.0 (TID 32) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [b0e60c1a-c6dc-4c75-ad61-03dcba212a90] (2 queries & 0 savepoints) is committed.
2025-06-14 08:36:11.013 [Executor task launch worker for task 0.0 in stage 20.0 (TID 32) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [6d5a893a-4de4-4fbe-bb9d-a48f6d454b68] (0 queries & 0 savepoints) is committed.
2025-06-14 08:36:11.013 [Executor task launch worker for task 1.0 in stage 20.0 (TID 33) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 20.0 (TID 33). 1646 bytes result sent to driver
2025-06-14 08:36:11.014 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 20.0 (TID 33) in 526 ms on phamviethoa (executor driver) (1/2)
2025-06-14 08:36:11.014 [Executor task launch worker for task 0.0 in stage 20.0 (TID 32) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 20.0 (TID 32). 1646 bytes result sent to driver
2025-06-14 08:36:11.016 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 20.0 (TID 32) in 528 ms on phamviethoa (executor driver) (2/2)
2025-06-14 08:36:11.016 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 20.0, whose tasks have all completed, from pool 
2025-06-14 08:36:11.016 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 20 (start at ClickstreamProcessor.java:66) finished in 0.533 s
2025-06-14 08:36:11.016 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:36:11.016 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 20: Stage finished
2025-06-14 08:36:11.016 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.spark.scheduler.DAGScheduler - Job 20 finished: start at ClickstreamProcessor.java:66, took 0.533759 s
2025-06-14 08:36:11.026 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/20 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.20.ef7f2adb-cb36-425e-a4fa-6e6c161f37be.tmp
2025-06-14 08:36:11.036 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.20.ef7f2adb-cb36-425e-a4fa-6e6c161f37be.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/20
2025-06-14 08:36:11.036 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:36:10.426Z",
  "batchId" : 20,
  "numInputRows" : 6,
  "inputRowsPerSecond" : 545.4545454545455,
  "processedRowsPerSecond" : 9.836065573770492,
  "durationMs" : {
    "addBatch" : 575,
    "commitOffsets" : 13,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 8,
    "triggerExecution" : 610,
    "walCommit" : 13
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 36,
        "0" : 45
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 40,
        "0" : 47
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 40,
        "0" : 47
      }
    },
    "numInputRows" : 6,
    "inputRowsPerSecond" : 545.4545454545455,
    "processedRowsPerSecond" : 9.836065573770492,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:36:14.019 [http-nio-8080-exec-5 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-14 08:36:14.019 [http-nio-8080-exec-5 INFO ] c.e.c.c.ClickstreamController - Received payload: {events=[{event_id=6d644510-a69f-47ff-b18b-76a594867eac, event_name=click, event_time=2025-06-14T01:36:11.555Z, user_id=user_pej5ild, session_id=session_mltrxqm, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 7
                        Compact and effi, element_type=div, element_name=null, track=product_click, productId=7}, {event_id=7d72a91d-8eee-4f3d-a922-10c9767ca734, event_name=click, event_time=2025-06-14T01:36:11.942Z, user_id=user_pej5ild, session_id=session_mltrxqm, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 8
                        Eco-friendly and, element_type=div, element_name=null, track=product_click, productId=8}, {event_id=938ad71c-1785-4816-bbf1-cd7c416aef6e, event_name=scroll, event_time=2025-06-14T01:36:14.017Z, user_id=user_pej5ild, session_id=session_mltrxqm, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=95}]}
2025-06-14 08:36:14.019 [http-nio-8080-exec-5 INFO ] c.e.c.c.ClickstreamController - Received 3 events
2025-06-14 08:36:14.019 [http-nio-8080-exec-5 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"6d644510-a69f-47ff-b18b-76a594867eac","eventName":"click","eventTimestamp":"2025-06-14T01:36:11.555Z","userId":"user_pej5ild","sessionId":"session_mltrxqm","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","productId":"7","element_text":"Product 7\n                        Compact and effi","viewport_size":"659x935","page_title":"E-Commerce Store","session_id":"session_mltrxqm","language":"vi","element_type":"div","screen_resolution":"1920x1080","element_class":"card product_card","platform":"web","event_id":"6d644510-a69f-47ff-b18b-76a594867eac","user_id":"user_pej5ild","event_name":"click","page_path":"/","page_referrer":"direct","track":"product_click","event_time":"2025-06-14T01:36:11.555Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:36:14.021 [http-nio-8080-exec-5 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 0, offset: 48
2025-06-14 08:36:14.021 [http-nio-8080-exec-5 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"7d72a91d-8eee-4f3d-a922-10c9767ca734","eventName":"click","eventTimestamp":"2025-06-14T01:36:11.942Z","userId":"user_pej5ild","sessionId":"session_mltrxqm","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","productId":"8","element_text":"Product 8\n                        Eco-friendly and","viewport_size":"659x935","page_title":"E-Commerce Store","session_id":"session_mltrxqm","language":"vi","element_type":"div","screen_resolution":"1920x1080","element_class":"card product_card","platform":"web","event_id":"7d72a91d-8eee-4f3d-a922-10c9767ca734","user_id":"user_pej5ild","event_name":"click","page_path":"/","page_referrer":"direct","track":"product_click","event_time":"2025-06-14T01:36:11.942Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:36:14.023 [http-nio-8080-exec-5 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 1, offset: 41
2025-06-14 08:36:14.023 [http-nio-8080-exec-5 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"938ad71c-1785-4816-bbf1-cd7c416aef6e","eventName":"scroll","eventTimestamp":"2025-06-14T01:36:14.017Z","userId":"user_pej5ild","sessionId":"session_mltrxqm","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","viewport_size":"659x935","page_title":"E-Commerce Store","session_id":"session_mltrxqm","language":"vi","screen_resolution":"1920x1080","platform":"web","scrollDepth":"95","event_id":"938ad71c-1785-4816-bbf1-cd7c416aef6e","user_id":"user_pej5ild","event_name":"scroll","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:36:14.017Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:36:14.024 [http-nio-8080-exec-5 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 1, offset: 43
2025-06-14 08:36:14.024 [http-nio-8080-exec-5 INFO ] c.e.c.c.ClickstreamController - Successfully processed 3 events
2025-06-14 08:36:14.030 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/21 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.21.bcf83391-a188-4379-80f1-962a02b0c85c.tmp
2025-06-14 08:36:14.040 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.21.bcf83391-a188-4379-80f1-962a02b0c85c.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/21
2025-06-14 08:36:14.040 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 21. Metadata OffsetSeqMetadata(0,1749864974027,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:36:14.048 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:14.048 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:14.053 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:14.054 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:14.087 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:36:14.087 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 21 (start at ClickstreamProcessor.java:66) with 2 output partitions
2025-06-14 08:36:14.087 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 21 (start at ClickstreamProcessor.java:66)
2025-06-14 08:36:14.087 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:36:14.087 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:36:14.087 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 21 (MapPartitionsRDD[197] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:36:14.089 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_21 stored as values in memory (estimated size 39.4 KiB, free 9.2 GiB)
2025-06-14 08:36:14.091 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_21_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 9.2 GiB)
2025-06-14 08:36:14.091 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_21_piece0 in memory on phamviethoa:45719 (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:36:14.091 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 21 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:36:14.091 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 21 (MapPartitionsRDD[197] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0, 1))
2025-06-14 08:36:14.091 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 21.0 with 2 tasks resource profile 0
2025-06-14 08:36:14.092 [dispatcher-event-loop-4 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 21.0 (TID 34) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:36:14.092 [dispatcher-event-loop-4 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 21.0 (TID 35) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:36:14.092 [Executor task launch worker for task 1.0 in stage 21.0 (TID 35) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 21.0 (TID 35)
2025-06-14 08:36:14.092 [Executor task launch worker for task 0.0 in stage 21.0 (TID 34) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 21.0 (TID 34)
2025-06-14 08:36:14.100 [Executor task launch worker for task 1.0 in stage 21.0 (TID 35) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 47 for partition clickstream-events-0
2025-06-14 08:36:14.100 [Executor task launch worker for task 0.0 in stage 21.0 (TID 34) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 40 for partition clickstream-events-1
2025-06-14 08:36:14.101 [Executor task launch worker for task 1.0 in stage 21.0 (TID 35) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-14 08:36:14.102 [Executor task launch worker for task 0.0 in stage 21.0 (TID 34) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-14 08:36:14.603 [Executor task launch worker for task 0.0 in stage 21.0 (TID 34) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:36:14.603 [Executor task launch worker for task 0.0 in stage 21.0 (TID 34) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-14 08:36:14.603 [Executor task launch worker for task 1.0 in stage 21.0 (TID 35) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:36:14.603 [Executor task launch worker for task 1.0 in stage 21.0 (TID 35) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-14 08:36:14.603 [Executor task launch worker for task 0.0 in stage 21.0 (TID 34) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=44, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:36:14.603 [Executor task launch worker for task 1.0 in stage 21.0 (TID 35) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=49, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:36:14.611 [Executor task launch worker for task 0.0 in stage 21.0 (TID 34) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:14.611 [Executor task launch worker for task 1.0 in stage 21.0 (TID 35) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:14.617 [Executor task launch worker for task 0.0 in stage 21.0 (TID 34) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:14.617 [Executor task launch worker for task 0.0 in stage 21.0 (TID 34) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [f91e16f2-e12e-4786-b6fb-78e41d139f28] (2 queries & 0 savepoints) is committed.
2025-06-14 08:36:14.617 [Executor task launch worker for task 0.0 in stage 21.0 (TID 34) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [31d99cd6-5f69-4e11-9021-051180263b4c] (0 queries & 0 savepoints) is committed.
2025-06-14 08:36:14.617 [Executor task launch worker for task 1.0 in stage 21.0 (TID 35) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:36:14.617 [Executor task launch worker for task 1.0 in stage 21.0 (TID 35) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [f028074d-f882-49d8-a5f3-32a26b9310ec] (2 queries & 0 savepoints) is committed.
2025-06-14 08:36:14.617 [Executor task launch worker for task 1.0 in stage 21.0 (TID 35) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [5def4bf7-2092-4dd3-b70a-8a3f5fb1c28d] (0 queries & 0 savepoints) is committed.
2025-06-14 08:36:14.618 [Executor task launch worker for task 0.0 in stage 21.0 (TID 34) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 21.0 (TID 34). 1646 bytes result sent to driver
2025-06-14 08:36:14.618 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 21.0 (TID 34) in 526 ms on phamviethoa (executor driver) (1/2)
2025-06-14 08:36:14.618 [Executor task launch worker for task 1.0 in stage 21.0 (TID 35) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 21.0 (TID 35). 1646 bytes result sent to driver
2025-06-14 08:36:14.620 [task-result-getter-3 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 21.0 (TID 35) in 528 ms on phamviethoa (executor driver) (2/2)
2025-06-14 08:36:14.620 [task-result-getter-3 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 21.0, whose tasks have all completed, from pool 
2025-06-14 08:36:14.620 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 21 (start at ClickstreamProcessor.java:66) finished in 0.533 s
2025-06-14 08:36:14.620 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:36:14.620 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 21: Stage finished
2025-06-14 08:36:14.620 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.spark.scheduler.DAGScheduler - Job 21 finished: start at ClickstreamProcessor.java:66, took 0.533438 s
2025-06-14 08:36:14.630 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/21 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.21.8f0897a9-2141-4495-a106-76869367da49.tmp
2025-06-14 08:36:14.641 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.21.8f0897a9-2141-4495-a106-76869367da49.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/21
2025-06-14 08:36:14.641 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:36:14.026Z",
  "batchId" : 21,
  "numInputRows" : 6,
  "inputRowsPerSecond" : 545.4545454545455,
  "processedRowsPerSecond" : 9.75609756097561,
  "durationMs" : {
    "addBatch" : 578,
    "commitOffsets" : 14,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 9,
    "triggerExecution" : 615,
    "walCommit" : 13
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 40,
        "0" : 47
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 44,
        "0" : 49
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 44,
        "0" : 49
      }
    },
    "numInputRows" : 6,
    "inputRowsPerSecond" : 545.4545454545455,
    "processedRowsPerSecond" : 9.75609756097561,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:36:24.646 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:36:24.645Z",
  "batchId" : 22,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 44,
        "0" : 49
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 44,
        "0" : 49
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 44,
        "0" : 49
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:36:34.650 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:36:34.649Z",
  "batchId" : 22,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 44,
        "0" : 49
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 44,
        "0" : 49
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 44,
        "0" : 49
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:36:44.661 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:36:44.659Z",
  "batchId" : 22,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 44,
        "0" : 49
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 44,
        "0" : 49
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 44,
        "0" : 49
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:36:54.666 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:36:54.665Z",
  "batchId" : 22,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 44,
        "0" : 49
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 44,
        "0" : 49
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 44,
        "0" : 49
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:36:59.420 [http-nio-8080-exec-6 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-14 08:36:59.420 [http-nio-8080-exec-6 INFO ] c.e.c.c.ClickstreamController - Received payload: {events=[{event_id=d1e0aa35-391b-423e-b479-1e5fbe229d85, event_name=click, event_time=2025-06-14T01:36:14.160Z, user_id=user_pej5ild, session_id=session_mltrxqm, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 9
                        Top-rated with e, element_type=div, element_name=null, track=product_click, productId=9}, {event_id=bcf57551-bd3c-4e9e-b2ce-7917d581386f, event_name=tab_change, event_time=2025-06-14T01:36:53.692Z, user_id=user_pej5ild, session_id=session_mltrxqm, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, is_visible=false, time_visible=0}, {event_id=f7559301-e29e-4d4e-b2ec-7f499b339e37, event_name=tab_change, event_time=2025-06-14T01:36:54.418Z, user_id=user_pej5ild, session_id=session_mltrxqm, platform=web, screen_resolution=1920x1080, viewport_size=659x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, is_visible=true, time_visible=0}]}
2025-06-14 08:36:59.420 [http-nio-8080-exec-6 INFO ] c.e.c.c.ClickstreamController - Received 3 events
2025-06-14 08:36:59.421 [http-nio-8080-exec-6 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"d1e0aa35-391b-423e-b479-1e5fbe229d85","eventName":"click","eventTimestamp":"2025-06-14T01:36:14.160Z","userId":"user_pej5ild","sessionId":"session_mltrxqm","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","productId":"9","element_text":"Product 9\n                        Top-rated with e","viewport_size":"659x935","page_title":"E-Commerce Store","session_id":"session_mltrxqm","language":"vi","element_type":"div","screen_resolution":"1920x1080","element_class":"card product_card","platform":"web","event_id":"d1e0aa35-391b-423e-b479-1e5fbe229d85","user_id":"user_pej5ild","event_name":"click","page_path":"/","page_referrer":"direct","track":"product_click","event_time":"2025-06-14T01:36:14.160Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:36:59.422 [http-nio-8080-exec-6 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 1, offset: 45
2025-06-14 08:36:59.426 [http-nio-8080-exec-6 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"bcf57551-bd3c-4e9e-b2ce-7917d581386f","eventName":"tab_change","eventTimestamp":"2025-06-14T01:36:53.692Z","userId":"user_pej5ild","sessionId":"session_mltrxqm","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","is_visible":"false","viewport_size":"659x935","page_title":"E-Commerce Store","session_id":"session_mltrxqm","language":"vi","screen_resolution":"1920x1080","platform":"web","time_visible":"0","event_id":"bcf57551-bd3c-4e9e-b2ce-7917d581386f","user_id":"user_pej5ild","event_name":"tab_change","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:36:53.692Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:36:59.427 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/22 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.22.fff7f58e-4865-4887-8f3b-5a30b93f22b4.tmp
2025-06-14 08:36:59.428 [http-nio-8080-exec-6 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 1, offset: 47
2025-06-14 08:36:59.428 [http-nio-8080-exec-6 INFO ] c.e.c.producer.ClickstreamProducer - Preparing to send event: {"eventId":"f7559301-e29e-4d4e-b2ec-7f499b339e37","eventName":"tab_change","eventTimestamp":"2025-06-14T01:36:54.418Z","userId":"user_pej5ild","sessionId":"session_mltrxqm","appId":null,"platform":"web","pageUrl":"http://localhost/#productList","eventParams":{"page_url":"http://localhost/#productList","is_visible":"true","viewport_size":"659x935","page_title":"E-Commerce Store","session_id":"session_mltrxqm","language":"vi","screen_resolution":"1920x1080","platform":"web","time_visible":"0","event_id":"f7559301-e29e-4d4e-b2ec-7f499b339e37","user_id":"user_pej5ild","event_name":"tab_change","page_path":"/","page_referrer":"direct","event_time":"2025-06-14T01:36:54.418Z","user_agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0"}}
2025-06-14 08:36:59.429 [http-nio-8080-exec-6 INFO ] c.e.c.producer.ClickstreamProducer - Event sent to Kafka topic: clickstream-events, partition: 0, offset: 50
2025-06-14 08:36:59.430 [http-nio-8080-exec-6 INFO ] c.e.c.c.ClickstreamController - Successfully processed 3 events
2025-06-14 08:36:59.440 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.22.fff7f58e-4865-4887-8f3b-5a30b93f22b4.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/22
2025-06-14 08:36:59.440 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 22. Metadata OffsetSeqMetadata(0,1749865019424,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:36:59.446 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:59.446 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:59.450 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:59.451 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:36:59.477 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:36:59.477 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 22 (start at ClickstreamProcessor.java:66) with 1 output partitions
2025-06-14 08:36:59.477 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 22 (start at ClickstreamProcessor.java:66)
2025-06-14 08:36:59.478 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:36:59.478 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:36:59.478 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 22 (MapPartitionsRDD[206] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:36:59.479 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_22 stored as values in memory (estimated size 39.3 KiB, free 9.2 GiB)
2025-06-14 08:36:59.481 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_22_piece0 stored as bytes in memory (estimated size 17.5 KiB, free 9.2 GiB)
2025-06-14 08:36:59.482 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_22_piece0 in memory on phamviethoa:45719 (size: 17.5 KiB, free: 9.2 GiB)
2025-06-14 08:36:59.482 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 22 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:36:59.482 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[206] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0))
2025-06-14 08:36:59.482 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 22.0 with 1 tasks resource profile 0
2025-06-14 08:36:59.482 [dispatcher-event-loop-0 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 22.0 (TID 36) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:36:59.483 [Executor task launch worker for task 0.0 in stage 22.0 (TID 36) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 22.0 (TID 36)
2025-06-14 08:36:59.491 [Executor task launch worker for task 0.0 in stage 22.0 (TID 36) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 44 for partition clickstream-events-1
2025-06-14 08:36:59.492 [Executor task launch worker for task 0.0 in stage 22.0 (TID 36) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-14 08:36:59.994 [Executor task launch worker for task 0.0 in stage 22.0 (TID 36) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:36:59.994 [Executor task launch worker for task 0.0 in stage 22.0 (TID 36) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-14 08:36:59.994 [Executor task launch worker for task 0.0 in stage 22.0 (TID 36) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=48, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:37:00.002 [Executor task launch worker for task 0.0 in stage 22.0 (TID 36) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:37:00.006 [Executor task launch worker for task 0.0 in stage 22.0 (TID 36) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:37:00.006 [Executor task launch worker for task 0.0 in stage 22.0 (TID 36) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [86347ec4-b8a9-458e-838b-4de0ca0effbb] (2 queries & 0 savepoints) is committed.
2025-06-14 08:37:00.006 [Executor task launch worker for task 0.0 in stage 22.0 (TID 36) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [e94c5cde-bb23-4a48-bf07-80d4b5d3cef4] (0 queries & 0 savepoints) is committed.
2025-06-14 08:37:00.007 [Executor task launch worker for task 0.0 in stage 22.0 (TID 36) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 22.0 (TID 36). 1646 bytes result sent to driver
2025-06-14 08:37:00.007 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 22.0 (TID 36) in 525 ms on phamviethoa (executor driver) (1/1)
2025-06-14 08:37:00.007 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 22.0, whose tasks have all completed, from pool 
2025-06-14 08:37:00.007 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 22 (start at ClickstreamProcessor.java:66) finished in 0.529 s
2025-06-14 08:37:00.007 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:37:00.007 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 22: Stage finished
2025-06-14 08:37:00.008 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.spark.scheduler.DAGScheduler - Job 22 finished: start at ClickstreamProcessor.java:66, took 0.530344 s
2025-06-14 08:37:00.018 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/22 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.22.09853d03-d362-4466-ba27-a74726cd8204.tmp
2025-06-14 08:37:00.028 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.22.09853d03-d362-4466-ba27-a74726cd8204.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/22
2025-06-14 08:37:00.029 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:36:59.423Z",
  "batchId" : 22,
  "numInputRows" : 2,
  "inputRowsPerSecond" : 181.81818181818184,
  "processedRowsPerSecond" : 3.3057851239669422,
  "durationMs" : {
    "addBatch" : 568,
    "commitOffsets" : 13,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 6,
    "triggerExecution" : 605,
    "walCommit" : 16
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 44,
        "0" : 49
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 46,
        "0" : 49
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 46,
        "0" : 49
      }
    },
    "numInputRows" : 2,
    "inputRowsPerSecond" : 181.81818181818184,
    "processedRowsPerSecond" : 3.3057851239669422,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:37:00.032 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/23 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.23.5dc476da-234f-4094-90ca-e6c010bfaedb.tmp
2025-06-14 08:37:00.042 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/.23.5dc476da-234f-4094-90ca-e6c010bfaedb.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/offsets/23
2025-06-14 08:37:00.042 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 23. Metadata OffsetSeqMetadata(0,1749865020030,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 8))
2025-06-14 08:37:00.047 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:37:00.048 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:37:00.052 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:37:00.053 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-14 08:37:00.080 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] org.apache.spark.SparkContext - Starting job: start at ClickstreamProcessor.java:66
2025-06-14 08:37:00.081 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 23 (start at ClickstreamProcessor.java:66) with 2 output partitions
2025-06-14 08:37:00.081 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 23 (start at ClickstreamProcessor.java:66)
2025-06-14 08:37:00.081 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-14 08:37:00.081 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-14 08:37:00.081 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 23 (MapPartitionsRDD[215] at start at ClickstreamProcessor.java:66), which has no missing parents
2025-06-14 08:37:00.083 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_23 stored as values in memory (estimated size 39.4 KiB, free 9.2 GiB)
2025-06-14 08:37:00.085 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_23_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 9.2 GiB)
2025-06-14 08:37:00.085 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_23_piece0 in memory on phamviethoa:45719 (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:37:00.085 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 23 from broadcast at DAGScheduler.scala:1535
2025-06-14 08:37:00.085 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 23 (MapPartitionsRDD[215] at start at ClickstreamProcessor.java:66) (first 15 tasks are for partitions Vector(0, 1))
2025-06-14 08:37:00.085 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 23.0 with 2 tasks resource profile 0
2025-06-14 08:37:00.086 [dispatcher-event-loop-1 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 23.0 (TID 37) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:37:00.086 [dispatcher-event-loop-1 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 23.0 (TID 38) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8313 bytes) 
2025-06-14 08:37:00.086 [Executor task launch worker for task 0.0 in stage 23.0 (TID 37) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 23.0 (TID 37)
2025-06-14 08:37:00.086 [Executor task launch worker for task 1.0 in stage 23.0 (TID 38) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 23.0 (TID 38)
2025-06-14 08:37:00.093 [Executor task launch worker for task 1.0 in stage 23.0 (TID 38) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to offset 49 for partition clickstream-events-0
2025-06-14 08:37:00.094 [Executor task launch worker for task 1.0 in stage 23.0 (TID 38) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-14 08:37:00.098 [Executor task launch worker for task 0.0 in stage 23.0 (TID 37) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:37:00.103 [Executor task launch worker for task 0.0 in stage 23.0 (TID 37) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:37:00.103 [Executor task launch worker for task 0.0 in stage 23.0 (TID 37) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [cc05124f-3f38-4e38-bc93-95ca5a2c5b49] (2 queries & 0 savepoints) is committed.
2025-06-14 08:37:00.103 [Executor task launch worker for task 0.0 in stage 23.0 (TID 37) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [55ebd758-15da-4690-9f14-2c2281513455] (0 queries & 0 savepoints) is committed.
2025-06-14 08:37:00.103 [Executor task launch worker for task 0.0 in stage 23.0 (TID 37) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 23.0 (TID 37). 1646 bytes result sent to driver
2025-06-14 08:37:00.104 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 23.0 (TID 37) in 18 ms on phamviethoa (executor driver) (1/2)
2025-06-14 08:37:00.595 [Executor task launch worker for task 1.0 in stage 23.0 (TID 38) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:37:00.595 [Executor task launch worker for task 1.0 in stage 23.0 (TID 38) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-14 08:37:00.595 [Executor task launch worker for task 1.0 in stage 23.0 (TID 38) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=51, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-14 08:37:00.602 [Executor task launch worker for task 1.0 in stage 23.0 (TID 38) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:37:00.606 [Executor task launch worker for task 1.0 in stage 23.0 (TID 38) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-14 08:37:00.606 [Executor task launch worker for task 1.0 in stage 23.0 (TID 38) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [d5b83ca7-25ef-49b7-a486-2320e30fbd18] (2 queries & 0 savepoints) is committed.
2025-06-14 08:37:00.606 [Executor task launch worker for task 1.0 in stage 23.0 (TID 38) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [157d062a-54cc-4d7e-a176-d71801750a8d] (0 queries & 0 savepoints) is committed.
2025-06-14 08:37:00.608 [Executor task launch worker for task 1.0 in stage 23.0 (TID 38) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 23.0 (TID 38). 1689 bytes result sent to driver
2025-06-14 08:37:00.608 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 23.0 (TID 38) in 522 ms on phamviethoa (executor driver) (2/2)
2025-06-14 08:37:00.608 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 23.0, whose tasks have all completed, from pool 
2025-06-14 08:37:00.608 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 23 (start at ClickstreamProcessor.java:66) finished in 0.527 s
2025-06-14 08:37:00.608 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-14 08:37:00.608 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 23: Stage finished
2025-06-14 08:37:00.609 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.spark.scheduler.DAGScheduler - Job 23 finished: start at ClickstreamProcessor.java:66, took 0.528261 s
2025-06-14 08:37:00.618 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/23 using temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.23.7958b279-5bcf-41c1-b2d4-dfc716b66d6c.tmp
2025-06-14 08:37:00.627 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/.23.7958b279-5bcf-41c1-b2d4-dfc716b66d6c.tmp to file:/tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c/commits/23
2025-06-14 08:37:00.628 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:37:00.029Z",
  "batchId" : 23,
  "numInputRows" : 4,
  "inputRowsPerSecond" : 6.600660066006601,
  "processedRowsPerSecond" : 6.688963210702341,
  "durationMs" : {
    "addBatch" : 566,
    "commitOffsets" : 12,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 6,
    "triggerExecution" : 598,
    "walCommit" : 12
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 46,
        "0" : 49
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 48,
        "0" : 51
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 48,
        "0" : 51
      }
    },
    "numInputRows" : 4,
    "inputRowsPerSecond" : 6.600660066006601,
    "processedRowsPerSecond" : 6.688963210702341,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:37:10.597 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_23_piece0 on phamviethoa:45719 in memory (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:37:10.598 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_22_piece0 on phamviethoa:45719 in memory (size: 17.5 KiB, free: 9.2 GiB)
2025-06-14 08:37:10.600 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_20_piece0 on phamviethoa:45719 in memory (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:37:10.602 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_15_piece0 on phamviethoa:45719 in memory (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:37:10.603 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_21_piece0 on phamviethoa:45719 in memory (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:37:10.604 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_16_piece0 on phamviethoa:45719 in memory (size: 17.5 KiB, free: 9.2 GiB)
2025-06-14 08:37:10.606 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_18_piece0 on phamviethoa:45719 in memory (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:37:10.606 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_13_piece0 on phamviethoa:45719 in memory (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:37:10.607 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_17_piece0 on phamviethoa:45719 in memory (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:37:10.609 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_19_piece0 on phamviethoa:45719 in memory (size: 17.6 KiB, free: 9.2 GiB)
2025-06-14 08:37:10.610 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_14_piece0 on phamviethoa:45719 in memory (size: 17.5 KiB, free: 9.2 GiB)
2025-06-14 08:37:10.630 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:37:10.629Z",
  "batchId" : 24,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 48,
        "0" : 51
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 48,
        "0" : 51
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 48,
        "0" : 51
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:37:20.638 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:37:20.637Z",
  "batchId" : 24,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 48,
        "0" : 51
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 48,
        "0" : 51
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 48,
        "0" : 51
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:37:30.643 [stream execution thread for [id = 5fa48a61-4890-4e02-a30a-eb665f4dc041, runId = ecb886d1-4aa2-4748-b80b-42940dc57c9f] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5fa48a61-4890-4e02-a30a-eb665f4dc041",
  "runId" : "ecb886d1-4aa2-4748-b80b-42940dc57c9f",
  "name" : null,
  "timestamp" : "2025-06-14T01:37:30.642Z",
  "batchId" : 24,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 48,
        "0" : 51
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 48,
        "0" : 51
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 48,
        "0" : 51
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-14 08:37:40.333 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-14 08:37:40.333 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-14 08:37:40.337 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-14 08:37:40.337 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-14 08:37:40.337 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-14 08:37:40.337 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-14 08:37:40.338 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-14 08:37:40.340 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-1 unregistered
2025-06-14 08:37:40.340 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-14 08:37:40.340 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2, groupId=spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-14 08:37:40.340 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-14 08:37:40.340 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-14 08:37:40.340 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-14 08:37:40.340 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-14 08:37:40.341 [SpringApplicationShutdownHook INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@8e09dcb{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-14 08:37:40.342 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-f5b3fdfe-e48b-4d11-ae38-5f15e0a78864-1892933956-executor-2 unregistered
2025-06-14 08:37:40.342 [shutdown-hook-0 INFO ] o.a.spark.storage.DiskBlockManager - Shutdown hook called
2025-06-14 08:37:40.342 [SpringApplicationShutdownHook INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-14 08:37:40.344 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-14 08:37:40.345 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-455cdf38-43d9-4545-b3a7-4fc05aee017c
2025-06-14 08:37:40.347 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-86df6411-1508-4ce3-ab30-f3ed61c14f7a/userFiles-ae9c61fd-34bd-4d2d-84f7-c079a89f9062
2025-06-14 08:37:40.348 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-86df6411-1508-4ce3-ab30-f3ed61c14f7a
2025-06-14 08:37:40.350 [dispatcher-event-loop-13 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-14 08:37:40.354 [SpringApplicationShutdownHook INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-14 08:37:40.354 [SpringApplicationShutdownHook INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-14 08:37:40.355 [SpringApplicationShutdownHook INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-14 08:37:40.357 [dispatcher-event-loop-15 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-14 08:37:40.359 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-14 08:37:40.360 [SpringApplicationShutdownHook INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-06-14 08:37:40.361 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-14 08:37:40.361 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-14 08:37:40.361 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-14 08:37:40.361 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-14 08:37:40.361 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-1 unregistered
