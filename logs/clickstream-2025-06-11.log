2025-06-11 07:41:09.693 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 14479 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-11 07:41:09.700 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-11 07:41:12.207 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-11 07:41:12.215 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-11 07:41:12.216 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-11 07:41:12.216 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-11 07:41:12.313 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-11 07:41:12.313 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 2549 ms
2025-06-11 07:41:12.633 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-11 07:41:12.661 [main WARN ] o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'requestMappingHandlerMapping' defined in class path resource [org/springframework/boot/autoconfigure/web/servlet/WebMvcAutoConfiguration$EnableWebMvcConfiguration.class]: Invocation of init method failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'app.cors.allowed-origins' in value "${app.cors.allowed-origins}"
2025-06-11 07:41:12.664 [main INFO ] o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-06-11 07:41:12.698 [main INFO ] o.s.b.a.l.ConditionEvaluationReportLoggingListener - 

Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
2025-06-11 07:41:12.715 [main ERROR] o.s.boot.SpringApplication - Application run failed
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'requestMappingHandlerMapping' defined in class path resource [org/springframework/boot/autoconfigure/web/servlet/WebMvcAutoConfiguration$EnableWebMvcConfiguration.class]: Invocation of init method failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'app.cors.allowed-origins' in value "${app.cors.allowed-origins}"
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1804)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:620)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:955)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:929)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:591)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:147)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:732)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:409)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:308)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1300)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1289)
	at com.example.Application.main(Application.java:10)
Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'app.cors.allowed-origins' in value "${app.cors.allowed-origins}"
	at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:180)
	at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:126)
	at org.springframework.core.env.AbstractPropertyResolver.doResolvePlaceholders(AbstractPropertyResolver.java:239)
	at org.springframework.core.env.AbstractPropertyResolver.resolveRequiredPlaceholders(AbstractPropertyResolver.java:210)
	at org.springframework.context.support.PropertySourcesPlaceholderConfigurer.lambda$processProperties$0(PropertySourcesPlaceholderConfigurer.java:191)
	at org.springframework.beans.factory.support.AbstractBeanFactory.resolveEmbeddedValue(AbstractBeanFactory.java:936)
	at org.springframework.beans.factory.config.EmbeddedValueResolver.resolveStringValue(EmbeddedValueResolver.java:54)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.resolveCorsAnnotationValue(RequestMappingHandlerMapping.java:511)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.updateCorsConfig(RequestMappingHandlerMapping.java:477)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.initCorsConfiguration(RequestMappingHandlerMapping.java:461)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.initCorsConfiguration(RequestMappingHandlerMapping.java:76)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping$MappingRegistry.register(AbstractHandlerMethodMapping.java:648)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.registerHandlerMethod(AbstractHandlerMethodMapping.java:332)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.registerHandlerMethod(RequestMappingHandlerMapping.java:420)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.registerHandlerMethod(RequestMappingHandlerMapping.java:76)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.lambda$detectHandlerMethods$2(AbstractHandlerMethodMapping.java:299)
	at java.base/java.util.LinkedHashMap.forEach(LinkedHashMap.java:684)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.detectHandlerMethods(AbstractHandlerMethodMapping.java:297)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.processCandidateBean(AbstractHandlerMethodMapping.java:266)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.initHandlerMethods(AbstractHandlerMethodMapping.java:225)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.afterPropertiesSet(AbstractHandlerMethodMapping.java:213)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.afterPropertiesSet(RequestMappingHandlerMapping.java:205)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1863)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1800)
	... 16 common frames omitted
2025-06-11 07:44:45.258 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 16535 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-11 07:44:45.261 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-11 07:44:46.882 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-11 07:44:46.889 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-11 07:44:46.890 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-11 07:44:46.890 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-11 07:44:47.004 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-11 07:44:47.004 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1637 ms
2025-06-11 07:44:47.385 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-11 07:44:47.413 [main WARN ] o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'requestMappingHandlerMapping' defined in class path resource [org/springframework/boot/autoconfigure/web/servlet/WebMvcAutoConfiguration$EnableWebMvcConfiguration.class]: Invocation of init method failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'app.cors.allowed-origins' in value "${app.cors.allowed-origins}"
2025-06-11 07:44:47.416 [main INFO ] o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-06-11 07:44:47.445 [main INFO ] o.s.b.a.l.ConditionEvaluationReportLoggingListener - 

Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
2025-06-11 07:44:47.472 [main ERROR] o.s.boot.SpringApplication - Application run failed
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'requestMappingHandlerMapping' defined in class path resource [org/springframework/boot/autoconfigure/web/servlet/WebMvcAutoConfiguration$EnableWebMvcConfiguration.class]: Invocation of init method failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'app.cors.allowed-origins' in value "${app.cors.allowed-origins}"
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1804)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:620)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:955)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:929)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:591)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:147)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:732)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:409)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:308)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1300)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1289)
	at com.example.Application.main(Application.java:10)
Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'app.cors.allowed-origins' in value "${app.cors.allowed-origins}"
	at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:180)
	at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:126)
	at org.springframework.core.env.AbstractPropertyResolver.doResolvePlaceholders(AbstractPropertyResolver.java:239)
	at org.springframework.core.env.AbstractPropertyResolver.resolveRequiredPlaceholders(AbstractPropertyResolver.java:210)
	at org.springframework.context.support.PropertySourcesPlaceholderConfigurer.lambda$processProperties$0(PropertySourcesPlaceholderConfigurer.java:191)
	at org.springframework.beans.factory.support.AbstractBeanFactory.resolveEmbeddedValue(AbstractBeanFactory.java:936)
	at org.springframework.beans.factory.config.EmbeddedValueResolver.resolveStringValue(EmbeddedValueResolver.java:54)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.resolveCorsAnnotationValue(RequestMappingHandlerMapping.java:511)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.updateCorsConfig(RequestMappingHandlerMapping.java:477)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.initCorsConfiguration(RequestMappingHandlerMapping.java:461)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.initCorsConfiguration(RequestMappingHandlerMapping.java:76)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping$MappingRegistry.register(AbstractHandlerMethodMapping.java:648)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.registerHandlerMethod(AbstractHandlerMethodMapping.java:332)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.registerHandlerMethod(RequestMappingHandlerMapping.java:420)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.registerHandlerMethod(RequestMappingHandlerMapping.java:76)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.lambda$detectHandlerMethods$2(AbstractHandlerMethodMapping.java:299)
	at java.base/java.util.LinkedHashMap.forEach(LinkedHashMap.java:684)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.detectHandlerMethods(AbstractHandlerMethodMapping.java:297)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.processCandidateBean(AbstractHandlerMethodMapping.java:266)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.initHandlerMethods(AbstractHandlerMethodMapping.java:225)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.afterPropertiesSet(AbstractHandlerMethodMapping.java:213)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.afterPropertiesSet(RequestMappingHandlerMapping.java:205)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1863)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1800)
	... 16 common frames omitted
2025-06-11 07:47:48.464 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 18217 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-11 07:47:48.467 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-11 07:47:49.541 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-11 07:47:49.547 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-11 07:47:49.548 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-11 07:47:49.548 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-11 07:47:49.632 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-11 07:47:49.632 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1114 ms
2025-06-11 07:47:49.932 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-11 07:47:49.960 [main WARN ] o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'requestMappingHandlerMapping' defined in class path resource [org/springframework/boot/autoconfigure/web/servlet/WebMvcAutoConfiguration$EnableWebMvcConfiguration.class]: Invocation of init method failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'app.cors.allowed-origins' in value "${app.cors.allowed-origins}"
2025-06-11 07:47:49.963 [main INFO ] o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-06-11 07:47:49.991 [main INFO ] o.s.b.a.l.ConditionEvaluationReportLoggingListener - 

Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
2025-06-11 07:47:50.007 [main ERROR] o.s.boot.SpringApplication - Application run failed
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'requestMappingHandlerMapping' defined in class path resource [org/springframework/boot/autoconfigure/web/servlet/WebMvcAutoConfiguration$EnableWebMvcConfiguration.class]: Invocation of init method failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'app.cors.allowed-origins' in value "${app.cors.allowed-origins}"
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1804)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:620)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:955)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:929)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:591)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:147)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:732)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:409)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:308)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1300)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1289)
	at com.example.Application.main(Application.java:10)
Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'app.cors.allowed-origins' in value "${app.cors.allowed-origins}"
	at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:180)
	at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:126)
	at org.springframework.core.env.AbstractPropertyResolver.doResolvePlaceholders(AbstractPropertyResolver.java:239)
	at org.springframework.core.env.AbstractPropertyResolver.resolveRequiredPlaceholders(AbstractPropertyResolver.java:210)
	at org.springframework.context.support.PropertySourcesPlaceholderConfigurer.lambda$processProperties$0(PropertySourcesPlaceholderConfigurer.java:191)
	at org.springframework.beans.factory.support.AbstractBeanFactory.resolveEmbeddedValue(AbstractBeanFactory.java:936)
	at org.springframework.beans.factory.config.EmbeddedValueResolver.resolveStringValue(EmbeddedValueResolver.java:54)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.resolveCorsAnnotationValue(RequestMappingHandlerMapping.java:511)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.updateCorsConfig(RequestMappingHandlerMapping.java:477)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.initCorsConfiguration(RequestMappingHandlerMapping.java:461)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.initCorsConfiguration(RequestMappingHandlerMapping.java:76)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping$MappingRegistry.register(AbstractHandlerMethodMapping.java:648)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.registerHandlerMethod(AbstractHandlerMethodMapping.java:332)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.registerHandlerMethod(RequestMappingHandlerMapping.java:420)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.registerHandlerMethod(RequestMappingHandlerMapping.java:76)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.lambda$detectHandlerMethods$2(AbstractHandlerMethodMapping.java:299)
	at java.base/java.util.LinkedHashMap.forEach(LinkedHashMap.java:684)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.detectHandlerMethods(AbstractHandlerMethodMapping.java:297)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.processCandidateBean(AbstractHandlerMethodMapping.java:266)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.initHandlerMethods(AbstractHandlerMethodMapping.java:225)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.afterPropertiesSet(AbstractHandlerMethodMapping.java:213)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.afterPropertiesSet(RequestMappingHandlerMapping.java:205)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1863)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1800)
	... 16 common frames omitted
2025-06-11 07:53:02.125 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 23611 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-11 07:53:02.127 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-11 07:53:03.468 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-11 07:53:03.473 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-11 07:53:03.474 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-11 07:53:03.474 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-11 07:53:03.833 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-11 07:53:03.833 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1662 ms
2025-06-11 07:53:04.222 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-11 07:53:04.263 [main WARN ] o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'requestMappingHandlerMapping' defined in class path resource [org/springframework/boot/autoconfigure/web/servlet/WebMvcAutoConfiguration$EnableWebMvcConfiguration.class]: Invocation of init method failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'app.cors.allowed-origins' in value "${app.cors.allowed-origins}"
2025-06-11 07:53:04.270 [main INFO ] o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-06-11 07:53:04.319 [main INFO ] o.s.b.a.l.ConditionEvaluationReportLoggingListener - 

Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
2025-06-11 07:53:04.356 [main ERROR] o.s.boot.SpringApplication - Application run failed
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'requestMappingHandlerMapping' defined in class path resource [org/springframework/boot/autoconfigure/web/servlet/WebMvcAutoConfiguration$EnableWebMvcConfiguration.class]: Invocation of init method failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'app.cors.allowed-origins' in value "${app.cors.allowed-origins}"
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1804)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:620)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:955)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:929)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:591)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:147)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:732)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:409)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:308)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1300)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1289)
	at com.example.Application.main(Application.java:10)
Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'app.cors.allowed-origins' in value "${app.cors.allowed-origins}"
	at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:180)
	at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:126)
	at org.springframework.core.env.AbstractPropertyResolver.doResolvePlaceholders(AbstractPropertyResolver.java:239)
	at org.springframework.core.env.AbstractPropertyResolver.resolveRequiredPlaceholders(AbstractPropertyResolver.java:210)
	at org.springframework.context.support.PropertySourcesPlaceholderConfigurer.lambda$processProperties$0(PropertySourcesPlaceholderConfigurer.java:191)
	at org.springframework.beans.factory.support.AbstractBeanFactory.resolveEmbeddedValue(AbstractBeanFactory.java:936)
	at org.springframework.beans.factory.config.EmbeddedValueResolver.resolveStringValue(EmbeddedValueResolver.java:54)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.resolveCorsAnnotationValue(RequestMappingHandlerMapping.java:511)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.updateCorsConfig(RequestMappingHandlerMapping.java:477)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.initCorsConfiguration(RequestMappingHandlerMapping.java:461)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.initCorsConfiguration(RequestMappingHandlerMapping.java:76)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping$MappingRegistry.register(AbstractHandlerMethodMapping.java:648)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.registerHandlerMethod(AbstractHandlerMethodMapping.java:332)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.registerHandlerMethod(RequestMappingHandlerMapping.java:420)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.registerHandlerMethod(RequestMappingHandlerMapping.java:76)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.lambda$detectHandlerMethods$2(AbstractHandlerMethodMapping.java:299)
	at java.base/java.util.LinkedHashMap.forEach(LinkedHashMap.java:684)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.detectHandlerMethods(AbstractHandlerMethodMapping.java:297)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.processCandidateBean(AbstractHandlerMethodMapping.java:266)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.initHandlerMethods(AbstractHandlerMethodMapping.java:225)
	at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.afterPropertiesSet(AbstractHandlerMethodMapping.java:213)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping.afterPropertiesSet(RequestMappingHandlerMapping.java:205)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1863)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1800)
	... 16 common frames omitted
2025-06-11 07:53:41.088 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 24189 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-11 07:53:41.090 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-11 07:53:42.385 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-11 07:53:42.390 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-11 07:53:42.391 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-11 07:53:42.391 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-11 07:53:42.477 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-11 07:53:42.477 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1340 ms
2025-06-11 07:53:42.783 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-11 07:53:43.116 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-11 07:53:43.155 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-11 07:53:43.191 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-11 07:53:43.192 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-11 07:53:43.192 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749603223190
2025-06-11 07:53:43.733 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-11 07:53:43.738 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-11 07:53:43.738 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-11 07:53:43.738 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-11 07:53:43.746 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-11 07:53:43.754 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-11 07:53:43.769 [main INFO ] com.example.Application - Started Application in 3.01 seconds (JVM running for 3.774)
2025-06-11 07:53:44.521 [RMI TCP Connection(3)-10.13.25.123 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-11 07:53:44.521 [RMI TCP Connection(3)-10.13.25.123 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-11 07:53:44.522 [RMI TCP Connection(3)-10.13.25.123 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-11 07:58:02.204 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 31619 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-11 07:58:02.207 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-11 07:58:03.407 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-11 07:58:03.413 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-11 07:58:03.414 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-11 07:58:03.414 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-11 07:58:03.535 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-11 07:58:03.535 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1287 ms
2025-06-11 07:58:03.824 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-11 07:58:04.119 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-11 07:58:04.148 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-11 07:58:04.182 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-11 07:58:04.183 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-11 07:58:04.183 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749603484181
2025-06-11 07:58:04.720 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-11 07:58:04.726 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-11 07:58:04.726 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-11 07:58:04.726 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-11 07:58:04.736 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-11 07:58:04.744 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-11 07:58:04.761 [main INFO ] com.example.Application - Started Application in 2.901 seconds (JVM running for 3.685)
2025-06-11 07:58:05.335 [RMI TCP Connection(4)-10.13.25.123 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-11 07:58:05.335 [RMI TCP Connection(4)-10.13.25.123 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-11 07:58:05.337 [RMI TCP Connection(4)-10.13.25.123 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 2 ms
2025-06-11 07:59:25.855 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 32869 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-11 07:59:25.858 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-11 07:59:26.979 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-11 07:59:26.984 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-11 07:59:26.985 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-11 07:59:26.985 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-11 07:59:27.065 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-11 07:59:27.065 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1160 ms
2025-06-11 07:59:27.326 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-11 07:59:27.626 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-11 07:59:27.652 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-11 07:59:27.679 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-11 07:59:27.680 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-11 07:59:27.680 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749603567678
2025-06-11 07:59:27.868 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-11 07:59:27.880 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-11 07:59:27.880 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-11 07:59:27.881 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-11 07:59:27.895 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-11 07:59:27.911 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-11 07:59:27.929 [main INFO ] com.example.Application - Started Application in 2.387 seconds (JVM running for 3.086)
2025-06-11 07:59:28.132 [RMI TCP Connection(3)-10.13.25.123 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-11 07:59:28.132 [RMI TCP Connection(3)-10.13.25.123 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-11 07:59:28.135 [RMI TCP Connection(3)-10.13.25.123 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 3 ms
2025-06-11 08:19:14.785 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 44848 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-11 08:19:14.789 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-11 08:19:15.921 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-11 08:19:15.927 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-11 08:19:15.928 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-11 08:19:15.928 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-11 08:19:16.021 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-11 08:19:16.022 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1192 ms
2025-06-11 08:19:16.320 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-11 08:19:16.633 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-11 08:19:16.666 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-11 08:19:16.702 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-11 08:19:16.703 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-11 08:19:16.703 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749604756701
2025-06-11 08:19:16.725 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:16.728 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:16.831 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:16.832 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:17.035 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:17.035 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:17.338 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:17.339 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:17.743 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:17.743 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:18.750 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:18.750 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:19.758 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:19.758 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:20.665 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:20.666 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:21.572 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:21.573 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:22.379 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:22.380 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:23.388 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:23.388 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:24.395 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:24.395 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:25.301 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:25.302 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:26.208 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:26.208 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:27.215 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:27.215 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:28.223 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:28.224 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:29.230 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:29.230 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:30.036 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:30.036 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:30.942 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:30.942 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:31.948 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:31.949 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:32.855 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:32.855 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:33.862 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:33.862 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:34.869 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:34.870 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:35.877 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:35.878 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:36.884 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:36.884 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:37.890 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:37.891 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:38.897 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:38.898 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:39.804 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:39.804 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:40.811 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:40.811 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:19:41.816 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-11 08:19:41.817 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-11 08:20:14.216 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 47685 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-11 08:20:14.220 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-11 08:20:15.737 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-11 08:20:15.743 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-11 08:20:15.744 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-11 08:20:15.745 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-11 08:20:16.059 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-11 08:20:16.060 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1790 ms
2025-06-11 08:20:16.375 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-11 08:20:16.696 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-11 08:20:16.727 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-11 08:20:16.766 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-11 08:20:16.768 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-11 08:20:16.768 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749604816766
2025-06-11 08:20:17.286 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-11 08:20:17.295 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-11 08:20:17.295 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-11 08:20:17.296 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-11 08:20:17.306 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-11 08:20:17.317 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-11 08:20:17.338 [main INFO ] com.example.Application - Started Application in 3.562 seconds (JVM running for 4.979)
2025-06-11 08:20:17.791 [RMI TCP Connection(1)-192.168.123.7 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-11 08:20:17.792 [RMI TCP Connection(1)-192.168.123.7 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-11 08:20:17.793 [RMI TCP Connection(1)-192.168.123.7 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-11 08:20:22.882 [http-nio-8080-exec-1 WARN ] o.s.w.s.m.s.DefaultHandlerExceptionResolver - Resolved [org.springframework.web.HttpMediaTypeNotSupportedException: Content type 'text/plain;charset=UTF-8' not supported]
2025-06-11 08:20:30.797 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=b8f6faff-aeee-4c87-a418-f0db5ce9ccfb, event_name=page_view, event_time=2025-06-11T01:20:24.939Z, user_id=user_4q6eh9x, session_id=session_x28fyxp, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/}, {event_id=20b5bd57-8213-4788-87dd-9d01b9e9a744, event_name=tab_change, event_time=2025-06-11T01:20:26.536Z, user_id=user_4q6eh9x, session_id=session_x28fyxp, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, is_visible=true, time_visible=0}, {event_id=b173ed8b-da5c-4f9f-bb00-602166b99c3b, event_name=scroll, event_time=2025-06-11T01:20:30.747Z, user_id=user_4q6eh9x, session_id=session_x28fyxp, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=0}]}
2025-06-11 08:20:30.799 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-11 08:20:30.828 [http-nio-8080-exec-2 INFO ] o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 1000
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-06-11 08:20:30.872 [http-nio-8080-exec-2 INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-11 08:20:30.888 [http-nio-8080-exec-2 INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-06-11 08:20:30.916 [http-nio-8080-exec-2 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-11 08:20:30.916 [http-nio-8080-exec-2 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-11 08:20:30.916 [http-nio-8080-exec-2 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749604830916
2025-06-11 08:20:30.972 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {clickstream-events=LEADER_NOT_AVAILABLE}
2025-06-11 08:20:30.974 [kafka-producer-network-thread | producer-1 INFO ] org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: 7ooi2p6vTN-ZXN5mav6Y4Q
2025-06-11 08:20:31.036 [kafka-producer-network-thread | producer-1 INFO ] o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 0 with epoch 0
2025-06-11 08:20:32.016 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-11 08:20:35.156 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=24094c01-bdbd-4024-b3a3-cd20200e7ed7, event_name=scroll, event_time=2025-06-11T01:20:31.750Z, user_id=user_4q6eh9x, session_id=session_x28fyxp, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=41}, {event_id=5ffb081e-691f-4908-aefb-7206b4e10c49, event_name=click, event_time=2025-06-11T01:20:32.475Z, user_id=user_4q6eh9x, session_id=session_x28fyxp, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 2
                        Durable and styl, element_type=div, element_name=null, track=product_click, productId=2}, {event_id=4fec205b-60e3-4435-ab3e-03193d6e9e5d, event_name=click, event_time=2025-06-11T01:20:35.146Z, user_id=user_4q6eh9x, session_id=session_x28fyxp, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 3
                        Customer favorit, element_type=div, element_name=null, track=product_click, productId=3}]}
2025-06-11 08:20:35.157 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-11 08:20:35.164 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-11 08:20:39.366 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=eea3aa30-4091-4ee7-9fb6-865bc9bf50e9, event_name=scroll, event_time=2025-06-11T01:20:37.866Z, user_id=user_4q6eh9x, session_id=session_x28fyxp, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=5}, {event_id=1e816e49-e142-42b1-a84f-034bf346e485, event_name=scroll, event_time=2025-06-11T01:20:38.866Z, user_id=user_4q6eh9x, session_id=session_x28fyxp, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=100}, {event_id=2845088e-18c3-4fcc-846c-96bc30dee91f, event_name=click, event_time=2025-06-11T01:20:39.355Z, user_id=user_4q6eh9x, session_id=session_x28fyxp, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 5
                        Bestseller with , element_type=div, element_name=null, track=product_click, productId=5}]}
2025-06-11 08:20:39.367 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-11 08:20:39.369 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-11 08:20:50.108 [http-nio-8080-exec-5 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=b46a031a-1cfe-47ad-bdc3-f6cf2cb73ce3, event_name=click, event_time=2025-06-11T01:20:40.374Z, user_id=user_4q6eh9x, session_id=session_x28fyxp, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 6
                        Sleek design and, element_type=div, element_name=null, track=product_click, productId=6}, {event_id=c78ef472-9a6d-4f34-97d2-60016ec4ccda, event_name=click, event_time=2025-06-11T01:20:41.671Z, user_id=user_4q6eh9x, session_id=session_x28fyxp, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 9
                        Top-rated with e, element_type=div, element_name=null, track=product_click, productId=9}, {event_id=db684330-8783-4bce-882a-fa3770509c96, event_name=click, event_time=2025-06-11T01:20:50.101Z, user_id=user_4q6eh9x, session_id=session_x28fyxp, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 7
                        Compact and effi, element_type=div, element_name=null, track=product_click, productId=7}]}
2025-06-11 08:20:50.108 [http-nio-8080-exec-5 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-11 08:20:50.112 [http-nio-8080-exec-5 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-11 08:21:01.329 [http-nio-8080-exec-6 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=ffc5c8bf-8a2d-43d3-8d81-c3b7701e14dc, event_name=click, event_time=2025-06-11T01:20:51.294Z, user_id=user_4q6eh9x, session_id=session_x28fyxp, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 4
                        Reliable and aff, element_type=div, element_name=null, track=product_click, productId=4}, {event_id=3615866f-7f49-429a-a49a-b206b0ffad80, event_name=scroll, event_time=2025-06-11T01:20:55.766Z, user_id=user_4q6eh9x, session_id=session_x28fyxp, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=48}, {event_id=3940bb65-53a2-4173-91d0-6fad8e5291e7, event_name=click, event_time=2025-06-11T01:21:01.316Z, user_id=user_4q6eh9x, session_id=session_x28fyxp, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 1
                        High-quality ite, element_type=div, element_name=null, track=product_click, productId=1}]}
2025-06-11 08:21:01.329 [http-nio-8080-exec-6 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-11 08:21:01.332 [http-nio-8080-exec-6 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-11 08:21:08.910 [http-nio-8080-exec-7 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=262f5b76-2014-4314-88e1-0a9ec3d35d14, event_name=scroll, event_time=2025-06-11T01:21:06.898Z, user_id=user_4q6eh9x, session_id=session_x28fyxp, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=52}, {event_id=d78753c0-07fe-45ab-bc2b-bf6fbae70f81, event_name=scroll, event_time=2025-06-11T01:21:07.900Z, user_id=user_4q6eh9x, session_id=session_x28fyxp, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=70}, {event_id=c781f1cb-8715-450c-a8f1-1295b8924062, event_name=scroll, event_time=2025-06-11T01:21:08.900Z, user_id=user_4q6eh9x, session_id=session_x28fyxp, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=42}]}
2025-06-11 08:21:08.911 [http-nio-8080-exec-7 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-11 08:21:08.917 [http-nio-8080-exec-7 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-11 08:29:32.335 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node -1 disconnected.
2025-06-11 08:39:31.247 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node -1 disconnected.
2025-06-11 08:47:51.811 [SpringApplicationShutdownHook INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-06-11 08:47:51.834 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-11 08:47:51.835 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-11 08:47:51.835 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-11 08:47:51.835 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-11 08:47:51.836 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-1 unregistered
2025-06-11 09:23:06.086 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-11 09:23:06.306 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-11 09:23:06.424 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:23:06.424 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-11 09:23:06.424 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:23:06.425 [main INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-11 09:23:06.454 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-11 09:23:06.465 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-11 09:23:06.466 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-11 09:23:06.541 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-11 09:23:06.542 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-11 09:23:06.542 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-11 09:23:06.543 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-11 09:23:06.543 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-11 09:23:06.793 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 34385.
2025-06-11 09:23:06.843 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-11 09:23:06.876 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-11 09:23:06.895 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-11 09:23:06.896 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-11 09:23:06.900 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-11 09:23:06.928 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-def23608-3cee-4afa-9536-1da261951c9b
2025-06-11 09:23:06.963 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 966.0 MiB
2025-06-11 09:23:06.977 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-11 09:23:07.035 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @2453ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-11 09:23:07.183 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-11 09:23:07.194 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-11 09:23:07.211 [main INFO ] org.sparkproject.jetty.server.Server - Started @2631ms
2025-06-11 09:23:07.239 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@6cf31447{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:23:07.240 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-11 09:23:07.268 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14f3c6fc{/,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.372 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-11 09:23:07.379 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-11 09:23:07.399 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34527.
2025-06-11 09:23:07.399 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:34527
2025-06-11 09:23:07.401 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-11 09:23:07.407 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 34527, None)
2025-06-11 09:23:07.412 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:34527 with 966.0 MiB RAM, BlockManagerId(driver, phamviethoa, 34527, None)
2025-06-11 09:23:07.415 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 34527, None)
2025-06-11 09:23:07.416 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 34527, None)
2025-06-11 09:23:07.621 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@14f3c6fc{/,null,STOPPED,@Spark}
2025-06-11 09:23:07.622 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31a3f4de{/jobs,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.623 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2dd2e270{/jobs/json,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.624 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d04529c{/jobs/job,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.625 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b16e202{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.626 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cd5122d{/stages,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.626 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10c07b8d{/stages/json,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.628 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1894e40d{/stages/stage,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.628 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7342e05d{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.629 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15383681{/stages/pool,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.630 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@109a2025{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.631 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@761956ac{/storage,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.631 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@304d0259{/storage/json,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.632 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133661d{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.633 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3414a8c3{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.634 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cf518cf{/environment,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.634 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68d651f2{/environment/json,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.635 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e43e323{/executors,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.636 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10643593{/executors/json,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.637 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@eca6a74{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.638 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48840594{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.647 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14823f76{/static,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.648 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@312b34e3{/,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.650 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a865273{/api,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.651 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46185a1b{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.652 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60cf62ad{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.656 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ba359bd{/metrics/json,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.855 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-11 09:23:07.875 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-11 09:23:07.908 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30c1da48{/SQL,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.912 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f1ef9d6{/SQL/json,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.915 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24a86066{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.917 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b3bb1f7{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-11 09:23:07.939 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d0d91a1{/static/sql,null,AVAILABLE,@Spark}
2025-06-11 09:23:09.106 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 201.986382 ms
2025-06-11 09:23:09.144 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 28.448331 ms
2025-06-11 09:23:10.971 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-11 09:23:11.082 [main INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-11 09:23:11.084 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-11 09:23:11.084 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-11 09:23:11.084 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749608591082
2025-06-11 09:23:11.467 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-11 09:23:11.472 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-11 09:23:11.472 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-11 09:23:11.472 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-11 09:23:11.473 [main INFO ] o.a.spark.sql.kafka010.KafkaRelation - GetBatch generating RDD of offset range: KafkaOffsetRange(clickstream-events-0,-2,-1,None), KafkaOffsetRange(clickstream-events-1,-2,-1,None)
2025-06-11 09:23:11.891 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 19.734839 ms
2025-06-11 09:23:12.118 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 20.358341 ms
2025-06-11 09:23:12.137 [main INFO ] org.apache.spark.SparkContext - Starting job: show at Processor.java:56
2025-06-11 09:23:12.151 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (show at Processor.java:56) with 1 output partitions
2025-06-11 09:23:12.152 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (show at Processor.java:56)
2025-06-11 09:23:12.152 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-11 09:23:12.153 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-11 09:23:12.157 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[8] at show at Processor.java:56), which has no missing parents
2025-06-11 09:23:12.275 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 40.0 KiB, free 966.0 MiB)
2025-06-11 09:23:12.307 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 965.9 MiB)
2025-06-11 09:23:12.310 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:34527 (size: 16.4 KiB, free: 966.0 MiB)
2025-06-11 09:23:12.313 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-11 09:23:12.331 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at show at Processor.java:56) (first 15 tasks are for partitions Vector(0))
2025-06-11 09:23:12.335 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
2025-06-11 09:23:12.456 [dispatcher-event-loop-2 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes) 
2025-06-11 09:23:12.476 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-11 09:23:12.672 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-11 09:23:12.706 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-11 09:23:12.792 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-11 09:23:12.792 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-11 09:23:12.792 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749608592792
2025-06-11 09:23:12.795 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-1, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Assigned to partition(s): clickstream-events-0
2025-06-11 09:23:12.799 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-1, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-11 09:23:12.817 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-1, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Cluster ID: 7ooi2p6vTN-ZXN5mav6Y4Q
2025-06-11 09:23:12.867 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-1, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:23:12.868 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-1, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-11 09:23:12.870 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-1, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=10, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:23:12.940 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 40.725918 ms
2025-06-11 09:23:12.965 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-1, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-11 09:23:13.137 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-1, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-11 09:23:13.721 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-1, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:23:13.722 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-1, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-11 09:23:13.726 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-1, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=10, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:23:13.919 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 48.694409 ms
2025-06-11 09:23:13.967 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3705 bytes result sent to driver
2025-06-11 09:23:13.980 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1538 ms on phamviethoa (executor driver) (1/1)
2025-06-11 09:23:13.983 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-11 09:23:13.990 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (show at Processor.java:56) finished in 1.819 s
2025-06-11 09:23:13.993 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-11 09:23:13.993 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-11 09:23:13.995 [main INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: show at Processor.java:56, took 1.857376 s
2025-06-11 09:23:14.012 [main INFO ] org.apache.spark.SparkContext - Starting job: show at Processor.java:56
2025-06-11 09:23:14.013 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 1 (show at Processor.java:56) with 1 output partitions
2025-06-11 09:23:14.014 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at Processor.java:56)
2025-06-11 09:23:14.014 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-11 09:23:14.014 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-11 09:23:14.016 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[8] at show at Processor.java:56), which has no missing parents
2025-06-11 09:23:14.023 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 40.0 KiB, free 965.9 MiB)
2025-06-11 09:23:14.031 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 965.9 MiB)
2025-06-11 09:23:14.034 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:34527 (size: 16.4 KiB, free: 966.0 MiB)
2025-06-11 09:23:14.035 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-11 09:23:14.036 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at show at Processor.java:56) (first 15 tasks are for partitions Vector(1))
2025-06-11 09:23:14.036 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-11 09:23:14.040 [dispatcher-event-loop-1 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 7555 bytes) 
2025-06-11 09:23:14.042 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
2025-06-11 09:23:14.053 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-11 09:23:14.054 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-11 09:23:14.062 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-11 09:23:14.062 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-11 09:23:14.062 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749608594062
2025-06-11 09:23:14.062 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-2, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Assigned to partition(s): clickstream-events-1
2025-06-11 09:23:14.063 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-2, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-11 09:23:14.068 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-2, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Cluster ID: 7ooi2p6vTN-ZXN5mav6Y4Q
2025-06-11 09:23:14.075 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-2, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:23:14.075 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-2, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-11 09:23:14.077 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-2, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:23:14.082 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-2, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-11 09:23:14.103 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-2, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-11 09:23:14.611 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-2, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:23:14.611 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-2, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-11 09:23:14.614 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-2, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:23:14.651 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 3390 bytes result sent to driver
2025-06-11 09:23:14.655 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 615 ms on phamviethoa (executor driver) (1/1)
2025-06-11 09:23:14.655 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-11 09:23:14.657 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (show at Processor.java:56) finished in 0.640 s
2025-06-11 09:23:14.658 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-11 09:23:14.658 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-11 09:23:14.659 [main INFO ] o.a.spark.scheduler.DAGScheduler - Job 1 finished: show at Processor.java:56, took 0.646903 s
2025-06-11 09:23:14.687 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 19.356289 ms
2025-06-11 09:23:14.785 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-1, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-11 09:23:14.785 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-1, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-11 09:23:14.796 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-11 09:23:14.796 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-11 09:23:14.797 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-11 09:23:14.797 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-11 09:23:14.801 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-1 unregistered
2025-06-11 09:23:14.802 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-2, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-11 09:23:14.802 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-2, groupId=spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-11 09:23:14.807 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-11 09:23:14.807 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-11 09:23:14.807 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-11 09:23:14.807 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-11 09:23:14.810 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-relation-d4ce0b12-53de-4c41-b49d-21f9618b075e-executor-2 unregistered
2025-06-11 09:23:14.810 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-11 09:23:14.811 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-11 09:23:14.820 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@6cf31447{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:23:14.830 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-11 09:23:14.853 [dispatcher-event-loop-1 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-11 09:23:14.878 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-11 09:23:14.879 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-11 09:23:14.889 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-11 09:23:14.893 [dispatcher-event-loop-1 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-11 09:23:14.903 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-11 09:23:14.903 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-11 09:23:14.903 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-142e219f-2e56-4275-8c22-96b7de61da03
2025-06-11 09:24:52.762 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-11 09:24:52.982 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-11 09:24:53.086 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:24:53.086 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-11 09:24:53.087 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:24:53.087 [main INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-11 09:24:53.115 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-11 09:24:53.126 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-11 09:24:53.126 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-11 09:24:53.194 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-11 09:24:53.195 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-11 09:24:53.195 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-11 09:24:53.196 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-11 09:24:53.196 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-11 09:24:53.474 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 43183.
2025-06-11 09:24:53.507 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-11 09:24:53.538 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-11 09:24:53.558 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-11 09:24:53.558 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-11 09:24:53.562 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-11 09:24:53.586 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-e5b97c22-2811-459d-990d-9c32285ada06
2025-06-11 09:24:53.618 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 966.0 MiB
2025-06-11 09:24:53.633 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-11 09:24:53.669 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @2163ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-11 09:24:53.767 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-11 09:24:53.780 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-11 09:24:53.798 [main INFO ] org.sparkproject.jetty.server.Server - Started @2292ms
2025-06-11 09:24:53.829 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@3f20f711{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:24:53.829 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-11 09:24:53.848 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4441d567{/,null,AVAILABLE,@Spark}
2025-06-11 09:24:53.940 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-11 09:24:53.947 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-11 09:24:53.968 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45539.
2025-06-11 09:24:53.968 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:45539
2025-06-11 09:24:53.970 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-11 09:24:53.979 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 45539, None)
2025-06-11 09:24:53.987 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:45539 with 966.0 MiB RAM, BlockManagerId(driver, phamviethoa, 45539, None)
2025-06-11 09:24:53.993 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 45539, None)
2025-06-11 09:24:53.995 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 45539, None)
2025-06-11 09:24:54.176 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4441d567{/,null,STOPPED,@Spark}
2025-06-11 09:24:54.178 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44b18fe4{/jobs,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.179 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d3ca6c7{/jobs/json,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.180 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44b194fe{/jobs/job,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.182 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fe46690{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.183 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b4d50b{/stages,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.184 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@476ee5b3{/stages/json,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.186 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68880c21{/stages/stage,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.186 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4195105b{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.187 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47ffe971{/stages/pool,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.189 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14fa92af{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.190 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@339a3670{/storage,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.191 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c7a8af2{/storage/json,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.193 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@720bf653{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.194 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4edef76c{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.195 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70c53dbe{/environment,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.196 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21c815e4{/environment/json,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.197 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a331b46{/executors,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.197 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@743e66f7{/executors/json,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.199 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241f05b{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.200 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.210 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/static,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.211 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42ea287{/,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.213 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f0b3cfe{/api,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.214 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7718a40f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.215 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26844abb{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.221 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fd9ebde{/metrics/json,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.424 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-11 09:24:54.436 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-11 09:24:54.461 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b612c6{/SQL,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.462 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22752544{/SQL/json,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.464 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4784efd9{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.465 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@427ae189{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-11 09:24:54.479 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@514de325{/static/sql,null,AVAILABLE,@Spark}
2025-06-11 09:24:55.641 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 232.095114 ms
2025-06-11 09:24:55.674 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 23.351759 ms
2025-06-11 09:24:57.556 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-11 09:24:57.641 [main INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-11 09:24:57.642 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-11 09:24:57.642 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-11 09:24:57.643 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749608697641
2025-06-11 09:24:57.978 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-11 09:24:57.983 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-11 09:24:57.983 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-11 09:24:57.984 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-11 09:24:57.985 [main INFO ] o.a.spark.sql.kafka010.KafkaRelation - GetBatch generating RDD of offset range: KafkaOffsetRange(clickstream-events-0,-2,-1,None), KafkaOffsetRange(clickstream-events-1,-2,-1,None)
2025-06-11 09:24:58.380 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 22.284172 ms
2025-06-11 09:24:58.576 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 11.215149 ms
2025-06-11 09:24:58.593 [main INFO ] org.apache.spark.SparkContext - Starting job: show at Processor.java:65
2025-06-11 09:24:58.610 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (show at Processor.java:65) with 1 output partitions
2025-06-11 09:24:58.611 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (show at Processor.java:65)
2025-06-11 09:24:58.611 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-11 09:24:58.612 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-11 09:24:58.619 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[9] at show at Processor.java:65), which has no missing parents
2025-06-11 09:24:58.754 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 44.6 KiB, free 966.0 MiB)
2025-06-11 09:24:58.782 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 18.3 KiB, free 965.9 MiB)
2025-06-11 09:24:58.786 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:45539 (size: 18.3 KiB, free: 966.0 MiB)
2025-06-11 09:24:58.789 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-11 09:24:58.805 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[9] at show at Processor.java:65) (first 15 tasks are for partitions Vector(0))
2025-06-11 09:24:58.807 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
2025-06-11 09:24:58.866 [dispatcher-event-loop-2 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes) 
2025-06-11 09:24:58.881 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-11 09:24:59.044 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-11 09:24:59.080 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-11 09:24:59.140 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-11 09:24:59.140 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-11 09:24:59.140 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749608699140
2025-06-11 09:24:59.142 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-1, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Assigned to partition(s): clickstream-events-0
2025-06-11 09:24:59.146 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-1, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-11 09:24:59.161 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-1, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Cluster ID: 7ooi2p6vTN-ZXN5mav6Y4Q
2025-06-11 09:24:59.174 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-1, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:24:59.176 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-1, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-11 09:24:59.179 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-1, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=10, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:24:59.226 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 22.313382 ms
2025-06-11 09:24:59.286 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 36.279245 ms
2025-06-11 09:24:59.294 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-1, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-11 09:24:59.342 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-1, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-11 09:24:59.843 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-1, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:24:59.844 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-1, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-11 09:24:59.847 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-1, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=10, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:25:00.004 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 35.398134 ms
2025-06-11 09:25:00.102 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2918 bytes result sent to driver
2025-06-11 09:25:00.115 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1260 ms on phamviethoa (executor driver) (1/1)
2025-06-11 09:25:00.117 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-11 09:25:00.125 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (show at Processor.java:65) finished in 1.487 s
2025-06-11 09:25:00.129 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-11 09:25:00.130 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-11 09:25:00.132 [main INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: show at Processor.java:65, took 1.537924 s
2025-06-11 09:25:00.148 [main INFO ] org.apache.spark.SparkContext - Starting job: show at Processor.java:65
2025-06-11 09:25:00.150 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 1 (show at Processor.java:65) with 1 output partitions
2025-06-11 09:25:00.150 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at Processor.java:65)
2025-06-11 09:25:00.150 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-11 09:25:00.151 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-11 09:25:00.152 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at show at Processor.java:65), which has no missing parents
2025-06-11 09:25:00.159 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 44.6 KiB, free 965.9 MiB)
2025-06-11 09:25:00.163 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 18.3 KiB, free 965.9 MiB)
2025-06-11 09:25:00.164 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:45539 (size: 18.3 KiB, free: 966.0 MiB)
2025-06-11 09:25:00.165 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-11 09:25:00.166 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at show at Processor.java:65) (first 15 tasks are for partitions Vector(1))
2025-06-11 09:25:00.166 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-11 09:25:00.168 [dispatcher-event-loop-1 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 7555 bytes) 
2025-06-11 09:25:00.170 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
2025-06-11 09:25:00.181 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-11 09:25:00.182 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-11 09:25:00.188 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-11 09:25:00.189 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-11 09:25:00.189 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749608700188
2025-06-11 09:25:00.189 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-2, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Assigned to partition(s): clickstream-events-1
2025-06-11 09:25:00.189 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-2, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-11 09:25:00.196 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-2, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Cluster ID: 7ooi2p6vTN-ZXN5mav6Y4Q
2025-06-11 09:25:00.202 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-2, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:25:00.203 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-2, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-11 09:25:00.204 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-2, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:25:00.216 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-2, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-11 09:25:00.223 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-2, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-11 09:25:00.736 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-2, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:25:00.736 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-2, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-11 09:25:00.739 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-2, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:25:00.773 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2636 bytes result sent to driver
2025-06-11 09:25:00.776 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 608 ms on phamviethoa (executor driver) (1/1)
2025-06-11 09:25:00.776 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-11 09:25:00.777 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (show at Processor.java:65) finished in 0.623 s
2025-06-11 09:25:00.778 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-11 09:25:00.778 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-11 09:25:00.778 [main INFO ] o.a.spark.scheduler.DAGScheduler - Job 1 finished: show at Processor.java:65, took 0.629924 s
2025-06-11 09:25:00.798 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 14.100965 ms
2025-06-11 09:25:00.823 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-2, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-11 09:25:00.824 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-2, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-11 09:25:00.831 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-11 09:25:00.831 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-11 09:25:00.831 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-11 09:25:00.831 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-11 09:25:00.836 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-2 unregistered
2025-06-11 09:25:00.837 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-1, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-11 09:25:00.837 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-1, groupId=spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-11 09:25:00.841 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-11 09:25:00.841 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-11 09:25:00.841 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-11 09:25:00.841 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-11 09:25:00.846 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-relation-c7321478-c73c-42a9-85a0-06ae96be69b7-executor-1 unregistered
2025-06-11 09:25:00.846 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-11 09:25:00.847 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-11 09:25:00.855 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@3f20f711{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:25:00.862 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-11 09:25:00.880 [dispatcher-event-loop-1 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-11 09:25:00.892 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-11 09:25:00.892 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-11 09:25:00.898 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-11 09:25:00.900 [dispatcher-event-loop-1 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-11 09:25:00.906 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-11 09:25:00.906 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-11 09:25:00.907 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-21e16f52-5a5b-4fe2-aaff-990d6a92a3b8
2025-06-11 09:28:00.053 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-11 09:28:00.242 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-11 09:28:00.325 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:28:00.326 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-11 09:28:00.326 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:28:00.327 [main INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-11 09:28:00.346 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-11 09:28:00.357 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-11 09:28:00.358 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-11 09:28:00.410 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-11 09:28:00.410 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-11 09:28:00.410 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-11 09:28:00.411 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-11 09:28:00.411 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-11 09:28:00.645 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 43027.
2025-06-11 09:28:00.671 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-11 09:28:00.701 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-11 09:28:00.717 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-11 09:28:00.718 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-11 09:28:00.721 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-11 09:28:00.739 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-b8e6daf2-7bbe-4a32-90f9-9c8e9c603939
2025-06-11 09:28:00.769 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 966.0 MiB
2025-06-11 09:28:00.785 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-11 09:28:00.822 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1926ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-11 09:28:00.920 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-11 09:28:00.932 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-11 09:28:00.953 [main INFO ] org.sparkproject.jetty.server.Server - Started @2058ms
2025-06-11 09:28:00.988 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@25ed1fa3{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:28:00.989 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-11 09:28:01.008 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4441d567{/,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.103 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-11 09:28:01.110 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-11 09:28:01.133 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39287.
2025-06-11 09:28:01.134 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:39287
2025-06-11 09:28:01.137 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-11 09:28:01.146 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 39287, None)
2025-06-11 09:28:01.153 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:39287 with 966.0 MiB RAM, BlockManagerId(driver, phamviethoa, 39287, None)
2025-06-11 09:28:01.157 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 39287, None)
2025-06-11 09:28:01.159 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 39287, None)
2025-06-11 09:28:01.314 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4441d567{/,null,STOPPED,@Spark}
2025-06-11 09:28:01.315 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44b18fe4{/jobs,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.316 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d3ca6c7{/jobs/json,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.317 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44b194fe{/jobs/job,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.318 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fe46690{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.319 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b4d50b{/stages,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.320 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@476ee5b3{/stages/json,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.321 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68880c21{/stages/stage,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.322 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4195105b{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.323 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47ffe971{/stages/pool,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.323 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14fa92af{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.324 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@339a3670{/storage,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.325 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c7a8af2{/storage/json,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.326 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@720bf653{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.326 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4edef76c{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.327 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70c53dbe{/environment,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.328 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21c815e4{/environment/json,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.329 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a331b46{/executors,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.330 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@743e66f7{/executors/json,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.331 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241f05b{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.332 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.340 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/static,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.341 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42ea287{/,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.343 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f0b3cfe{/api,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.344 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7718a40f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.346 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26844abb{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.353 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fd9ebde{/metrics/json,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.537 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-11 09:28:01.545 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-11 09:28:01.564 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b612c6{/SQL,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.565 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22752544{/SQL/json,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.566 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4784efd9{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.567 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@427ae189{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-11 09:28:01.582 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@514de325{/static/sql,null,AVAILABLE,@Spark}
2025-06-11 09:28:02.644 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 191.336287 ms
2025-06-11 09:28:02.680 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 25.347304 ms
2025-06-11 09:28:04.420 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-11 09:28:04.468 [main INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-11 09:28:04.470 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-11 09:28:04.470 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-11 09:28:04.470 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749608884468
2025-06-11 09:28:04.819 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-11 09:28:04.824 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-11 09:28:04.824 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-11 09:28:04.824 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-11 09:28:04.825 [main INFO ] o.a.spark.sql.kafka010.KafkaRelation - GetBatch generating RDD of offset range: KafkaOffsetRange(clickstream-events-0,-2,-1,None), KafkaOffsetRange(clickstream-events-1,-2,-1,None)
2025-06-11 09:28:05.171 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 19.52509 ms
2025-06-11 09:28:05.356 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 13.480099 ms
2025-06-11 09:28:05.372 [main INFO ] org.apache.spark.SparkContext - Starting job: show at Processor.java:64
2025-06-11 09:28:05.397 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (show at Processor.java:64) with 1 output partitions
2025-06-11 09:28:05.398 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (show at Processor.java:64)
2025-06-11 09:28:05.399 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-11 09:28:05.400 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-11 09:28:05.404 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[9] at show at Processor.java:64), which has no missing parents
2025-06-11 09:28:05.544 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 44.6 KiB, free 966.0 MiB)
2025-06-11 09:28:05.569 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 18.3 KiB, free 965.9 MiB)
2025-06-11 09:28:05.571 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:39287 (size: 18.3 KiB, free: 966.0 MiB)
2025-06-11 09:28:05.574 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-11 09:28:05.589 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[9] at show at Processor.java:64) (first 15 tasks are for partitions Vector(0))
2025-06-11 09:28:05.591 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
2025-06-11 09:28:05.671 [dispatcher-event-loop-2 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes) 
2025-06-11 09:28:05.694 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-11 09:28:05.868 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-11 09:28:05.917 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-11 09:28:06.003 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-11 09:28:06.003 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-11 09:28:06.003 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749608886003
2025-06-11 09:28:06.006 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor-1, groupId=spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor] Assigned to partition(s): clickstream-events-0
2025-06-11 09:28:06.011 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor-1, groupId=spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-11 09:28:06.043 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor-1, groupId=spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor] Cluster ID: 7ooi2p6vTN-ZXN5mav6Y4Q
2025-06-11 09:28:06.061 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor-1, groupId=spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:28:06.062 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor-1, groupId=spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-11 09:28:06.065 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor-1, groupId=spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=10, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:28:06.118 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 29.060945 ms
2025-06-11 09:28:06.225 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 52.603974 ms
2025-06-11 09:28:06.234 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor-1, groupId=spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-11 09:28:06.274 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor-1, groupId=spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-11 09:28:06.780 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor-1, groupId=spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:28:06.780 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor-1, groupId=spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-11 09:28:06.785 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor-1, groupId=spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=10, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:28:06.975 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 38.977817 ms
2025-06-11 09:28:07.071 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2501 bytes result sent to driver
2025-06-11 09:28:07.084 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1428 ms on phamviethoa (executor driver) (1/1)
2025-06-11 09:28:07.087 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-11 09:28:07.094 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (show at Processor.java:64) finished in 1.678 s
2025-06-11 09:28:07.097 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-11 09:28:07.097 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-11 09:28:07.099 [main INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: show at Processor.java:64, took 1.726465 s
2025-06-11 09:28:07.119 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 11.680546 ms
2025-06-11 09:28:07.142 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor-1, groupId=spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-11 09:28:07.142 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor-1, groupId=spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-11 09:28:07.150 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-11 09:28:07.150 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-11 09:28:07.150 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-11 09:28:07.150 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-11 09:28:07.159 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-relation-039a054d-4e39-4998-9059-fbe9f1bce112-executor-1 unregistered
2025-06-11 09:28:07.160 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-11 09:28:07.161 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-11 09:28:07.169 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@25ed1fa3{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:28:07.175 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-11 09:28:07.193 [dispatcher-event-loop-2 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-11 09:28:07.206 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-11 09:28:07.207 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-11 09:28:07.211 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-11 09:28:07.214 [dispatcher-event-loop-2 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-11 09:28:07.219 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-11 09:28:07.220 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-11 09:28:07.220 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-9aab8adc-172a-4c01-aff9-0792c1dfd9ce
2025-06-11 09:30:27.224 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-11 09:30:27.376 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-11 09:30:27.460 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:30:27.461 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-11 09:30:27.461 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:30:27.462 [main INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-11 09:30:27.482 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-11 09:30:27.493 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-11 09:30:27.494 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-11 09:30:27.544 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-11 09:30:27.544 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-11 09:30:27.544 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-11 09:30:27.545 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-11 09:30:27.545 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-11 09:30:27.776 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 36521.
2025-06-11 09:30:27.800 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-11 09:30:27.830 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-11 09:30:27.846 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-11 09:30:27.847 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-11 09:30:27.851 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-11 09:30:27.869 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-92ad5c63-fe48-496f-b684-45b336d97b6c
2025-06-11 09:30:27.899 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 966.0 MiB
2025-06-11 09:30:27.913 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-11 09:30:27.948 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @2119ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-11 09:30:28.093 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-11 09:30:28.104 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-11 09:30:28.123 [main INFO ] org.sparkproject.jetty.server.Server - Started @2295ms
2025-06-11 09:30:28.166 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@76c21277{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:30:28.166 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-11 09:30:28.191 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4441d567{/,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.310 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-11 09:30:28.322 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-11 09:30:28.351 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32959.
2025-06-11 09:30:28.352 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:32959
2025-06-11 09:30:28.355 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-11 09:30:28.364 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 32959, None)
2025-06-11 09:30:28.371 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:32959 with 966.0 MiB RAM, BlockManagerId(driver, phamviethoa, 32959, None)
2025-06-11 09:30:28.376 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 32959, None)
2025-06-11 09:30:28.377 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 32959, None)
2025-06-11 09:30:28.545 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4441d567{/,null,STOPPED,@Spark}
2025-06-11 09:30:28.547 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44b18fe4{/jobs,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.549 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d3ca6c7{/jobs/json,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.551 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44b194fe{/jobs/job,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.552 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fe46690{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.554 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b4d50b{/stages,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.555 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@476ee5b3{/stages/json,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.558 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68880c21{/stages/stage,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.560 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4195105b{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.561 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47ffe971{/stages/pool,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.563 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14fa92af{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.564 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@339a3670{/storage,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.566 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c7a8af2{/storage/json,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.568 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@720bf653{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.569 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4edef76c{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.571 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70c53dbe{/environment,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.573 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21c815e4{/environment/json,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.575 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a331b46{/executors,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.576 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@743e66f7{/executors/json,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.578 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241f05b{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.579 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.593 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/static,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.595 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42ea287{/,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.598 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f0b3cfe{/api,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.600 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7718a40f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.602 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26844abb{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.607 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fd9ebde{/metrics/json,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.871 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-11 09:30:28.879 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-11 09:30:28.893 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b612c6{/SQL,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.894 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22752544{/SQL/json,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.895 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4784efd9{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.896 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@427ae189{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-11 09:30:28.908 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@514de325{/static/sql,null,AVAILABLE,@Spark}
2025-06-11 09:30:30.009 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 194.788265 ms
2025-06-11 09:30:30.044 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 25.073307 ms
2025-06-11 09:30:31.758 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-11 09:30:31.759 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-11 09:30:31.767 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@76c21277{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:30:31.771 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-11 09:30:31.792 [dispatcher-event-loop-3 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-11 09:30:31.806 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-11 09:30:31.807 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-11 09:30:31.818 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-11 09:30:31.821 [dispatcher-event-loop-3 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-11 09:30:31.827 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-11 09:30:31.827 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-11 09:30:31.828 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-d1aa84d7-afbb-4273-a80f-bb0833b1bc55
2025-06-11 09:31:39.880 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-11 09:31:40.076 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-11 09:31:40.256 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:31:40.256 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-11 09:31:40.256 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:31:40.257 [main INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-11 09:31:40.285 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-11 09:31:40.299 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-11 09:31:40.299 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-11 09:31:40.370 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-11 09:31:40.370 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-11 09:31:40.371 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-11 09:31:40.371 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-11 09:31:40.371 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-11 09:31:40.675 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 43667.
2025-06-11 09:31:40.707 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-11 09:31:40.742 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-11 09:31:40.760 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-11 09:31:40.760 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-11 09:31:40.764 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-11 09:31:40.784 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-bacc2201-7cc3-438f-b8aa-f82f3b9e147d
2025-06-11 09:31:40.851 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 966.0 MiB
2025-06-11 09:31:40.866 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-11 09:31:40.912 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @2411ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-11 09:31:41.015 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-11 09:31:41.028 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-11 09:31:41.044 [main INFO ] org.sparkproject.jetty.server.Server - Started @2543ms
2025-06-11 09:31:41.074 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@5a684072{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:31:41.075 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-11 09:31:41.093 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b969c4{/,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.218 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-11 09:31:41.229 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-11 09:31:41.255 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34121.
2025-06-11 09:31:41.255 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:34121
2025-06-11 09:31:41.257 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-11 09:31:41.264 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 34121, None)
2025-06-11 09:31:41.270 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:34121 with 966.0 MiB RAM, BlockManagerId(driver, phamviethoa, 34121, None)
2025-06-11 09:31:41.275 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 34121, None)
2025-06-11 09:31:41.276 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 34121, None)
2025-06-11 09:31:41.463 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@62b969c4{/,null,STOPPED,@Spark}
2025-06-11 09:31:41.466 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d3ca6c7{/jobs,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.467 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a638c79{/jobs/json,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.471 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fe46690{/jobs/job,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.472 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b4d50b{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.474 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@476ee5b3{/stages,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.476 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cd4a4d7{/stages/json,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.478 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4195105b{/stages/stage,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.480 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47ffe971{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.482 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14fa92af{/stages/pool,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.484 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@339a3670{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.485 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c7a8af2{/storage,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.488 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@720bf653{/storage/json,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.490 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4edef76c{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.491 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70c53dbe{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.494 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21c815e4{/environment,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.496 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a331b46{/environment/json,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.497 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@743e66f7{/executors,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.499 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241f05b{/executors/json,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.503 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.506 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.518 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c9320c2{/static,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.520 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f0b3cfe{/,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.522 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65a48602{/api,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.524 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26844abb{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.525 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@288ca5f0{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.530 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ee5b2d9{/metrics/json,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.723 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-11 09:31:41.734 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-11 09:31:41.753 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22752544{/SQL,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.755 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69d23296{/SQL/json,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.759 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@427ae189{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.762 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76332405{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-11 09:31:41.782 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@43a65cd8{/static/sql,null,AVAILABLE,@Spark}
2025-06-11 09:31:42.790 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 180.433546 ms
2025-06-11 09:31:42.824 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 24.384948 ms
2025-06-11 09:31:44.490 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-11 09:31:44.490 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-11 09:31:44.500 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@5a684072{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:31:44.504 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-11 09:31:44.518 [dispatcher-event-loop-3 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-11 09:31:44.531 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-11 09:31:44.532 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-11 09:31:44.540 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-11 09:31:44.543 [dispatcher-event-loop-3 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-11 09:31:44.548 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-11 09:31:44.549 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-11 09:31:44.549 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-a1c85ac8-91f7-4c9e-a376-d65198bdd30a
2025-06-11 09:33:22.309 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-11 09:33:22.457 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-11 09:33:22.544 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:33:22.544 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-11 09:33:22.544 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:33:22.545 [main INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-11 09:33:22.565 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-11 09:33:22.576 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-11 09:33:22.577 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-11 09:33:22.628 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-11 09:33:22.629 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-11 09:33:22.629 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-11 09:33:22.630 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-11 09:33:22.630 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-11 09:33:22.852 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 37331.
2025-06-11 09:33:22.876 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-11 09:33:22.905 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-11 09:33:22.922 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-11 09:33:22.923 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-11 09:33:22.926 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-11 09:33:22.945 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-ef781c85-e540-4477-9bcb-4af7e1e4b508
2025-06-11 09:33:22.976 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 966.0 MiB
2025-06-11 09:33:22.991 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-11 09:33:23.031 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @2232ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-11 09:33:23.136 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-11 09:33:23.152 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-11 09:33:23.169 [main INFO ] org.sparkproject.jetty.server.Server - Started @2371ms
2025-06-11 09:33:23.200 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@46aa7087{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:33:23.201 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-11 09:33:23.220 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cb2651f{/,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.308 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-11 09:33:23.315 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-11 09:33:23.334 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42549.
2025-06-11 09:33:23.335 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:42549
2025-06-11 09:33:23.336 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-11 09:33:23.347 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 42549, None)
2025-06-11 09:33:23.356 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:42549 with 966.0 MiB RAM, BlockManagerId(driver, phamviethoa, 42549, None)
2025-06-11 09:33:23.364 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 42549, None)
2025-06-11 09:33:23.366 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 42549, None)
2025-06-11 09:33:23.541 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7cb2651f{/,null,STOPPED,@Spark}
2025-06-11 09:33:23.543 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67b100fe{/jobs,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.544 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ba5aa7a{/jobs/json,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.545 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c931d35{/jobs/job,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.546 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49122b8f{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.547 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4beabeec{/stages,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.547 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b22d8a1{/stages/json,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.549 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31a3f4de{/stages/stage,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.550 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2dd2e270{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.551 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f3e19b3{/stages/pool,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.552 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d04529c{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.554 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b16e202{/storage,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.555 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cd5122d{/storage/json,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.556 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10c07b8d{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.557 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@360bc645{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.560 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d51e129{/environment,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.562 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1894e40d{/environment/json,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.564 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7342e05d{/executors,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.565 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15383681{/executors/json,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.567 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@109a2025{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.568 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@761956ac{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.581 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@304d0259{/static,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.582 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e08acf9{/,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.584 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78cd163b{/api,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.586 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@672b72ba{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.587 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@312b34e3{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.593 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ac4ccad{/metrics/json,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.797 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-11 09:33:23.809 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-11 09:33:23.825 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@806996{/SQL,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.828 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@257e0827{/SQL/json,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.831 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@376c7d7d{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.833 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fba233d{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-11 09:33:23.856 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b48e183{/static/sql,null,AVAILABLE,@Spark}
2025-06-11 09:33:24.880 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 181.86735 ms
2025-06-11 09:33:24.916 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 25.64109 ms
2025-06-11 09:33:26.915 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-11 09:33:26.995 [main INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-11 09:33:26.996 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-11 09:33:26.996 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-11 09:33:26.996 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749609206995
2025-06-11 09:33:27.340 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-11 09:33:27.349 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-11 09:33:27.350 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-11 09:33:27.350 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-11 09:33:27.351 [main INFO ] o.a.spark.sql.kafka010.KafkaRelation - GetBatch generating RDD of offset range: KafkaOffsetRange(clickstream-events-0,-2,-1,None), KafkaOffsetRange(clickstream-events-1,-2,-1,None)
2025-06-11 09:33:27.749 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 24.616147 ms
2025-06-11 09:33:28.029 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 45.340866 ms
2025-06-11 09:33:28.042 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.122831 ms
2025-06-11 09:33:28.063 [main INFO ] org.apache.spark.SparkContext - Starting job: show at Processor.java:68
2025-06-11 09:33:28.078 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (show at Processor.java:68) with 1 output partitions
2025-06-11 09:33:28.078 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (show at Processor.java:68)
2025-06-11 09:33:28.079 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-11 09:33:28.080 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-11 09:33:28.083 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[10] at show at Processor.java:68), which has no missing parents
2025-06-11 09:33:28.213 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 61.0 KiB, free 965.9 MiB)
2025-06-11 09:33:28.244 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 21.8 KiB, free 965.9 MiB)
2025-06-11 09:33:28.248 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:42549 (size: 21.8 KiB, free: 966.0 MiB)
2025-06-11 09:33:28.253 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-11 09:33:28.275 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[10] at show at Processor.java:68) (first 15 tasks are for partitions Vector(0))
2025-06-11 09:33:28.276 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
2025-06-11 09:33:28.345 [dispatcher-event-loop-2 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes) 
2025-06-11 09:33:28.359 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-11 09:33:28.572 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-11 09:33:28.606 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-11 09:33:28.654 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-11 09:33:28.655 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-11 09:33:28.655 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749609208654
2025-06-11 09:33:28.657 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor-1, groupId=spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor] Assigned to partition(s): clickstream-events-0
2025-06-11 09:33:28.661 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor-1, groupId=spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-11 09:33:28.673 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor-1, groupId=spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor] Cluster ID: 7ooi2p6vTN-ZXN5mav6Y4Q
2025-06-11 09:33:28.683 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor-1, groupId=spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:33:28.684 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor-1, groupId=spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-11 09:33:28.686 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor-1, groupId=spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=10, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:33:28.723 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 17.297802 ms
2025-06-11 09:33:28.748 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 15.853104 ms
2025-06-11 09:33:28.755 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor-1, groupId=spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-11 09:33:28.795 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor-1, groupId=spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-11 09:33:29.299 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor-1, groupId=spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:33:29.300 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor-1, groupId=spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-11 09:33:29.304 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor-1, groupId=spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=10, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:33:29.454 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 34.034366 ms
2025-06-11 09:33:29.534 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2661 bytes result sent to driver
2025-06-11 09:33:29.548 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1213 ms on phamviethoa (executor driver) (1/1)
2025-06-11 09:33:29.551 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-11 09:33:29.556 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (show at Processor.java:68) finished in 1.462 s
2025-06-11 09:33:29.559 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-11 09:33:29.559 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-11 09:33:29.561 [main INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: show at Processor.java:68, took 1.497558 s
2025-06-11 09:33:29.591 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 16.889037 ms
2025-06-11 09:33:29.611 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor-1, groupId=spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-11 09:33:29.611 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor-1, groupId=spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-11 09:33:29.620 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-11 09:33:29.621 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-11 09:33:29.621 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-11 09:33:29.621 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-11 09:33:29.628 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-relation-270245e4-2574-4af3-976b-a7692636257e-executor-1 unregistered
2025-06-11 09:33:29.628 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-11 09:33:29.629 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-11 09:33:29.636 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@46aa7087{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:33:29.641 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-11 09:33:29.656 [dispatcher-event-loop-2 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-11 09:33:29.668 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-11 09:33:29.668 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-11 09:33:29.675 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-11 09:33:29.678 [dispatcher-event-loop-2 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-11 09:33:29.684 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-11 09:33:29.685 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-11 09:33:29.685 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-e8edf98a-6186-4450-a482-fcd41bf055f0
2025-06-11 09:34:50.541 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-11 09:34:50.725 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-11 09:34:50.812 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:34:50.812 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-11 09:34:50.813 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:34:50.813 [main INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-11 09:34:50.834 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-11 09:34:50.845 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-11 09:34:50.845 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-11 09:34:50.899 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-11 09:34:50.900 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-11 09:34:50.900 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-11 09:34:50.901 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-11 09:34:50.901 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-11 09:34:51.159 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 42427.
2025-06-11 09:34:51.186 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-11 09:34:51.216 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-11 09:34:51.234 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-11 09:34:51.235 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-11 09:34:51.238 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-11 09:34:51.257 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-5394b6f0-7b94-4fff-9abc-209900bb20f0
2025-06-11 09:34:51.288 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 966.0 MiB
2025-06-11 09:34:51.303 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-11 09:34:51.340 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @2213ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-11 09:34:51.455 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-11 09:34:51.469 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-11 09:34:51.486 [main INFO ] org.sparkproject.jetty.server.Server - Started @2360ms
2025-06-11 09:34:51.517 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@7cb2651f{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:34:51.517 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-11 09:34:51.535 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49293b43{/,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.634 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-11 09:34:51.641 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-11 09:34:51.666 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37833.
2025-06-11 09:34:51.667 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:37833
2025-06-11 09:34:51.671 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-11 09:34:51.681 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 37833, None)
2025-06-11 09:34:51.687 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:37833 with 966.0 MiB RAM, BlockManagerId(driver, phamviethoa, 37833, None)
2025-06-11 09:34:51.693 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 37833, None)
2025-06-11 09:34:51.695 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 37833, None)
2025-06-11 09:34:51.862 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@49293b43{/,null,STOPPED,@Spark}
2025-06-11 09:34:51.865 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/jobs,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.866 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397ef2{/jobs/json,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.868 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9b21bd3{/jobs/job,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.870 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7661b5a{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.872 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65c33b92{/stages,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.874 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e08acf9{/stages/json,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.877 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75483843{/stages/stage,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.880 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ec4ff02{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.882 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b2f5fcf{/stages/pool,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.884 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@711d1a52{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.886 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@302edb74{/storage,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.889 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@672b72ba{/storage/json,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.892 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@312b34e3{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.895 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a865273{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.898 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4068102e{/environment,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.901 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c008c24{/environment/json,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.904 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21079a12{/executors,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.906 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67c5ac52{/executors/json,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.910 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b8bb184{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.912 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dc79225{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.928 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46185a1b{/static,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.929 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62db3891{/,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.935 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6724cdec{/api,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.937 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38bb9d7a{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.940 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78d6447a{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-11 09:34:51.947 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6eb17ec8{/metrics/json,null,AVAILABLE,@Spark}
2025-06-11 09:34:52.128 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-11 09:34:52.138 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-11 09:34:52.154 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41853299{/SQL,null,AVAILABLE,@Spark}
2025-06-11 09:34:52.155 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e5b7fba{/SQL/json,null,AVAILABLE,@Spark}
2025-06-11 09:34:52.157 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77681ce4{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-11 09:34:52.158 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f76c2cc{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-11 09:34:52.172 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1df1ced0{/static/sql,null,AVAILABLE,@Spark}
2025-06-11 09:34:53.192 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 186.52439 ms
2025-06-11 09:34:53.231 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 28.138037 ms
2025-06-11 09:34:54.997 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-11 09:34:54.997 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-11 09:34:55.004 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@7cb2651f{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:34:55.009 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-11 09:34:55.024 [dispatcher-event-loop-3 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-11 09:34:55.034 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-11 09:34:55.035 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-11 09:34:55.040 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-11 09:34:55.044 [dispatcher-event-loop-3 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-11 09:34:55.049 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-11 09:34:55.050 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-11 09:34:55.051 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-da336166-d515-4385-a95b-84240b94f413
2025-06-11 09:35:25.463 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-11 09:35:25.620 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-11 09:35:25.708 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:35:25.708 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-11 09:35:25.708 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:35:25.709 [main INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-11 09:35:25.729 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-11 09:35:25.741 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-11 09:35:25.741 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-11 09:35:25.793 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-11 09:35:25.793 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-11 09:35:25.794 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-11 09:35:25.794 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-11 09:35:25.794 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-11 09:35:26.041 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 34495.
2025-06-11 09:35:26.082 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-11 09:35:26.117 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-11 09:35:26.134 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-11 09:35:26.134 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-11 09:35:26.138 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-11 09:35:26.156 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-d11d2197-5b2a-4edc-ae44-701f0ae55b09
2025-06-11 09:35:26.186 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 966.0 MiB
2025-06-11 09:35:26.200 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-11 09:35:26.234 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @2251ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-11 09:35:26.350 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-11 09:35:26.360 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-11 09:35:26.385 [main INFO ] org.sparkproject.jetty.server.Server - Started @2403ms
2025-06-11 09:35:26.423 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@58d73d19{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:35:26.424 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-11 09:35:26.444 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cb2651f{/,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.537 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-11 09:35:26.545 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-11 09:35:26.573 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36775.
2025-06-11 09:35:26.573 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:36775
2025-06-11 09:35:26.575 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-11 09:35:26.582 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 36775, None)
2025-06-11 09:35:26.589 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:36775 with 966.0 MiB RAM, BlockManagerId(driver, phamviethoa, 36775, None)
2025-06-11 09:35:26.596 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 36775, None)
2025-06-11 09:35:26.598 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 36775, None)
2025-06-11 09:35:26.750 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7cb2651f{/,null,STOPPED,@Spark}
2025-06-11 09:35:26.753 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67b100fe{/jobs,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.753 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ba5aa7a{/jobs/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.754 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c931d35{/jobs/job,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.755 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49122b8f{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.756 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4beabeec{/stages,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.757 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b22d8a1{/stages/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.758 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31a3f4de{/stages/stage,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.759 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2dd2e270{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.760 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f3e19b3{/stages/pool,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.761 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d04529c{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.761 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b16e202{/storage,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.762 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cd5122d{/storage/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.763 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10c07b8d{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.765 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@360bc645{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.766 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d51e129{/environment,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.767 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1894e40d{/environment/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.768 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7342e05d{/executors,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.769 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15383681{/executors/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.770 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@109a2025{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.771 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@761956ac{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.780 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@304d0259{/static,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.781 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e08acf9{/,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.784 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78cd163b{/api,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.785 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@672b72ba{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.788 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@312b34e3{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.796 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ac4ccad{/metrics/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:26.995 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-11 09:35:27.010 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-11 09:35:27.028 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@806996{/SQL,null,AVAILABLE,@Spark}
2025-06-11 09:35:27.030 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@257e0827{/SQL/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:27.031 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@376c7d7d{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-11 09:35:27.032 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fba233d{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:27.045 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b48e183{/static/sql,null,AVAILABLE,@Spark}
2025-06-11 09:35:28.064 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 186.668686 ms
2025-06-11 09:35:28.099 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 24.449886 ms
2025-06-11 09:35:29.826 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-11 09:35:29.827 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-11 09:35:29.836 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@58d73d19{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:35:29.840 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-11 09:35:29.855 [dispatcher-event-loop-3 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-11 09:35:29.865 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-11 09:35:29.865 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-11 09:35:29.871 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-11 09:35:29.873 [dispatcher-event-loop-3 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-11 09:35:29.880 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-11 09:35:29.881 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-11 09:35:29.882 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-cc264d22-8aa5-4e59-939a-6d070d3744fa
2025-06-11 09:35:50.108 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-11 09:35:50.280 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-11 09:35:50.377 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:35:50.377 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-11 09:35:50.378 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:35:50.378 [main INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-11 09:35:50.398 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-11 09:35:50.409 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-11 09:35:50.410 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-11 09:35:50.461 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-11 09:35:50.461 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-11 09:35:50.462 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-11 09:35:50.462 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-11 09:35:50.462 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-11 09:35:50.692 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 34447.
2025-06-11 09:35:50.718 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-11 09:35:50.750 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-11 09:35:50.767 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-11 09:35:50.767 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-11 09:35:50.771 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-11 09:35:50.788 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-e00cf53e-2ad9-4a7e-9c3d-6534f845729a
2025-06-11 09:35:50.818 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 966.0 MiB
2025-06-11 09:35:50.832 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-11 09:35:50.867 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @2165ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-11 09:35:50.965 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-11 09:35:50.980 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-11 09:35:50.999 [main INFO ] org.sparkproject.jetty.server.Server - Started @2298ms
2025-06-11 09:35:51.033 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@58d73d19{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:35:51.033 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-11 09:35:51.054 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@610df783{/,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.161 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-11 09:35:51.169 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-11 09:35:51.206 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44903.
2025-06-11 09:35:51.206 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:44903
2025-06-11 09:35:51.209 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-11 09:35:51.218 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 44903, None)
2025-06-11 09:35:51.225 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:44903 with 966.0 MiB RAM, BlockManagerId(driver, phamviethoa, 44903, None)
2025-06-11 09:35:51.233 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 44903, None)
2025-06-11 09:35:51.235 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 44903, None)
2025-06-11 09:35:51.410 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@610df783{/,null,STOPPED,@Spark}
2025-06-11 09:35:51.412 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44b194fe{/jobs,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.414 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fe46690{/jobs/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.416 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@476ee5b3{/jobs/job,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.418 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cd4a4d7{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.419 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18da4dd{/stages,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.420 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68880c21{/stages/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.421 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14fa92af{/stages/stage,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.423 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@339a3670{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.424 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c7a8af2{/stages/pool,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.425 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@720bf653{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.426 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4edef76c{/storage,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.427 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70c53dbe{/storage/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.429 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21c815e4{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.430 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a331b46{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.432 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@743e66f7{/environment,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.434 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241f05b{/environment/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.435 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/executors,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.436 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/executors/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.438 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c9320c2{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.438 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cc9385{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.448 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7915bca3{/static,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.449 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@336206d8{/,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.451 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f11f64e{/api,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.452 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44bd4b0a{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.454 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@216e0771{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.458 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b6d92e{/metrics/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.613 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-11 09:35:51.621 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-11 09:35:51.636 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@376c7d7d{/SQL,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.637 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fba233d{/SQL/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.638 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d1d8e1a{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.639 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b48e183{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-11 09:35:51.651 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e682398{/static/sql,null,AVAILABLE,@Spark}
2025-06-11 09:35:52.677 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 187.849926 ms
2025-06-11 09:35:52.714 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 26.983842 ms
2025-06-11 09:35:54.563 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-11 09:35:54.612 [main INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-11 09:35:54.613 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-11 09:35:54.613 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-11 09:35:54.613 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749609354612
2025-06-11 09:35:54.956 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-11 09:35:54.960 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-11 09:35:54.960 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-11 09:35:54.960 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-11 09:35:54.962 [main INFO ] o.a.spark.sql.kafka010.KafkaRelation - GetBatch generating RDD of offset range: KafkaOffsetRange(clickstream-events-0,-2,-1,None), KafkaOffsetRange(clickstream-events-1,-2,-1,None)
2025-06-11 09:35:55.302 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 21.95614 ms
2025-06-11 09:35:55.534 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 39.14382 ms
2025-06-11 09:35:55.547 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.517253 ms
2025-06-11 09:35:55.568 [main INFO ] org.apache.spark.SparkContext - Starting job: show at Processor.java:68
2025-06-11 09:35:55.582 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (show at Processor.java:68) with 1 output partitions
2025-06-11 09:35:55.582 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (show at Processor.java:68)
2025-06-11 09:35:55.583 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-11 09:35:55.584 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-11 09:35:55.587 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[10] at show at Processor.java:68), which has no missing parents
2025-06-11 09:35:55.734 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 61.0 KiB, free 965.9 MiB)
2025-06-11 09:35:55.775 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 21.8 KiB, free 965.9 MiB)
2025-06-11 09:35:55.779 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:44903 (size: 21.8 KiB, free: 966.0 MiB)
2025-06-11 09:35:55.786 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-11 09:35:55.805 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[10] at show at Processor.java:68) (first 15 tasks are for partitions Vector(0))
2025-06-11 09:35:55.807 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
2025-06-11 09:35:55.866 [dispatcher-event-loop-2 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes) 
2025-06-11 09:35:55.879 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-11 09:35:56.076 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-11 09:35:56.110 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-11 09:35:56.155 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-11 09:35:56.155 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-11 09:35:56.155 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749609356155
2025-06-11 09:35:56.158 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor-1, groupId=spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor] Assigned to partition(s): clickstream-events-0
2025-06-11 09:35:56.162 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor-1, groupId=spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-11 09:35:56.176 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor-1, groupId=spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor] Cluster ID: 7ooi2p6vTN-ZXN5mav6Y4Q
2025-06-11 09:35:56.190 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor-1, groupId=spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:35:56.191 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor-1, groupId=spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-11 09:35:56.193 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor-1, groupId=spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=10, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:35:56.238 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 19.554979 ms
2025-06-11 09:35:56.263 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 15.322215 ms
2025-06-11 09:35:56.272 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor-1, groupId=spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-11 09:35:56.318 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor-1, groupId=spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-11 09:35:56.822 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor-1, groupId=spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:35:56.823 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor-1, groupId=spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-11 09:35:56.826 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor-1, groupId=spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=10, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:35:57.001 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 42.81269 ms
2025-06-11 09:35:57.088 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2661 bytes result sent to driver
2025-06-11 09:35:57.099 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1242 ms on phamviethoa (executor driver) (1/1)
2025-06-11 09:35:57.101 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-11 09:35:57.106 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (show at Processor.java:68) finished in 1.507 s
2025-06-11 09:35:57.109 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-11 09:35:57.110 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-11 09:35:57.112 [main INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: show at Processor.java:68, took 1.543541 s
2025-06-11 09:35:57.133 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 11.22988 ms
2025-06-11 09:35:57.154 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor-1, groupId=spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-11 09:35:57.154 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor-1, groupId=spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-11 09:35:57.162 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-11 09:35:57.162 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-11 09:35:57.162 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-11 09:35:57.162 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-11 09:35:57.167 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-relation-3d81b07f-0e70-42f9-ab34-7e100ac84880-executor-1 unregistered
2025-06-11 09:35:57.168 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-11 09:35:57.168 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-11 09:35:57.176 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@58d73d19{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:35:57.182 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-11 09:35:57.196 [dispatcher-event-loop-2 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-11 09:35:57.206 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-11 09:35:57.206 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-11 09:35:57.214 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-11 09:35:57.216 [dispatcher-event-loop-2 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-11 09:35:57.223 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-11 09:35:57.223 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-11 09:35:57.224 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-fec5eccf-5bfd-4f9e-925c-cc045ea76a9f
2025-06-11 09:39:55.764 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-11 09:39:55.957 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-11 09:39:56.074 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:39:56.074 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-11 09:39:56.075 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:39:56.075 [main INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-11 09:39:56.113 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-11 09:39:56.130 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-11 09:39:56.130 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-11 09:39:56.209 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-11 09:39:56.209 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-11 09:39:56.210 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-11 09:39:56.210 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-11 09:39:56.210 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-11 09:39:56.449 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 45093.
2025-06-11 09:39:56.472 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-11 09:39:56.503 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-11 09:39:56.519 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-11 09:39:56.520 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-11 09:39:56.523 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-11 09:39:56.542 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-1bb048df-7354-4ba1-b461-7da262688733
2025-06-11 09:39:56.575 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 966.0 MiB
2025-06-11 09:39:56.590 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-11 09:39:56.625 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @2264ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-11 09:39:56.727 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-11 09:39:56.741 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-11 09:39:56.763 [main INFO ] org.sparkproject.jetty.server.Server - Started @2403ms
2025-06-11 09:39:56.802 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@344afa98{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:39:56.803 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-11 09:39:56.824 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b969c4{/,null,AVAILABLE,@Spark}
2025-06-11 09:39:56.922 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-11 09:39:56.929 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-11 09:39:56.958 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41941.
2025-06-11 09:39:56.958 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:41941
2025-06-11 09:39:56.961 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-11 09:39:56.969 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 41941, None)
2025-06-11 09:39:56.977 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:41941 with 966.0 MiB RAM, BlockManagerId(driver, phamviethoa, 41941, None)
2025-06-11 09:39:56.983 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 41941, None)
2025-06-11 09:39:56.985 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 41941, None)
2025-06-11 09:39:57.210 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@62b969c4{/,null,STOPPED,@Spark}
2025-06-11 09:39:57.212 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d3ca6c7{/jobs,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.213 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a638c79{/jobs/json,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.214 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fe46690{/jobs/job,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.215 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b4d50b{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.217 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@476ee5b3{/stages,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.218 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cd4a4d7{/stages/json,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.220 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4195105b{/stages/stage,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.221 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47ffe971{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.222 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14fa92af{/stages/pool,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.223 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@339a3670{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.224 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c7a8af2{/storage,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.225 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@720bf653{/storage/json,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.226 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4edef76c{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.227 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70c53dbe{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.228 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21c815e4{/environment,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.229 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a331b46{/environment/json,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.229 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@743e66f7{/executors,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.230 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241f05b{/executors/json,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.232 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.233 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.244 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c9320c2{/static,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.245 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f0b3cfe{/,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.247 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65a48602{/api,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.248 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26844abb{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.249 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@288ca5f0{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.254 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ee5b2d9{/metrics/json,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.508 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-11 09:39:57.520 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-11 09:39:57.542 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22752544{/SQL,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.543 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69d23296{/SQL/json,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.544 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@427ae189{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.544 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76332405{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-11 09:39:57.556 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@43a65cd8{/static/sql,null,AVAILABLE,@Spark}
2025-06-11 09:39:58.825 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 235.808799 ms
2025-06-11 09:39:58.860 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 24.894089 ms
2025-06-11 09:40:00.851 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-11 09:40:00.910 [main INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-11 09:40:00.911 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-11 09:40:00.912 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-11 09:40:00.912 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749609600910
2025-06-11 09:40:01.284 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-11 09:40:01.289 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-11 09:40:01.289 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-11 09:40:01.289 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-11 09:40:01.290 [main INFO ] o.a.spark.sql.kafka010.KafkaRelation - GetBatch generating RDD of offset range: KafkaOffsetRange(clickstream-events-0,-2,-1,None), KafkaOffsetRange(clickstream-events-1,-2,-1,None)
2025-06-11 09:40:01.644 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 19.163996 ms
2025-06-11 09:40:01.898 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 42.543403 ms
2025-06-11 09:40:01.916 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 12.673856 ms
2025-06-11 09:40:01.939 [main INFO ] org.apache.spark.SparkContext - Starting job: show at Processor.java:70
2025-06-11 09:40:01.959 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (show at Processor.java:70) with 1 output partitions
2025-06-11 09:40:01.960 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (show at Processor.java:70)
2025-06-11 09:40:01.960 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-11 09:40:01.961 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-11 09:40:01.966 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[10] at show at Processor.java:70), which has no missing parents
2025-06-11 09:40:02.103 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 61.8 KiB, free 965.9 MiB)
2025-06-11 09:40:02.142 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 21.9 KiB, free 965.9 MiB)
2025-06-11 09:40:02.147 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:41941 (size: 21.9 KiB, free: 966.0 MiB)
2025-06-11 09:40:02.153 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-11 09:40:02.181 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[10] at show at Processor.java:70) (first 15 tasks are for partitions Vector(0))
2025-06-11 09:40:02.182 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
2025-06-11 09:40:02.246 [dispatcher-event-loop-2 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes) 
2025-06-11 09:40:02.262 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-11 09:40:02.474 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-11 09:40:02.507 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-11 09:40:02.555 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-11 09:40:02.555 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-11 09:40:02.555 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749609602555
2025-06-11 09:40:02.558 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor-1, groupId=spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor] Assigned to partition(s): clickstream-events-0
2025-06-11 09:40:02.562 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor-1, groupId=spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-11 09:40:02.577 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor-1, groupId=spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor] Cluster ID: 7ooi2p6vTN-ZXN5mav6Y4Q
2025-06-11 09:40:02.589 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor-1, groupId=spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:40:02.590 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor-1, groupId=spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-11 09:40:02.593 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor-1, groupId=spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=10, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:40:02.649 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 22.3153 ms
2025-06-11 09:40:02.675 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 16.709918 ms
2025-06-11 09:40:02.683 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor-1, groupId=spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-11 09:40:02.733 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor-1, groupId=spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-11 09:40:03.240 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor-1, groupId=spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:40:03.240 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor-1, groupId=spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-11 09:40:03.243 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor-1, groupId=spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=10, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:40:03.393 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 30.840573 ms
2025-06-11 09:40:03.477 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2721 bytes result sent to driver
2025-06-11 09:40:03.492 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1259 ms on phamviethoa (executor driver) (1/1)
2025-06-11 09:40:03.494 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-11 09:40:03.500 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (show at Processor.java:70) finished in 1.521 s
2025-06-11 09:40:03.502 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-11 09:40:03.503 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-11 09:40:03.505 [main INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: show at Processor.java:70, took 1.564617 s
2025-06-11 09:40:03.524 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.318273 ms
2025-06-11 09:40:03.543 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor-1, groupId=spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-11 09:40:03.544 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor-1, groupId=spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-11 09:40:03.550 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-11 09:40:03.550 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-11 09:40:03.551 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-11 09:40:03.551 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-11 09:40:03.555 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-relation-553ed04c-cd55-46a4-b6e3-ed2d1c8c3fd6-executor-1 unregistered
2025-06-11 09:40:03.556 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-11 09:40:03.556 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-11 09:40:03.564 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@344afa98{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:40:03.569 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-11 09:40:03.587 [dispatcher-event-loop-2 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-11 09:40:03.602 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-11 09:40:03.602 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-11 09:40:03.610 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-11 09:40:03.615 [dispatcher-event-loop-2 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-11 09:40:03.623 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-11 09:40:03.623 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-11 09:40:03.624 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-b071dff9-6dfc-4549-af34-c4222cfa6324
2025-06-11 09:41:48.837 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-11 09:41:49.083 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-11 09:41:49.192 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:41:49.193 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-11 09:41:49.193 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:41:49.193 [main INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-11 09:41:49.215 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-11 09:41:49.228 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-11 09:41:49.229 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-11 09:41:49.310 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-11 09:41:49.311 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-11 09:41:49.311 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-11 09:41:49.312 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-11 09:41:49.312 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-11 09:41:49.576 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 33347.
2025-06-11 09:41:49.609 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-11 09:41:49.640 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-11 09:41:49.656 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-11 09:41:49.657 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-11 09:41:49.660 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-11 09:41:49.688 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-06d585f1-56f4-490c-b840-b7536a41623f
2025-06-11 09:41:49.720 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 966.0 MiB
2025-06-11 09:41:49.734 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-11 09:41:49.774 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @2221ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-11 09:41:49.889 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-11 09:41:49.899 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-11 09:41:49.922 [main INFO ] org.sparkproject.jetty.server.Server - Started @2370ms
2025-06-11 09:41:49.954 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@486c36a4{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:41:49.954 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-11 09:41:49.973 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e1624c7{/,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.085 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-11 09:41:50.092 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-11 09:41:50.117 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37955.
2025-06-11 09:41:50.117 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:37955
2025-06-11 09:41:50.119 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-11 09:41:50.128 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 37955, None)
2025-06-11 09:41:50.134 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:37955 with 966.0 MiB RAM, BlockManagerId(driver, phamviethoa, 37955, None)
2025-06-11 09:41:50.141 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 37955, None)
2025-06-11 09:41:50.143 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 37955, None)
2025-06-11 09:41:50.302 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@3e1624c7{/,null,STOPPED,@Spark}
2025-06-11 09:41:50.305 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ba5aa7a{/jobs,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.307 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22e5f96e{/jobs/json,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.309 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49122b8f{/jobs/job,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.310 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4beabeec{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.312 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b22d8a1{/stages,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.313 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59ed3e6c{/stages/json,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.315 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2dd2e270{/stages/stage,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.316 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f3e19b3{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.317 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d04529c{/stages/pool,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.319 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b16e202{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.320 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cd5122d{/storage,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.320 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10c07b8d{/storage/json,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.322 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@360bc645{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.323 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d51e129{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.324 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1894e40d{/environment,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.325 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7342e05d{/environment/json,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.326 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15383681{/executors,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.328 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@109a2025{/executors/json,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.330 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@761956ac{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.331 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@304d0259{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.345 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133661d{/static,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.347 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78cd163b{/,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.349 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14ef2482{/api,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.350 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@312b34e3{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.351 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a865273{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.359 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14982a82{/metrics/json,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.634 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-11 09:41:50.641 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-11 09:41:50.654 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@257e0827{/SQL,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.655 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21ba2445{/SQL/json,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.656 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fba233d{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.657 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16a9eb2e{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-11 09:41:50.667 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30c1da48{/static/sql,null,AVAILABLE,@Spark}
2025-06-11 09:41:51.727 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 202.261174 ms
2025-06-11 09:41:51.762 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 24.654469 ms
2025-06-11 09:41:53.627 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-11 09:41:53.628 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-11 09:41:53.639 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@486c36a4{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:41:53.644 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-11 09:41:53.661 [dispatcher-event-loop-3 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-11 09:41:53.674 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-11 09:41:53.675 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-11 09:41:53.682 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-11 09:41:53.688 [dispatcher-event-loop-3 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-11 09:41:53.698 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-11 09:41:53.699 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-11 09:41:53.703 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-dac9b69d-db50-4290-93f2-23a1848c250c
2025-06-11 09:42:47.521 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-11 09:42:47.686 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-11 09:42:47.777 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:42:47.778 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-11 09:42:47.778 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:42:47.779 [main INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-11 09:42:47.800 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-11 09:42:47.812 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-11 09:42:47.813 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-11 09:42:47.867 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-11 09:42:47.868 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-11 09:42:47.868 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-11 09:42:47.869 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-11 09:42:47.869 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-11 09:42:48.104 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 46343.
2025-06-11 09:42:48.130 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-11 09:42:48.163 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-11 09:42:48.195 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-11 09:42:48.196 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-11 09:42:48.199 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-11 09:42:48.217 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-fb4aea58-2fd0-47be-8dfb-b4724d1135b5
2025-06-11 09:42:48.248 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 966.0 MiB
2025-06-11 09:42:48.263 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-11 09:42:48.300 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @2020ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-11 09:42:48.427 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-11 09:42:48.437 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-11 09:42:48.461 [main INFO ] org.sparkproject.jetty.server.Server - Started @2183ms
2025-06-11 09:42:48.495 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@76e22d04{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:42:48.496 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-11 09:42:48.518 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4441d567{/,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.626 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-11 09:42:48.634 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-11 09:42:48.665 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35825.
2025-06-11 09:42:48.666 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:35825
2025-06-11 09:42:48.669 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-11 09:42:48.677 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 35825, None)
2025-06-11 09:42:48.686 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:35825 with 966.0 MiB RAM, BlockManagerId(driver, phamviethoa, 35825, None)
2025-06-11 09:42:48.696 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 35825, None)
2025-06-11 09:42:48.698 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 35825, None)
2025-06-11 09:42:48.895 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4441d567{/,null,STOPPED,@Spark}
2025-06-11 09:42:48.898 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44b18fe4{/jobs,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.900 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d3ca6c7{/jobs/json,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.902 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44b194fe{/jobs/job,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.904 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fe46690{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.906 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b4d50b{/stages,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.908 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@476ee5b3{/stages/json,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.913 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68880c21{/stages/stage,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.914 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4195105b{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.916 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47ffe971{/stages/pool,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.918 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14fa92af{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.920 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@339a3670{/storage,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.923 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c7a8af2{/storage/json,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.925 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@720bf653{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.926 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4edef76c{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.927 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70c53dbe{/environment,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.927 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21c815e4{/environment/json,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.928 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a331b46{/executors,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.929 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@743e66f7{/executors/json,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.930 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241f05b{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.931 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.941 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/static,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.942 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42ea287{/,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.943 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f0b3cfe{/api,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.944 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7718a40f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.945 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26844abb{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-11 09:42:48.949 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fd9ebde{/metrics/json,null,AVAILABLE,@Spark}
2025-06-11 09:42:49.230 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-11 09:42:49.242 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-11 09:42:49.259 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b612c6{/SQL,null,AVAILABLE,@Spark}
2025-06-11 09:42:49.260 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22752544{/SQL/json,null,AVAILABLE,@Spark}
2025-06-11 09:42:49.261 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4784efd9{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-11 09:42:49.262 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@427ae189{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-11 09:42:49.273 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@514de325{/static/sql,null,AVAILABLE,@Spark}
2025-06-11 09:42:50.447 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 199.935365 ms
2025-06-11 09:42:50.484 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 26.116722 ms
2025-06-11 09:42:52.292 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-11 09:42:52.292 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-11 09:42:52.304 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@76e22d04{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:42:52.311 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-11 09:42:52.334 [dispatcher-event-loop-3 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-11 09:42:52.345 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-11 09:42:52.345 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-11 09:42:52.354 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-11 09:42:52.357 [dispatcher-event-loop-3 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-11 09:42:52.364 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-11 09:42:52.364 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-11 09:42:52.365 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-26f45029-e3e3-4b94-97a3-11d51d542318
2025-06-11 09:43:11.154 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-11 09:43:11.328 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-11 09:43:11.411 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:43:11.411 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-11 09:43:11.411 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:43:11.412 [main INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-11 09:43:11.431 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-11 09:43:11.442 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-11 09:43:11.443 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-11 09:43:11.496 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-11 09:43:11.497 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-11 09:43:11.497 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-11 09:43:11.497 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-11 09:43:11.498 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-11 09:43:11.737 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 42377.
2025-06-11 09:43:11.763 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-11 09:43:11.793 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-11 09:43:11.809 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-11 09:43:11.809 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-11 09:43:11.813 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-11 09:43:11.830 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-8f5c3fed-8040-4609-9305-319f9ba48c18
2025-06-11 09:43:11.859 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 966.0 MiB
2025-06-11 09:43:11.873 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-11 09:43:11.907 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @2192ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-11 09:43:12.008 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-11 09:43:12.022 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-11 09:43:12.044 [main INFO ] org.sparkproject.jetty.server.Server - Started @2330ms
2025-06-11 09:43:12.078 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@579fd95f{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:43:12.078 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-11 09:43:12.098 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4441d567{/,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.205 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-11 09:43:12.214 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-11 09:43:12.251 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46239.
2025-06-11 09:43:12.251 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:46239
2025-06-11 09:43:12.255 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-11 09:43:12.270 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 46239, None)
2025-06-11 09:43:12.278 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:46239 with 966.0 MiB RAM, BlockManagerId(driver, phamviethoa, 46239, None)
2025-06-11 09:43:12.282 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 46239, None)
2025-06-11 09:43:12.284 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 46239, None)
2025-06-11 09:43:12.451 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4441d567{/,null,STOPPED,@Spark}
2025-06-11 09:43:12.453 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44b18fe4{/jobs,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.455 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d3ca6c7{/jobs/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.459 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44b194fe{/jobs/job,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.461 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fe46690{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.464 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b4d50b{/stages,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.466 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@476ee5b3{/stages/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.470 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68880c21{/stages/stage,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.474 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4195105b{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.477 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47ffe971{/stages/pool,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.479 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14fa92af{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.483 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@339a3670{/storage,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.485 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c7a8af2{/storage/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.487 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@720bf653{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.490 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4edef76c{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.492 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70c53dbe{/environment,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.494 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21c815e4{/environment/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.495 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a331b46{/executors,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.496 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@743e66f7{/executors/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.499 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241f05b{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.500 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.514 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/static,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.516 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42ea287{/,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.518 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f0b3cfe{/api,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.520 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7718a40f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.521 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26844abb{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.527 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fd9ebde{/metrics/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.701 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-11 09:43:12.710 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-11 09:43:12.728 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b612c6{/SQL,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.729 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22752544{/SQL/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.731 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4784efd9{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.732 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@427ae189{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:12.747 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@514de325{/static/sql,null,AVAILABLE,@Spark}
2025-06-11 09:43:13.779 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 203.971122 ms
2025-06-11 09:43:13.815 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 24.682895 ms
2025-06-11 09:43:15.772 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-11 09:43:15.845 [main INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-11 09:43:15.847 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-11 09:43:15.847 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-11 09:43:15.847 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749609795845
2025-06-11 09:43:16.185 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-11 09:43:16.190 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-11 09:43:16.190 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-11 09:43:16.190 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-11 09:43:16.192 [main INFO ] o.a.spark.sql.kafka010.KafkaRelation - GetBatch generating RDD of offset range: KafkaOffsetRange(clickstream-events-0,-2,-1,None), KafkaOffsetRange(clickstream-events-1,-2,-1,None)
2025-06-11 09:43:16.541 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 19.095033 ms
2025-06-11 09:43:16.828 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 61.610884 ms
2025-06-11 09:43:16.839 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.069676 ms
2025-06-11 09:43:16.860 [main INFO ] org.apache.spark.SparkContext - Starting job: show at Processor.java:74
2025-06-11 09:43:16.874 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (show at Processor.java:74) with 1 output partitions
2025-06-11 09:43:16.874 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (show at Processor.java:74)
2025-06-11 09:43:16.874 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-11 09:43:16.875 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-11 09:43:16.879 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[10] at show at Processor.java:74), which has no missing parents
2025-06-11 09:43:17.009 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 72.5 KiB, free 965.9 MiB)
2025-06-11 09:43:17.035 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.5 KiB, free 965.9 MiB)
2025-06-11 09:43:17.040 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:46239 (size: 24.5 KiB, free: 966.0 MiB)
2025-06-11 09:43:17.045 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-11 09:43:17.062 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[10] at show at Processor.java:74) (first 15 tasks are for partitions Vector(0))
2025-06-11 09:43:17.064 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
2025-06-11 09:43:17.129 [dispatcher-event-loop-2 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes) 
2025-06-11 09:43:17.142 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-11 09:43:17.349 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-11 09:43:17.380 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-11 09:43:17.424 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-11 09:43:17.424 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-11 09:43:17.424 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749609797424
2025-06-11 09:43:17.426 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor-1, groupId=spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor] Assigned to partition(s): clickstream-events-0
2025-06-11 09:43:17.430 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor-1, groupId=spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-11 09:43:17.444 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor-1, groupId=spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor] Cluster ID: 7ooi2p6vTN-ZXN5mav6Y4Q
2025-06-11 09:43:17.457 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor-1, groupId=spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:43:17.458 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor-1, groupId=spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-11 09:43:17.460 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor-1, groupId=spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=10, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:43:17.498 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 15.359556 ms
2025-06-11 09:43:17.520 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 14.370596 ms
2025-06-11 09:43:17.528 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor-1, groupId=spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-11 09:43:17.564 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor-1, groupId=spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-11 09:43:18.065 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor-1, groupId=spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:43:18.066 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor-1, groupId=spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-11 09:43:18.069 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor-1, groupId=spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=10, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:43:18.213 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 32.870476 ms
2025-06-11 09:43:18.289 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2916 bytes result sent to driver
2025-06-11 09:43:18.307 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1188 ms on phamviethoa (executor driver) (1/1)
2025-06-11 09:43:18.310 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-11 09:43:18.314 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (show at Processor.java:74) finished in 1.424 s
2025-06-11 09:43:18.318 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-11 09:43:18.318 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-11 09:43:18.320 [main INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: show at Processor.java:74, took 1.460129 s
2025-06-11 09:43:18.351 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 13.825143 ms
2025-06-11 09:43:18.374 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor-1, groupId=spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-11 09:43:18.374 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor-1, groupId=spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-11 09:43:18.382 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-11 09:43:18.383 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-11 09:43:18.383 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-11 09:43:18.383 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-11 09:43:18.389 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-relation-593cffba-74f6-466e-94d1-e34cda892981-executor-1 unregistered
2025-06-11 09:43:18.390 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-11 09:43:18.390 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-11 09:43:18.403 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@579fd95f{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:43:18.409 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-11 09:43:18.424 [dispatcher-event-loop-2 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-11 09:43:18.435 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-11 09:43:18.435 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-11 09:43:18.440 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-11 09:43:18.443 [dispatcher-event-loop-2 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-11 09:43:18.450 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-11 09:43:18.451 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-11 09:43:18.452 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-3b81b158-9a63-4ebb-bf9b-20643f6fb576
2025-06-11 09:43:55.278 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-11 09:43:55.446 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-11 09:43:55.545 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:43:55.546 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-11 09:43:55.546 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:43:55.547 [main INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-11 09:43:55.569 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-11 09:43:55.581 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-11 09:43:55.581 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-11 09:43:55.633 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-11 09:43:55.634 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-11 09:43:55.634 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-11 09:43:55.634 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-11 09:43:55.634 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-11 09:43:55.896 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 37269.
2025-06-11 09:43:55.923 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-11 09:43:55.956 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-11 09:43:55.973 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-11 09:43:55.974 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-11 09:43:55.980 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-11 09:43:55.998 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-6eba2dec-6bc2-4533-8d98-15085ac214a8
2025-06-11 09:43:56.029 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 966.0 MiB
2025-06-11 09:43:56.051 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-11 09:43:56.096 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @2104ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-11 09:43:56.245 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-11 09:43:56.257 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-11 09:43:56.275 [main INFO ] org.sparkproject.jetty.server.Server - Started @2284ms
2025-06-11 09:43:56.307 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@19dc4295{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:43:56.307 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-11 09:43:56.335 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@610df783{/,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.435 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-11 09:43:56.442 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-11 09:43:56.468 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42603.
2025-06-11 09:43:56.468 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:42603
2025-06-11 09:43:56.471 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-11 09:43:56.479 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 42603, None)
2025-06-11 09:43:56.486 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:42603 with 966.0 MiB RAM, BlockManagerId(driver, phamviethoa, 42603, None)
2025-06-11 09:43:56.493 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 42603, None)
2025-06-11 09:43:56.495 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 42603, None)
2025-06-11 09:43:56.668 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@610df783{/,null,STOPPED,@Spark}
2025-06-11 09:43:56.670 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44b194fe{/jobs,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.673 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fe46690{/jobs/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.675 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@476ee5b3{/jobs/job,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.677 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cd4a4d7{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.680 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18da4dd{/stages,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.683 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68880c21{/stages/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.687 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14fa92af{/stages/stage,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.689 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@339a3670{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.691 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c7a8af2{/stages/pool,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.692 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@720bf653{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.694 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4edef76c{/storage,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.695 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70c53dbe{/storage/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.697 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21c815e4{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.698 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a331b46{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.700 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@743e66f7{/environment,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.702 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241f05b{/environment/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.703 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/executors,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.704 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/executors/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.708 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c9320c2{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.710 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cc9385{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.724 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7915bca3{/static,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.727 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@336206d8{/,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.731 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f11f64e{/api,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.733 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44bd4b0a{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.735 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@216e0771{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.744 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b6d92e{/metrics/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.948 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-11 09:43:56.960 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-11 09:43:56.983 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@376c7d7d{/SQL,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.985 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fba233d{/SQL/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.987 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d1d8e1a{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-11 09:43:56.988 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b48e183{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-11 09:43:57.001 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e682398{/static/sql,null,AVAILABLE,@Spark}
2025-06-11 09:43:58.044 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 189.703548 ms
2025-06-11 09:43:58.081 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 25.944451 ms
2025-06-11 09:44:00.030 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-11 09:44:00.086 [main INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-11 09:44:00.087 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-11 09:44:00.087 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-11 09:44:00.087 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749609840086
2025-06-11 09:44:00.428 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-11 09:44:00.435 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-11 09:44:00.436 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-11 09:44:00.436 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-11 09:44:00.437 [main INFO ] o.a.spark.sql.kafka010.KafkaRelation - GetBatch generating RDD of offset range: KafkaOffsetRange(clickstream-events-0,-2,-1,None), KafkaOffsetRange(clickstream-events-1,-2,-1,None)
2025-06-11 09:44:00.768 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 19.31576 ms
2025-06-11 09:44:01.029 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 56.060285 ms
2025-06-11 09:44:01.040 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.085176 ms
2025-06-11 09:44:01.061 [main INFO ] org.apache.spark.SparkContext - Starting job: show at Processor.java:74
2025-06-11 09:44:01.076 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (show at Processor.java:74) with 1 output partitions
2025-06-11 09:44:01.076 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (show at Processor.java:74)
2025-06-11 09:44:01.077 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-11 09:44:01.078 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-11 09:44:01.082 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[10] at show at Processor.java:74), which has no missing parents
2025-06-11 09:44:01.254 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 72.5 KiB, free 965.9 MiB)
2025-06-11 09:44:01.293 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.6 KiB, free 965.9 MiB)
2025-06-11 09:44:01.300 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:42603 (size: 24.6 KiB, free: 966.0 MiB)
2025-06-11 09:44:01.306 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-11 09:44:01.324 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[10] at show at Processor.java:74) (first 15 tasks are for partitions Vector(0))
2025-06-11 09:44:01.325 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
2025-06-11 09:44:01.379 [dispatcher-event-loop-2 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes) 
2025-06-11 09:44:01.394 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-11 09:44:01.564 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-11 09:44:01.599 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-11 09:44:01.645 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-11 09:44:01.646 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-11 09:44:01.646 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749609841645
2025-06-11 09:44:01.648 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor-1, groupId=spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor] Assigned to partition(s): clickstream-events-0
2025-06-11 09:44:01.653 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor-1, groupId=spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-11 09:44:01.667 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor-1, groupId=spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor] Cluster ID: 7ooi2p6vTN-ZXN5mav6Y4Q
2025-06-11 09:44:01.682 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor-1, groupId=spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:44:01.684 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor-1, groupId=spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-11 09:44:01.686 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor-1, groupId=spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=10, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:44:01.728 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 17.267372 ms
2025-06-11 09:44:01.754 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 14.801442 ms
2025-06-11 09:44:01.762 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor-1, groupId=spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-11 09:44:01.799 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor-1, groupId=spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-11 09:44:02.302 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor-1, groupId=spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:44:02.302 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor-1, groupId=spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-11 09:44:02.305 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor-1, groupId=spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=10, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-11 09:44:02.438 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 29.241408 ms
2025-06-11 09:44:02.502 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2956 bytes result sent to driver
2025-06-11 09:44:02.515 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1146 ms on phamviethoa (executor driver) (1/1)
2025-06-11 09:44:02.517 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-11 09:44:02.525 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (show at Processor.java:74) finished in 1.429 s
2025-06-11 09:44:02.531 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-11 09:44:02.532 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-11 09:44:02.535 [main INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: show at Processor.java:74, took 1.473244 s
2025-06-11 09:44:02.559 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 11.859435 ms
2025-06-11 09:44:02.579 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor-1, groupId=spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-11 09:44:02.579 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor-1, groupId=spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-11 09:44:02.585 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-11 09:44:02.586 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-11 09:44:02.586 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-11 09:44:02.586 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-11 09:44:02.591 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-relation-fb3a7261-7026-4a9d-8d68-b7898f0628e4-executor-1 unregistered
2025-06-11 09:44:02.592 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-11 09:44:02.592 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-11 09:44:02.599 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@19dc4295{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:44:02.603 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-11 09:44:02.615 [dispatcher-event-loop-2 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-11 09:44:02.624 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-11 09:44:02.625 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-11 09:44:02.630 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-11 09:44:02.632 [dispatcher-event-loop-2 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-11 09:44:02.638 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-11 09:44:02.638 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-11 09:44:02.639 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-8e5716cd-7d19-4081-82ba-1556183d3e11
2025-06-11 09:47:19.706 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-11 09:47:19.901 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-11 09:47:20.019 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:47:20.020 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-11 09:47:20.020 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:47:20.021 [main INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-11 09:47:20.044 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-11 09:47:20.055 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-11 09:47:20.056 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-11 09:47:20.118 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-11 09:47:20.119 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-11 09:47:20.119 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-11 09:47:20.119 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-11 09:47:20.120 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-11 09:47:20.376 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 34249.
2025-06-11 09:47:20.409 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-11 09:47:20.449 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-11 09:47:20.467 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-11 09:47:20.468 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-11 09:47:20.472 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-11 09:47:20.489 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-ca88123c-7ece-449a-adc9-61e53c26c351
2025-06-11 09:47:20.529 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 966.0 MiB
2025-06-11 09:47:20.546 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-11 09:47:20.592 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @2290ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-11 09:47:20.700 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-11 09:47:20.710 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-11 09:47:20.725 [main INFO ] org.sparkproject.jetty.server.Server - Started @2425ms
2025-06-11 09:47:20.756 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@5d3c54d4{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:47:20.756 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-11 09:47:20.775 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dcc6211{/,null,AVAILABLE,@Spark}
2025-06-11 09:47:20.892 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-11 09:47:20.902 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-11 09:47:20.923 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33209.
2025-06-11 09:47:20.923 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:33209
2025-06-11 09:47:20.925 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-11 09:47:20.932 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 33209, None)
2025-06-11 09:47:20.938 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:33209 with 966.0 MiB RAM, BlockManagerId(driver, phamviethoa, 33209, None)
2025-06-11 09:47:20.943 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 33209, None)
2025-06-11 09:47:20.944 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 33209, None)
2025-06-11 09:47:21.095 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@dcc6211{/,null,STOPPED,@Spark}
2025-06-11 09:47:21.098 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22e5f96e{/jobs,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.099 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c931d35{/jobs/json,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.101 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4beabeec{/jobs/job,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.102 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b22d8a1{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.103 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59ed3e6c{/stages,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.105 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@317e9c3c{/stages/json,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.106 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f3e19b3{/stages/stage,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.108 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d04529c{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.109 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b16e202{/stages/pool,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.110 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cd5122d{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.112 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10c07b8d{/storage,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.113 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@360bc645{/storage/json,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.115 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d51e129{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.116 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1894e40d{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.117 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7342e05d{/environment,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.118 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15383681{/environment/json,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.119 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@109a2025{/executors,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.120 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@761956ac{/executors/json,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.123 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@304d0259{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.124 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133661d{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.135 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3414a8c3{/static,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.136 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14ef2482{/,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.138 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75483843{/api,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.140 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a865273{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.141 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4068102e{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.147 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72f8ae0c{/metrics/json,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.327 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-11 09:47:21.337 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-11 09:47:21.353 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21ba2445{/SQL,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.354 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c3820bb{/SQL/json,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.355 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16a9eb2e{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.357 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@187e5235{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-11 09:47:21.374 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f1ef9d6{/static/sql,null,AVAILABLE,@Spark}
2025-06-11 09:47:22.472 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 199.759169 ms
2025-06-11 09:47:22.513 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 29.873019 ms
2025-06-11 09:47:25.665 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-11 09:47:25.666 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-11 09:47:25.673 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@5d3c54d4{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:47:25.677 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-11 09:47:25.692 [dispatcher-event-loop-3 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-11 09:47:25.703 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-11 09:47:25.703 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-11 09:47:25.711 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-11 09:47:25.714 [dispatcher-event-loop-3 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-11 09:47:25.720 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-11 09:47:25.720 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-11 09:47:25.721 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-fd08f733-1467-4a46-84f4-b1755cdc0144
2025-06-11 09:50:14.866 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-11 09:50:15.112 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-11 09:50:15.221 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:50:15.221 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-11 09:50:15.222 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:50:15.222 [main INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-11 09:50:15.244 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-11 09:50:15.256 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-11 09:50:15.257 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-11 09:50:15.392 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-11 09:50:15.393 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-11 09:50:15.393 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-11 09:50:15.394 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-11 09:50:15.394 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-11 09:50:15.646 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 42149.
2025-06-11 09:50:15.674 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-11 09:50:15.709 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-11 09:50:15.727 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-11 09:50:15.727 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-11 09:50:15.731 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-11 09:50:15.760 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-8e603426-6d0e-44d4-99da-2b277b271d1f
2025-06-11 09:50:15.794 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 966.0 MiB
2025-06-11 09:50:15.809 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-11 09:50:15.847 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @2379ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-11 09:50:15.958 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-11 09:50:15.969 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-11 09:50:15.985 [main INFO ] org.sparkproject.jetty.server.Server - Started @2518ms
2025-06-11 09:50:16.020 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@20a6c75a{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:50:16.020 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-11 09:50:16.039 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4441d567{/,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.137 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-11 09:50:16.144 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-11 09:50:16.165 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32793.
2025-06-11 09:50:16.165 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:32793
2025-06-11 09:50:16.168 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-11 09:50:16.177 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 32793, None)
2025-06-11 09:50:16.185 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:32793 with 966.0 MiB RAM, BlockManagerId(driver, phamviethoa, 32793, None)
2025-06-11 09:50:16.190 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 32793, None)
2025-06-11 09:50:16.192 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 32793, None)
2025-06-11 09:50:16.372 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4441d567{/,null,STOPPED,@Spark}
2025-06-11 09:50:16.375 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44b18fe4{/jobs,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.377 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d3ca6c7{/jobs/json,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.380 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44b194fe{/jobs/job,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.383 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fe46690{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.386 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b4d50b{/stages,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.389 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@476ee5b3{/stages/json,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.394 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68880c21{/stages/stage,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.396 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4195105b{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.399 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47ffe971{/stages/pool,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.402 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14fa92af{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.405 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@339a3670{/storage,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.409 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c7a8af2{/storage/json,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.412 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@720bf653{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.415 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4edef76c{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.419 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70c53dbe{/environment,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.422 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21c815e4{/environment/json,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.425 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a331b46{/executors,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.429 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@743e66f7{/executors/json,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.435 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241f05b{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.438 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.451 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/static,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.453 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42ea287{/,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.455 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f0b3cfe{/api,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.456 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7718a40f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.457 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26844abb{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.462 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fd9ebde{/metrics/json,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.676 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-11 09:50:16.687 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-11 09:50:16.702 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b612c6{/SQL,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.703 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22752544{/SQL/json,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.704 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4784efd9{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.705 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@427ae189{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-11 09:50:16.717 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@514de325{/static/sql,null,AVAILABLE,@Spark}
2025-06-11 09:50:17.828 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 195.875817 ms
2025-06-11 09:50:17.863 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 24.310698 ms
2025-06-11 09:50:20.303 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-11 09:50:20.304 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-11 09:50:20.311 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@20a6c75a{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:50:20.316 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-11 09:50:20.330 [dispatcher-event-loop-3 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-11 09:50:20.341 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-11 09:50:20.342 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-11 09:50:20.347 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-11 09:50:20.350 [dispatcher-event-loop-3 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-11 09:50:20.356 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-11 09:50:20.356 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-11 09:50:20.357 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-45fa4a47-39dc-4972-90d7-3af0735b4246
2025-06-11 09:51:13.155 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-11 09:51:13.326 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-11 09:51:13.416 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:51:13.416 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-11 09:51:13.417 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:51:13.417 [main INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-11 09:51:13.438 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-11 09:51:13.449 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-11 09:51:13.450 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-11 09:51:13.505 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-11 09:51:13.506 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-11 09:51:13.507 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-11 09:51:13.507 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-11 09:51:13.507 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-11 09:51:13.756 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 38403.
2025-06-11 09:51:13.788 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-11 09:51:13.819 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-11 09:51:13.837 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-11 09:51:13.838 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-11 09:51:13.841 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-11 09:51:13.859 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-36121ff0-9d78-4c02-9b9c-f3a59d23a934
2025-06-11 09:51:13.890 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 966.0 MiB
2025-06-11 09:51:13.908 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-11 09:51:13.947 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @2169ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-11 09:51:14.065 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-11 09:51:14.076 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-11 09:51:14.100 [main INFO ] org.sparkproject.jetty.server.Server - Started @2323ms
2025-06-11 09:51:14.134 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@6444f79f{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:51:14.135 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-11 09:51:14.158 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f387978{/,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.257 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-11 09:51:14.265 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-11 09:51:14.298 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34091.
2025-06-11 09:51:14.299 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:34091
2025-06-11 09:51:14.303 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-11 09:51:14.312 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 34091, None)
2025-06-11 09:51:14.318 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:34091 with 966.0 MiB RAM, BlockManagerId(driver, phamviethoa, 34091, None)
2025-06-11 09:51:14.324 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 34091, None)
2025-06-11 09:51:14.326 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 34091, None)
2025-06-11 09:51:14.483 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1f387978{/,null,STOPPED,@Spark}
2025-06-11 09:51:14.484 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@320a8ebf{/jobs,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.485 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44b18fe4{/jobs/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.486 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a638c79{/jobs/job,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.487 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44b194fe{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.488 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fe46690{/stages,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.488 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b4d50b{/stages/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.490 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18da4dd{/stages/stage,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.490 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68880c21{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.491 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4195105b{/stages/pool,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.492 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47ffe971{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.493 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14fa92af{/storage,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.493 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@339a3670{/storage/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.494 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c7a8af2{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.495 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@720bf653{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.496 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4edef76c{/environment,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.497 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70c53dbe{/environment/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.498 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21c815e4{/executors,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.498 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a331b46{/executors/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.500 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@743e66f7{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.501 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241f05b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.509 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/static,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.510 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7903d448{/,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.513 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42ea287{/api,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.515 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7efd28bd{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.517 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7718a40f{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.523 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e0895f5{/metrics/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.710 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-11 09:51:14.720 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-11 09:51:14.738 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38e7ed69{/SQL,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.739 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b612c6{/SQL/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.741 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c3820bb{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.742 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4784efd9{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:14.759 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5434e40c{/static/sql,null,AVAILABLE,@Spark}
2025-06-11 09:51:15.957 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 247.440294 ms
2025-06-11 09:51:16.007 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 38.366727 ms
2025-06-11 09:51:18.456 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-11 09:51:18.456 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-11 09:51:18.463 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@6444f79f{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:51:18.468 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-11 09:51:18.483 [dispatcher-event-loop-3 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-11 09:51:18.495 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-11 09:51:18.496 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-11 09:51:18.502 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-11 09:51:18.506 [dispatcher-event-loop-3 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-11 09:51:18.512 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-11 09:51:18.512 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-11 09:51:18.513 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-7c30e500-9a45-46fb-bdff-a06f47c6abb1
2025-06-11 09:51:34.363 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-11 09:51:34.527 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-11 09:51:34.611 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:51:34.611 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-11 09:51:34.612 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:51:34.612 [main INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-11 09:51:34.632 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-11 09:51:34.643 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-11 09:51:34.644 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-11 09:51:34.695 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-11 09:51:34.696 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-11 09:51:34.696 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-11 09:51:34.696 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-11 09:51:34.697 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-11 09:51:34.930 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 46419.
2025-06-11 09:51:34.957 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-11 09:51:34.990 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-11 09:51:35.006 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-11 09:51:35.007 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-11 09:51:35.011 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-11 09:51:35.028 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-ea918cff-8f5e-4137-b6e1-ce3361831726
2025-06-11 09:51:35.059 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 966.0 MiB
2025-06-11 09:51:35.073 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-11 09:51:35.107 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @2128ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-11 09:51:35.205 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-11 09:51:35.219 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-11 09:51:35.241 [main INFO ] org.sparkproject.jetty.server.Server - Started @2263ms
2025-06-11 09:51:35.273 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@1ec3a17f{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:51:35.274 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-11 09:51:35.293 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4441d567{/,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.385 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-11 09:51:35.392 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-11 09:51:35.414 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34019.
2025-06-11 09:51:35.414 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:34019
2025-06-11 09:51:35.416 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-11 09:51:35.425 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 34019, None)
2025-06-11 09:51:35.433 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:34019 with 966.0 MiB RAM, BlockManagerId(driver, phamviethoa, 34019, None)
2025-06-11 09:51:35.438 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 34019, None)
2025-06-11 09:51:35.440 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 34019, None)
2025-06-11 09:51:35.595 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4441d567{/,null,STOPPED,@Spark}
2025-06-11 09:51:35.597 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44b18fe4{/jobs,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.598 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d3ca6c7{/jobs/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.599 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44b194fe{/jobs/job,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.600 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fe46690{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.601 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b4d50b{/stages,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.602 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@476ee5b3{/stages/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.603 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68880c21{/stages/stage,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.604 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4195105b{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.605 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47ffe971{/stages/pool,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.606 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14fa92af{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.607 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@339a3670{/storage,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.608 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c7a8af2{/storage/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.609 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@720bf653{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.609 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4edef76c{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.611 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70c53dbe{/environment,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.611 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21c815e4{/environment/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.612 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a331b46{/executors,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.613 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@743e66f7{/executors/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.615 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241f05b{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.616 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.625 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/static,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.626 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42ea287{/,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.630 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f0b3cfe{/api,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.631 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7718a40f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.633 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26844abb{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.639 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fd9ebde{/metrics/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.826 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-11 09:51:35.837 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-11 09:51:35.853 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b612c6{/SQL,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.854 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22752544{/SQL/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.855 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4784efd9{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.856 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@427ae189{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-11 09:51:35.872 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@514de325{/static/sql,null,AVAILABLE,@Spark}
2025-06-11 09:51:36.920 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 224.990728 ms
2025-06-11 09:51:36.954 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 23.654662 ms
2025-06-11 09:51:39.355 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-11 09:51:39.355 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-11 09:51:39.364 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@1ec3a17f{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:51:39.386 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-11 09:51:39.407 [dispatcher-event-loop-3 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-11 09:51:39.421 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-11 09:51:39.422 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-11 09:51:39.431 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-11 09:51:39.436 [dispatcher-event-loop-3 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-11 09:51:39.443 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-11 09:51:39.443 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-11 09:51:39.444 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-55cb7b72-5b55-4135-b736-a04e422172a4
2025-06-11 09:53:51.368 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-11 09:53:51.604 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-11 09:53:51.709 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:53:51.709 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-11 09:53:51.709 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:53:51.710 [main INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-11 09:53:51.739 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-11 09:53:51.750 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-11 09:53:51.751 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-11 09:53:51.818 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-11 09:53:51.819 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-11 09:53:51.820 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-11 09:53:51.821 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-11 09:53:51.821 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-11 09:53:52.172 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 43485.
2025-06-11 09:53:52.204 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-11 09:53:52.234 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-11 09:53:52.254 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-11 09:53:52.254 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-11 09:53:52.257 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-11 09:53:52.278 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-cdae089b-b3b6-4be5-b773-afa566d2c3a7
2025-06-11 09:53:52.310 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 966.0 MiB
2025-06-11 09:53:52.325 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-11 09:53:52.361 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @2229ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-11 09:53:52.491 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-11 09:53:52.501 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-11 09:53:52.516 [main INFO ] org.sparkproject.jetty.server.Server - Started @2384ms
2025-06-11 09:53:52.548 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@58d73d19{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:53:52.548 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-11 09:53:52.569 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cb2651f{/,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.674 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-11 09:53:52.681 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-11 09:53:52.701 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39967.
2025-06-11 09:53:52.702 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:39967
2025-06-11 09:53:52.705 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-11 09:53:52.713 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 39967, None)
2025-06-11 09:53:52.719 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:39967 with 966.0 MiB RAM, BlockManagerId(driver, phamviethoa, 39967, None)
2025-06-11 09:53:52.725 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 39967, None)
2025-06-11 09:53:52.726 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 39967, None)
2025-06-11 09:53:52.894 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7cb2651f{/,null,STOPPED,@Spark}
2025-06-11 09:53:52.896 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67b100fe{/jobs,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.897 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ba5aa7a{/jobs/json,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.898 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c931d35{/jobs/job,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.899 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49122b8f{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.900 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4beabeec{/stages,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.901 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b22d8a1{/stages/json,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.902 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31a3f4de{/stages/stage,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.903 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2dd2e270{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.904 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f3e19b3{/stages/pool,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.906 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d04529c{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.907 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b16e202{/storage,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.908 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cd5122d{/storage/json,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.909 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10c07b8d{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.910 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@360bc645{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.911 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d51e129{/environment,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.912 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1894e40d{/environment/json,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.914 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7342e05d{/executors,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.914 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15383681{/executors/json,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.917 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@109a2025{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.918 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@761956ac{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.932 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@304d0259{/static,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.933 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e08acf9{/,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.936 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78cd163b{/api,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.938 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@672b72ba{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.940 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@312b34e3{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-11 09:53:52.948 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ac4ccad{/metrics/json,null,AVAILABLE,@Spark}
2025-06-11 09:53:53.190 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-11 09:53:53.201 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-11 09:53:53.221 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@806996{/SQL,null,AVAILABLE,@Spark}
2025-06-11 09:53:53.222 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@257e0827{/SQL/json,null,AVAILABLE,@Spark}
2025-06-11 09:53:53.224 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@376c7d7d{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-11 09:53:53.225 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fba233d{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-11 09:53:53.239 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b48e183{/static/sql,null,AVAILABLE,@Spark}
2025-06-11 09:53:54.385 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 198.549637 ms
2025-06-11 09:53:54.421 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 26.097809 ms
2025-06-11 09:53:56.853 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-11 09:53:56.854 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-11 09:53:56.864 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@58d73d19{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:53:56.871 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-11 09:53:56.893 [dispatcher-event-loop-3 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-11 09:53:56.903 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-11 09:53:56.904 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-11 09:53:56.913 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-11 09:53:56.918 [dispatcher-event-loop-0 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-11 09:53:56.926 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-11 09:53:56.926 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-11 09:53:56.927 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-d5f2064b-03e7-45d0-bf37-b2f112ac9388
2025-06-11 09:55:33.414 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-11 09:55:33.604 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-11 09:55:33.710 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:55:33.711 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-11 09:55:33.711 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:55:33.712 [main INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-11 09:55:33.733 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-11 09:55:33.744 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-11 09:55:33.745 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-11 09:55:33.795 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-11 09:55:33.796 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-11 09:55:33.796 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-11 09:55:33.797 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-11 09:55:33.797 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-11 09:55:34.037 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 36707.
2025-06-11 09:55:34.065 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-11 09:55:34.097 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-11 09:55:34.113 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-11 09:55:34.114 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-11 09:55:34.118 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-11 09:55:34.137 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-7fa092cf-8f0f-4b37-bb24-06898500f7de
2025-06-11 09:55:34.167 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 966.0 MiB
2025-06-11 09:55:34.181 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-11 09:55:34.215 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @2028ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-11 09:55:34.315 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-11 09:55:34.331 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-11 09:55:34.358 [main INFO ] org.sparkproject.jetty.server.Server - Started @2171ms
2025-06-11 09:55:34.399 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@6dc0feb1{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:55:34.399 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-11 09:55:34.425 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4441d567{/,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.529 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-11 09:55:34.543 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-11 09:55:34.568 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45053.
2025-06-11 09:55:34.568 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:45053
2025-06-11 09:55:34.570 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-11 09:55:34.577 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 45053, None)
2025-06-11 09:55:34.583 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:45053 with 966.0 MiB RAM, BlockManagerId(driver, phamviethoa, 45053, None)
2025-06-11 09:55:34.588 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 45053, None)
2025-06-11 09:55:34.589 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 45053, None)
2025-06-11 09:55:34.735 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4441d567{/,null,STOPPED,@Spark}
2025-06-11 09:55:34.738 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44b18fe4{/jobs,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.739 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d3ca6c7{/jobs/json,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.741 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44b194fe{/jobs/job,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.742 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fe46690{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.743 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b4d50b{/stages,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.744 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@476ee5b3{/stages/json,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.747 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68880c21{/stages/stage,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.748 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4195105b{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.749 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47ffe971{/stages/pool,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.751 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14fa92af{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.752 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@339a3670{/storage,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.754 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c7a8af2{/storage/json,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.755 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@720bf653{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.756 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4edef76c{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.757 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70c53dbe{/environment,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.758 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21c815e4{/environment/json,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.759 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a331b46{/executors,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.760 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@743e66f7{/executors/json,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.762 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241f05b{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.764 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.775 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/static,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.778 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42ea287{/,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.781 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f0b3cfe{/api,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.782 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7718a40f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.783 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26844abb{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.790 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fd9ebde{/metrics/json,null,AVAILABLE,@Spark}
2025-06-11 09:55:34.978 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-11 09:55:34.987 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-11 09:55:35.005 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b612c6{/SQL,null,AVAILABLE,@Spark}
2025-06-11 09:55:35.006 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22752544{/SQL/json,null,AVAILABLE,@Spark}
2025-06-11 09:55:35.008 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4784efd9{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-11 09:55:35.009 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@427ae189{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-11 09:55:35.022 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@514de325{/static/sql,null,AVAILABLE,@Spark}
2025-06-11 09:55:36.109 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 199.389617 ms
2025-06-11 09:55:36.147 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 26.681272 ms
2025-06-11 09:55:38.680 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-11 09:55:38.680 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-11 09:55:38.706 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@6dc0feb1{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:55:38.713 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-11 09:55:38.736 [dispatcher-event-loop-3 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-11 09:55:38.748 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-11 09:55:38.749 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-11 09:55:38.756 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-11 09:55:38.760 [dispatcher-event-loop-3 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-11 09:55:38.766 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-11 09:55:38.767 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-11 09:55:38.767 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-22d01222-07a2-4e75-930f-462a87eb0128
2025-06-11 09:58:07.979 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-11 09:58:08.143 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-11 09:58:08.234 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:58:08.234 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-11 09:58:08.235 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-11 09:58:08.235 [main INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-11 09:58:08.255 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-11 09:58:08.266 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-11 09:58:08.267 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-11 09:58:08.324 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-11 09:58:08.325 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-11 09:58:08.325 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-11 09:58:08.325 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-11 09:58:08.326 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-11 09:58:08.568 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 46859.
2025-06-11 09:58:08.595 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-11 09:58:08.627 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-11 09:58:08.643 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-11 09:58:08.644 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-11 09:58:08.648 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-11 09:58:08.670 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-d3ce50a5-b7ba-40c1-9e46-20adb7596f42
2025-06-11 09:58:08.706 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 966.0 MiB
2025-06-11 09:58:08.722 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-11 09:58:08.762 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @2082ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-11 09:58:08.874 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-11 09:58:08.886 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-11 09:58:08.903 [main INFO ] org.sparkproject.jetty.server.Server - Started @2225ms
2025-06-11 09:58:08.937 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@1ec3a17f{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:58:08.937 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-11 09:58:08.955 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cb2651f{/,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.046 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-11 09:58:09.053 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-11 09:58:09.071 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34789.
2025-06-11 09:58:09.071 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:34789
2025-06-11 09:58:09.072 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-11 09:58:09.079 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 34789, None)
2025-06-11 09:58:09.085 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:34789 with 966.0 MiB RAM, BlockManagerId(driver, phamviethoa, 34789, None)
2025-06-11 09:58:09.091 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 34789, None)
2025-06-11 09:58:09.093 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 34789, None)
2025-06-11 09:58:09.284 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7cb2651f{/,null,STOPPED,@Spark}
2025-06-11 09:58:09.287 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67b100fe{/jobs,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.289 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ba5aa7a{/jobs/json,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.290 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c931d35{/jobs/job,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.291 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49122b8f{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.293 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4beabeec{/stages,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.294 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b22d8a1{/stages/json,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.297 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31a3f4de{/stages/stage,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.298 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2dd2e270{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.299 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f3e19b3{/stages/pool,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.300 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d04529c{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.302 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b16e202{/storage,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.303 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cd5122d{/storage/json,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.304 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10c07b8d{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.305 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@360bc645{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.306 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d51e129{/environment,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.307 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1894e40d{/environment/json,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.309 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7342e05d{/executors,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.310 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15383681{/executors/json,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.312 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@109a2025{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.313 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@761956ac{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.323 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@304d0259{/static,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.325 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e08acf9{/,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.327 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78cd163b{/api,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.328 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@672b72ba{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.329 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@312b34e3{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.334 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ac4ccad{/metrics/json,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.513 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-11 09:58:09.523 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-11 09:58:09.540 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@806996{/SQL,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.541 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@257e0827{/SQL/json,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.543 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@376c7d7d{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.544 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fba233d{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-11 09:58:09.558 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b48e183{/static/sql,null,AVAILABLE,@Spark}
2025-06-11 09:58:10.687 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 218.338346 ms
2025-06-11 09:58:10.724 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 25.907261 ms
2025-06-11 09:58:13.278 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-11 09:58:13.278 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-11 09:58:13.307 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@1ec3a17f{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-11 09:58:13.312 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-11 09:58:13.330 [dispatcher-event-loop-3 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-11 09:58:13.342 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-11 09:58:13.343 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-11 09:58:13.350 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-11 09:58:13.355 [dispatcher-event-loop-3 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-11 09:58:13.361 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-11 09:58:13.362 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-11 09:58:13.363 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-e899450e-2dc1-4864-9b20-4bcb63ad4034
