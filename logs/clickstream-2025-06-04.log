2025-06-04 22:30:53.509 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 35011 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-04 22:30:53.511 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-04 22:30:54.002 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-04 22:30:54.005 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-04 22:30:54.005 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-04 22:30:54.005 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-04 22:30:54.050 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-04 22:30:54.050 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 520 ms
2025-06-04 22:30:54.196 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-04 22:30:54.353 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-04 22:30:54.367 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-04 22:30:54.383 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 22:30:54.383 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 22:30:54.383 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749051054382
2025-06-04 22:30:54.483 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-04 22:30:54.486 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 22:30:54.486 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 22:30:54.486 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 22:30:54.490 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-04 22:30:54.494 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-04 22:30:54.502 [main INFO ] com.example.Application - Started Application in 1.172 seconds (JVM running for 1.491)
2025-06-04 22:30:55.006 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-04 22:30:55.006 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-04 22:30:55.007 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-04 22:31:00.243 [http-nio-8080-exec-1 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=2d6a7760-f7e6-4901-b4f7-1fcaa4b02ded, event_name=page_view, event_time=2025-06-04T15:30:16.379Z, user_id=user_4q6eh9x, sessionId=session_hyhabjd, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/}, {event_id=0382d484-0e0a-4c26-be0c-7217ca5288d8, event_name=tab_change, event_time=2025-06-04T15:30:16.905Z, user_id=user_4q6eh9x, sessionId=session_hyhabjd, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, is_visible=true, time_visible=0}, {event_id=1376c5a4-f77b-4629-880b-15237ce2e3bb, event_name=tab_change, event_time=2025-06-04T15:30:18.163Z, user_id=user_4q6eh9x, sessionId=session_hyhabjd, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, is_visible=false, time_visible=0}, {event_id=95c78196-0614-43d4-9879-f9bea2e2ebce, event_name=scroll, event_time=2025-06-04T15:30:43.661Z, user_id=user_4q6eh9x, sessionId=session_hyhabjd, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=3}, {event_id=22f78cec-4615-4f9e-b3d3-d7ed4f83acba, event_name=scroll, event_time=2025-06-04T15:30:44.661Z, user_id=user_4q6eh9x, sessionId=session_hyhabjd, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=100}, {event_id=513f8578-4030-4366-9b52-2ace8ff2aaba, event_name=click, event_time=2025-06-04T15:30:44.706Z, user_id=user_4q6eh9x, sessionId=session_hyhabjd, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 3
                        Customer favorit, element_type=div, element_name=null, track=product_click, productId=3}, {event_id=2c911ff0-e945-4239-b83e-a56921aedbb8, event_name=click, event_time=2025-06-04T15:30:45.405Z, user_id=user_4q6eh9x, sessionId=session_hyhabjd, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 6
                        Sleek design and, element_type=div, element_name=null, track=product_click, productId=6}, {event_id=adefd2a6-70dc-4c57-880b-a5de970b55f8, event_name=scroll, event_time=2025-06-04T15:30:46.480Z, user_id=user_4q6eh9x, sessionId=session_hyhabjd, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=88}, {event_id=eceb0a2a-f29a-4b02-95ad-cc21be8da0fa, event_name=click, event_time=2025-06-04T15:31:00.190Z, user_id=user_4q6eh9x, sessionId=session_hyhabjd, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 2
                        Durable and styl, element_type=div, element_name=null, track=product_click, productId=2}]}
2025-06-04 22:31:00.243 [http-nio-8080-exec-1 INFO ] c.e.controller.ClickstreamController - Received 9 events
2025-06-04 22:31:00.251 [http-nio-8080-exec-1 INFO ] o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 1000
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-06-04 22:31:00.270 [http-nio-8080-exec-1 INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-04 22:31:00.275 [http-nio-8080-exec-1 INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-06-04 22:31:00.287 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 22:31:00.288 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 22:31:00.288 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749051060287
2025-06-04 22:31:00.295 [kafka-producer-network-thread | producer-1 INFO ] org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-04 22:31:00.301 [http-nio-8080-exec-1 INFO ] c.e.controller.ClickstreamController - Successfully processed 9 events
2025-06-04 22:31:00.308 [kafka-producer-network-thread | producer-1 INFO ] o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 3000 with epoch 0
2025-06-04 22:31:03.963 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=cbddb406-a87b-40a6-8885-bffd388bb608, event_name=click, event_time=2025-06-04T15:31:01.285Z, user_id=user_4q6eh9x, sessionId=session_hyhabjd, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 5
                        Bestseller with , element_type=div, element_name=null, track=product_click, productId=5}, {event_id=4c08fe9f-74d6-4eba-829c-c0786a4056d3, event_name=click, event_time=2025-06-04T15:31:03.418Z, user_id=user_4q6eh9x, sessionId=session_hyhabjd, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 1
                        High-quality ite, element_type=div, element_name=null, track=product_click, productId=1}, {event_id=5f4f7101-f401-443b-8bbb-825f5207f524, event_name=click, event_time=2025-06-04T15:31:03.959Z, user_id=user_4q6eh9x, sessionId=session_hyhabjd, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 7
                        Compact and effi, element_type=div, element_name=null, track=product_click, productId=7}]}
2025-06-04 22:31:03.963 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-04 22:31:03.963 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-04 22:31:06.499 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=3f43b659-3a18-4b5b-ae4c-20c34688a710, event_name=click, event_time=2025-06-04T15:31:04.850Z, user_id=user_4q6eh9x, sessionId=session_hyhabjd, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 9
                        Top-rated with e, element_type=div, element_name=null, track=product_click, productId=9}, {event_id=93d0fc2b-b385-4c74-859d-95e32175a1c0, event_name=click, event_time=2025-06-04T15:31:05.366Z, user_id=user_4q6eh9x, sessionId=session_hyhabjd, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 8
                        Eco-friendly and, element_type=div, element_name=null, track=product_click, productId=8}, {event_id=5d75b09b-2deb-4154-b975-101fcfe7af69, event_name=scroll, event_time=2025-06-04T15:31:06.496Z, user_id=user_4q6eh9x, sessionId=session_hyhabjd, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=87}]}
2025-06-04 22:31:06.499 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-04 22:31:06.500 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-04 22:32:26.439 [SpringApplicationShutdownHook INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-06-04 22:32:26.441 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 22:32:26.441 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 22:32:26.441 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-04 22:32:26.441 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 22:32:26.442 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-1 unregistered
2025-06-04 22:35:37.738 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-04 22:35:37.839 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-04 22:35:37.891 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 22:35:37.891 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-04 22:35:37.891 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 22:35:37.891 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-04 22:35:37.902 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-04 22:35:37.908 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-04 22:35:37.908 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-04 22:35:37.938 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-04 22:35:37.938 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-04 22:35:37.938 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-04 22:35:37.938 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-04 22:35:37.938 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-04 22:35:38.061 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 44743.
2025-06-04 22:35:38.082 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-04 22:35:38.099 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-04 22:35:38.109 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-04 22:35:38.110 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-04 22:35:38.112 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-04 22:35:38.122 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-a36dc38c-7b4a-4e12-8563-1fb25b10b331
2025-06-04 22:35:38.138 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-04 22:35:38.146 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-04 22:35:38.166 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1055ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-04 22:35:38.206 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-04 22:35:38.211 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-04 22:35:38.219 [main INFO ] org.sparkproject.jetty.server.Server - Started @1109ms
2025-06-04 22:35:38.234 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@66f659e6{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 22:35:38.234 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-04 22:35:38.244 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7144655b{/,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.293 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-04 22:35:38.297 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-04 22:35:38.309 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39407.
2025-06-04 22:35:38.310 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:39407
2025-06-04 22:35:38.311 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-04 22:35:38.314 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 39407, None)
2025-06-04 22:35:38.316 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:39407 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 39407, None)
2025-06-04 22:35:38.318 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 39407, None)
2025-06-04 22:35:38.318 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 39407, None)
2025-06-04 22:35:38.395 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7144655b{/,null,STOPPED,@Spark}
2025-06-04 22:35:38.396 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241f05b{/jobs,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.396 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/jobs/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.397 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c9320c2{/jobs/job,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.397 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cc9385{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.398 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7915bca3{/stages,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.398 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/stages/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.399 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79fd6f95{/stages/stage,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.400 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.400 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/stages/pool,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.401 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.401 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/storage,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.401 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/storage/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.402 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.402 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.403 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/environment,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.403 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/environment/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.403 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/executors,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.404 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/executors/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.405 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397ef2{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.405 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e93c1f{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.409 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9b21bd3{/static,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.410 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@323f3c96{/,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.411 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b6d92e{/api,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.411 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25d93198{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.412 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f951a7f{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.414 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e7f0216{/metrics/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.486 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-04 22:35:38.490 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-04 22:35:38.499 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46aa712c{/SQL,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.499 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7412ed6b{/SQL/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.500 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@298f0a0b{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.500 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31dfc6f5{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:38.506 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188ac8a3{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 22:35:39.813 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-04 22:35:39.821 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b41b8bb{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-04 22:35:39.821 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7538cfe6{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:39.822 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@598e5cb8{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-04 22:35:39.822 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@300aa927{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:39.823 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@611a2d82{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 22:35:39.839 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-04 22:35:39.840 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-04 22:35:39.844 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@66f659e6{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 22:35:39.846 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-04 22:35:39.854 [dispatcher-event-loop-8 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-04 22:35:39.859 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-04 22:35:39.859 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-04 22:35:39.862 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-04 22:35:39.864 [dispatcher-event-loop-12 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-04 22:35:39.867 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-04 22:35:39.867 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-04 22:35:39.867 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-817b4b4b-c037-4d80-8b1b-068299af05db
2025-06-04 22:35:57.296 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-04 22:35:57.379 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-04 22:35:57.422 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 22:35:57.422 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-04 22:35:57.422 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 22:35:57.422 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-04 22:35:57.433 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-04 22:35:57.438 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-04 22:35:57.439 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-04 22:35:57.465 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-04 22:35:57.465 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-04 22:35:57.466 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-04 22:35:57.466 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-04 22:35:57.466 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-04 22:35:57.581 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 32959.
2025-06-04 22:35:57.594 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-04 22:35:57.610 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-04 22:35:57.619 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-04 22:35:57.619 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-04 22:35:57.621 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-04 22:35:57.631 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-d0086c26-2785-44d6-8759-1dcfe0bb654a
2025-06-04 22:35:57.648 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-04 22:35:57.657 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-04 22:35:57.677 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1024ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-04 22:35:57.720 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-04 22:35:57.725 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-04 22:35:57.733 [main INFO ] org.sparkproject.jetty.server.Server - Started @1081ms
2025-06-04 22:35:57.749 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@414cc890{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 22:35:57.749 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-04 22:35:57.760 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49cf9028{/,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.807 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-04 22:35:57.811 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-04 22:35:57.822 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36287.
2025-06-04 22:35:57.822 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:36287
2025-06-04 22:35:57.823 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-04 22:35:57.827 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 36287, None)
2025-06-04 22:35:57.829 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:36287 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 36287, None)
2025-06-04 22:35:57.831 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 36287, None)
2025-06-04 22:35:57.832 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 36287, None)
2025-06-04 22:35:57.908 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@49cf9028{/,null,STOPPED,@Spark}
2025-06-04 22:35:57.909 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3414a8c3{/jobs,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.910 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cf518cf{/jobs/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.910 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e43e323{/jobs/job,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.911 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10643593{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.911 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@eca6a74{/stages,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.911 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48840594{/stages/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.912 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@113e13f9{/stages/stage,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.912 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7979b8b7{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.913 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bc49bc5{/stages/pool,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.913 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f66ffc8{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.914 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2def7a7a{/storage,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.914 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c080ef3{/storage/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.915 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ee6291f{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.915 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37e0292a{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.915 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35267fd4{/environment,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.916 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36a6bea6{/environment/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.916 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42373389{/executors,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.916 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a62c7cd{/executors/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.917 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c36db44{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.917 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7903d448{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.921 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42ea287{/static,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.922 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@644ded04{/,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.923 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13d9261f{/api,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.923 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e8a1ab4{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.924 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1aabf50d{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.926 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bc7e78e{/metrics/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:57.990 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-04 22:35:57.993 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-04 22:35:58.000 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@298f0a0b{/SQL,null,AVAILABLE,@Spark}
2025-06-04 22:35:58.000 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31dfc6f5{/SQL/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:58.001 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e5843db{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-04 22:35:58.001 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188ac8a3{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:58.006 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70e94ecb{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 22:35:59.213 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-04 22:35:59.220 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@598e5cb8{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-04 22:35:59.221 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@300aa927{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:59.221 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1feb586d{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-04 22:35:59.222 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@611a2d82{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-04 22:35:59.223 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2548fc01{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 22:35:59.227 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-6e2891cf-abef-4cd7-a88d-488ea9ba4717. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-04 22:35:59.238 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-6e2891cf-abef-4cd7-a88d-488ea9ba4717 resolved to file:/tmp/temporary-6e2891cf-abef-4cd7-a88d-488ea9ba4717.
2025-06-04 22:35:59.239 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-04 22:35:59.290 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-6e2891cf-abef-4cd7-a88d-488ea9ba4717/metadata using temp file file:/tmp/temporary-6e2891cf-abef-4cd7-a88d-488ea9ba4717/.metadata.6ba4f593-58c8-47a2-927d-d0f9a3eba477.tmp
2025-06-04 22:35:59.341 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-6e2891cf-abef-4cd7-a88d-488ea9ba4717/.metadata.6ba4f593-58c8-47a2-927d-d0f9a3eba477.tmp to file:/tmp/temporary-6e2891cf-abef-4cd7-a88d-488ea9ba4717/metadata
2025-06-04 22:35:59.360 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d]. Use file:/tmp/temporary-6e2891cf-abef-4cd7-a88d-488ea9ba4717 to store the query checkpoint.
2025-06-04 22:35:59.365 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@4823e2e] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@421c17ff]
2025-06-04 22:35:59.379 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-04 22:35:59.380 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-04 22:35:59.380 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-04 22:35:59.381 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-04 22:35:59.531 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-04 22:35:59.560 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-04 22:35:59.561 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 22:35:59.561 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 22:35:59.561 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749051359560
2025-06-04 22:35:59.732 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-6e2891cf-abef-4cd7-a88d-488ea9ba4717/sources/0/0 using temp file file:/tmp/temporary-6e2891cf-abef-4cd7-a88d-488ea9ba4717/sources/0/.0.d44bb697-d338-47aa-bab0-ab32dd2391c9.tmp
2025-06-04 22:35:59.745 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-6e2891cf-abef-4cd7-a88d-488ea9ba4717/sources/0/.0.d44bb697-d338-47aa-bab0-ab32dd2391c9.tmp to file:/tmp/temporary-6e2891cf-abef-4cd7-a88d-488ea9ba4717/sources/0/0
2025-06-04 22:35:59.745 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events-1k":{"1":1706,"0":1604}}
2025-06-04 22:35:59.757 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-6e2891cf-abef-4cd7-a88d-488ea9ba4717/offsets/0 using temp file file:/tmp/temporary-6e2891cf-abef-4cd7-a88d-488ea9ba4717/offsets/.0.6c6a81fe-15f3-444c-abfb-a7bc55701344.tmp
2025-06-04 22:35:59.774 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-6e2891cf-abef-4cd7-a88d-488ea9ba4717/offsets/.0.6c6a81fe-15f3-444c-abfb-a7bc55701344.tmp to file:/tmp/temporary-6e2891cf-abef-4cd7-a88d-488ea9ba4717/offsets/0
2025-06-04 22:35:59.775 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749051359753,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-04 22:35:59.979 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749051359753
2025-06-04 22:36:00.063 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 22:36:00.098 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 22:36:00.133 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749051359753
2025-06-04 22:36:00.135 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 22:36:00.136 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 22:36:00.383 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 148.051848 ms
2025-06-04 22:36:00.538 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.128238 ms
2025-06-04 22:36:00.550 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 5.526833 ms
2025-06-04 22:36:00.614 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:85
2025-06-04 22:36:00.622 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:85) as input to shuffle 0
2025-06-04 22:36:00.625 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:85) with 1 output partitions
2025-06-04 22:36:00.625 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:85)
2025-06-04 22:36:00.625 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-04 22:36:00.626 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-04 22:36:00.627 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:85), which has no missing parents
2025-06-04 22:36:00.662 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-04 22:36:00.689 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-04 22:36:00.690 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:36287 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-04 22:36:00.692 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-04 22:36:00.699 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:85) (first 15 tasks are for partitions Vector(0))
2025-06-04 22:36:00.700 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-04 22:36:00.727 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-04 22:36:00.735 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
2025-06-04 22:36:00.802 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-04 22:36:00.803 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
2025-06-04 22:36:00.819 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 4031 bytes result sent to driver
2025-06-04 22:36:00.825 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 104 ms on phamviethoa (executor driver) (1/1)
2025-06-04 22:36:00.827 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-04 22:36:00.830 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:85) finished in 0.195 s
2025-06-04 22:36:00.831 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-04 22:36:00.832 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-04 22:36:00.833 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:85, took 0.218316 s
2025-06-04 22:36:01.042 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] terminated with error
org.apache.spark.sql.AnalysisException: Column geo_region not found in schema Some(StructType(StructField(event_id,StringType,false),StructField(event_name,StringType,false),StructField(event_time,TimestampType,false),StructField(user_id,StringType,false),StructField(session_id,StringType,false),StructField(app_id,StringType,false),StructField(platform,StringType,false),StructField(page_url,StringType,false),StructField(geo_country,StringType,false),StructField(geo_city,StringType,false),StructField(traffic_source,StringType,false),StructField(traffic_medium,StringType,false),StructField(item_id,StringType,true),StructField(item_price,DoubleType,true),StructField(kafka_timestamp,TimestampType,false),StructField(processing_time,TimestampType,false))).
	at org.apache.spark.sql.errors.QueryCompilationErrors$.columnNotFoundInSchemaError(QueryCompilationErrors.scala:1612)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$4(JdbcUtils.scala:126)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$2(JdbcUtils.scala:126)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getInsertStatement(JdbcUtils.scala:124)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:883)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.SparkClickstreamProcessor.lambda$startProcessing$8292e96f$1(SparkClickstreamProcessor.java:81)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
2025-06-04 22:36:01.043 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-04 22:36:01.046 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 22:36:01.046 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 22:36:01.046 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 22:36:01.047 [stream execution thread for [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = a6a1fc1d-b258-429f-96dc-6c9898147e5a, runId = c6338ad8-7be7-4c87-8ee7-25eced82b59d] has been shutdown
2025-06-04 22:36:01.057 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-04 22:36:01.058 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-04 22:36:01.061 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@414cc890{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 22:36:01.063 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-04 22:36:01.070 [dispatcher-event-loop-12 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-04 22:36:01.076 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-04 22:36:01.076 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-04 22:36:01.079 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-04 22:36:01.080 [dispatcher-event-loop-0 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-04 22:36:01.083 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-04 22:36:01.083 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-04 22:36:01.084 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-6e2891cf-abef-4cd7-a88d-488ea9ba4717
2025-06-04 22:36:01.085 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-ccb7c573-32e5-4fea-96fe-92ac03c1d318
2025-06-04 22:36:26.788 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-04 22:36:26.868 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-04 22:36:26.911 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 22:36:26.911 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-04 22:36:26.911 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 22:36:26.911 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-04 22:36:26.922 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-04 22:36:26.928 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-04 22:36:26.928 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-04 22:36:26.955 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-04 22:36:26.955 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-04 22:36:26.955 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-04 22:36:26.955 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-04 22:36:26.956 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-04 22:36:27.072 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 43817.
2025-06-04 22:36:27.088 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-04 22:36:27.104 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-04 22:36:27.113 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-04 22:36:27.114 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-04 22:36:27.116 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-04 22:36:27.126 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-88ce42ee-7f40-4cec-805c-ae157f0845ca
2025-06-04 22:36:27.143 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-04 22:36:27.151 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-04 22:36:27.171 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @977ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-04 22:36:27.214 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-04 22:36:27.219 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-04 22:36:27.227 [main INFO ] org.sparkproject.jetty.server.Server - Started @1034ms
2025-06-04 22:36:27.243 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@69b452ce{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 22:36:27.243 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-04 22:36:27.254 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17740dae{/,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.306 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-04 22:36:27.311 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-04 22:36:27.322 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46447.
2025-06-04 22:36:27.322 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:46447
2025-06-04 22:36:27.323 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-04 22:36:27.327 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 46447, None)
2025-06-04 22:36:27.329 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:46447 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 46447, None)
2025-06-04 22:36:27.331 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 46447, None)
2025-06-04 22:36:27.331 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 46447, None)
2025-06-04 22:36:27.407 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@17740dae{/,null,STOPPED,@Spark}
2025-06-04 22:36:27.408 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21c815e4{/jobs,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.408 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a331b46{/jobs/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.409 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241f05b{/jobs/job,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.409 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.410 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/stages,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.411 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c9320c2{/stages/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.412 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/stages/stage,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.412 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67b4ec{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.413 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f91da5e{/stages/pool,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.417 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79fd6f95{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.418 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/storage,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.419 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/storage/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.419 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.419 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.420 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/environment,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.420 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/environment/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.420 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/executors,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.421 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/executors/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.421 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.422 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.426 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/static,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.426 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e0895f5{/,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.427 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fd9ebde{/api,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.428 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e9ef5b6{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.428 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4110765e{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.430 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53ec2968{/metrics/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.498 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-04 22:36:27.501 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-04 22:36:27.508 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2da3b078{/SQL,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.509 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7fb66650{/SQL/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.509 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ada9c0c{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.510 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e7c351d{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:27.515 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37b52340{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 22:36:28.748 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-04 22:36:28.755 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@778197c0{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-04 22:36:28.755 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c28f97e{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:28.756 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4cadd4d4{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-04 22:36:28.756 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f4dd016{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:28.757 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fa11fda{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 22:36:28.760 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-98445a8f-4980-49c9-8685-0fbc78671132. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-04 22:36:28.770 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-98445a8f-4980-49c9-8685-0fbc78671132 resolved to file:/tmp/temporary-98445a8f-4980-49c9-8685-0fbc78671132.
2025-06-04 22:36:28.770 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-04 22:36:28.810 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-98445a8f-4980-49c9-8685-0fbc78671132/metadata using temp file file:/tmp/temporary-98445a8f-4980-49c9-8685-0fbc78671132/.metadata.53af8f46-1967-476e-a776-a9ca41d54281.tmp
2025-06-04 22:36:28.857 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-98445a8f-4980-49c9-8685-0fbc78671132/.metadata.53af8f46-1967-476e-a776-a9ca41d54281.tmp to file:/tmp/temporary-98445a8f-4980-49c9-8685-0fbc78671132/metadata
2025-06-04 22:36:28.871 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa]. Use file:/tmp/temporary-98445a8f-4980-49c9-8685-0fbc78671132 to store the query checkpoint.
2025-06-04 22:36:28.877 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@3b6179] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@29d09f4]
2025-06-04 22:36:28.891 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-04 22:36:28.892 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-04 22:36:28.892 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-04 22:36:28.893 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-04 22:36:29.047 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-04 22:36:29.079 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-04 22:36:29.080 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 22:36:29.080 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 22:36:29.080 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749051389079
2025-06-04 22:36:29.260 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-98445a8f-4980-49c9-8685-0fbc78671132/sources/0/0 using temp file file:/tmp/temporary-98445a8f-4980-49c9-8685-0fbc78671132/sources/0/.0.24cf4a3c-04a4-4f92-9399-5f14f1f10442.tmp
2025-06-04 22:36:29.274 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-98445a8f-4980-49c9-8685-0fbc78671132/sources/0/.0.24cf4a3c-04a4-4f92-9399-5f14f1f10442.tmp to file:/tmp/temporary-98445a8f-4980-49c9-8685-0fbc78671132/sources/0/0
2025-06-04 22:36:29.274 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events-1k":{"1":1706,"0":1604}}
2025-06-04 22:36:29.287 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-98445a8f-4980-49c9-8685-0fbc78671132/offsets/0 using temp file file:/tmp/temporary-98445a8f-4980-49c9-8685-0fbc78671132/offsets/.0.61157017-15c3-4ae2-a724-4f030d41cbe0.tmp
2025-06-04 22:36:29.306 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-98445a8f-4980-49c9-8685-0fbc78671132/offsets/.0.61157017-15c3-4ae2-a724-4f030d41cbe0.tmp to file:/tmp/temporary-98445a8f-4980-49c9-8685-0fbc78671132/offsets/0
2025-06-04 22:36:29.307 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749051389283,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-04 22:36:29.482 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749051389283
2025-06-04 22:36:29.536 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 22:36:29.564 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 22:36:29.598 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749051389283
2025-06-04 22:36:29.600 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 22:36:29.601 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 22:36:29.826 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 135.557281 ms
2025-06-04 22:36:29.973 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.104541 ms
2025-06-04 22:36:29.985 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 5.566015 ms
2025-06-04 22:36:30.037 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:84
2025-06-04 22:36:30.045 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:84) as input to shuffle 0
2025-06-04 22:36:30.048 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:84) with 1 output partitions
2025-06-04 22:36:30.049 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:84)
2025-06-04 22:36:30.049 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-04 22:36:30.050 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-04 22:36:30.053 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:84), which has no missing parents
2025-06-04 22:36:30.086 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-04 22:36:30.101 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-04 22:36:30.103 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:46447 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-04 22:36:30.105 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-04 22:36:30.113 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:84) (first 15 tasks are for partitions Vector(0))
2025-06-04 22:36:30.114 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-04 22:36:30.141 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-04 22:36:30.148 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
2025-06-04 22:36:30.212 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-04 22:36:30.214 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
2025-06-04 22:36:30.229 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 4031 bytes result sent to driver
2025-06-04 22:36:30.236 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 100 ms on phamviethoa (executor driver) (1/1)
2025-06-04 22:36:30.237 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-04 22:36:30.240 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:84) finished in 0.179 s
2025-06-04 22:36:30.242 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-04 22:36:30.242 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-04 22:36:30.243 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:84, took 0.206020 s
2025-06-04 22:36:30.389 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] terminated with error
org.apache.spark.sql.AnalysisException: Column event_params not found in schema Some(StructType(StructField(event_id,StringType,false),StructField(event_name,StringType,false),StructField(event_time,TimestampType,false),StructField(user_id,StringType,false),StructField(session_id,StringType,false),StructField(app_id,StringType,false),StructField(platform,StringType,false),StructField(page_url,StringType,false),StructField(geo_country,StringType,false),StructField(geo_city,StringType,false),StructField(traffic_source,StringType,false),StructField(traffic_medium,StringType,false),StructField(item_id,StringType,true),StructField(item_price,DoubleType,true),StructField(kafka_timestamp,TimestampType,false),StructField(processing_time,TimestampType,false))).
	at org.apache.spark.sql.errors.QueryCompilationErrors$.columnNotFoundInSchemaError(QueryCompilationErrors.scala:1612)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$4(JdbcUtils.scala:126)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$2(JdbcUtils.scala:126)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getInsertStatement(JdbcUtils.scala:124)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:883)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.SparkClickstreamProcessor.lambda$startProcessing$8292e96f$1(SparkClickstreamProcessor.java:80)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
2025-06-04 22:36:30.390 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-04 22:36:30.393 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 22:36:30.393 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 22:36:30.393 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 22:36:30.394 [stream execution thread for [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = 026ce901-b8b1-4aae-8e53-c83cb3c9df56, runId = 7e65ecb8-9d98-4742-ab79-8077fd3d4caa] has been shutdown
2025-06-04 22:36:30.403 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-04 22:36:30.403 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-04 22:36:30.407 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@69b452ce{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 22:36:30.408 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-04 22:36:30.416 [dispatcher-event-loop-12 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-04 22:36:30.421 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-04 22:36:30.421 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-04 22:36:30.424 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-04 22:36:30.425 [dispatcher-event-loop-0 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-04 22:36:30.428 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-04 22:36:30.428 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-04 22:36:30.429 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-c5abaf1e-d9b6-4ceb-9748-8491e3ca1f4f
2025-06-04 22:36:30.430 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-98445a8f-4980-49c9-8685-0fbc78671132
2025-06-04 22:36:55.157 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-04 22:36:55.252 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-04 22:36:55.296 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 22:36:55.297 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-04 22:36:55.297 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 22:36:55.297 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-04 22:36:55.308 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-04 22:36:55.314 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-04 22:36:55.314 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-04 22:36:55.345 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-04 22:36:55.346 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-04 22:36:55.346 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-04 22:36:55.346 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-04 22:36:55.346 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-04 22:36:55.473 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 34215.
2025-06-04 22:36:55.486 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-04 22:36:55.503 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-04 22:36:55.512 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-04 22:36:55.512 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-04 22:36:55.514 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-04 22:36:55.524 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-53fad767-593a-454f-9805-7b0a5e1418e0
2025-06-04 22:36:55.540 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-04 22:36:55.548 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-04 22:36:55.567 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @964ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-04 22:36:55.611 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-04 22:36:55.616 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-04 22:36:55.624 [main INFO ] org.sparkproject.jetty.server.Server - Started @1022ms
2025-06-04 22:36:55.639 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@330f6509{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 22:36:55.640 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-04 22:36:55.650 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a231dfd{/,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.697 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-04 22:36:55.701 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-04 22:36:55.714 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37457.
2025-06-04 22:36:55.714 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:37457
2025-06-04 22:36:55.715 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-04 22:36:55.719 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 37457, None)
2025-06-04 22:36:55.721 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:37457 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 37457, None)
2025-06-04 22:36:55.723 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 37457, None)
2025-06-04 22:36:55.724 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 37457, None)
2025-06-04 22:36:55.801 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7a231dfd{/,null,STOPPED,@Spark}
2025-06-04 22:36:55.802 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/jobs,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.802 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c9320c2{/jobs/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.803 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7915bca3{/jobs/job,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.803 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.804 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67b4ec{/stages,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.804 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f91da5e{/stages/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.805 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/stages/stage,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.805 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.806 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/stages/pool,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.806 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.807 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/storage,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.807 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/storage/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.808 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.808 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.809 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/environment,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.809 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/environment/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.810 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397ef2{/executors,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.810 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e93c1f{/executors/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.811 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9b21bd3{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.811 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7661b5a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.817 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65c33b92{/static,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.818 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7899de11{/,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.819 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bc0d349{/api,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.820 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c777e7b{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.820 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78e22d35{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.822 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6724cdec{/metrics/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.896 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-04 22:36:55.899 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-04 22:36:55.907 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b4a0aef{/SQL,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.907 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26d41711{/SQL/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.908 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@663bb8ef{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.908 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60e9c3a5{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:55.914 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a8b9166{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 22:36:57.178 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-04 22:36:57.186 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30ef32eb{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-04 22:36:57.186 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@bb5f9d{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:57.187 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b76b7d8{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-04 22:36:57.187 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63c4d16{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-04 22:36:57.188 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4627dfda{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 22:36:57.192 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2d6ea2df-4f01-484a-b9ba-034af4fac136. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-04 22:36:57.203 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-2d6ea2df-4f01-484a-b9ba-034af4fac136 resolved to file:/tmp/temporary-2d6ea2df-4f01-484a-b9ba-034af4fac136.
2025-06-04 22:36:57.203 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-04 22:36:57.245 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-2d6ea2df-4f01-484a-b9ba-034af4fac136/metadata using temp file file:/tmp/temporary-2d6ea2df-4f01-484a-b9ba-034af4fac136/.metadata.021d2149-4803-420c-bffa-588d9e091a78.tmp
2025-06-04 22:36:57.295 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-2d6ea2df-4f01-484a-b9ba-034af4fac136/.metadata.021d2149-4803-420c-bffa-588d9e091a78.tmp to file:/tmp/temporary-2d6ea2df-4f01-484a-b9ba-034af4fac136/metadata
2025-06-04 22:36:57.311 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3]. Use file:/tmp/temporary-2d6ea2df-4f01-484a-b9ba-034af4fac136 to store the query checkpoint.
2025-06-04 22:36:57.322 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@4102f305] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@23d03ae4]
2025-06-04 22:36:57.335 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-04 22:36:57.336 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-04 22:36:57.336 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-04 22:36:57.338 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-04 22:36:57.491 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-04 22:36:57.518 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-04 22:36:57.519 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 22:36:57.519 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 22:36:57.519 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749051417518
2025-06-04 22:36:57.677 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-2d6ea2df-4f01-484a-b9ba-034af4fac136/sources/0/0 using temp file file:/tmp/temporary-2d6ea2df-4f01-484a-b9ba-034af4fac136/sources/0/.0.07f2ed0f-e1d1-4e44-8789-7550ace3ab56.tmp
2025-06-04 22:36:57.689 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-2d6ea2df-4f01-484a-b9ba-034af4fac136/sources/0/.0.07f2ed0f-e1d1-4e44-8789-7550ace3ab56.tmp to file:/tmp/temporary-2d6ea2df-4f01-484a-b9ba-034af4fac136/sources/0/0
2025-06-04 22:36:57.690 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events-1k":{"1":1706,"0":1604}}
2025-06-04 22:36:57.703 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-2d6ea2df-4f01-484a-b9ba-034af4fac136/offsets/0 using temp file file:/tmp/temporary-2d6ea2df-4f01-484a-b9ba-034af4fac136/offsets/.0.8e17c648-ad88-40ac-895e-4bd021309dfb.tmp
2025-06-04 22:36:57.722 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-2d6ea2df-4f01-484a-b9ba-034af4fac136/offsets/.0.8e17c648-ad88-40ac-895e-4bd021309dfb.tmp to file:/tmp/temporary-2d6ea2df-4f01-484a-b9ba-034af4fac136/offsets/0
2025-06-04 22:36:57.723 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749051417698,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-04 22:36:57.884 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749051417698
2025-06-04 22:36:57.935 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 22:36:57.962 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 22:36:57.994 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749051417698
2025-06-04 22:36:57.996 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 22:36:57.997 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 22:36:58.207 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 120.753582 ms
2025-06-04 22:36:58.324 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.392285 ms
2025-06-04 22:36:58.336 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 5.784222 ms
2025-06-04 22:36:58.383 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:84
2025-06-04 22:36:58.391 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:84) as input to shuffle 0
2025-06-04 22:36:58.393 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:84) with 1 output partitions
2025-06-04 22:36:58.394 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:84)
2025-06-04 22:36:58.394 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-04 22:36:58.395 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-04 22:36:58.396 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:84), which has no missing parents
2025-06-04 22:36:58.427 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-04 22:36:58.444 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-04 22:36:58.446 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:37457 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-04 22:36:58.448 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-04 22:36:58.456 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:84) (first 15 tasks are for partitions Vector(0))
2025-06-04 22:36:58.457 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-04 22:36:58.488 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-04 22:36:58.497 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
2025-06-04 22:36:58.561 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-04 22:36:58.563 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
2025-06-04 22:36:58.578 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 4031 bytes result sent to driver
2025-06-04 22:36:58.585 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 104 ms on phamviethoa (executor driver) (1/1)
2025-06-04 22:36:58.586 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-04 22:36:58.589 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:84) finished in 0.186 s
2025-06-04 22:36:58.591 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-04 22:36:58.591 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-04 22:36:58.592 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:84, took 0.208349 s
2025-06-04 22:36:58.746 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.059304 ms
2025-06-04 22:36:58.796 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:84
2025-06-04 22:36:58.797 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.spark.scheduler.DAGScheduler - Job 1 finished: start at SparkClickstreamProcessor.java:84, took 0.000227 s
2025-06-04 22:36:58.821 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-2d6ea2df-4f01-484a-b9ba-034af4fac136/commits/0 using temp file file:/tmp/temporary-2d6ea2df-4f01-484a-b9ba-034af4fac136/commits/.0.de81af1c-506f-4df8-b3c5-7bfa33c0ea74.tmp
2025-06-04 22:36:58.834 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-2d6ea2df-4f01-484a-b9ba-034af4fac136/commits/.0.de81af1c-506f-4df8-b3c5-7bfa33c0ea74.tmp to file:/tmp/temporary-2d6ea2df-4f01-484a-b9ba-034af4fac136/commits/0
2025-06-04 22:36:58.855 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:36:57.334Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 839,
    "commitOffsets" : 18,
    "getBatch" : 27,
    "latestOffset" : 359,
    "queryPlanning" : 220,
    "triggerExecution" : 1501,
    "walCommit" : 24
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : null,
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:37:30.005 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:37:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:38:00.007 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:38:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 5,
    "triggerExecution" : 6
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:38:30.004 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:38:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:39:00.005 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:39:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 5
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:39:30.005 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:39:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:40:00.004 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:40:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:40:30.006 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:40:30.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:41:00.005 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:41:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:41:30.006 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:41:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 5
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:41:57.659 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-04 22:42:00.004 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:42:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:42:30.005 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:42:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:43:00.004 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:43:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:43:30.004 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:43:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:44:00.005 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:44:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:44:30.004 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:44:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:45:00.005 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:45:00.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:45:30.005 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:45:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 5
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:46:00.004 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:46:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:46:30.005 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:46:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:47:00.005 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:47:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:47:30.005 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:47:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:48:00.003 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:48:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:48:30.004 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:48:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:49:00.004 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:49:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:49:30.003 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:49:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:50:00.005 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:50:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:50:30.003 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:50:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:51:00.004 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:51:00.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:51:30.004 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:51:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:52:00.004 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:52:00.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:52:30.003 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:52:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:53:00.004 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:53:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:53:30.004 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:53:30.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:54:00.003 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:54:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:54:30.003 [stream execution thread for [id = d89d9cff-98f9-4980-bdbc-71b312a71bd1, runId = 26d208a4-7999-49ef-a30f-8d02813528d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "d89d9cff-98f9-4980-bdbc-71b312a71bd1",
  "runId" : "26d208a4-7999-49ef-a30f-8d02813528d3",
  "name" : null,
  "timestamp" : "2025-06-04T15:54:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:54:32.424 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-04 22:54:32.424 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-04 22:54:32.428 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@330f6509{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 22:54:32.430 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-04 22:54:32.437 [dispatcher-event-loop-3 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-04 22:54:32.444 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-04 22:54:32.445 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-04 22:54:32.447 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-04 22:54:32.448 [dispatcher-event-loop-7 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-04 22:54:32.451 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-04 22:54:32.451 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-04 22:54:32.451 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-fc67777c-fd92-4a73-a936-8bf2791de89d
2025-06-04 22:54:32.452 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-2d6ea2df-4f01-484a-b9ba-034af4fac136
2025-06-04 22:55:28.714 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 55292 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-04 22:55:28.715 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-04 22:55:29.191 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-04 22:55:29.194 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-04 22:55:29.194 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-04 22:55:29.194 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-04 22:55:29.237 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-04 22:55:29.237 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 498 ms
2025-06-04 22:55:29.377 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-04 22:55:29.538 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-04 22:55:29.552 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-04 22:55:29.567 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 22:55:29.568 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 22:55:29.568 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749052529567
2025-06-04 22:55:29.657 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-04 22:55:29.659 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 22:55:29.659 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 22:55:29.659 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 22:55:29.664 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-04 22:55:29.668 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-04 22:55:29.676 [main INFO ] com.example.Application - Started Application in 1.138 seconds (JVM running for 1.459)
2025-06-04 22:55:29.940 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-04 22:55:29.959 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-04 22:55:29.960 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-04 22:55:29.961 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-04 22:55:30.005 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-04 22:55:30.038 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 22:55:30.038 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-04 22:55:30.038 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 22:55:30.038 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-04 22:55:30.046 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-04 22:55:30.052 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-04 22:55:30.052 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-04 22:55:30.069 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-04 22:55:30.070 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-04 22:55:30.070 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-04 22:55:30.070 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-04 22:55:30.070 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-04 22:55:30.151 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 33333.
2025-06-04 22:55:30.166 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-04 22:55:30.183 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-04 22:55:30.190 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-04 22:55:30.190 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-04 22:55:30.192 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-04 22:55:30.196 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-7496e02e-64f0-42c2-8473-9c21adc1ecd6
2025-06-04 22:55:30.210 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-04 22:55:30.216 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-04 22:55:30.227 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @2010ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-04 22:55:30.262 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-04 22:55:30.266 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-04 22:55:30.271 [main INFO ] org.sparkproject.jetty.server.Server - Started @2054ms
2025-06-04 22:55:30.281 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@10d466f5{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 22:55:30.282 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-04 22:55:30.288 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6516181f{/,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.327 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-04 22:55:30.331 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-04 22:55:30.340 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41287.
2025-06-04 22:55:30.340 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:41287
2025-06-04 22:55:30.340 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-04 22:55:30.343 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 41287, None)
2025-06-04 22:55:30.345 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:41287 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 41287, None)
2025-06-04 22:55:30.347 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 41287, None)
2025-06-04 22:55:30.347 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 41287, None)
2025-06-04 22:55:30.364 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6516181f{/,null,STOPPED,@Spark}
2025-06-04 22:55:30.365 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@152035eb{/jobs,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.365 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3605ab16{/jobs/json,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.366 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ba402b5{/jobs/job,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.367 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d02c00{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.368 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29dfc68f{/stages,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.368 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@333a2df2{/stages/json,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.369 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3051e476{/stages/stage,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.369 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d9ee75a{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.369 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cf6377{/stages/pool,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.370 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3151277f{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.370 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67f266bd{/storage,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.371 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@912747d{/storage/json,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.372 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cd93621{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.372 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21ba0d33{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.373 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fd63c43{/environment,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.373 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cea67b1{/environment/json,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.373 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23d23d98{/executors,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.374 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40db6136{/executors/json,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.375 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ee1ddcf{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.375 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70aa03c0{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.378 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2435c6ae{/static,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.378 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@468f2a6f{/,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.379 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ed84be9{/api,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.380 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5065bdac{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.380 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e617c0e{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.382 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e1598e5{/metrics/json,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.437 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-04 22:55:30.438 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-04 22:55:30.441 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-04 22:55:30.446 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e9f1a4c{/SQL,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.446 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@457b8fc3{/SQL/json,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.447 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3da61af2{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.447 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@417751d3{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-04 22:55:30.448 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d3a28b5{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 22:55:31.285 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-04 22:55:31.290 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c7bc8ac{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-04 22:55:31.291 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d7f4cbb{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-04 22:55:31.291 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3284f91f{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-04 22:55:31.291 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77e4ffe5{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-04 22:55:31.292 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ddf9fd{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 22:55:31.294 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-58ac1fc8-0b8b-431b-b681-57019501bef9. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-04 22:55:31.302 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-58ac1fc8-0b8b-431b-b681-57019501bef9 resolved to file:/tmp/temporary-58ac1fc8-0b8b-431b-b681-57019501bef9.
2025-06-04 22:55:31.302 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-04 22:55:31.334 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-58ac1fc8-0b8b-431b-b681-57019501bef9/metadata using temp file file:/tmp/temporary-58ac1fc8-0b8b-431b-b681-57019501bef9/.metadata.d25b3683-ce40-42f0-9195-95d8b3dd11c1.tmp
2025-06-04 22:55:31.370 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-58ac1fc8-0b8b-431b-b681-57019501bef9/.metadata.d25b3683-ce40-42f0-9195-95d8b3dd11c1.tmp to file:/tmp/temporary-58ac1fc8-0b8b-431b-b681-57019501bef9/metadata
2025-06-04 22:55:31.382 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8]. Use file:/tmp/temporary-58ac1fc8-0b8b-431b-b681-57019501bef9 to store the query checkpoint.
2025-06-04 22:55:31.387 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@2b1fdeb0] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@32f20d60]
2025-06-04 22:55:31.398 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-04 22:55:31.398 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-04 22:55:31.398 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-04 22:55:31.399 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-04 22:55:31.498 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-04 22:55:31.500 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-04 22:55:31.500 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 22:55:31.500 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 22:55:31.500 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749052531500
2025-06-04 22:55:31.522 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-58ac1fc8-0b8b-431b-b681-57019501bef9/sources/0/0 using temp file file:/tmp/temporary-58ac1fc8-0b8b-431b-b681-57019501bef9/sources/0/.0.568ac409-88e0-4c38-8005-5ad7fbd012c5.tmp
2025-06-04 22:55:31.535 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-58ac1fc8-0b8b-431b-b681-57019501bef9/sources/0/.0.568ac409-88e0-4c38-8005-5ad7fbd012c5.tmp to file:/tmp/temporary-58ac1fc8-0b8b-431b-b681-57019501bef9/sources/0/0
2025-06-04 22:55:31.536 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events-1k":{"1":1706,"0":1604}}
2025-06-04 22:55:31.546 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-58ac1fc8-0b8b-431b-b681-57019501bef9/offsets/0 using temp file file:/tmp/temporary-58ac1fc8-0b8b-431b-b681-57019501bef9/offsets/.0.ae47f95c-dd8c-4de8-b053-2cab6f668cd4.tmp
2025-06-04 22:55:31.564 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-58ac1fc8-0b8b-431b-b681-57019501bef9/offsets/.0.ae47f95c-dd8c-4de8-b053-2cab6f668cd4.tmp to file:/tmp/temporary-58ac1fc8-0b8b-431b-b681-57019501bef9/offsets/0
2025-06-04 22:55:31.565 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749052531542,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-04 22:55:31.678 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749052531542
2025-06-04 22:55:31.707 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 22:55:31.729 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 22:55:31.764 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749052531542
2025-06-04 22:55:31.766 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 22:55:31.766 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 22:55:31.947 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 113.635761 ms
2025-06-04 22:55:32.038 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.577757 ms
2025-06-04 22:55:32.048 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 5.181682 ms
2025-06-04 22:55:32.083 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:84
2025-06-04 22:55:32.088 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:84) as input to shuffle 0
2025-06-04 22:55:32.090 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:84) with 1 output partitions
2025-06-04 22:55:32.090 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:84)
2025-06-04 22:55:32.090 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-04 22:55:32.090 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-04 22:55:32.091 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:84), which has no missing parents
2025-06-04 22:55:32.110 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-04 22:55:32.121 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-04 22:55:32.122 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:41287 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-04 22:55:32.124 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-04 22:55:32.128 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:84) (first 15 tasks are for partitions Vector(0))
2025-06-04 22:55:32.128 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-04 22:55:32.146 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-04 22:55:32.150 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
2025-06-04 22:55:32.192 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-04 22:55:32.193 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
2025-06-04 22:55:32.205 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 4031 bytes result sent to driver
2025-06-04 22:55:32.209 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 67 ms on phamviethoa (executor driver) (1/1)
2025-06-04 22:55:32.210 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-04 22:55:32.212 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:84) finished in 0.116 s
2025-06-04 22:55:32.213 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-04 22:55:32.214 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-04 22:55:32.214 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:84, took 0.131379 s
2025-06-04 22:55:32.320 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.412668 ms
2025-06-04 22:55:32.343 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:84
2025-06-04 22:55:32.343 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.spark.scheduler.DAGScheduler - Job 1 finished: start at SparkClickstreamProcessor.java:84, took 0.000163 s
2025-06-04 22:55:32.360 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-58ac1fc8-0b8b-431b-b681-57019501bef9/commits/0 using temp file file:/tmp/temporary-58ac1fc8-0b8b-431b-b681-57019501bef9/commits/.0.827ecdf6-af51-4c36-a5ce-e1835b65c8f3.tmp
2025-06-04 22:55:32.373 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-58ac1fc8-0b8b-431b-b681-57019501bef9/commits/.0.827ecdf6-af51-4c36-a5ce-e1835b65c8f3.tmp to file:/tmp/temporary-58ac1fc8-0b8b-431b-b681-57019501bef9/commits/0
2025-06-04 22:55:32.387 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "3fba59e1-ef68-46ad-9634-37a8a1f85e1c",
  "runId" : "7149a732-51b4-4755-9056-8bdcacd436c8",
  "name" : null,
  "timestamp" : "2025-06-04T15:55:31.396Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 603,
    "commitOffsets" : 16,
    "getBatch" : 12,
    "latestOffset" : 141,
    "queryPlanning" : 169,
    "triggerExecution" : 977,
    "walCommit" : 22
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : null,
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:55:48.677 [http-nio-8080-exec-1 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=f4e6b2b4-1f35-496d-8bb4-4ebc9207b636, event_name=page_view, event_time=2025-06-04T15:55:45.697Z, user_id=user_4q6eh9x, sessionId=session_q11x8z6, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/}, {event_id=577d4897-fbd6-474f-b9ff-d8597ace3237, event_name=tab_change, event_time=2025-06-04T15:55:46.405Z, user_id=user_4q6eh9x, sessionId=session_q11x8z6, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, is_visible=true, time_visible=0}, {event_id=7051a246-f058-4012-95a3-de48f57540f6, event_name=click, event_time=2025-06-04T15:55:48.632Z, user_id=user_4q6eh9x, sessionId=session_q11x8z6, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 2
                        Durable and styl, element_type=div, element_name=null, track=product_click, productId=2}]}
2025-06-04 22:55:48.677 [http-nio-8080-exec-1 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-04 22:55:48.683 [http-nio-8080-exec-1 INFO ] o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 1000
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-06-04 22:55:48.692 [http-nio-8080-exec-1 INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-04 22:55:48.696 [http-nio-8080-exec-1 INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-06-04 22:55:48.704 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 22:55:48.704 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 22:55:48.704 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749052548704
2025-06-04 22:55:48.709 [kafka-producer-network-thread | producer-1 INFO ] org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-04 22:55:48.709 [kafka-producer-network-thread | producer-1 INFO ] o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 3001 with epoch 0
2025-06-04 22:55:48.715 [http-nio-8080-exec-1 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-04 22:55:50.514 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=dff40e8d-3c59-4ae0-9a20-a16b4611589e, event_name=click, event_time=2025-06-04T15:55:49.257Z, user_id=user_4q6eh9x, sessionId=session_q11x8z6, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 3
                        Customer favorit, element_type=div, element_name=null, track=product_click, productId=3}, {event_id=3735e9cf-ed46-47fb-8f39-736c69fa6e68, event_name=scroll, event_time=2025-06-04T15:55:49.665Z, user_id=user_4q6eh9x, sessionId=session_q11x8z6, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=1}, {event_id=ef4560a3-84be-4141-a21b-9ea139deed28, event_name=click, event_time=2025-06-04T15:55:50.510Z, user_id=user_4q6eh9x, sessionId=session_q11x8z6, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 6
                        Sleek design and, element_type=div, element_name=null, track=product_click, productId=6}]}
2025-06-04 22:55:50.514 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-04 22:55:50.515 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-04 22:55:52.057 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=6b566d2f-ead6-4fa8-b5bb-b744057c9ba1, event_name=scroll, event_time=2025-06-04T15:55:50.665Z, user_id=user_4q6eh9x, sessionId=session_q11x8z6, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=100}, {event_id=4594bcdc-ed1b-452e-9fe4-4a9b07d31ccc, event_name=click, event_time=2025-06-04T15:55:50.962Z, user_id=user_4q6eh9x, sessionId=session_q11x8z6, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 5
                        Bestseller with , element_type=div, element_name=null, track=product_click, productId=5}, {event_id=88a5377c-2082-46d5-8485-87ad725754c6, event_name=click, event_time=2025-06-04T15:55:52.054Z, user_id=user_4q6eh9x, sessionId=session_q11x8z6, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 4
                        Reliable and aff, element_type=div, element_name=null, track=product_click, productId=4}]}
2025-06-04 22:55:52.058 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-04 22:55:52.058 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-04 22:55:53.162 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=0e3d18d7-2e87-4377-9eeb-89d911aba3f1, event_name=click, event_time=2025-06-04T15:55:52.408Z, user_id=user_4q6eh9x, sessionId=session_q11x8z6, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 1
                        High-quality ite, element_type=div, element_name=null, track=product_click, productId=1}, {event_id=ead3af82-a83c-4ff4-acf0-593a4b0419fe, event_name=click, event_time=2025-06-04T15:55:52.824Z, user_id=user_4q6eh9x, sessionId=session_q11x8z6, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 7
                        Compact and effi, element_type=div, element_name=null, track=product_click, productId=7}, {event_id=30e48caf-1278-419c-9160-005db2b8419c, event_name=click, event_time=2025-06-04T15:55:53.158Z, user_id=user_4q6eh9x, sessionId=session_q11x8z6, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 8
                        Eco-friendly and, element_type=div, element_name=null, track=product_click, productId=8}]}
2025-06-04 22:55:53.162 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-04 22:55:53.163 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-04 22:55:54.785 [http-nio-8080-exec-5 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=a558fa6f-4d75-495c-b7f3-355cf2e8eeab, event_name=click, event_time=2025-06-04T15:55:53.463Z, user_id=user_4q6eh9x, sessionId=session_q11x8z6, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 9
                        Top-rated with e, element_type=div, element_name=null, track=product_click, productId=9}, {event_id=b50addb2-e72b-4d56-aea2-3f7846a739c0, event_name=scroll, event_time=2025-06-04T15:55:53.782Z, user_id=user_4q6eh9x, sessionId=session_q11x8z6, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=99}, {event_id=aa94fbb7-5cd3-42d0-97d4-996facd1003f, event_name=scroll, event_time=2025-06-04T15:55:54.782Z, user_id=user_4q6eh9x, sessionId=session_q11x8z6, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=24}]}
2025-06-04 22:55:54.785 [http-nio-8080-exec-5 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-04 22:55:54.786 [http-nio-8080-exec-5 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-04 22:56:00.005 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "3fba59e1-ef68-46ad-9634-37a8a1f85e1c",
  "runId" : "7149a732-51b4-4755-9056-8bdcacd436c8",
  "name" : null,
  "timestamp" : "2025-06-04T15:56:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:56:30.005 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "3fba59e1-ef68-46ad-9634-37a8a1f85e1c",
  "runId" : "7149a732-51b4-4755-9056-8bdcacd436c8",
  "name" : null,
  "timestamp" : "2025-06-04T15:56:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:57:00.004 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "3fba59e1-ef68-46ad-9634-37a8a1f85e1c",
  "runId" : "7149a732-51b4-4755-9056-8bdcacd436c8",
  "name" : null,
  "timestamp" : "2025-06-04T15:57:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:57:30.004 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "3fba59e1-ef68-46ad-9634-37a8a1f85e1c",
  "runId" : "7149a732-51b4-4755-9056-8bdcacd436c8",
  "name" : null,
  "timestamp" : "2025-06-04T15:57:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:58:00.004 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "3fba59e1-ef68-46ad-9634-37a8a1f85e1c",
  "runId" : "7149a732-51b4-4755-9056-8bdcacd436c8",
  "name" : null,
  "timestamp" : "2025-06-04T15:58:00.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:58:30.004 [stream execution thread for [id = 3fba59e1-ef68-46ad-9634-37a8a1f85e1c, runId = 7149a732-51b4-4755-9056-8bdcacd436c8] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "3fba59e1-ef68-46ad-9634-37a8a1f85e1c",
  "runId" : "7149a732-51b4-4755-9056-8bdcacd436c8",
  "name" : null,
  "timestamp" : "2025-06-04T15:58:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:58:42.220 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-04 22:58:42.220 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-04 22:58:42.225 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@10d466f5{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 22:58:42.227 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-04 22:58:42.229 [SpringApplicationShutdownHook INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-06-04 22:58:42.232 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 22:58:42.233 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 22:58:42.233 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-04 22:58:42.233 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 22:58:42.233 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-1 unregistered
2025-06-04 22:58:42.234 [dispatcher-event-loop-13 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-04 22:59:08.459 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 58404 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-04 22:59:08.461 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-04 22:59:08.942 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-04 22:59:08.945 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-04 22:59:08.946 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-04 22:59:08.946 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-04 22:59:08.990 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-04 22:59:08.990 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 507 ms
2025-06-04 22:59:09.134 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-04 22:59:09.294 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-04 22:59:09.309 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-04 22:59:09.323 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 22:59:09.323 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 22:59:09.323 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749052749322
2025-06-04 22:59:09.409 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-04 22:59:09.411 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 22:59:09.411 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 22:59:09.411 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 22:59:09.415 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-04 22:59:09.419 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-04 22:59:09.428 [main INFO ] com.example.Application - Started Application in 1.126 seconds (JVM running for 1.424)
2025-06-04 22:59:09.622 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-04 22:59:09.622 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-04 22:59:09.623 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-04 22:59:09.691 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-04 22:59:09.740 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-04 22:59:09.772 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 22:59:09.772 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-04 22:59:09.772 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 22:59:09.772 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-04 22:59:09.780 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-04 22:59:09.785 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-04 22:59:09.785 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-04 22:59:09.802 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-04 22:59:09.803 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-04 22:59:09.803 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-04 22:59:09.803 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-04 22:59:09.803 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-04 22:59:09.893 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 39687.
2025-06-04 22:59:09.907 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-04 22:59:09.921 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-04 22:59:09.926 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-04 22:59:09.927 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-04 22:59:09.928 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-04 22:59:09.933 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-2a279ddb-6eaf-4eef-a84d-5d1836c2928f
2025-06-04 22:59:09.946 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-04 22:59:09.955 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-04 22:59:09.966 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1962ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-04 22:59:10.000 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-04 22:59:10.004 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-04 22:59:10.009 [main INFO ] org.sparkproject.jetty.server.Server - Started @2005ms
2025-06-04 22:59:10.019 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@60e3cae6{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 22:59:10.019 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-04 22:59:10.026 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5614ae05{/,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.052 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-04 22:59:10.055 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-04 22:59:10.063 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38025.
2025-06-04 22:59:10.063 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:38025
2025-06-04 22:59:10.063 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-04 22:59:10.066 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 38025, None)
2025-06-04 22:59:10.068 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:38025 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 38025, None)
2025-06-04 22:59:10.069 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 38025, None)
2025-06-04 22:59:10.069 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 38025, None)
2025-06-04 22:59:10.082 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@5614ae05{/,null,STOPPED,@Spark}
2025-06-04 22:59:10.082 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@470b5213{/jobs,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.083 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64887fbc{/jobs/json,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.083 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7836c79{/jobs/job,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.083 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28269c65{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.084 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18b58c77{/stages,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.084 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c20e9d6{/stages/json,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.085 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3605ab16{/stages/stage,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.085 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24d0c6a4{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.085 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ba402b5{/stages/pool,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.086 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d02c00{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.086 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29dfc68f{/storage,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.086 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@333a2df2{/storage/json,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.087 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8c18bde{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.087 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ae16aa{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.087 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3051e476{/environment,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.087 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d9ee75a{/environment/json,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.088 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cf6377{/executors,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.088 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3151277f{/executors/json,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.089 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67f266bd{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.089 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@912747d{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.091 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cd93621{/static,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.092 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@762f8ff6{/,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.092 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bf10539{/api,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.093 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@468f2a6f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.093 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ed84be9{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.095 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@510203de{/metrics/json,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.128 [main INFO ] c.e.c.p.SparkClickstreamProcessor - SparkClickstreamProcessor initialized with SparkSession
2025-06-04 22:59:10.128 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Starting Kafka stream processing...
2025-06-04 22:59:10.153 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-04 22:59:10.154 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-04 22:59:10.157 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-04 22:59:10.162 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@becb93a{/SQL,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.162 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d1fd2aa{/SQL/json,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.162 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@43ca96a0{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.163 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5443086a{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.163 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e9f1a4c{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.888 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Successfully connected to Kafka
2025-06-04 22:59:10.976 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Data transformation completed, starting write stream...
2025-06-04 22:59:10.982 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-04 22:59:10.987 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d5ec2ed{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.987 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37596b44{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.988 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@653c0c9c{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.988 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@221961f2{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-04 22:59:10.988 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c7bc8ac{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 22:59:11.002 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/checkpoint/8c2c7b3b-df14-48fa-af06-645e002a3192 resolved to file:/tmp/checkpoint/8c2c7b3b-df14-48fa-af06-645e002a3192.
2025-06-04 22:59:11.003 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-04 22:59:11.038 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/8c2c7b3b-df14-48fa-af06-645e002a3192/metadata using temp file file:/tmp/checkpoint/8c2c7b3b-df14-48fa-af06-645e002a3192/.metadata.80a50eeb-5ae4-418e-9378-a063d868cb47.tmp
2025-06-04 22:59:11.075 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/8c2c7b3b-df14-48fa-af06-645e002a3192/.metadata.80a50eeb-5ae4-418e-9378-a063d868cb47.tmp to file:/tmp/checkpoint/8c2c7b3b-df14-48fa-af06-645e002a3192/metadata
2025-06-04 22:59:11.087 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532]. Use file:/tmp/checkpoint/8c2c7b3b-df14-48fa-af06-645e002a3192 to store the query checkpoint.
2025-06-04 22:59:11.089 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Stream processing started successfully
2025-06-04 22:59:11.091 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@170e72b6] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@64179539]
2025-06-04 22:59:11.105 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-04 22:59:11.106 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-04 22:59:11.106 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-04 22:59:11.107 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-04 22:59:11.214 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-04 22:59:11.216 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-04 22:59:11.216 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 22:59:11.216 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 22:59:11.216 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749052751216
2025-06-04 22:59:11.238 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/8c2c7b3b-df14-48fa-af06-645e002a3192/sources/0/0 using temp file file:/tmp/checkpoint/8c2c7b3b-df14-48fa-af06-645e002a3192/sources/0/.0.2bb06085-b4f5-4c93-808c-b3b542c67010.tmp
2025-06-04 22:59:11.250 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/8c2c7b3b-df14-48fa-af06-645e002a3192/sources/0/.0.2bb06085-b4f5-4c93-808c-b3b542c67010.tmp to file:/tmp/checkpoint/8c2c7b3b-df14-48fa-af06-645e002a3192/sources/0/0
2025-06-04 22:59:11.250 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events-1k":{"1":1706,"0":1604}}
2025-06-04 22:59:11.260 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/8c2c7b3b-df14-48fa-af06-645e002a3192/offsets/0 using temp file file:/tmp/checkpoint/8c2c7b3b-df14-48fa-af06-645e002a3192/offsets/.0.3bfcdf99-32c5-450d-b455-0983a46b0ed3.tmp
2025-06-04 22:59:11.276 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/8c2c7b3b-df14-48fa-af06-645e002a3192/offsets/.0.3bfcdf99-32c5-450d-b455-0983a46b0ed3.tmp to file:/tmp/checkpoint/8c2c7b3b-df14-48fa-af06-645e002a3192/offsets/0
2025-06-04 22:59:11.276 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749052751256,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-04 22:59:11.398 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749052751256
2025-06-04 22:59:11.426 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 22:59:11.446 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 22:59:11.479 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749052751256
2025-06-04 22:59:11.480 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 22:59:11.481 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 22:59:11.662 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 114.897352 ms
2025-06-04 22:59:11.748 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.152136 ms
2025-06-04 22:59:11.756 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 3.851737 ms
2025-06-04 22:59:11.790 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:110
2025-06-04 22:59:11.795 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:110) as input to shuffle 0
2025-06-04 22:59:11.797 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:110) with 1 output partitions
2025-06-04 22:59:11.798 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:110)
2025-06-04 22:59:11.798 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-04 22:59:11.798 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-04 22:59:11.799 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:110), which has no missing parents
2025-06-04 22:59:11.818 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-04 22:59:11.829 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-04 22:59:11.830 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:38025 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-04 22:59:11.831 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-04 22:59:11.835 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:110) (first 15 tasks are for partitions Vector(0))
2025-06-04 22:59:11.836 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-04 22:59:11.854 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-04 22:59:11.858 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
2025-06-04 22:59:11.901 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-04 22:59:11.902 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
2025-06-04 22:59:11.913 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 4031 bytes result sent to driver
2025-06-04 22:59:11.917 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 67 ms on phamviethoa (executor driver) (1/1)
2025-06-04 22:59:11.918 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-04 22:59:11.920 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:110) finished in 0.116 s
2025-06-04 22:59:11.922 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-04 22:59:11.922 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-04 22:59:11.922 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:110, took 0.132723 s
2025-06-04 22:59:11.926 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] c.e.c.p.SparkClickstreamProcessor - Processing batch 0 with 0 records
2025-06-04 22:59:11.926 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] c.e.c.p.SparkClickstreamProcessor - Batch 0 is empty, skipping write to ClickHouse
2025-06-04 22:59:11.932 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/8c2c7b3b-df14-48fa-af06-645e002a3192/commits/0 using temp file file:/tmp/checkpoint/8c2c7b3b-df14-48fa-af06-645e002a3192/commits/.0.e66386ba-641d-4130-b0d2-318733d1cfbc.tmp
2025-06-04 22:59:11.945 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/8c2c7b3b-df14-48fa-af06-645e002a3192/commits/.0.e66386ba-641d-4130-b0d2-318733d1cfbc.tmp to file:/tmp/checkpoint/8c2c7b3b-df14-48fa-af06-645e002a3192/commits/0
2025-06-04 22:59:11.959 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "f03b5496-a0ad-4bb8-9d83-8bb18ad4a392",
  "runId" : "37ff5d5f-d1eb-4454-9da7-af2023583532",
  "name" : null,
  "timestamp" : "2025-06-04T15:59:11.103Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 461,
    "commitOffsets" : 16,
    "getBatch" : 11,
    "latestOffset" : 147,
    "queryPlanning" : 174,
    "triggerExecution" : 841,
    "walCommit" : 19
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : null,
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 22:59:15.829 [http-nio-8080-exec-1 WARN ] o.s.w.s.m.s.DefaultHandlerExceptionResolver - Resolved [org.springframework.web.HttpMediaTypeNotSupportedException: Content type 'text/plain;charset=UTF-8' not supported]
2025-06-04 22:59:20.047 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=93ee3872-cd32-4170-8b90-e2cfa4cb3478, event_name=page_view, event_time=2025-06-04T15:59:17.766Z, user_id=user_4q6eh9x, sessionId=session_5ccw5qc, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/}, {event_id=7f364390-7a45-45d5-a7eb-c3afafb0d9cf, event_name=tab_change, event_time=2025-06-04T15:59:18.425Z, user_id=user_4q6eh9x, sessionId=session_5ccw5qc, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, is_visible=true, time_visible=0}, {event_id=49a54da5-a5d1-41a4-abc0-43bb15f915cf, event_name=scroll, event_time=2025-06-04T15:59:20.040Z, user_id=user_4q6eh9x, sessionId=session_5ccw5qc, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=3}]}
2025-06-04 22:59:20.047 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-04 22:59:20.053 [http-nio-8080-exec-2 INFO ] o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 1000
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-06-04 22:59:20.063 [http-nio-8080-exec-2 INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-04 22:59:20.066 [http-nio-8080-exec-2 INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-06-04 22:59:20.075 [http-nio-8080-exec-2 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 22:59:20.075 [http-nio-8080-exec-2 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 22:59:20.075 [http-nio-8080-exec-2 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749052760075
2025-06-04 22:59:20.080 [kafka-producer-network-thread | producer-1 INFO ] org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-04 22:59:20.081 [kafka-producer-network-thread | producer-1 INFO ] o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 3002 with epoch 0
2025-06-04 22:59:20.086 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-04 22:59:21.779 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=a984c4f9-75c9-4cc9-a0a4-80c2a92c9f12, event_name=click, event_time=2025-06-04T15:59:21.023Z, user_id=user_4q6eh9x, sessionId=session_5ccw5qc, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 2
                        Durable and styl, element_type=div, element_name=null, track=product_click, productId=2}, {event_id=9d268208-8a60-4611-8ae1-f7fccbd5c8ec, event_name=scroll, event_time=2025-06-04T15:59:21.041Z, user_id=user_4q6eh9x, sessionId=session_5ccw5qc, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=100}, {event_id=2a3681f8-2397-4694-81a1-1f74edc1c1b5, event_name=click, event_time=2025-06-04T15:59:21.776Z, user_id=user_4q6eh9x, sessionId=session_5ccw5qc, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 1
                        High-quality ite, element_type=div, element_name=null, track=product_click, productId=1}]}
2025-06-04 22:59:21.779 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-04 22:59:21.780 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-04 22:59:22.927 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=6e09e351-ccd2-4700-87b3-10e1ac269339, event_name=click, event_time=2025-06-04T15:59:22.183Z, user_id=user_4q6eh9x, sessionId=session_5ccw5qc, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 4
                        Reliable and aff, element_type=div, element_name=null, track=product_click, productId=4}, {event_id=a7125cd9-d454-415e-8285-08339d47d61a, event_name=click, event_time=2025-06-04T15:59:22.559Z, user_id=user_4q6eh9x, sessionId=session_5ccw5qc, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 5
                        Bestseller with , element_type=div, element_name=null, track=product_click, productId=5}, {event_id=2e8e867c-cdfd-493c-b991-4e7f3e4f8322, event_name=click, event_time=2025-06-04T15:59:22.924Z, user_id=user_4q6eh9x, sessionId=session_5ccw5qc, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 8
                        Eco-friendly and, element_type=div, element_name=null, track=product_click, productId=8}]}
2025-06-04 22:59:22.928 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-04 22:59:22.929 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-04 22:59:24.032 [http-nio-8080-exec-5 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=1a8573d2-ec5d-4226-95c2-38df956411c8, event_name=click, event_time=2025-06-04T15:59:23.325Z, user_id=user_4q6eh9x, sessionId=session_5ccw5qc, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 7
                        Compact and effi, element_type=div, element_name=null, track=product_click, productId=7}, {event_id=683d0335-94c3-4b3d-bd72-39081899b77c, event_name=click, event_time=2025-06-04T15:59:23.713Z, user_id=user_4q6eh9x, sessionId=session_5ccw5qc, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 9
                        Top-rated with e, element_type=div, element_name=null, track=product_click, productId=9}, {event_id=ae50b51c-211a-4ca3-bf0b-41f6d9ec4f58, event_name=click, event_time=2025-06-04T15:59:24.028Z, user_id=user_4q6eh9x, sessionId=session_5ccw5qc, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 6
                        Sleek design and, element_type=div, element_name=null, track=product_click, productId=6}]}
2025-06-04 22:59:24.032 [http-nio-8080-exec-5 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-04 22:59:24.033 [http-nio-8080-exec-5 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-04 22:59:26.110 [http-nio-8080-exec-7 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=cc171b50-6e0f-4d90-b634-2ed3b9871ec7, event_name=click, event_time=2025-06-04T15:59:24.389Z, user_id=user_4q6eh9x, sessionId=session_5ccw5qc, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 3
                        Customer favorit, element_type=div, element_name=null, track=product_click, productId=3}, {event_id=e7f59e46-a175-4c29-9660-776cb5e435ed, event_name=scroll, event_time=2025-06-04T15:59:25.107Z, user_id=user_4q6eh9x, sessionId=session_5ccw5qc, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=99}, {event_id=e2cd6de6-499b-487f-9261-d16bb5c5b590, event_name=scroll, event_time=2025-06-04T15:59:26.107Z, user_id=user_4q6eh9x, sessionId=session_5ccw5qc, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=22}]}
2025-06-04 22:59:26.110 [http-nio-8080-exec-7 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-04 22:59:26.111 [http-nio-8080-exec-7 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-04 22:59:29.110 [http-nio-8080-exec-8 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=cfac0cf1-9945-4f6f-8995-99614cbcebba, event_name=scroll, event_time=2025-06-04T15:59:27.107Z, user_id=user_4q6eh9x, sessionId=session_5ccw5qc, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=76}, {event_id=83e0b692-f0ed-4143-a8e2-72a638de8fcb, event_name=scroll, event_time=2025-06-04T15:59:28.107Z, user_id=user_4q6eh9x, sessionId=session_5ccw5qc, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=31}, {event_id=6e119f1c-bf44-4513-9e5b-6826135d096a, event_name=scroll, event_time=2025-06-04T15:59:29.107Z, user_id=user_4q6eh9x, sessionId=session_5ccw5qc, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=0}]}
2025-06-04 22:59:29.110 [http-nio-8080-exec-8 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-04 22:59:29.112 [http-nio-8080-exec-8 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-04 22:59:30.005 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "f03b5496-a0ad-4bb8-9d83-8bb18ad4a392",
  "runId" : "37ff5d5f-d1eb-4454-9da7-af2023583532",
  "name" : null,
  "timestamp" : "2025-06-04T15:59:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:00:00.004 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "f03b5496-a0ad-4bb8-9d83-8bb18ad4a392",
  "runId" : "37ff5d5f-d1eb-4454-9da7-af2023583532",
  "name" : null,
  "timestamp" : "2025-06-04T16:00:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:00:30.003 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "f03b5496-a0ad-4bb8-9d83-8bb18ad4a392",
  "runId" : "37ff5d5f-d1eb-4454-9da7-af2023583532",
  "name" : null,
  "timestamp" : "2025-06-04T16:00:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:01:00.004 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "f03b5496-a0ad-4bb8-9d83-8bb18ad4a392",
  "runId" : "37ff5d5f-d1eb-4454-9da7-af2023583532",
  "name" : null,
  "timestamp" : "2025-06-04T16:01:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:01:30.004 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "f03b5496-a0ad-4bb8-9d83-8bb18ad4a392",
  "runId" : "37ff5d5f-d1eb-4454-9da7-af2023583532",
  "name" : null,
  "timestamp" : "2025-06-04T16:01:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:02:00.004 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "f03b5496-a0ad-4bb8-9d83-8bb18ad4a392",
  "runId" : "37ff5d5f-d1eb-4454-9da7-af2023583532",
  "name" : null,
  "timestamp" : "2025-06-04T16:02:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:02:30.004 [stream execution thread for [id = f03b5496-a0ad-4bb8-9d83-8bb18ad4a392, runId = 37ff5d5f-d1eb-4454-9da7-af2023583532] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "f03b5496-a0ad-4bb8-9d83-8bb18ad4a392",
  "runId" : "37ff5d5f-d1eb-4454-9da7-af2023583532",
  "name" : null,
  "timestamp" : "2025-06-04T16:02:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:02:54.141 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-04 23:02:54.141 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-04 23:02:54.145 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@60e3cae6{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:02:54.146 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-04 23:02:54.154 [dispatcher-event-loop-0 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-04 23:02:54.159 [SpringApplicationShutdownHook INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-06-04 23:02:54.161 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-04 23:02:54.161 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-04 23:02:54.161 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 23:02:54.161 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 23:02:54.161 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-04 23:02:54.161 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 23:02:54.162 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-1 unregistered
2025-06-04 23:02:54.163 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-04 23:03:00.313 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-04 23:03:00.395 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-04 23:03:00.441 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:03:00.442 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-04 23:03:00.442 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:03:00.442 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-04 23:03:00.454 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-04 23:03:00.460 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-04 23:03:00.460 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-04 23:03:00.491 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-04 23:03:00.492 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-04 23:03:00.492 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-04 23:03:00.492 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-04 23:03:00.492 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-04 23:03:00.635 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 43589.
2025-06-04 23:03:00.650 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-04 23:03:00.668 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-04 23:03:00.678 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-04 23:03:00.678 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-04 23:03:00.680 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-04 23:03:00.690 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-5906c757-e64d-4565-a6a7-2a719800a7c6
2025-06-04 23:03:00.707 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-04 23:03:00.716 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-04 23:03:00.736 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1104ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-04 23:03:00.783 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-04 23:03:00.789 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-04 23:03:00.798 [main INFO ] org.sparkproject.jetty.server.Server - Started @1166ms
2025-06-04 23:03:00.815 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@4a0fab74{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:03:00.816 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-04 23:03:00.827 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d56aaa6{/,null,AVAILABLE,@Spark}
2025-06-04 23:03:00.889 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-04 23:03:00.893 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-04 23:03:00.905 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43997.
2025-06-04 23:03:00.905 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:43997
2025-06-04 23:03:00.906 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-04 23:03:00.910 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 43997, None)
2025-06-04 23:03:00.912 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:43997 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 43997, None)
2025-06-04 23:03:00.914 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 43997, None)
2025-06-04 23:03:00.915 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 43997, None)
2025-06-04 23:03:00.991 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@d56aaa6{/,null,STOPPED,@Spark}
2025-06-04 23:03:00.992 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@711d1a52{/jobs,null,AVAILABLE,@Spark}
2025-06-04 23:03:00.993 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@302edb74{/jobs/json,null,AVAILABLE,@Spark}
2025-06-04 23:03:00.993 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@312b34e3{/jobs/job,null,AVAILABLE,@Spark}
2025-06-04 23:03:00.994 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a865273{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-04 23:03:00.994 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4068102e{/stages,null,AVAILABLE,@Spark}
2025-06-04 23:03:00.994 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c008c24{/stages/json,null,AVAILABLE,@Spark}
2025-06-04 23:03:00.995 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b8bb184{/stages/stage,null,AVAILABLE,@Spark}
2025-06-04 23:03:00.996 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dc79225{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-04 23:03:00.996 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46185a1b{/stages/pool,null,AVAILABLE,@Spark}
2025-06-04 23:03:00.996 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60cf62ad{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-04 23:03:00.997 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ac4ccad{/storage,null,AVAILABLE,@Spark}
2025-06-04 23:03:00.997 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14982a82{/storage/json,null,AVAILABLE,@Spark}
2025-06-04 23:03:00.998 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72f8ae0c{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-04 23:03:00.998 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6726cc69{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-04 23:03:00.999 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33899f7a{/environment,null,AVAILABLE,@Spark}
2025-06-04 23:03:01.000 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@290d10ef{/environment/json,null,AVAILABLE,@Spark}
2025-06-04 23:03:01.000 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@644ded04{/executors,null,AVAILABLE,@Spark}
2025-06-04 23:03:01.001 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13d9261f{/executors/json,null,AVAILABLE,@Spark}
2025-06-04 23:03:01.002 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5300cac{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-04 23:03:01.002 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ba359bd{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-04 23:03:01.007 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@673919a7{/static,null,AVAILABLE,@Spark}
2025-06-04 23:03:01.007 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b9396d3{/,null,AVAILABLE,@Spark}
2025-06-04 23:03:01.008 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a472566{/api,null,AVAILABLE,@Spark}
2025-06-04 23:03:01.009 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@277bf091{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-04 23:03:01.009 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a094db2{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-04 23:03:01.011 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7add323c{/metrics/json,null,AVAILABLE,@Spark}
2025-06-04 23:03:01.047 [main INFO ] c.e.c.p.SparkClickstreamProcessor - SparkClickstreamProcessor initialized with SparkSession
2025-06-04 23:03:01.047 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Starting Kafka stream processing...
2025-06-04 23:03:01.082 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-04 23:03:01.086 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-04 23:03:01.093 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@714f3da4{/SQL,null,AVAILABLE,@Spark}
2025-06-04 23:03:01.094 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f53481b{/SQL/json,null,AVAILABLE,@Spark}
2025-06-04 23:03:01.095 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e869098{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-04 23:03:01.095 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d497a91{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-04 23:03:01.101 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b0f2299{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:03:02.275 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Successfully connected to Kafka
2025-06-04 23:03:02.414 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Data transformation completed, starting write stream...
2025-06-04 23:03:02.431 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-04 23:03:02.441 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1169fdfd{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-04 23:03:02.442 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@224d537d{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-04 23:03:02.442 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65d23aa3{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-04 23:03:02.442 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63d0e8d{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-04 23:03:02.443 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e691624{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:03:02.463 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/checkpoint/aa6a1374-d5f8-4de1-89ed-ca85266b4058 resolved to file:/tmp/checkpoint/aa6a1374-d5f8-4de1-89ed-ca85266b4058.
2025-06-04 23:03:02.463 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-04 23:03:02.506 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/aa6a1374-d5f8-4de1-89ed-ca85266b4058/metadata using temp file file:/tmp/checkpoint/aa6a1374-d5f8-4de1-89ed-ca85266b4058/.metadata.f9278564-e1fd-4b7b-9c1b-a66054706f38.tmp
2025-06-04 23:03:02.565 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/aa6a1374-d5f8-4de1-89ed-ca85266b4058/.metadata.f9278564-e1fd-4b7b-9c1b-a66054706f38.tmp to file:/tmp/checkpoint/aa6a1374-d5f8-4de1-89ed-ca85266b4058/metadata
2025-06-04 23:03:02.580 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b]. Use file:/tmp/checkpoint/aa6a1374-d5f8-4de1-89ed-ca85266b4058 to store the query checkpoint.
2025-06-04 23:03:02.583 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Stream processing started successfully
2025-06-04 23:03:02.586 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@59dc4ca4] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@77623e7f]
2025-06-04 23:03:02.599 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-04 23:03:02.600 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-04 23:03:02.601 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-04 23:03:02.602 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-04 23:03:02.755 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-04 23:03:02.786 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-04 23:03:02.787 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:03:02.787 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:03:02.787 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749052982786
2025-06-04 23:03:02.971 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/aa6a1374-d5f8-4de1-89ed-ca85266b4058/sources/0/0 using temp file file:/tmp/checkpoint/aa6a1374-d5f8-4de1-89ed-ca85266b4058/sources/0/.0.81fc52af-8e1e-46c2-b63c-ef0a416f4a67.tmp
2025-06-04 23:03:02.984 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/aa6a1374-d5f8-4de1-89ed-ca85266b4058/sources/0/.0.81fc52af-8e1e-46c2-b63c-ef0a416f4a67.tmp to file:/tmp/checkpoint/aa6a1374-d5f8-4de1-89ed-ca85266b4058/sources/0/0
2025-06-04 23:03:02.984 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events-1k":{"1":1706,"0":1604}}
2025-06-04 23:03:02.997 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/aa6a1374-d5f8-4de1-89ed-ca85266b4058/offsets/0 using temp file file:/tmp/checkpoint/aa6a1374-d5f8-4de1-89ed-ca85266b4058/offsets/.0.ee2e8e7b-47a3-4f1d-9bde-e611b15be1cd.tmp
2025-06-04 23:03:03.018 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/aa6a1374-d5f8-4de1-89ed-ca85266b4058/offsets/.0.ee2e8e7b-47a3-4f1d-9bde-e611b15be1cd.tmp to file:/tmp/checkpoint/aa6a1374-d5f8-4de1-89ed-ca85266b4058/offsets/0
2025-06-04 23:03:03.019 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749052982992,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-04 23:03:03.203 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749052982992
2025-06-04 23:03:03.260 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:03:03.288 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:03:03.320 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749052982992
2025-06-04 23:03:03.323 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:03:03.324 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:03:03.550 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 130.935173 ms
2025-06-04 23:03:03.680 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.345105 ms
2025-06-04 23:03:03.694 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.370357 ms
2025-06-04 23:03:03.744 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:110
2025-06-04 23:03:03.751 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:110) as input to shuffle 0
2025-06-04 23:03:03.754 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:110) with 1 output partitions
2025-06-04 23:03:03.754 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:110)
2025-06-04 23:03:03.754 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-04 23:03:03.755 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-04 23:03:03.757 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:110), which has no missing parents
2025-06-04 23:03:03.787 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-04 23:03:03.806 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-04 23:03:03.807 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:43997 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-04 23:03:03.810 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-04 23:03:03.818 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:110) (first 15 tasks are for partitions Vector(0))
2025-06-04 23:03:03.819 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-04 23:03:03.849 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-04 23:03:03.856 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
2025-06-04 23:03:03.940 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-04 23:03:03.942 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
2025-06-04 23:03:03.959 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 4031 bytes result sent to driver
2025-06-04 23:03:03.965 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 123 ms on phamviethoa (executor driver) (1/1)
2025-06-04 23:03:03.967 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-04 23:03:03.970 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:110) finished in 0.206 s
2025-06-04 23:03:03.972 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-04 23:03:03.972 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-04 23:03:03.973 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:110, took 0.228992 s
2025-06-04 23:03:03.978 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] c.e.c.p.SparkClickstreamProcessor - Processing batch 0 with 0 records
2025-06-04 23:03:03.978 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] c.e.c.p.SparkClickstreamProcessor - Batch 0 is empty, skipping write to ClickHouse
2025-06-04 23:03:03.986 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/aa6a1374-d5f8-4de1-89ed-ca85266b4058/commits/0 using temp file file:/tmp/checkpoint/aa6a1374-d5f8-4de1-89ed-ca85266b4058/commits/.0.ffaddbe5-604a-4ed2-8a3d-d4a7303ab5e6.tmp
2025-06-04 23:03:04.000 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/aa6a1374-d5f8-4de1-89ed-ca85266b4058/commits/.0.ffaddbe5-604a-4ed2-8a3d-d4a7303ab5e6.tmp to file:/tmp/checkpoint/aa6a1374-d5f8-4de1-89ed-ca85266b4058/commits/0
2025-06-04 23:03:04.019 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "267ef6b9-6661-46a5-af11-116221eeb5c0",
  "runId" : "f0d0d2c6-cb9b-4028-9675-043b5323f09b",
  "name" : null,
  "timestamp" : "2025-06-04T16:03:02.598Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 676,
    "commitOffsets" : 20,
    "getBatch" : 14,
    "latestOffset" : 388,
    "queryPlanning" : 265,
    "triggerExecution" : 1403,
    "walCommit" : 26
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : null,
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:03:30.005 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "267ef6b9-6661-46a5-af11-116221eeb5c0",
  "runId" : "f0d0d2c6-cb9b-4028-9675-043b5323f09b",
  "name" : null,
  "timestamp" : "2025-06-04T16:03:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:04:00.004 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "267ef6b9-6661-46a5-af11-116221eeb5c0",
  "runId" : "f0d0d2c6-cb9b-4028-9675-043b5323f09b",
  "name" : null,
  "timestamp" : "2025-06-04T16:04:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:04:30.004 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "267ef6b9-6661-46a5-af11-116221eeb5c0",
  "runId" : "f0d0d2c6-cb9b-4028-9675-043b5323f09b",
  "name" : null,
  "timestamp" : "2025-06-04T16:04:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:05:00.006 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "267ef6b9-6661-46a5-af11-116221eeb5c0",
  "runId" : "f0d0d2c6-cb9b-4028-9675-043b5323f09b",
  "name" : null,
  "timestamp" : "2025-06-04T16:05:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:05:30.004 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "267ef6b9-6661-46a5-af11-116221eeb5c0",
  "runId" : "f0d0d2c6-cb9b-4028-9675-043b5323f09b",
  "name" : null,
  "timestamp" : "2025-06-04T16:05:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:06:00.004 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "267ef6b9-6661-46a5-af11-116221eeb5c0",
  "runId" : "f0d0d2c6-cb9b-4028-9675-043b5323f09b",
  "name" : null,
  "timestamp" : "2025-06-04T16:06:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:06:30.004 [stream execution thread for [id = 267ef6b9-6661-46a5-af11-116221eeb5c0, runId = f0d0d2c6-cb9b-4028-9675-043b5323f09b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "267ef6b9-6661-46a5-af11-116221eeb5c0",
  "runId" : "f0d0d2c6-cb9b-4028-9675-043b5323f09b",
  "name" : null,
  "timestamp" : "2025-06-04T16:06:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:06:54.902 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-04 23:06:54.902 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-04 23:06:54.907 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@4a0fab74{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:06:54.909 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-04 23:06:54.919 [dispatcher-event-loop-2 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-04 23:06:54.925 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-04 23:06:54.925 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-04 23:06:54.928 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-04 23:06:54.929 [dispatcher-event-loop-6 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-04 23:06:54.932 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-04 23:06:54.932 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-04 23:06:54.933 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-6de07151-204f-4721-b403-c36cb2044cbe
2025-06-04 23:06:57.382 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-04 23:06:57.475 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-04 23:06:57.527 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:06:57.527 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-04 23:06:57.528 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:06:57.528 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-04 23:06:57.541 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-04 23:06:57.547 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-04 23:06:57.548 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-04 23:06:57.578 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-04 23:06:57.578 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-04 23:06:57.578 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-04 23:06:57.579 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-04 23:06:57.579 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-04 23:06:57.717 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 39135.
2025-06-04 23:06:57.732 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-04 23:06:57.751 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-04 23:06:57.760 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-04 23:06:57.760 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-04 23:06:57.762 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-04 23:06:57.772 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-bea4d625-cb20-4770-82d2-c32e734598b7
2025-06-04 23:06:57.789 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-04 23:06:57.800 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-04 23:06:57.820 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1083ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-04 23:06:57.862 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-04 23:06:57.868 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-04 23:06:57.876 [main INFO ] org.sparkproject.jetty.server.Server - Started @1139ms
2025-06-04 23:06:57.892 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@5cc175b6{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:06:57.892 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-04 23:06:57.903 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d2a1da3{/,null,AVAILABLE,@Spark}
2025-06-04 23:06:57.955 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-04 23:06:57.959 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-04 23:06:57.972 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37843.
2025-06-04 23:06:57.973 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:37843
2025-06-04 23:06:57.974 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-04 23:06:57.977 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 37843, None)
2025-06-04 23:06:57.980 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:37843 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 37843, None)
2025-06-04 23:06:57.983 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 37843, None)
2025-06-04 23:06:57.984 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 37843, None)
2025-06-04 23:06:58.065 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4d2a1da3{/,null,STOPPED,@Spark}
2025-06-04 23:06:58.066 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@472a11ae{/jobs,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.066 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30e9ca13{/jobs/json,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.067 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e0895f5{/jobs/job,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.067 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fd9ebde{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.068 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ee5b2d9{/stages,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.068 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@323f3c96{/stages/json,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.069 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bc0d349{/stages/stage,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.069 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5292ceca{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.070 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e9ef5b6{/stages/pool,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.071 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4110765e{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.071 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62e93c3a{/storage,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.071 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25d93198{/storage/json,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.072 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f951a7f{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.072 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c777e7b{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.073 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78e22d35{/environment,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.073 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59f93db8{/environment/json,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.074 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73c9e8e8{/executors,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.074 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1deceb67{/executors/json,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.075 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64984b0f{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.075 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53ec2968{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.080 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e19755a{/static,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.080 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65bdd558{/,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.081 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4beaf6bd{/api,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.081 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@660f0c{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.082 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b1f5012{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.084 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38d17d80{/metrics/json,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.119 [main INFO ] c.e.c.p.SparkClickstreamProcessor - SparkClickstreamProcessor initialized with SparkSession
2025-06-04 23:06:58.119 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Starting Kafka stream processing...
2025-06-04 23:06:58.155 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-04 23:06:58.159 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-04 23:06:58.166 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@352ed70d{/SQL,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.166 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5793b87{/SQL/json,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.167 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33634f04{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.167 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7601bc96{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-04 23:06:58.173 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57562473{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:06:59.379 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Successfully connected to Kafka
2025-06-04 23:06:59.402 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Kafka DataFrame Schema: {
  "type" : "struct",
  "fields" : [ {
    "name" : "key",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "value",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "topic",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "partition",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "offset",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "timestamp",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "timestampType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
2025-06-04 23:06:59.541 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Processed DataFrame Schema: {
  "type" : "struct",
  "fields" : [ {
    "name" : "event_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "session_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "app_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "platform",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "page_url",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "geo_country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "geo_city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "traffic_source",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "traffic_medium",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "kafka_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "processing_time",
    "type" : "string",
    "nullable" : false,
    "metadata" : { }
  } ]
}
2025-06-04 23:06:59.541 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Data transformation completed, starting write stream...
2025-06-04 23:06:59.558 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-04 23:06:59.566 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f6ff62{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-04 23:06:59.566 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@651caa2e{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-04 23:06:59.567 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e9f8160{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-04 23:06:59.567 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e1a150c{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-04 23:06:59.568 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72c704f1{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:06:59.589 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/checkpoint/8857ea91-7779-4bc8-a95a-d338d593c62a resolved to file:/tmp/checkpoint/8857ea91-7779-4bc8-a95a-d338d593c62a.
2025-06-04 23:06:59.589 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-04 23:06:59.627 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/8857ea91-7779-4bc8-a95a-d338d593c62a/metadata using temp file file:/tmp/checkpoint/8857ea91-7779-4bc8-a95a-d338d593c62a/.metadata.5789bba3-0c05-4c48-8160-434460d56b9e.tmp
2025-06-04 23:06:59.668 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/8857ea91-7779-4bc8-a95a-d338d593c62a/.metadata.5789bba3-0c05-4c48-8160-434460d56b9e.tmp to file:/tmp/checkpoint/8857ea91-7779-4bc8-a95a-d338d593c62a/metadata
2025-06-04 23:06:59.685 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959]. Use file:/tmp/checkpoint/8857ea91-7779-4bc8-a95a-d338d593c62a to store the query checkpoint.
2025-06-04 23:06:59.688 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Stream processing started successfully
2025-06-04 23:06:59.691 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@a4a3f6e] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@2b22759f]
2025-06-04 23:06:59.705 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-04 23:06:59.706 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-04 23:06:59.707 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-04 23:06:59.708 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-04 23:06:59.869 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-04 23:06:59.901 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-04 23:06:59.902 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:06:59.902 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:06:59.902 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749053219901
2025-06-04 23:07:00.091 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/8857ea91-7779-4bc8-a95a-d338d593c62a/sources/0/0 using temp file file:/tmp/checkpoint/8857ea91-7779-4bc8-a95a-d338d593c62a/sources/0/.0.5647aa08-6cf2-49fb-aada-3b8118066687.tmp
2025-06-04 23:07:00.104 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/8857ea91-7779-4bc8-a95a-d338d593c62a/sources/0/.0.5647aa08-6cf2-49fb-aada-3b8118066687.tmp to file:/tmp/checkpoint/8857ea91-7779-4bc8-a95a-d338d593c62a/sources/0/0
2025-06-04 23:07:00.105 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events-1k":{"1":1706,"0":1604}}
2025-06-04 23:07:00.117 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/8857ea91-7779-4bc8-a95a-d338d593c62a/offsets/0 using temp file file:/tmp/checkpoint/8857ea91-7779-4bc8-a95a-d338d593c62a/offsets/.0.35b19f96-bb91-45b1-960d-203eaac754e0.tmp
2025-06-04 23:07:00.135 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/8857ea91-7779-4bc8-a95a-d338d593c62a/offsets/.0.35b19f96-bb91-45b1-960d-203eaac754e0.tmp to file:/tmp/checkpoint/8857ea91-7779-4bc8-a95a-d338d593c62a/offsets/0
2025-06-04 23:07:00.136 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749053220112,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-04 23:07:00.292 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749053220112
2025-06-04 23:07:00.342 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:07:00.367 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:07:00.398 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749053220112
2025-06-04 23:07:00.401 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:07:00.402 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:07:00.606 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 123.325747 ms
2025-06-04 23:07:00.719 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.870647 ms
2025-06-04 23:07:00.731 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 5.647619 ms
2025-06-04 23:07:00.779 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:127
2025-06-04 23:07:00.788 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:127) as input to shuffle 0
2025-06-04 23:07:00.790 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:127) with 1 output partitions
2025-06-04 23:07:00.791 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:127)
2025-06-04 23:07:00.791 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-04 23:07:00.792 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-04 23:07:00.793 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:127), which has no missing parents
2025-06-04 23:07:00.821 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-04 23:07:00.840 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-04 23:07:00.841 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:37843 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-04 23:07:00.843 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-04 23:07:00.852 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:127) (first 15 tasks are for partitions Vector(0))
2025-06-04 23:07:00.853 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-04 23:07:00.887 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-04 23:07:00.894 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
2025-06-04 23:07:00.963 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-04 23:07:00.964 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
2025-06-04 23:07:00.980 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 4031 bytes result sent to driver
2025-06-04 23:07:00.988 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 108 ms on phamviethoa (executor driver) (1/1)
2025-06-04 23:07:00.989 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-04 23:07:00.993 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:127) finished in 0.193 s
2025-06-04 23:07:00.995 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-04 23:07:00.995 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-04 23:07:00.996 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:127, took 0.216304 s
2025-06-04 23:07:01.000 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] c.e.c.p.SparkClickstreamProcessor - Processing batch 0 with 0 records
2025-06-04 23:07:01.000 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] c.e.c.p.SparkClickstreamProcessor - Batch 0 is empty, skipping write to ClickHouse
2025-06-04 23:07:01.007 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/8857ea91-7779-4bc8-a95a-d338d593c62a/commits/0 using temp file file:/tmp/checkpoint/8857ea91-7779-4bc8-a95a-d338d593c62a/commits/.0.f1cdb0f8-1841-4246-b466-cda9cd76013d.tmp
2025-06-04 23:07:01.021 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/8857ea91-7779-4bc8-a95a-d338d593c62a/commits/.0.f1cdb0f8-1841-4246-b466-cda9cd76013d.tmp to file:/tmp/checkpoint/8857ea91-7779-4bc8-a95a-d338d593c62a/commits/0
2025-06-04 23:07:01.036 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "e66947f7-4cab-48db-bb45-84f0a6081792",
  "runId" : "a3da446c-790e-49d0-a097-01cb4257a959",
  "name" : null,
  "timestamp" : "2025-06-04T16:06:59.703Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 619,
    "commitOffsets" : 18,
    "getBatch" : 14,
    "latestOffset" : 403,
    "queryPlanning" : 226,
    "triggerExecution" : 1317,
    "walCommit" : 23
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : null,
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:07:30.005 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "e66947f7-4cab-48db-bb45-84f0a6081792",
  "runId" : "a3da446c-790e-49d0-a097-01cb4257a959",
  "name" : null,
  "timestamp" : "2025-06-04T16:07:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 5
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:08:00.004 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "e66947f7-4cab-48db-bb45-84f0a6081792",
  "runId" : "a3da446c-790e-49d0-a097-01cb4257a959",
  "name" : null,
  "timestamp" : "2025-06-04T16:08:00.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:08:30.006 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "e66947f7-4cab-48db-bb45-84f0a6081792",
  "runId" : "a3da446c-790e-49d0-a097-01cb4257a959",
  "name" : null,
  "timestamp" : "2025-06-04T16:08:30.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:09:00.005 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "e66947f7-4cab-48db-bb45-84f0a6081792",
  "runId" : "a3da446c-790e-49d0-a097-01cb4257a959",
  "name" : null,
  "timestamp" : "2025-06-04T16:09:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:09:30.005 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "e66947f7-4cab-48db-bb45-84f0a6081792",
  "runId" : "a3da446c-790e-49d0-a097-01cb4257a959",
  "name" : null,
  "timestamp" : "2025-06-04T16:09:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:10:00.005 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "e66947f7-4cab-48db-bb45-84f0a6081792",
  "runId" : "a3da446c-790e-49d0-a097-01cb4257a959",
  "name" : null,
  "timestamp" : "2025-06-04T16:10:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 5
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:10:30.004 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "e66947f7-4cab-48db-bb45-84f0a6081792",
  "runId" : "a3da446c-790e-49d0-a097-01cb4257a959",
  "name" : null,
  "timestamp" : "2025-06-04T16:10:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:11:00.004 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "e66947f7-4cab-48db-bb45-84f0a6081792",
  "runId" : "a3da446c-790e-49d0-a097-01cb4257a959",
  "name" : null,
  "timestamp" : "2025-06-04T16:11:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:11:30.004 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "e66947f7-4cab-48db-bb45-84f0a6081792",
  "runId" : "a3da446c-790e-49d0-a097-01cb4257a959",
  "name" : null,
  "timestamp" : "2025-06-04T16:11:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:12:00.004 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "e66947f7-4cab-48db-bb45-84f0a6081792",
  "runId" : "a3da446c-790e-49d0-a097-01cb4257a959",
  "name" : null,
  "timestamp" : "2025-06-04T16:12:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:12:00.056 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-04 23:12:30.004 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "e66947f7-4cab-48db-bb45-84f0a6081792",
  "runId" : "a3da446c-790e-49d0-a097-01cb4257a959",
  "name" : null,
  "timestamp" : "2025-06-04T16:12:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:13:00.006 [stream execution thread for [id = e66947f7-4cab-48db-bb45-84f0a6081792, runId = a3da446c-790e-49d0-a097-01cb4257a959] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "e66947f7-4cab-48db-bb45-84f0a6081792",
  "runId" : "a3da446c-790e-49d0-a097-01cb4257a959",
  "name" : null,
  "timestamp" : "2025-06-04T16:13:00.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:13:03.596 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-04 23:13:03.596 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-04 23:13:03.599 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@5cc175b6{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:13:03.601 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-04 23:13:03.609 [dispatcher-event-loop-15 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-04 23:13:03.615 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-04 23:13:03.615 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-04 23:13:03.617 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-04 23:13:03.618 [dispatcher-event-loop-3 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-04 23:13:03.621 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-04 23:13:03.622 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-04 23:13:03.622 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-72089895-a3ad-4b4c-b3ff-96d0f54f4db8
2025-06-04 23:13:05.817 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-04 23:13:05.904 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-04 23:13:05.947 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:13:05.947 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-04 23:13:05.948 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:13:05.948 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-04 23:13:05.959 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-04 23:13:05.965 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-04 23:13:05.965 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-04 23:13:05.992 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-04 23:13:05.993 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-04 23:13:05.993 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-04 23:13:05.993 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-04 23:13:05.993 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-04 23:13:06.121 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 46429.
2025-06-04 23:13:06.134 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-04 23:13:06.152 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-04 23:13:06.161 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-04 23:13:06.161 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-04 23:13:06.163 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-04 23:13:06.172 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-9dca5fce-54ec-4bd9-ba49-fb4bf1d5c46e
2025-06-04 23:13:06.188 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-04 23:13:06.197 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-04 23:13:06.217 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1023ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-04 23:13:06.264 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-04 23:13:06.269 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-04 23:13:06.276 [main INFO ] org.sparkproject.jetty.server.Server - Started @1083ms
2025-06-04 23:13:06.291 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@bc4d5e1{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:13:06.292 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-04 23:13:06.302 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20e6c4dc{/,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.350 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-04 23:13:06.354 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-04 23:13:06.366 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34759.
2025-06-04 23:13:06.366 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:34759
2025-06-04 23:13:06.367 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-04 23:13:06.371 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 34759, None)
2025-06-04 23:13:06.373 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:34759 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 34759, None)
2025-06-04 23:13:06.375 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 34759, None)
2025-06-04 23:13:06.375 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 34759, None)
2025-06-04 23:13:06.453 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@20e6c4dc{/,null,STOPPED,@Spark}
2025-06-04 23:13:06.453 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36417a54{/jobs,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.454 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@472a11ae{/jobs/json,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.454 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51288417{/jobs/job,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.455 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e0895f5{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.455 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fd9ebde{/stages,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.456 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ee5b2d9{/stages/json,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.456 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7899de11{/stages/stage,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.457 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bc0d349{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.457 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5292ceca{/stages/pool,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.458 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e9ef5b6{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.458 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4110765e{/storage,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.459 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62e93c3a{/storage/json,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.460 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25d93198{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.460 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f951a7f{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.461 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c777e7b{/environment,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.461 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78e22d35{/environment/json,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.461 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59f93db8{/executors,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.462 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73c9e8e8{/executors/json,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.462 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1deceb67{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.463 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64984b0f{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.467 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53ec2968{/static,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.468 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fab35b1{/,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.468 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65bdd558{/api,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.469 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22587507{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.469 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@660f0c{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.471 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4649d70a{/metrics/json,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.506 [main INFO ] c.e.c.p.SparkClickstreamProcessor - SparkClickstreamProcessor initialized with SparkSession
2025-06-04 23:13:06.506 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Starting Kafka stream processing...
2025-06-04 23:13:06.542 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-04 23:13:06.546 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-04 23:13:06.553 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33063f5b{/SQL,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.553 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@352ed70d{/SQL/json,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.554 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f1a16fe{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.554 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33634f04{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-04 23:13:06.560 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2975a9e{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:13:07.661 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Successfully connected to Kafka
2025-06-04 23:13:07.686 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Kafka DataFrame Schema: {
  "type" : "struct",
  "fields" : [ {
    "name" : "key",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "value",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "topic",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "partition",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "offset",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "timestamp",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "timestampType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
2025-06-04 23:13:07.826 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Processed DataFrame Schema: {
  "type" : "struct",
  "fields" : [ {
    "name" : "event_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "session_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "app_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "platform",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "page_url",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "geo_country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "geo_city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "traffic_source",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "traffic_medium",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "kafka_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "processing_time",
    "type" : "string",
    "nullable" : false,
    "metadata" : { }
  } ]
}
2025-06-04 23:13:07.826 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Data transformation completed, starting write stream...
2025-06-04 23:13:07.843 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-04 23:13:07.851 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e20f4e3{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-04 23:13:07.852 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f6ff62{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-04 23:13:07.852 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7af9595d{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-04 23:13:07.853 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e9f8160{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-04 23:13:07.853 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54ca9420{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:13:07.872 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/checkpoint/9cde8bf3-61b0-4f02-97a3-d9423b68f3f2 resolved to file:/tmp/checkpoint/9cde8bf3-61b0-4f02-97a3-d9423b68f3f2.
2025-06-04 23:13:07.873 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-04 23:13:07.910 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/9cde8bf3-61b0-4f02-97a3-d9423b68f3f2/metadata using temp file file:/tmp/checkpoint/9cde8bf3-61b0-4f02-97a3-d9423b68f3f2/.metadata.4f7aa7ff-8485-49f4-9a8e-1342810376c7.tmp
2025-06-04 23:13:07.951 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/9cde8bf3-61b0-4f02-97a3-d9423b68f3f2/.metadata.4f7aa7ff-8485-49f4-9a8e-1342810376c7.tmp to file:/tmp/checkpoint/9cde8bf3-61b0-4f02-97a3-d9423b68f3f2/metadata
2025-06-04 23:13:07.966 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a]. Use file:/tmp/checkpoint/9cde8bf3-61b0-4f02-97a3-d9423b68f3f2 to store the query checkpoint.
2025-06-04 23:13:07.968 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Stream processing started successfully
2025-06-04 23:13:07.972 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@3091eaf7] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@2a23fb81]
2025-06-04 23:13:07.988 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-04 23:13:07.989 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-04 23:13:07.989 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-04 23:13:07.991 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-04 23:13:08.168 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-04 23:13:08.200 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-04 23:13:08.201 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:13:08.201 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:13:08.201 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749053588200
2025-06-04 23:13:08.397 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/9cde8bf3-61b0-4f02-97a3-d9423b68f3f2/sources/0/0 using temp file file:/tmp/checkpoint/9cde8bf3-61b0-4f02-97a3-d9423b68f3f2/sources/0/.0.005696d5-bcda-4708-9314-748725c73e36.tmp
2025-06-04 23:13:08.411 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/9cde8bf3-61b0-4f02-97a3-d9423b68f3f2/sources/0/.0.005696d5-bcda-4708-9314-748725c73e36.tmp to file:/tmp/checkpoint/9cde8bf3-61b0-4f02-97a3-d9423b68f3f2/sources/0/0
2025-06-04 23:13:08.411 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events-1k":{"1":1706,"0":1604}}
2025-06-04 23:13:08.424 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/9cde8bf3-61b0-4f02-97a3-d9423b68f3f2/offsets/0 using temp file file:/tmp/checkpoint/9cde8bf3-61b0-4f02-97a3-d9423b68f3f2/offsets/.0.4599c3ed-5db3-420d-b43d-fad9120511ed.tmp
2025-06-04 23:13:08.442 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/9cde8bf3-61b0-4f02-97a3-d9423b68f3f2/offsets/.0.4599c3ed-5db3-420d-b43d-fad9120511ed.tmp to file:/tmp/checkpoint/9cde8bf3-61b0-4f02-97a3-d9423b68f3f2/offsets/0
2025-06-04 23:13:08.443 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749053588419,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-04 23:13:08.593 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749053588419
2025-06-04 23:13:08.649 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:13:08.675 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:13:08.707 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749053588419
2025-06-04 23:13:08.709 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:13:08.710 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:13:08.941 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 134.346867 ms
2025-06-04 23:13:09.069 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.713683 ms
2025-06-04 23:13:09.081 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 5.85134 ms
2025-06-04 23:13:09.129 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:129
2025-06-04 23:13:09.138 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:129) as input to shuffle 0
2025-06-04 23:13:09.141 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:129) with 1 output partitions
2025-06-04 23:13:09.142 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:129)
2025-06-04 23:13:09.142 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-04 23:13:09.143 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-04 23:13:09.145 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:129), which has no missing parents
2025-06-04 23:13:09.178 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-04 23:13:09.196 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-04 23:13:09.198 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:34759 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-04 23:13:09.200 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-04 23:13:09.208 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:129) (first 15 tasks are for partitions Vector(0))
2025-06-04 23:13:09.209 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-04 23:13:09.237 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-04 23:13:09.246 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
2025-06-04 23:13:09.309 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-04 23:13:09.310 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
2025-06-04 23:13:09.329 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 4031 bytes result sent to driver
2025-06-04 23:13:09.338 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 106 ms on phamviethoa (executor driver) (1/1)
2025-06-04 23:13:09.339 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-04 23:13:09.343 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:129) finished in 0.190 s
2025-06-04 23:13:09.345 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-04 23:13:09.346 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-04 23:13:09.347 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:129, took 0.216854 s
2025-06-04 23:13:09.351 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] c.e.c.p.SparkClickstreamProcessor - Processing batch 0 with 0 records
2025-06-04 23:13:09.423 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 14.705221 ms
2025-06-04 23:13:09.461 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/9cde8bf3-61b0-4f02-97a3-d9423b68f3f2/commits/0 using temp file file:/tmp/checkpoint/9cde8bf3-61b0-4f02-97a3-d9423b68f3f2/commits/.0.53d17cba-edbf-4448-956b-3434d12c657a.tmp
2025-06-04 23:13:09.475 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/9cde8bf3-61b0-4f02-97a3-d9423b68f3f2/commits/.0.53d17cba-edbf-4448-956b-3434d12c657a.tmp to file:/tmp/checkpoint/9cde8bf3-61b0-4f02-97a3-d9423b68f3f2/commits/0
2025-06-04 23:13:09.490 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:13:07.986Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 741,
    "commitOffsets" : 42,
    "getBatch" : 14,
    "latestOffset" : 426,
    "queryPlanning" : 227,
    "triggerExecution" : 1488,
    "walCommit" : 23
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : null,
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:13:30.005 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:13:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:14:00.004 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:14:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:14:30.004 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:14:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:15:00.005 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:15:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:15:30.004 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:15:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:15:36.714 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 71384 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-04 23:15:36.716 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-04 23:15:37.234 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-04 23:15:37.237 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-04 23:15:37.238 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-04 23:15:37.238 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-04 23:15:37.284 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-04 23:15:37.284 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 545 ms
2025-06-04 23:15:37.434 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-04 23:15:37.587 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-04 23:15:37.601 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-04 23:15:37.616 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:15:37.616 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:15:37.616 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749053737616
2025-06-04 23:15:37.706 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-04 23:15:37.708 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 23:15:37.708 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 23:15:37.708 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 23:15:37.712 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-04 23:15:37.717 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-04 23:15:37.726 [main INFO ] com.example.Application - Started Application in 1.171 seconds (JVM running for 1.469)
2025-06-04 23:15:37.864 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-04 23:15:37.864 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-04 23:15:37.865 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-04 23:15:37.963 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-04 23:15:38.013 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-04 23:15:38.045 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:15:38.045 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-04 23:15:38.045 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:15:38.045 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-04 23:15:38.053 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-04 23:15:38.058 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-04 23:15:38.058 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-04 23:15:38.076 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-04 23:15:38.076 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-04 23:15:38.076 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-04 23:15:38.076 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-04 23:15:38.076 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-04 23:15:38.174 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 44373.
2025-06-04 23:15:38.182 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-04 23:15:38.195 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-04 23:15:38.205 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-04 23:15:38.205 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-04 23:15:38.207 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-04 23:15:38.214 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-1dbc0d0b-27bb-4aa8-9263-dde4d2be07df
2025-06-04 23:15:38.230 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-04 23:15:38.236 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-04 23:15:38.244 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1987ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-04 23:15:38.276 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-04 23:15:38.280 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-04 23:15:38.286 [main INFO ] org.sparkproject.jetty.server.Server - Started @2029ms
2025-06-04 23:15:38.295 [main WARN ] org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2025-06-04 23:15:38.298 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@10897a5b{HTTP/1.1, (http/1.1)}{0.0.0.0:4041}
2025-06-04 23:15:38.299 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4041.
2025-06-04 23:15:38.305 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6516181f{/,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.333 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-04 23:15:38.335 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-04 23:15:38.343 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37813.
2025-06-04 23:15:38.343 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:37813
2025-06-04 23:15:38.344 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-04 23:15:38.347 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 37813, None)
2025-06-04 23:15:38.348 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:37813 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 37813, None)
2025-06-04 23:15:38.350 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 37813, None)
2025-06-04 23:15:38.350 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 37813, None)
2025-06-04 23:15:38.364 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6516181f{/,null,STOPPED,@Spark}
2025-06-04 23:15:38.365 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@152035eb{/jobs,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.365 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3605ab16{/jobs/json,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.366 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ba402b5{/jobs/job,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.366 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d02c00{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.367 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29dfc68f{/stages,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.367 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@333a2df2{/stages/json,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.368 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3051e476{/stages/stage,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.368 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d9ee75a{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.368 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cf6377{/stages/pool,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.369 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3151277f{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.369 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67f266bd{/storage,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.369 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@912747d{/storage/json,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.369 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cd93621{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.370 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21ba0d33{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.370 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fd63c43{/environment,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.370 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cea67b1{/environment/json,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.371 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23d23d98{/executors,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.371 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40db6136{/executors/json,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.372 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ee1ddcf{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.372 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70aa03c0{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.374 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2435c6ae{/static,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.375 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@468f2a6f{/,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.375 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ed84be9{/api,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.376 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5065bdac{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.376 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e617c0e{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.378 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e1598e5{/metrics/json,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.404 [main INFO ] c.e.c.p.SparkClickstreamProcessor - SparkClickstreamProcessor initialized with SparkSession
2025-06-04 23:15:38.404 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Starting Kafka stream processing...
2025-06-04 23:15:38.427 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-04 23:15:38.427 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-04 23:15:38.430 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-04 23:15:38.434 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e9f1a4c{/SQL,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.434 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@457b8fc3{/SQL/json,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.435 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3da61af2{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.435 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@417751d3{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-04 23:15:38.435 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d3a28b5{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:15:39.172 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Successfully connected to Kafka
2025-06-04 23:15:39.185 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Kafka DataFrame Schema: {
  "type" : "struct",
  "fields" : [ {
    "name" : "key",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "value",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "topic",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "partition",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "offset",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "timestamp",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "timestampType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
2025-06-04 23:15:39.282 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Processed DataFrame Schema: {
  "type" : "struct",
  "fields" : [ {
    "name" : "event_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "session_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "app_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "platform",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "page_url",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "geo_country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "geo_city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "traffic_source",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "traffic_medium",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "kafka_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "processing_time",
    "type" : "string",
    "nullable" : false,
    "metadata" : { }
  } ]
}
2025-06-04 23:15:39.282 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Data transformation completed, starting write stream...
2025-06-04 23:15:39.289 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-04 23:15:39.294 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2aadeb31{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-04 23:15:39.294 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23973547{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-04 23:15:39.294 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67e12e28{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-04 23:15:39.295 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@96271d8{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-04 23:15:39.295 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64404db6{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:15:39.308 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/checkpoint/cdda47b1-df80-408a-acf7-a13ba177540f resolved to file:/tmp/checkpoint/cdda47b1-df80-408a-acf7-a13ba177540f.
2025-06-04 23:15:39.309 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-04 23:15:39.338 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/cdda47b1-df80-408a-acf7-a13ba177540f/metadata using temp file file:/tmp/checkpoint/cdda47b1-df80-408a-acf7-a13ba177540f/.metadata.79bb7f29-c831-4c2d-b067-e213cfe4d67c.tmp
2025-06-04 23:15:39.368 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/cdda47b1-df80-408a-acf7-a13ba177540f/.metadata.79bb7f29-c831-4c2d-b067-e213cfe4d67c.tmp to file:/tmp/checkpoint/cdda47b1-df80-408a-acf7-a13ba177540f/metadata
2025-06-04 23:15:39.383 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1]. Use file:/tmp/checkpoint/cdda47b1-df80-408a-acf7-a13ba177540f to store the query checkpoint.
2025-06-04 23:15:39.384 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Stream processing started successfully
2025-06-04 23:15:39.386 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@408bfab2] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@1dfeee39]
2025-06-04 23:15:39.398 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-04 23:15:39.399 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-04 23:15:39.399 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-04 23:15:39.400 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-04 23:15:39.504 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-04 23:15:39.506 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-04 23:15:39.506 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:15:39.506 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:15:39.506 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749053739506
2025-06-04 23:15:39.528 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/cdda47b1-df80-408a-acf7-a13ba177540f/sources/0/0 using temp file file:/tmp/checkpoint/cdda47b1-df80-408a-acf7-a13ba177540f/sources/0/.0.2dbc20fe-c354-45b6-ac3d-f4477aaf3778.tmp
2025-06-04 23:15:39.540 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/cdda47b1-df80-408a-acf7-a13ba177540f/sources/0/.0.2dbc20fe-c354-45b6-ac3d-f4477aaf3778.tmp to file:/tmp/checkpoint/cdda47b1-df80-408a-acf7-a13ba177540f/sources/0/0
2025-06-04 23:15:39.541 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events-1k":{"1":1706,"0":1604}}
2025-06-04 23:15:39.550 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/cdda47b1-df80-408a-acf7-a13ba177540f/offsets/0 using temp file file:/tmp/checkpoint/cdda47b1-df80-408a-acf7-a13ba177540f/offsets/.0.65082470-316a-458f-82e0-2173dde6d1d7.tmp
2025-06-04 23:15:39.565 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/cdda47b1-df80-408a-acf7-a13ba177540f/offsets/.0.65082470-316a-458f-82e0-2173dde6d1d7.tmp to file:/tmp/checkpoint/cdda47b1-df80-408a-acf7-a13ba177540f/offsets/0
2025-06-04 23:15:39.566 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749053739546,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-04 23:15:39.675 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749053739546
2025-06-04 23:15:39.703 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:15:39.723 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:15:39.757 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749053739546
2025-06-04 23:15:39.759 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:15:39.759 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:15:39.937 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 114.562845 ms
2025-06-04 23:15:40.026 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.358153 ms
2025-06-04 23:15:40.034 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 4.214279 ms
2025-06-04 23:15:40.071 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:129
2025-06-04 23:15:40.078 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:129) as input to shuffle 0
2025-06-04 23:15:40.081 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:129) with 1 output partitions
2025-06-04 23:15:40.081 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:129)
2025-06-04 23:15:40.082 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-04 23:15:40.083 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-04 23:15:40.084 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:129), which has no missing parents
2025-06-04 23:15:40.107 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-04 23:15:40.118 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-04 23:15:40.119 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:37813 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-04 23:15:40.121 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-04 23:15:40.127 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:129) (first 15 tasks are for partitions Vector(0))
2025-06-04 23:15:40.128 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-04 23:15:40.147 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7363 bytes) 
2025-06-04 23:15:40.152 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
2025-06-04 23:15:40.195 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-04 23:15:40.196 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
2025-06-04 23:15:40.207 [Executor task launch worker for task 0.0 in stage 1.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 4031 bytes result sent to driver
2025-06-04 23:15:40.211 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 68 ms on phamviethoa (executor driver) (1/1)
2025-06-04 23:15:40.212 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-04 23:15:40.215 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:129) finished in 0.123 s
2025-06-04 23:15:40.216 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-04 23:15:40.216 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-04 23:15:40.217 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:129, took 0.146286 s
2025-06-04 23:15:40.221 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] c.e.c.p.SparkClickstreamProcessor - Processing batch 0 with 0 records
2025-06-04 23:15:40.265 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.286377 ms
2025-06-04 23:15:40.275 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/cdda47b1-df80-408a-acf7-a13ba177540f/commits/0 using temp file file:/tmp/checkpoint/cdda47b1-df80-408a-acf7-a13ba177540f/commits/.0.22d5b786-b8f4-4f0a-96ca-79dba65f80b9.tmp
2025-06-04 23:15:40.286 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/cdda47b1-df80-408a-acf7-a13ba177540f/commits/.0.22d5b786-b8f4-4f0a-96ca-79dba65f80b9.tmp to file:/tmp/checkpoint/cdda47b1-df80-408a-acf7-a13ba177540f/commits/0
2025-06-04 23:15:40.297 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5acc4639-7d23-4063-8886-5a1b81904b2f",
  "runId" : "590f944e-206a-4351-a3f7-9a1f2436efa1",
  "name" : null,
  "timestamp" : "2025-06-04T16:15:39.396Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 524,
    "commitOffsets" : 14,
    "getBatch" : 10,
    "latestOffset" : 145,
    "queryPlanning" : 166,
    "triggerExecution" : 890,
    "walCommit" : 19
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : null,
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:15:43.021 [http-nio-8080-exec-1 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=34b587ce-548e-41e6-af57-5e51c371b62c, event_name=page_view, event_time=2025-06-04T16:14:02.713Z, user_id=user_4q6eh9x, sessionId=session_pkfgk1s, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/}, {event_id=9055ae7f-a5ee-4c88-848e-f7d0d7b502be, event_name=tab_change, event_time=2025-06-04T16:14:03.179Z, user_id=user_4q6eh9x, sessionId=session_pkfgk1s, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, is_visible=true, time_visible=0}, {event_id=0c3847c9-8a24-4055-840a-e5422db64e41, event_name=scroll, event_time=2025-06-04T16:14:05.475Z, user_id=user_4q6eh9x, sessionId=session_pkfgk1s, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=1}, {event_id=053b9397-00cb-43a0-bb62-3e8a914793fa, event_name=click, event_time=2025-06-04T16:14:06.467Z, user_id=user_4q6eh9x, sessionId=session_pkfgk1s, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 2
                        Durable and styl, element_type=div, element_name=null, track=product_click, productId=2}, {event_id=9f39a493-89ba-479f-9780-42057d3ff5b0, event_name=scroll, event_time=2025-06-04T16:14:06.475Z, user_id=user_4q6eh9x, sessionId=session_pkfgk1s, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=100}, {event_id=692ae90a-d0ee-4f4e-87da-3a13d167672b, event_name=click, event_time=2025-06-04T16:14:07.206Z, user_id=user_4q6eh9x, sessionId=session_pkfgk1s, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 3
                        Customer favorit, element_type=div, element_name=null, track=product_click, productId=3}, {event_id=85e38fe5-a5ad-447d-9e6b-bded3ad409f7, event_name=click, event_time=2025-06-04T16:14:07.756Z, user_id=user_4q6eh9x, sessionId=session_pkfgk1s, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 1
                        High-quality ite, element_type=div, element_name=null, track=product_click, productId=1}, {event_id=daa78bf3-591d-43cf-9ccf-baadf4d4ee7c, event_name=click, event_time=2025-06-04T16:14:08.186Z, user_id=user_4q6eh9x, sessionId=session_pkfgk1s, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 4
                        Reliable and aff, element_type=div, element_name=null, track=product_click, productId=4}, {event_id=2592fcc8-02bf-4e1a-9355-15fc839622aa, event_name=click, event_time=2025-06-04T16:14:08.510Z, user_id=user_4q6eh9x, sessionId=session_pkfgk1s, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 5
                        Bestseller with , element_type=div, element_name=null, track=product_click, productId=5}, {event_id=a5e219b0-cbaf-47e8-a7fa-0c50f1c6e455, event_name=click, event_time=2025-06-04T16:14:08.871Z, user_id=user_4q6eh9x, sessionId=session_pkfgk1s, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 6
                        Sleek design and, element_type=div, element_name=null, track=product_click, productId=6}, {event_id=739ef90f-80dd-483e-bd48-110bfcfa099c, event_name=click, event_time=2025-06-04T16:14:09.209Z, user_id=user_4q6eh9x, sessionId=session_pkfgk1s, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 9
                        Top-rated with e, element_type=div, element_name=null, track=product_click, productId=9}, {event_id=aa6d216d-ab87-41a7-8334-c52449745012, event_name=click, event_time=2025-06-04T16:14:09.485Z, user_id=user_4q6eh9x, sessionId=session_pkfgk1s, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 8
                        Eco-friendly and, element_type=div, element_name=null, track=product_click, productId=8}, {event_id=8fd49906-fcd0-4f3a-9ee3-c29f6989fea9, event_name=click, event_time=2025-06-04T16:14:09.793Z, user_id=user_4q6eh9x, sessionId=session_pkfgk1s, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 7
                        Compact and effi, element_type=div, element_name=null, track=product_click, productId=7}, {event_id=3d516538-3969-48dd-b3f9-1e49a3ec3eaa, event_name=scroll, event_time=2025-06-04T16:14:10.658Z, user_id=user_4q6eh9x, sessionId=session_pkfgk1s, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=99}, {event_id=3ba1e476-df47-4d8c-8b4b-60f68fd2c004, event_name=scroll, event_time=2025-06-04T16:14:11.658Z, user_id=user_4q6eh9x, sessionId=session_pkfgk1s, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=23}, {event_id=cefd812a-2ed1-445d-9bce-402e3397c5b9, event_name=scroll, event_time=2025-06-04T16:14:12.659Z, user_id=user_4q6eh9x, sessionId=session_pkfgk1s, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=76}, {event_id=c2612ee3-f425-44f1-977a-57a1fd7eeecd, event_name=scroll, event_time=2025-06-04T16:14:14.659Z, user_id=user_4q6eh9x, sessionId=session_pkfgk1s, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=0}, {event_id=d1ce02e5-f85b-436e-bc28-a3c0cfd64108, event_name=scroll, event_time=2025-06-04T16:15:42.978Z, user_id=user_4q6eh9x, sessionId=session_pkfgk1s, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=3}]}
2025-06-04 23:15:43.021 [http-nio-8080-exec-1 INFO ] c.e.controller.ClickstreamController - Received 18 events
2025-06-04 23:15:43.027 [http-nio-8080-exec-1 INFO ] o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 1000
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-06-04 23:15:43.036 [http-nio-8080-exec-1 INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-04 23:15:43.039 [http-nio-8080-exec-1 INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-06-04 23:15:43.048 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:15:43.048 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:15:43.048 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749053743048
2025-06-04 23:15:43.052 [kafka-producer-network-thread | producer-1 INFO ] org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-04 23:15:43.053 [kafka-producer-network-thread | producer-1 INFO ] o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 3003 with epoch 0
2025-06-04 23:15:43.064 [http-nio-8080-exec-1 INFO ] c.e.controller.ClickstreamController - Successfully processed 18 events
2025-06-04 23:16:00.004 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5acc4639-7d23-4063-8886-5a1b81904b2f",
  "runId" : "590f944e-206a-4351-a3f7-9a1f2436efa1",
  "name" : null,
  "timestamp" : "2025-06-04T16:16:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:16:00.005 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:16:00.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:16:30.004 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5acc4639-7d23-4063-8886-5a1b81904b2f",
  "runId" : "590f944e-206a-4351-a3f7-9a1f2436efa1",
  "name" : null,
  "timestamp" : "2025-06-04T16:16:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:16:30.004 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:16:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:17:00.004 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5acc4639-7d23-4063-8886-5a1b81904b2f",
  "runId" : "590f944e-206a-4351-a3f7-9a1f2436efa1",
  "name" : null,
  "timestamp" : "2025-06-04T16:17:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:17:00.004 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:17:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:17:30.003 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:17:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:17:30.004 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5acc4639-7d23-4063-8886-5a1b81904b2f",
  "runId" : "590f944e-206a-4351-a3f7-9a1f2436efa1",
  "name" : null,
  "timestamp" : "2025-06-04T16:17:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:18:00.004 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5acc4639-7d23-4063-8886-5a1b81904b2f",
  "runId" : "590f944e-206a-4351-a3f7-9a1f2436efa1",
  "name" : null,
  "timestamp" : "2025-06-04T16:18:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:18:00.004 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:18:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 2,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:18:08.365 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-04 23:18:30.004 [stream execution thread for [id = 5acc4639-7d23-4063-8886-5a1b81904b2f, runId = 590f944e-206a-4351-a3f7-9a1f2436efa1] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "5acc4639-7d23-4063-8886-5a1b81904b2f",
  "runId" : "590f944e-206a-4351-a3f7-9a1f2436efa1",
  "name" : null,
  "timestamp" : "2025-06-04T16:18:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:18:30.004 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:18:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:18:44.256 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-04 23:18:44.256 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-04 23:18:44.261 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@10897a5b{HTTP/1.1, (http/1.1)}{0.0.0.0:4041}
2025-06-04 23:18:44.263 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4041
2025-06-04 23:18:44.271 [dispatcher-event-loop-13 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-04 23:18:44.278 [SpringApplicationShutdownHook INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-06-04 23:18:44.279 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-04 23:18:44.279 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-04 23:18:44.281 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 23:18:44.281 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 23:18:44.281 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-04 23:18:44.281 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 23:18:44.281 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-1 unregistered
2025-06-04 23:18:44.282 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-04 23:18:46.313 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 73931 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-04 23:18:46.316 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-04 23:18:46.896 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-04 23:18:46.899 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-04 23:18:46.900 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-04 23:18:46.900 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-04 23:18:46.957 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-04 23:18:46.957 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 621 ms
2025-06-04 23:18:47.126 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-04 23:18:47.301 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-04 23:18:47.316 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-04 23:18:47.332 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:18:47.333 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:18:47.333 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749053927332
2025-06-04 23:18:47.422 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-04 23:18:47.424 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 23:18:47.424 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 23:18:47.424 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 23:18:47.429 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-04 23:18:47.434 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-04 23:18:47.443 [main INFO ] com.example.Application - Started Application in 1.299 seconds (JVM running for 1.598)
2025-06-04 23:18:47.645 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-04 23:18:47.696 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-04 23:18:47.728 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:18:47.729 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-04 23:18:47.729 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:18:47.729 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-04 23:18:47.738 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-04 23:18:47.744 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-04 23:18:47.744 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-04 23:18:47.765 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-04 23:18:47.765 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-04 23:18:47.766 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-04 23:18:47.766 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-04 23:18:47.766 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-04 23:18:47.856 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 40267.
2025-06-04 23:18:47.865 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-04 23:18:47.879 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-04 23:18:47.885 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-04 23:18:47.886 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-04 23:18:47.887 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-04 23:18:47.892 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-663a1c9e-2f2d-4768-96e2-d85494a89424
2025-06-04 23:18:47.920 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-04 23:18:47.926 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-04 23:18:47.936 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @2091ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-04 23:18:47.978 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-04 23:18:47.978 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-04 23:18:47.979 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-04 23:18:47.987 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-04 23:18:47.991 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-04 23:18:47.998 [main INFO ] org.sparkproject.jetty.server.Server - Started @2153ms
2025-06-04 23:18:48.009 [main WARN ] org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2025-06-04 23:18:48.013 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@665136b9{HTTP/1.1, (http/1.1)}{0.0.0.0:4041}
2025-06-04 23:18:48.013 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4041.
2025-06-04 23:18:48.022 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7744195{/,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.053 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-04 23:18:48.057 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-04 23:18:48.066 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36223.
2025-06-04 23:18:48.066 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:36223
2025-06-04 23:18:48.067 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-04 23:18:48.070 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 36223, None)
2025-06-04 23:18:48.072 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:36223 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 36223, None)
2025-06-04 23:18:48.073 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 36223, None)
2025-06-04 23:18:48.074 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 36223, None)
2025-06-04 23:18:48.089 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7744195{/,null,STOPPED,@Spark}
2025-06-04 23:18:48.089 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76e2a621{/jobs,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.090 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e7517aa{/jobs/json,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.090 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30cb223b{/jobs/job,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.090 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50e24ea4{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.091 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22172b00{/stages,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.091 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4205d5d0{/stages/json,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.091 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6719f206{/stages/stage,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.092 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@344a065a{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.092 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1870b9b8{/stages/pool,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.092 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9825465{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.093 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2befb16f{/storage,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.093 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2aeb7c4c{/storage/json,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.093 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31c0c7e5{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.094 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78ea700f{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.094 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b1137b0{/environment,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.094 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@afb7b03{/environment/json,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.095 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d483ebe{/executors,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.095 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d98364c{/executors/json,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.096 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1fd35a92{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.096 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ced0537{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.099 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b630d4b{/static,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.099 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2431050d{/,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.100 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@522f0bb8{/api,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.100 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e26040f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.101 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8bde368{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.103 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67a3dd86{/metrics/json,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.132 [main INFO ] c.e.c.p.SparkClickstreamProcessor - SparkClickstreamProcessor initialized with SparkSession
2025-06-04 23:18:48.132 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Starting Kafka stream processing...
2025-06-04 23:18:48.159 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-04 23:18:48.159 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-04 23:18:48.163 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-04 23:18:48.168 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35840ecc{/SQL,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.168 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65a2e14e{/SQL/json,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.169 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11ca8f71{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.169 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c8689e{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-04 23:18:48.170 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e27bb89{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:18:49.027 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Successfully connected to Kafka
2025-06-04 23:18:49.044 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Kafka DataFrame Schema: {
  "type" : "struct",
  "fields" : [ {
    "name" : "key",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "value",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "topic",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "partition",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "offset",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "timestamp",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "timestampType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
2025-06-04 23:18:49.151 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Processed DataFrame Schema: {
  "type" : "struct",
  "fields" : [ {
    "name" : "event_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "session_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "app_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "platform",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "page_url",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "geo_country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "geo_city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "traffic_source",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "traffic_medium",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "kafka_timestamp",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "processing_time",
    "type" : "string",
    "nullable" : false,
    "metadata" : { }
  } ]
}
2025-06-04 23:18:49.151 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Data transformation completed, starting write stream...
2025-06-04 23:18:49.159 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-04 23:18:49.165 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e609b42{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-04 23:18:49.165 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24673720{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-04 23:18:49.166 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6558aa{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-04 23:18:49.166 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7fec6c2f{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-04 23:18:49.166 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@530aa75c{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:18:49.181 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d resolved to file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d.
2025-06-04 23:18:49.181 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-04 23:18:49.215 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d/metadata using temp file file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d/.metadata.d3d50540-bde4-4543-8f2e-f50e464c26c8.tmp
2025-06-04 23:18:49.249 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d/.metadata.d3d50540-bde4-4543-8f2e-f50e464c26c8.tmp to file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d/metadata
2025-06-04 23:18:49.265 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5]. Use file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d to store the query checkpoint.
2025-06-04 23:18:49.267 [main INFO ] c.e.c.p.SparkClickstreamProcessor - Stream processing started successfully
2025-06-04 23:18:49.270 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@545ed6e2] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@31ebd8a3]
2025-06-04 23:18:49.284 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-04 23:18:49.285 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-04 23:18:49.286 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-04 23:18:49.287 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-04 23:18:49.400 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-04 23:18:49.402 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-04 23:18:49.402 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:18:49.402 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:18:49.402 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749053929402
2025-06-04 23:18:49.425 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d/sources/0/0 using temp file file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d/sources/0/.0.cc26bf80-91d2-4a2b-b2fa-ca95180d72ae.tmp
2025-06-04 23:18:49.440 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d/sources/0/.0.cc26bf80-91d2-4a2b-b2fa-ca95180d72ae.tmp to file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d/sources/0/0
2025-06-04 23:18:49.441 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-04 23:18:49.452 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d/offsets/0 using temp file file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d/offsets/.0.8f126490-c1c9-423e-a1d4-c74d32b47c83.tmp
2025-06-04 23:18:49.471 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d/offsets/.0.8f126490-c1c9-423e-a1d4-c74d32b47c83.tmp to file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d/offsets/0
2025-06-04 23:18:49.472 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749053929447,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-04 23:18:49.592 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749053929447
2025-06-04 23:18:49.622 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:18:49.643 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:18:49.681 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749053929447
2025-06-04 23:18:49.683 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:18:49.684 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:18:49.874 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 113.79003 ms
2025-06-04 23:18:49.967 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.831427 ms
2025-06-04 23:18:49.976 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 4.388128 ms
2025-06-04 23:18:50.014 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:129
2025-06-04 23:18:50.021 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:129) as input to shuffle 0
2025-06-04 23:18:50.024 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:129) with 1 output partitions
2025-06-04 23:18:50.024 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:129)
2025-06-04 23:18:50.024 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-04 23:18:50.024 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
2025-06-04 23:18:50.026 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:129), which has no missing parents
2025-06-04 23:18:50.074 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 34.7 KiB, free 9.2 GiB)
2025-06-04 23:18:50.085 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 15.0 KiB, free 9.2 GiB)
2025-06-04 23:18:50.087 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:36223 (size: 15.0 KiB, free: 9.2 GiB)
2025-06-04 23:18:50.088 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-04 23:18:50.093 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:129) (first 15 tasks are for partitions Vector(0, 1))
2025-06-04 23:18:50.093 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-04 23:18:50.112 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8303 bytes) 
2025-06-04 23:18:50.113 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8303 bytes) 
2025-06-04 23:18:50.116 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-04 23:18:50.117 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-04 23:18:50.179 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.5521 ms
2025-06-04 23:18:50.201 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 5.944638 ms
2025-06-04 23:18:50.213 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.805839 ms
2025-06-04 23:18:50.221 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-04 23:18:50.227 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-04 23:18:50.236 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-04 23:18:50.236 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-04 23:18:50.258 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:18:50.258 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:18:50.258 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749053930258
2025-06-04 23:18:50.259 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:18:50.259 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:18:50.259 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749053930258
2025-06-04 23:18:50.259 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-1, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Assigned to partition(s): clickstream-events-0
2025-06-04 23:18:50.259 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Assigned to partition(s): clickstream-events-1
2025-06-04 23:18:50.264 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-04 23:18:50.264 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-1, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-04 23:18:50.269 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-1, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-04 23:18:50.269 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-04 23:18:50.313 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-04 23:18:50.313 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-1, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-04 23:18:50.822 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:18:50.822 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-1, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:18:50.822 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-1, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-04 23:18:50.822 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-04 23:18:50.823 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=42, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:18:50.823 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-1, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=48, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:18:50.886 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 2342 bytes result sent to driver
2025-06-04 23:18:50.886 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2342 bytes result sent to driver
2025-06-04 23:18:50.891 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 777 ms on phamviethoa (executor driver) (1/2)
2025-06-04 23:18:50.891 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 784 ms on phamviethoa (executor driver) (2/2)
2025-06-04 23:18:50.892 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-04 23:18:50.895 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (start at SparkClickstreamProcessor.java:129) finished in 0.864 s
2025-06-04 23:18:50.895 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
2025-06-04 23:18:50.895 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - running: Set()
2025-06-04 23:18:50.895 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
2025-06-04 23:18:50.895 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - failed: Set()
2025-06-04 23:18:50.896 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:129), which has no missing parents
2025-06-04 23:18:50.899 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-04 23:18:50.901 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-04 23:18:50.901 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:36223 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-04 23:18:50.902 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-04 23:18:50.902 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:129) (first 15 tasks are for partitions Vector(0))
2025-06-04 23:18:50.902 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-04 23:18:50.905 [dispatcher-event-loop-12 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
2025-06-04 23:18:50.905 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-04 23:18:50.925 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-04 23:18:50.926 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
2025-06-04 23:18:50.933 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 4038 bytes result sent to driver
2025-06-04 23:18:50.934 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 30 ms on phamviethoa (executor driver) (1/1)
2025-06-04 23:18:50.934 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-04 23:18:50.935 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:129) finished in 0.037 s
2025-06-04 23:18:50.936 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-04 23:18:50.936 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-04 23:18:50.937 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:129, took 0.922666 s
2025-06-04 23:18:50.940 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] c.e.c.p.SparkClickstreamProcessor - Processing batch 0 with 90 records
2025-06-04 23:18:50.991 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.411693 ms
2025-06-04 23:18:50.995 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:129
2025-06-04 23:18:50.996 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 1 (start at SparkClickstreamProcessor.java:129) with 1 output partitions
2025-06-04 23:18:50.996 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (start at SparkClickstreamProcessor.java:129)
2025-06-04 23:18:50.996 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-04 23:18:50.996 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-04 23:18:50.997 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[11] at start at SparkClickstreamProcessor.java:129), which has no missing parents
2025-06-04 23:18:50.999 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 35.0 KiB, free 9.2 GiB)
2025-06-04 23:18:51.000 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.5 KiB, free 9.2 GiB)
2025-06-04 23:18:51.001 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on phamviethoa:36223 (size: 14.5 KiB, free: 9.2 GiB)
2025-06-04 23:18:51.001 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1535
2025-06-04 23:18:51.001 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at start at SparkClickstreamProcessor.java:129) (first 15 tasks are for partitions Vector(0))
2025-06-04 23:18:51.001 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks resource profile 0
2025-06-04 23:18:51.002 [dispatcher-event-loop-15 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 3) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8314 bytes) 
2025-06-04 23:18:51.002 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 3)
2025-06-04 23:18:51.014 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-04 23:18:51.017 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-04 23:18:51.519 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:18:51.519 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-04 23:18:51.520 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=42, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:18:51.526 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 3). 2124 bytes result sent to driver
2025-06-04 23:18:51.527 [task-result-getter-3 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 3) in 525 ms on phamviethoa (executor driver) (1/1)
2025-06-04 23:18:51.527 [task-result-getter-3 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2025-06-04 23:18:51.527 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 2 (start at SparkClickstreamProcessor.java:129) finished in 0.530 s
2025-06-04 23:18:51.527 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-04 23:18:51.527 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage finished
2025-06-04 23:18:51.527 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.spark.scheduler.DAGScheduler - Job 1 finished: start at SparkClickstreamProcessor.java:129, took 0.532003 s
2025-06-04 23:18:51.543 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.081657 ms
2025-06-04 23:18:51.555 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d/commits/0 using temp file file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d/commits/.0.68c4b544-1166-4c54-9ec2-23682fa273aa.tmp
2025-06-04 23:18:51.569 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d/commits/.0.68c4b544-1166-4c54-9ec2-23682fa273aa.tmp to file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d/commits/0
2025-06-04 23:18:51.580 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "61820dfc-92a5-4fbe-933e-8fa480b1575f",
  "runId" : "a68bb1f6-b907-4299-b847-831175229ef5",
  "name" : null,
  "timestamp" : "2025-06-04T16:18:49.282Z",
  "batchId" : 0,
  "numInputRows" : 111,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 48.53519895059029,
  "durationMs" : {
    "addBatch" : 1881,
    "commitOffsets" : 18,
    "getBatch" : 12,
    "latestOffset" : 158,
    "queryPlanning" : 179,
    "triggerExecution" : 2287,
    "walCommit" : 24
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : null,
    "endOffset" : {
      "clickstream-events" : {
        "1" : 42,
        "0" : 48
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 42,
        "0" : 48
      }
    },
    "numInputRows" : 111,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 48.53519895059029,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:19:00.004 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:19:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:19:02.948 [http-nio-8080-exec-1 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=30675fac-856b-4f93-80bf-bca8ec259926, event_name=scroll, event_time=2025-06-04T15:31:07.497Z, user_id=user_4q6eh9x, sessionId=session_hyhabjd, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=25}, {event_id=017c93ee-b7f4-4325-962e-c5b18027a36d, event_name=scroll, event_time=2025-06-04T15:31:08.498Z, user_id=user_4q6eh9x, sessionId=session_hyhabjd, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=0}, {event_id=9f02775c-2b8f-4e98-8b60-996842116c60, event_name=session_renew, event_time=2025-06-04T16:02:22.638Z, user_id=user_4q6eh9x, sessionId=session_0nuv6es, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/}, {event_id=7e65de07-8d38-41c5-9e63-6e8c25dee3a9, event_name=scroll, event_time=2025-06-04T16:19:02.901Z, user_id=user_4q6eh9x, sessionId=session_gkx5eeg, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=1}]}
2025-06-04 23:19:02.948 [http-nio-8080-exec-1 INFO ] c.e.controller.ClickstreamController - Received 4 events
2025-06-04 23:19:02.953 [http-nio-8080-exec-1 INFO ] o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 1000
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-06-04 23:19:02.954 [http-nio-8080-exec-1 INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-04 23:19:02.957 [http-nio-8080-exec-1 INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-06-04 23:19:02.962 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:19:02.962 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:19:02.962 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749053942962
2025-06-04 23:19:02.966 [kafka-producer-network-thread | producer-1 INFO ] org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-04 23:19:02.967 [kafka-producer-network-thread | producer-1 INFO ] o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 3004 with epoch 0
2025-06-04 23:19:02.973 [http-nio-8080-exec-1 INFO ] c.e.controller.ClickstreamController - Successfully processed 4 events
2025-06-04 23:19:11.134 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=87d4c8b0-7cc9-4fc6-a9e5-dbd414428309, event_name=scroll, event_time=2025-06-04T16:19:03.902Z, user_id=user_4q6eh9x, sessionId=session_gkx5eeg, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=43}, {event_id=4ce094e0-04bc-4e8a-8da7-cc4e5f624bca, event_name=scroll, event_time=2025-06-04T16:19:10.131Z, user_id=user_4q6eh9x, sessionId=session_gkx5eeg, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=99}, {event_id=d28b15f2-8f47-4467-a54d-2680f95b7b62, event_name=scroll, event_time=2025-06-04T16:19:11.131Z, user_id=user_4q6eh9x, sessionId=session_gkx5eeg, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=69}]}
2025-06-04 23:19:11.134 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-04 23:19:11.134 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-04 23:19:30.005 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:19:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:19:30.006 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d/offsets/1 using temp file file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d/offsets/.1.e10389d4-3610-4d71-9015-0195d5a7956e.tmp
2025-06-04 23:19:30.016 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d/offsets/.1.e10389d4-3610-4d71-9015-0195d5a7956e.tmp to file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d/offsets/1
2025-06-04 23:19:30.016 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1749053970002,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-04 23:19:30.027 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749053970002
2025-06-04 23:19:30.028 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:19:30.029 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:19:30.036 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749053970002
2025-06-04 23:19:30.037 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:19:30.038 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:19:30.051 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.614976 ms
2025-06-04 23:19:30.074 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:129
2025-06-04 23:19:30.074 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 18 (start at SparkClickstreamProcessor.java:129) as input to shuffle 1
2025-06-04 23:19:30.074 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 2 (start at SparkClickstreamProcessor.java:129) with 1 output partitions
2025-06-04 23:19:30.074 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 4 (start at SparkClickstreamProcessor.java:129)
2025-06-04 23:19:30.074 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 3)
2025-06-04 23:19:30.074 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 3)
2025-06-04 23:19:30.075 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[18] at start at SparkClickstreamProcessor.java:129), which has no missing parents
2025-06-04 23:19:30.077 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 34.7 KiB, free 9.2 GiB)
2025-06-04 23:19:30.079 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 15.1 KiB, free 9.2 GiB)
2025-06-04 23:19:30.079 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on phamviethoa:36223 (size: 15.1 KiB, free: 9.2 GiB)
2025-06-04 23:19:30.079 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1535
2025-06-04 23:19:30.080 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[18] at start at SparkClickstreamProcessor.java:129) (first 15 tasks are for partitions Vector(0, 1))
2025-06-04 23:19:30.080 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 2 tasks resource profile 0
2025-06-04 23:19:30.080 [dispatcher-event-loop-5 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 4) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8303 bytes) 
2025-06-04 23:19:30.080 [dispatcher-event-loop-5 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 5) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8303 bytes) 
2025-06-04 23:19:30.081 [Executor task launch worker for task 1.0 in stage 3.0 (TID 5) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 5)
2025-06-04 23:19:30.081 [Executor task launch worker for task 0.0 in stage 3.0 (TID 4) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 4)
2025-06-04 23:19:30.088 [Executor task launch worker for task 0.0 in stage 3.0 (TID 4) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Seeking to offset 42 for partition clickstream-events-1
2025-06-04 23:19:30.088 [Executor task launch worker for task 1.0 in stage 3.0 (TID 5) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-1, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Seeking to offset 48 for partition clickstream-events-0
2025-06-04 23:19:30.090 [Executor task launch worker for task 1.0 in stage 3.0 (TID 5) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-1, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-04 23:19:30.090 [Executor task launch worker for task 0.0 in stage 3.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-04 23:19:30.592 [Executor task launch worker for task 1.0 in stage 3.0 (TID 5) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-1, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:19:30.592 [Executor task launch worker for task 0.0 in stage 3.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:19:30.592 [Executor task launch worker for task 0.0 in stage 3.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-04 23:19:30.592 [Executor task launch worker for task 1.0 in stage 3.0 (TID 5) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-1, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-04 23:19:30.592 [Executor task launch worker for task 0.0 in stage 3.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:19:30.593 [Executor task launch worker for task 1.0 in stage 3.0 (TID 5) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-1, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:19:30.596 [Executor task launch worker for task 0.0 in stage 3.0 (TID 4) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 4). 2299 bytes result sent to driver
2025-06-04 23:19:30.596 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 4) in 516 ms on phamviethoa (executor driver) (1/2)
2025-06-04 23:19:30.597 [Executor task launch worker for task 1.0 in stage 3.0 (TID 5) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 5). 2299 bytes result sent to driver
2025-06-04 23:19:30.597 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 5) in 517 ms on phamviethoa (executor driver) (2/2)
2025-06-04 23:19:30.597 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2025-06-04 23:19:30.598 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (start at SparkClickstreamProcessor.java:129) finished in 0.523 s
2025-06-04 23:19:30.598 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
2025-06-04 23:19:30.598 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - running: Set()
2025-06-04 23:19:30.598 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 4)
2025-06-04 23:19:30.598 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - failed: Set()
2025-06-04 23:19:30.598 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 4 (MapPartitionsRDD[21] at start at SparkClickstreamProcessor.java:129), which has no missing parents
2025-06-04 23:19:30.600 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-04 23:19:30.601 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-04 23:19:30.602 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on phamviethoa:36223 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-04 23:19:30.602 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1535
2025-06-04 23:19:30.602 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[21] at start at SparkClickstreamProcessor.java:129) (first 15 tasks are for partitions Vector(0))
2025-06-04 23:19:30.602 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks resource profile 0
2025-06-04 23:19:30.603 [dispatcher-event-loop-11 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 6) (phamviethoa, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
2025-06-04 23:19:30.603 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 6)
2025-06-04 23:19:30.606 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-04 23:19:30.606 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
2025-06-04 23:19:30.608 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 6). 3995 bytes result sent to driver
2025-06-04 23:19:30.609 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 6) in 6 ms on phamviethoa (executor driver) (1/1)
2025-06-04 23:19:30.609 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2025-06-04 23:19:30.609 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 4 (start at SparkClickstreamProcessor.java:129) finished in 0.010 s
2025-06-04 23:19:30.609 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-04 23:19:30.609 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 4: Stage finished
2025-06-04 23:19:30.609 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.spark.scheduler.DAGScheduler - Job 2 finished: start at SparkClickstreamProcessor.java:129, took 0.535657 s
2025-06-04 23:19:30.610 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] c.e.c.p.SparkClickstreamProcessor - Processing batch 1 with 7 records
2025-06-04 23:19:30.635 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:129
2025-06-04 23:19:30.636 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 3 (start at SparkClickstreamProcessor.java:129) with 1 output partitions
2025-06-04 23:19:30.636 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (start at SparkClickstreamProcessor.java:129)
2025-06-04 23:19:30.636 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-04 23:19:30.636 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-04 23:19:30.636 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[23] at start at SparkClickstreamProcessor.java:129), which has no missing parents
2025-06-04 23:19:30.638 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 35.0 KiB, free 9.2 GiB)
2025-06-04 23:19:30.640 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 14.6 KiB, free 9.2 GiB)
2025-06-04 23:19:30.640 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on phamviethoa:36223 (size: 14.6 KiB, free: 9.2 GiB)
2025-06-04 23:19:30.641 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1535
2025-06-04 23:19:30.641 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[23] at start at SparkClickstreamProcessor.java:129) (first 15 tasks are for partitions Vector(0))
2025-06-04 23:19:30.641 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks resource profile 0
2025-06-04 23:19:30.642 [dispatcher-event-loop-13 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 7) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8314 bytes) 
2025-06-04 23:19:30.642 [Executor task launch worker for task 0.0 in stage 5.0 (TID 7) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 7)
2025-06-04 23:19:30.650 [Executor task launch worker for task 0.0 in stage 5.0 (TID 7) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Seeking to offset 42 for partition clickstream-events-1
2025-06-04 23:19:30.651 [Executor task launch worker for task 0.0 in stage 5.0 (TID 7) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-04 23:19:31.153 [Executor task launch worker for task 0.0 in stage 5.0 (TID 7) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:19:31.153 [Executor task launch worker for task 0.0 in stage 5.0 (TID 7) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-04 23:19:31.153 [Executor task launch worker for task 0.0 in stage 5.0 (TID 7) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:19:31.156 [Executor task launch worker for task 0.0 in stage 5.0 (TID 7) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 7). 1957 bytes result sent to driver
2025-06-04 23:19:31.156 [task-result-getter-3 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 7) in 515 ms on phamviethoa (executor driver) (1/1)
2025-06-04 23:19:31.156 [task-result-getter-3 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2025-06-04 23:19:31.157 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 5 (start at SparkClickstreamProcessor.java:129) finished in 0.519 s
2025-06-04 23:19:31.157 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-04 23:19:31.157 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 5: Stage finished
2025-06-04 23:19:31.157 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.spark.scheduler.DAGScheduler - Job 3 finished: start at SparkClickstreamProcessor.java:129, took 0.521256 s
2025-06-04 23:19:31.160 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:129
2025-06-04 23:19:31.160 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 4 (start at SparkClickstreamProcessor.java:129) with 1 output partitions
2025-06-04 23:19:31.160 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (start at SparkClickstreamProcessor.java:129)
2025-06-04 23:19:31.160 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-04 23:19:31.160 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-04 23:19:31.161 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (MapPartitionsRDD[23] at start at SparkClickstreamProcessor.java:129), which has no missing parents
2025-06-04 23:19:31.162 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 35.0 KiB, free 9.2 GiB)
2025-06-04 23:19:31.164 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.6 KiB, free 9.2 GiB)
2025-06-04 23:19:31.164 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on phamviethoa:36223 (size: 14.6 KiB, free: 9.2 GiB)
2025-06-04 23:19:31.164 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1535
2025-06-04 23:19:31.164 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[23] at start at SparkClickstreamProcessor.java:129) (first 15 tasks are for partitions Vector(1))
2025-06-04 23:19:31.164 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks resource profile 0
2025-06-04 23:19:31.165 [dispatcher-event-loop-0 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 8) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8314 bytes) 
2025-06-04 23:19:31.165 [Executor task launch worker for task 0.0 in stage 6.0 (TID 8) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 8)
2025-06-04 23:19:31.172 [Executor task launch worker for task 0.0 in stage 6.0 (TID 8) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-1, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Seeking to offset 48 for partition clickstream-events-0
2025-06-04 23:19:31.174 [Executor task launch worker for task 0.0 in stage 6.0 (TID 8) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-1, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-04 23:19:31.675 [Executor task launch worker for task 0.0 in stage 6.0 (TID 8) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-1, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:19:31.675 [Executor task launch worker for task 0.0 in stage 6.0 (TID 8) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-1, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-04 23:19:31.676 [Executor task launch worker for task 0.0 in stage 6.0 (TID 8) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-1, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:19:31.678 [Executor task launch worker for task 0.0 in stage 6.0 (TID 8) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 8). 1968 bytes result sent to driver
2025-06-04 23:19:31.679 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 8) in 514 ms on phamviethoa (executor driver) (1/1)
2025-06-04 23:19:31.679 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
2025-06-04 23:19:31.679 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 6 (start at SparkClickstreamProcessor.java:129) finished in 0.518 s
2025-06-04 23:19:31.679 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-04 23:19:31.679 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 6: Stage finished
2025-06-04 23:19:31.679 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.spark.scheduler.DAGScheduler - Job 4 finished: start at SparkClickstreamProcessor.java:129, took 0.519105 s
2025-06-04 23:19:31.685 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d/commits/1 using temp file file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d/commits/.1.17345fea-c207-4a14-98bf-23ac43bd9683.tmp
2025-06-04 23:19:31.695 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d/commits/.1.17345fea-c207-4a14-98bf-23ac43bd9683.tmp to file:/tmp/checkpoint/e89cf3ff-ebae-45ef-85a6-ab40f113b60d/commits/1
2025-06-04 23:19:31.696 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "61820dfc-92a5-4fbe-933e-8fa480b1575f",
  "runId" : "a68bb1f6-b907-4299-b847-831175229ef5",
  "name" : null,
  "timestamp" : "2025-06-04T16:19:30.000Z",
  "batchId" : 1,
  "numInputRows" : 14,
  "inputRowsPerSecond" : 0.4666666666666667,
  "processedRowsPerSecond" : 8.259587020648967,
  "durationMs" : {
    "addBatch" : 1651,
    "commitOffsets" : 12,
    "getBatch" : 1,
    "latestOffset" : 2,
    "queryPlanning" : 12,
    "triggerExecution" : 1695,
    "walCommit" : 15
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 42,
        "0" : 48
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "numInputRows" : 14,
    "inputRowsPerSecond" : 0.4666666666666667,
    "processedRowsPerSecond" : 8.259587020648967,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:20:00.004 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "61820dfc-92a5-4fbe-933e-8fa480b1575f",
  "runId" : "a68bb1f6-b907-4299-b847-831175229ef5",
  "name" : null,
  "timestamp" : "2025-06-04T16:20:00.000Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:20:00.004 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:20:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:20:30.004 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "61820dfc-92a5-4fbe-933e-8fa480b1575f",
  "runId" : "a68bb1f6-b907-4299-b847-831175229ef5",
  "name" : null,
  "timestamp" : "2025-06-04T16:20:30.000Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:20:30.004 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:20:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:21:00.005 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "61820dfc-92a5-4fbe-933e-8fa480b1575f",
  "runId" : "a68bb1f6-b907-4299-b847-831175229ef5",
  "name" : null,
  "timestamp" : "2025-06-04T16:21:00.000Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:21:00.006 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:21:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 5
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:21:30.004 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:21:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:21:30.004 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "61820dfc-92a5-4fbe-933e-8fa480b1575f",
  "runId" : "a68bb1f6-b907-4299-b847-831175229ef5",
  "name" : null,
  "timestamp" : "2025-06-04T16:21:30.000Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:22:00.004 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:22:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:22:00.005 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "61820dfc-92a5-4fbe-933e-8fa480b1575f",
  "runId" : "a68bb1f6-b907-4299-b847-831175229ef5",
  "name" : null,
  "timestamp" : "2025-06-04T16:22:00.000Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:22:30.004 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:22:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:22:30.004 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "61820dfc-92a5-4fbe-933e-8fa480b1575f",
  "runId" : "a68bb1f6-b907-4299-b847-831175229ef5",
  "name" : null,
  "timestamp" : "2025-06-04T16:22:30.000Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:23:00.005 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "61820dfc-92a5-4fbe-933e-8fa480b1575f",
  "runId" : "a68bb1f6-b907-4299-b847-831175229ef5",
  "name" : null,
  "timestamp" : "2025-06-04T16:23:00.000Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:23:00.005 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:23:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:23:30.003 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "61820dfc-92a5-4fbe-933e-8fa480b1575f",
  "runId" : "a68bb1f6-b907-4299-b847-831175229ef5",
  "name" : null,
  "timestamp" : "2025-06-04T16:23:30.000Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:23:30.004 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:23:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:23:49.425 [kafka-admin-client-thread | adminclient-2 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-2] Node -1 disconnected.
2025-06-04 23:24:00.003 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "61820dfc-92a5-4fbe-933e-8fa480b1575f",
  "runId" : "a68bb1f6-b907-4299-b847-831175229ef5",
  "name" : null,
  "timestamp" : "2025-06-04T16:24:00.000Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:24:00.004 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:24:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:24:30.004 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:24:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:24:30.004 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "61820dfc-92a5-4fbe-933e-8fa480b1575f",
  "runId" : "a68bb1f6-b907-4299-b847-831175229ef5",
  "name" : null,
  "timestamp" : "2025-06-04T16:24:30.000Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:24:50.192 [commons-pool-evictor INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-1, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-04 23:24:50.192 [commons-pool-evictor INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-1, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-04 23:24:50.197 [commons-pool-evictor INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 23:24:50.197 [commons-pool-evictor INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 23:24:50.197 [commons-pool-evictor INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-04 23:24:50.197 [commons-pool-evictor INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 23:24:50.200 [commons-pool-evictor INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-1 unregistered
2025-06-04 23:24:50.200 [commons-pool-evictor INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-04 23:24:50.200 [commons-pool-evictor INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2, groupId=spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-04 23:24:50.202 [commons-pool-evictor INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 23:24:50.202 [commons-pool-evictor INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 23:24:50.202 [commons-pool-evictor INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-04 23:24:50.202 [commons-pool-evictor INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 23:24:50.203 [commons-pool-evictor INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-a2940384-b16c-4b9f-9613-ec5504ace853--1723881453-executor-2 unregistered
2025-06-04 23:25:00.005 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "61820dfc-92a5-4fbe-933e-8fa480b1575f",
  "runId" : "a68bb1f6-b907-4299-b847-831175229ef5",
  "name" : null,
  "timestamp" : "2025-06-04T16:25:00.001Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:25:00.005 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:25:00.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:25:30.004 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:25:30.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:25:30.004 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "61820dfc-92a5-4fbe-933e-8fa480b1575f",
  "runId" : "a68bb1f6-b907-4299-b847-831175229ef5",
  "name" : null,
  "timestamp" : "2025-06-04T16:25:30.000Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:26:00.004 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:26:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:26:00.004 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "61820dfc-92a5-4fbe-933e-8fa480b1575f",
  "runId" : "a68bb1f6-b907-4299-b847-831175229ef5",
  "name" : null,
  "timestamp" : "2025-06-04T16:26:00.000Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:26:30.004 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "61820dfc-92a5-4fbe-933e-8fa480b1575f",
  "runId" : "a68bb1f6-b907-4299-b847-831175229ef5",
  "name" : null,
  "timestamp" : "2025-06-04T16:26:30.000Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:26:30.004 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:26:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:27:00.003 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "61820dfc-92a5-4fbe-933e-8fa480b1575f",
  "runId" : "a68bb1f6-b907-4299-b847-831175229ef5",
  "name" : null,
  "timestamp" : "2025-06-04T16:27:00.000Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:27:00.004 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:27:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:27:30.003 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "61820dfc-92a5-4fbe-933e-8fa480b1575f",
  "runId" : "a68bb1f6-b907-4299-b847-831175229ef5",
  "name" : null,
  "timestamp" : "2025-06-04T16:27:30.000Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:27:30.004 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:27:30.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:28:00.004 [stream execution thread for [id = 61820dfc-92a5-4fbe-933e-8fa480b1575f, runId = a68bb1f6-b907-4299-b847-831175229ef5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "61820dfc-92a5-4fbe-933e-8fa480b1575f",
  "runId" : "a68bb1f6-b907-4299-b847-831175229ef5",
  "name" : null,
  "timestamp" : "2025-06-04T16:28:00.001Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 45,
        "0" : 52
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:28:00.004 [stream execution thread for [id = ac80d613-1cf0-4184-8299-d422666bd08c, runId = 9e9b5e89-22f2-45c6-92b6-eb3b2c15565a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "ac80d613-1cf0-4184-8299-d422666bd08c",
  "runId" : "9e9b5e89-22f2-45c6-92b6-eb3b2c15565a",
  "name" : null,
  "timestamp" : "2025-06-04T16:28:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events-1k]]",
    "startOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "endOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "latestOffset" : {
      "clickstream-events-1k" : {
        "1" : 1706,
        "0" : 1604
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-04 23:28:03.201 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node -1 disconnected.
2025-06-04 23:28:21.340 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-04 23:28:21.341 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-04 23:28:21.345 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-04 23:28:21.346 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-04 23:28:21.346 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@bc4d5e1{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:28:21.348 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-04 23:28:21.351 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@665136b9{HTTP/1.1, (http/1.1)}{0.0.0.0:4041}
2025-06-04 23:28:21.354 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4041
2025-06-04 23:28:21.355 [dispatcher-event-loop-6 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-04 23:28:21.361 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-04 23:28:21.361 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-04 23:28:21.362 [dispatcher-event-loop-8 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-04 23:28:21.363 [SpringApplicationShutdownHook INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-06-04 23:28:21.364 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-04 23:28:21.364 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 23:28:21.364 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 23:28:21.365 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-04 23:28:21.365 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 23:28:21.365 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-1 unregistered
2025-06-04 23:28:21.365 [dispatcher-event-loop-10 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-04 23:28:21.368 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-04 23:28:21.369 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-04 23:28:21.369 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-573d72dd-3dd5-4df4-aaa5-cfbe27c8d047
2025-06-04 23:28:51.181 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-04 23:28:51.255 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-04 23:28:51.299 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:28:51.299 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-04 23:28:51.299 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:28:51.300 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-04 23:28:51.310 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-04 23:28:51.317 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-04 23:28:51.317 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-04 23:28:51.348 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-04 23:28:51.348 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-04 23:28:51.349 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-04 23:28:51.349 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-04 23:28:51.349 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-04 23:28:51.467 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 36271.
2025-06-04 23:28:51.481 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-04 23:28:51.497 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-04 23:28:51.506 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-04 23:28:51.507 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-04 23:28:51.508 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-04 23:28:51.518 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-9bb1ae92-b908-42c5-8ac9-0e9085fdadbb
2025-06-04 23:28:51.534 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-04 23:28:51.542 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-04 23:28:51.562 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1006ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-04 23:28:51.606 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-04 23:28:51.611 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-04 23:28:51.619 [main INFO ] org.sparkproject.jetty.server.Server - Started @1063ms
2025-06-04 23:28:51.633 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@414cc890{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:28:51.634 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-04 23:28:51.645 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7144655b{/,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.695 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-04 23:28:51.700 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-04 23:28:51.713 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35463.
2025-06-04 23:28:51.713 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:35463
2025-06-04 23:28:51.714 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-04 23:28:51.718 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 35463, None)
2025-06-04 23:28:51.721 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:35463 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 35463, None)
2025-06-04 23:28:51.724 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 35463, None)
2025-06-04 23:28:51.725 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 35463, None)
2025-06-04 23:28:51.817 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7144655b{/,null,STOPPED,@Spark}
2025-06-04 23:28:51.818 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241f05b{/jobs,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.818 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/jobs/json,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.819 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c9320c2{/jobs/job,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.819 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cc9385{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.820 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7915bca3{/stages,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.820 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/stages/json,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.821 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79fd6f95{/stages/stage,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.821 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.821 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/stages/pool,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.822 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.822 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/storage,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.823 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/storage/json,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.823 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.824 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.824 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/environment,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.824 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/environment/json,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.825 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/executors,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.825 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/executors/json,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.826 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397ef2{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.826 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e93c1f{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.830 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9b21bd3{/static,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.831 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@323f3c96{/,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.832 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b6d92e{/api,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.832 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25d93198{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.833 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f951a7f{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.835 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e7f0216{/metrics/json,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.916 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-04 23:28:51.920 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-04 23:28:51.927 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c86da0c{/SQL,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.927 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6732726{/SQL/json,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.928 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47d023b7{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.928 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d64c100{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-04 23:28:51.935 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d904ff1{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:28:53.136 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-04 23:28:53.144 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5dd3727c{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-04 23:28:53.144 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@229cb4d8{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-04 23:28:53.145 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@391dfe7c{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-04 23:28:53.145 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d3bcd3{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-04 23:28:53.146 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@112530c3{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:28:53.149 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-252952d6-1e23-4976-b419-3bb0ad0308c0. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-04 23:28:53.160 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-252952d6-1e23-4976-b419-3bb0ad0308c0 resolved to file:/tmp/temporary-252952d6-1e23-4976-b419-3bb0ad0308c0.
2025-06-04 23:28:53.160 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-04 23:28:53.201 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-252952d6-1e23-4976-b419-3bb0ad0308c0/metadata using temp file file:/tmp/temporary-252952d6-1e23-4976-b419-3bb0ad0308c0/.metadata.117e794a-96aa-4068-95c4-9c66bf77bdf7.tmp
2025-06-04 23:28:53.250 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-252952d6-1e23-4976-b419-3bb0ad0308c0/.metadata.117e794a-96aa-4068-95c4-9c66bf77bdf7.tmp to file:/tmp/temporary-252952d6-1e23-4976-b419-3bb0ad0308c0/metadata
2025-06-04 23:28:53.266 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a]. Use file:/tmp/temporary-252952d6-1e23-4976-b419-3bb0ad0308c0 to store the query checkpoint.
2025-06-04 23:28:53.271 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@3bd923f1] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@cd6f59c]
2025-06-04 23:28:53.285 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-04 23:28:53.286 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-04 23:28:53.286 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-04 23:28:53.288 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-04 23:28:53.440 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-04 23:28:53.471 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-04 23:28:53.472 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:28:53.472 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:28:53.472 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749054533471
2025-06-04 23:28:53.652 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-252952d6-1e23-4976-b419-3bb0ad0308c0/sources/0/0 using temp file file:/tmp/temporary-252952d6-1e23-4976-b419-3bb0ad0308c0/sources/0/.0.c5e20da3-a88f-4c8f-9f07-7305c79862b9.tmp
2025-06-04 23:28:53.664 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-252952d6-1e23-4976-b419-3bb0ad0308c0/sources/0/.0.c5e20da3-a88f-4c8f-9f07-7305c79862b9.tmp to file:/tmp/temporary-252952d6-1e23-4976-b419-3bb0ad0308c0/sources/0/0
2025-06-04 23:28:53.664 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-04 23:28:53.675 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-252952d6-1e23-4976-b419-3bb0ad0308c0/offsets/0 using temp file file:/tmp/temporary-252952d6-1e23-4976-b419-3bb0ad0308c0/offsets/.0.c1866167-1b9a-42a1-96b8-3a993a0febba.tmp
2025-06-04 23:28:53.692 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-252952d6-1e23-4976-b419-3bb0ad0308c0/offsets/.0.c1866167-1b9a-42a1-96b8-3a993a0febba.tmp to file:/tmp/temporary-252952d6-1e23-4976-b419-3bb0ad0308c0/offsets/0
2025-06-04 23:28:53.693 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749054533671,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-04 23:28:53.841 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749054533671
2025-06-04 23:28:53.891 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:28:53.917 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:28:53.949 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749054533671
2025-06-04 23:28:53.952 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:28:53.953 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-04 23:28:54.156 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 119.850955 ms
2025-06-04 23:28:54.277 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.128328 ms
2025-06-04 23:28:54.290 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 5.740613 ms
2025-06-04 23:28:54.341 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:228
2025-06-04 23:28:54.349 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:228) as input to shuffle 0
2025-06-04 23:28:54.351 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:228) with 1 output partitions
2025-06-04 23:28:54.352 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:228)
2025-06-04 23:28:54.352 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-04 23:28:54.353 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
2025-06-04 23:28:54.355 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:228), which has no missing parents
2025-06-04 23:28:54.421 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 37.0 KiB, free 9.2 GiB)
2025-06-04 23:28:54.438 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 15.8 KiB, free 9.2 GiB)
2025-06-04 23:28:54.456 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:35463 (size: 15.8 KiB, free: 9.2 GiB)
2025-06-04 23:28:54.458 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-04 23:28:54.466 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:228) (first 15 tasks are for partitions Vector(0, 1))
2025-06-04 23:28:54.468 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-04 23:28:54.495 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8302 bytes) 
2025-06-04 23:28:54.497 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8302 bytes) 
2025-06-04 23:28:54.504 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-04 23:28:54.504 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-04 23:28:54.613 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 14.077809 ms
2025-06-04 23:28:54.647 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.836507 ms
2025-06-04 23:28:54.670 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 15.395553 ms
2025-06-04 23:28:54.681 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-04 23:28:54.681 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-04 23:28:54.703 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-04 23:28:54.703 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-04 23:28:54.732 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:28:54.733 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:28:54.733 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749054534732
2025-06-04 23:28:54.733 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:28:54.733 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:28:54.733 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749054534732
2025-06-04 23:28:54.734 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-1, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Assigned to partition(s): clickstream-events-1
2025-06-04 23:28:54.734 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-2, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Assigned to partition(s): clickstream-events-0
2025-06-04 23:28:54.739 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-2, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-04 23:28:54.739 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-1, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-04 23:28:54.745 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-2, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-04 23:28:54.745 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-1, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-04 23:28:54.771 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-2, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-04 23:28:54.771 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-1, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-04 23:28:55.273 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-1, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:28:55.273 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-2, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:28:55.273 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-1, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-04 23:28:55.273 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-2, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-04 23:28:55.274 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-2, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:28:55.274 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-1, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:28:55.347 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 2342 bytes result sent to driver
2025-06-04 23:28:55.347 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2342 bytes result sent to driver
2025-06-04 23:28:55.353 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 864 ms on phamviethoa (executor driver) (1/2)
2025-06-04 23:28:55.354 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 857 ms on phamviethoa (executor driver) (2/2)
2025-06-04 23:28:55.355 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-04 23:28:55.360 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (start at SparkClickstreamProcessor.java:228) finished in 0.999 s
2025-06-04 23:28:55.361 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
2025-06-04 23:28:55.361 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - running: Set()
2025-06-04 23:28:55.361 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
2025-06-04 23:28:55.361 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - failed: Set()
2025-06-04 23:28:55.362 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:228), which has no missing parents
2025-06-04 23:28:55.367 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-04 23:28:55.369 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-04 23:28:55.369 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:35463 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-04 23:28:55.370 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-04 23:28:55.370 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:228) (first 15 tasks are for partitions Vector(0))
2025-06-04 23:28:55.371 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-04 23:28:55.374 [dispatcher-event-loop-12 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
2025-06-04 23:28:55.375 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-04 23:28:55.416 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-04 23:28:55.418 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
2025-06-04 23:28:55.429 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 4038 bytes result sent to driver
2025-06-04 23:28:55.430 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 57 ms on phamviethoa (executor driver) (1/1)
2025-06-04 23:28:55.430 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-04 23:28:55.431 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:228) finished in 0.065 s
2025-06-04 23:28:55.433 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-04 23:28:55.433 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-04 23:28:55.434 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:228, took 1.092937 s
2025-06-04 23:28:55.593 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.621811 ms
2025-06-04 23:28:55.624 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:228
2025-06-04 23:28:55.624 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 1 (start at SparkClickstreamProcessor.java:228) with 2 output partitions
2025-06-04 23:28:55.625 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (start at SparkClickstreamProcessor.java:228)
2025-06-04 23:28:55.625 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-04 23:28:55.625 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-04 23:28:55.625 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[13] at start at SparkClickstreamProcessor.java:228), which has no missing parents
2025-06-04 23:28:55.638 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 46.8 KiB, free 9.2 GiB)
2025-06-04 23:28:55.641 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 9.2 GiB)
2025-06-04 23:28:55.642 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on phamviethoa:35463 (size: 19.1 KiB, free: 9.2 GiB)
2025-06-04 23:28:55.642 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1535
2025-06-04 23:28:55.643 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at start at SparkClickstreamProcessor.java:228) (first 15 tasks are for partitions Vector(0, 1))
2025-06-04 23:28:55.643 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 2 tasks resource profile 0
2025-06-04 23:28:55.644 [dispatcher-event-loop-15 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 3) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-04 23:28:55.644 [dispatcher-event-loop-15 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 4) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8313 bytes) 
2025-06-04 23:28:55.645 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 4)
2025-06-04 23:28:55.645 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 3)
2025-06-04 23:28:55.693 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.99165 ms
2025-06-04 23:28:55.700 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-2, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-04 23:28:55.703 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-1, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-04 23:28:55.705 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-2, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-04 23:28:55.707 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-1, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-04 23:28:56.207 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-2, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:28:56.207 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-2, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-04 23:28:56.208 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-2, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:28:56.208 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-1, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:28:56.208 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-1, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-04 23:28:56.209 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-1, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:28:56.219 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-04 23:28:56.219 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-04 23:28:56.226 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-04 23:28:56.226 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-04 23:28:56.226 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [bc19fde1-9ae0-4d77-8c70-b61204ef0192] (1 queries & 0 savepoints) is rolled back.
2025-06-04 23:28:56.226 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [65460a02-6b3b-426d-8d8e-dab0fd300bac] (1 queries & 0 savepoints) is rolled back.
2025-06-04 23:28:56.226 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [3a80e78e-90d4-4e00-bb23-41b6980c32f2] (0 queries & 0 savepoints) is committed.
2025-06-04 23:28:56.227 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [0c947a8c-f655-4192-8e22-94b6eee2dc20] (0 queries & 0 savepoints) is committed.
2025-06-04 23:28:56.229 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) ERROR] org.apache.spark.executor.Executor - Exception in task 1.0 in stage 2.0 (TID 4)
java.sql.SQLException: Cannot set null to non-nullable column #1 [event_id String]
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:340)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-04 23:28:56.229 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) ERROR] org.apache.spark.executor.Executor - Exception in task 0.0 in stage 2.0 (TID 3)
java.sql.SQLException: Cannot set null to non-nullable column #1 [event_id String]
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:340)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-04 23:28:56.237 [task-result-getter-3 WARN ] o.a.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 2.0 (TID 3) (phamviethoa executor driver): java.sql.SQLException: Cannot set null to non-nullable column #1 [event_id String]
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:340)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

2025-06-04 23:28:56.238 [task-result-getter-3 ERROR] o.a.spark.scheduler.TaskSetManager - Task 0 in stage 2.0 failed 1 times; aborting job
2025-06-04 23:28:56.238 [task-result-getter-3 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2025-06-04 23:28:56.239 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 2.0 (TID 4) on phamviethoa, executor driver: java.sql.SQLException (Cannot set null to non-nullable column #1 [event_id String]) [duplicate 1]
2025-06-04 23:28:56.239 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2025-06-04 23:28:56.239 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Cancelling stage 2
2025-06-04 23:28:56.239 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage cancelled
2025-06-04 23:28:56.239 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 2 (start at SparkClickstreamProcessor.java:228) failed in 0.613 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 3) (phamviethoa executor driver): java.sql.SQLException: Cannot set null to non-nullable column #1 [event_id String]
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:340)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Driver stacktrace:
2025-06-04 23:28:56.240 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.spark.scheduler.DAGScheduler - Job 1 failed: start at SparkClickstreamProcessor.java:228, took 0.616610 s
2025-06-04 23:28:56.250 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] terminated with error
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 3) (phamviethoa executor driver): java.sql.SQLException: Cannot set null to non-nullable column #1 [event_id String]
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:340)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1009)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1007)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:890)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.SparkClickstreamProcessor.lambda$main$a450ce84$1(SparkClickstreamProcessor.java:224)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
Caused by: java.sql.SQLException: Cannot set null to non-nullable column #1 [event_id String]
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:340)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-04 23:28:56.251 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-04 23:28:56.253 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 23:28:56.253 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 23:28:56.253 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 23:28:56.253 [stream execution thread for [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = f27549a2-3c69-4e73-8d78-6b054a91c6b8, runId = c2cbc1be-786b-4f63-acc1-8f34037f385a] has been shutdown
2025-06-04 23:28:56.263 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-1, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-04 23:28:56.263 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-1, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-04 23:28:56.266 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 23:28:56.266 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 23:28:56.266 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-04 23:28:56.266 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 23:28:56.267 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-1 unregistered
2025-06-04 23:28:56.268 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-2, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-04 23:28:56.268 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-2, groupId=spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-04 23:28:56.269 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 23:28:56.269 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 23:28:56.269 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-04 23:28:56.269 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 23:28:56.271 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-8ae731ad-18e0-4a0d-b34b-b656388c573a--199692267-executor-2 unregistered
2025-06-04 23:28:56.271 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-04 23:28:56.271 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-04 23:28:56.275 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@414cc890{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:28:56.276 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-04 23:28:56.283 [dispatcher-event-loop-6 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-04 23:28:56.289 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-04 23:28:56.289 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-04 23:28:56.292 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-04 23:28:56.293 [dispatcher-event-loop-11 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-04 23:28:56.296 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-04 23:28:56.296 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-04 23:28:56.296 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-4384a2a3-6c59-46b5-b4d1-6b5e5845082b
2025-06-04 23:28:56.297 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-252952d6-1e23-4976-b419-3bb0ad0308c0
2025-06-04 23:30:45.675 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-04 23:30:45.784 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-04 23:30:45.845 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:30:45.845 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-04 23:30:45.845 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:30:45.846 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-04 23:30:45.859 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-04 23:30:45.866 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-04 23:30:45.866 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-04 23:30:45.901 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-04 23:30:45.901 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-04 23:30:45.901 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-04 23:30:45.901 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-04 23:30:45.902 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-04 23:30:46.050 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 33061.
2025-06-04 23:30:46.071 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-04 23:30:46.093 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-04 23:30:46.105 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-04 23:30:46.106 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-04 23:30:46.110 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-04 23:30:46.130 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-28c069fd-cab2-4e2e-8eaa-e0ada11bbe84
2025-06-04 23:30:46.152 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-04 23:30:46.162 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-04 23:30:46.185 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1364ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-04 23:30:46.237 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-04 23:30:46.244 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-04 23:30:46.254 [main INFO ] org.sparkproject.jetty.server.Server - Started @1433ms
2025-06-04 23:30:46.285 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@70f69ec6{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:30:46.285 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-04 23:30:46.300 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e33fcae{/,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.423 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-04 23:30:46.429 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-04 23:30:46.446 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46781.
2025-06-04 23:30:46.446 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:46781
2025-06-04 23:30:46.448 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-04 23:30:46.452 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 46781, None)
2025-06-04 23:30:46.455 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:46781 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 46781, None)
2025-06-04 23:30:46.457 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 46781, None)
2025-06-04 23:30:46.459 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 46781, None)
2025-06-04 23:30:46.549 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6e33fcae{/,null,STOPPED,@Spark}
2025-06-04 23:30:46.550 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34332b8d{/jobs,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.550 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f8aba08{/jobs/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.551 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@462abec3{/jobs/job,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.552 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a4d582c{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.552 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45e9b12d{/stages,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.552 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d55e826{/stages/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.554 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d247660{/stages/stage,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.554 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49d30c6f{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.555 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fdca00a{/stages/pool,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.555 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a8c93{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.556 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@119b0892{/storage,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.557 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ed4a7e4{/storage/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.557 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55651434{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.558 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50448409{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.558 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b0dc227{/environment,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.559 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@43bdaa1b{/environment/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.560 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75e09567{/executors,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.560 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@470d183{/executors/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.561 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ea52184{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.561 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c854752{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.567 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a500561{/static,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.567 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6bccd036{/,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.569 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a756082{/api,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.570 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76a14c8d{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.570 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ee99964{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.573 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fb9a67f{/metrics/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.681 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-04 23:30:46.686 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-04 23:30:46.695 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59939293{/SQL,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.695 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d74c81b{/SQL/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.696 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@991cbde{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.696 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@456bcb74{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:46.703 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b332962{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:30:47.961 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-04 23:30:47.961 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-04 23:30:47.968 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@70f69ec6{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:30:47.972 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-04 23:30:47.984 [dispatcher-event-loop-7 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-04 23:30:47.991 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-04 23:30:47.991 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-04 23:30:47.996 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-04 23:30:47.998 [dispatcher-event-loop-11 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-04 23:30:48.002 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-04 23:30:48.002 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-04 23:30:48.003 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-6744c8ef-cd20-4f18-b638-f8f2a9351edd
2025-06-04 23:30:50.190 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-04 23:30:50.285 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-04 23:30:50.333 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:30:50.333 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-04 23:30:50.333 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:30:50.334 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-04 23:30:50.345 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-04 23:30:50.352 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-04 23:30:50.352 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-04 23:30:50.380 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-04 23:30:50.380 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-04 23:30:50.381 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-04 23:30:50.381 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-04 23:30:50.381 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-04 23:30:50.505 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 41215.
2025-06-04 23:30:50.520 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-04 23:30:50.537 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-04 23:30:50.546 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-04 23:30:50.547 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-04 23:30:50.549 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-04 23:30:50.560 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-37bce0b8-3301-48ac-b3af-ad4fdcdb8336
2025-06-04 23:30:50.577 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-04 23:30:50.585 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-04 23:30:50.605 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1114ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-04 23:30:50.650 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-04 23:30:50.655 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-04 23:30:50.664 [main INFO ] org.sparkproject.jetty.server.Server - Started @1174ms
2025-06-04 23:30:50.680 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@4005aaf6{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:30:50.680 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-04 23:30:50.691 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c504e66{/,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.744 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-04 23:30:50.748 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-04 23:30:50.761 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38195.
2025-06-04 23:30:50.761 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:38195
2025-06-04 23:30:50.762 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-04 23:30:50.766 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 38195, None)
2025-06-04 23:30:50.769 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:38195 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 38195, None)
2025-06-04 23:30:50.771 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 38195, None)
2025-06-04 23:30:50.772 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 38195, None)
2025-06-04 23:30:50.852 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1c504e66{/,null,STOPPED,@Spark}
2025-06-04 23:30:50.852 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7342e05d{/jobs,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.853 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15383681{/jobs/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.853 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@761956ac{/jobs/job,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.854 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@304d0259{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.854 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133661d{/stages,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.855 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3414a8c3{/stages/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.855 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e43e323{/stages/stage,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.856 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10643593{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.856 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@eca6a74{/stages/pool,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.857 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48840594{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.857 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14823f76{/storage,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.858 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ed16657{/storage/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.858 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@113e13f9{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.859 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7979b8b7{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.859 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bc49bc5{/environment,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.860 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f66ffc8{/environment/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.860 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2def7a7a{/executors,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.861 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c080ef3{/executors/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.861 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ee6291f{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.862 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37e0292a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.866 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35267fd4{/static,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.867 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ac4ccad{/,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.868 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14982a82{/api,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.868 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5300cac{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.868 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ba359bd{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.871 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58ff8d79{/metrics/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.970 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-04 23:30:50.974 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-04 23:30:50.981 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1dbd580{/SQL,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.982 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d0d91a1{/SQL/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.982 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6732726{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.983 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d64c581{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-04 23:30:50.989 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fdf17dc{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:30:52.064 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-04 23:30:52.065 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-04 23:30:52.069 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@4005aaf6{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:30:52.071 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-04 23:30:52.080 [dispatcher-event-loop-7 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-04 23:30:52.088 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-04 23:30:52.088 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-04 23:30:52.093 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-04 23:30:52.096 [dispatcher-event-loop-11 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-04 23:30:52.100 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-04 23:30:52.100 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-04 23:30:52.101 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-ddeb71f0-84a8-409e-aa68-94715819f949
2025-06-04 23:33:00.019 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-04 23:33:00.096 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-04 23:33:00.139 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:33:00.140 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-04 23:33:00.140 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:33:00.140 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-04 23:33:00.151 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-04 23:33:00.158 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-04 23:33:00.158 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-04 23:33:00.185 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-04 23:33:00.186 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-04 23:33:00.186 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-04 23:33:00.186 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-04 23:33:00.186 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-04 23:33:00.305 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 39899.
2025-06-04 23:33:00.318 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-04 23:33:00.339 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-04 23:33:00.349 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-04 23:33:00.349 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-04 23:33:00.351 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-04 23:33:00.362 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-2eaf529a-2262-43cc-b0f3-6461c98bacad
2025-06-04 23:33:00.377 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-04 23:33:00.385 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-04 23:33:00.404 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1010ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-04 23:33:00.445 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-04 23:33:00.450 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-04 23:33:00.458 [main INFO ] org.sparkproject.jetty.server.Server - Started @1065ms
2025-06-04 23:33:00.473 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@1ecaad17{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:33:00.474 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-04 23:33:00.484 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35835e65{/,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.533 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-04 23:33:00.537 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-04 23:33:00.547 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39945.
2025-06-04 23:33:00.547 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:39945
2025-06-04 23:33:00.549 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-04 23:33:00.552 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 39945, None)
2025-06-04 23:33:00.555 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:39945 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 39945, None)
2025-06-04 23:33:00.557 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 39945, None)
2025-06-04 23:33:00.557 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 39945, None)
2025-06-04 23:33:00.633 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@35835e65{/,null,STOPPED,@Spark}
2025-06-04 23:33:00.633 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/jobs,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.634 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/jobs/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.635 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cc9385{/jobs/job,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.635 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7915bca3{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.635 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/stages,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.636 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67b4ec{/stages/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.637 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/stages/stage,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.637 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.638 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/stages/pool,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.639 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.639 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/storage,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.640 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/storage/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.641 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.641 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.642 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/environment,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.642 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/environment/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.643 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/executors,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.643 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397ef2{/executors/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.644 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e93c1f{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.645 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9b21bd3{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.649 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7661b5a{/static,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.649 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b6d92e{/,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.650 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7899de11{/api,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.651 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f951a7f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.651 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c777e7b{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.653 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62db3891{/metrics/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.734 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-04 23:33:00.737 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-04 23:33:00.744 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6732726{/SQL,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.745 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d64c581{/SQL/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.746 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d64c100{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.746 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fdf17dc{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:00.752 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ff8a9dc{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:33:02.333 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-04 23:33:02.334 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-04 23:33:02.338 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@1ecaad17{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:33:02.340 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-04 23:33:02.355 [dispatcher-event-loop-7 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-04 23:33:02.361 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-04 23:33:02.361 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-04 23:33:02.364 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-04 23:33:02.365 [dispatcher-event-loop-11 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-04 23:33:02.368 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-04 23:33:02.368 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-04 23:33:02.369 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-4cd0e1b1-ce10-4bd8-bb7a-7d09d8f117e4
2025-06-04 23:33:46.405 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-04 23:33:46.488 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-04 23:33:46.531 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:33:46.531 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-04 23:33:46.531 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:33:46.532 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-04 23:33:46.543 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-04 23:33:46.549 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-04 23:33:46.549 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-04 23:33:46.578 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-04 23:33:46.578 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-04 23:33:46.578 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-04 23:33:46.578 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-04 23:33:46.579 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-04 23:33:46.694 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 34857.
2025-06-04 23:33:46.707 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-04 23:33:46.723 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-04 23:33:46.732 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-04 23:33:46.733 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-04 23:33:46.735 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-04 23:33:46.746 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-806e382f-28e7-486e-b38f-2ed1ce4d4365
2025-06-04 23:33:46.762 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-04 23:33:46.770 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-04 23:33:46.789 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @957ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-04 23:33:46.833 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-04 23:33:46.839 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-04 23:33:46.847 [main INFO ] org.sparkproject.jetty.server.Server - Started @1016ms
2025-06-04 23:33:46.865 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@4caff132{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:33:46.866 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-04 23:33:46.876 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35835e65{/,null,AVAILABLE,@Spark}
2025-06-04 23:33:46.922 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-04 23:33:46.926 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-04 23:33:46.939 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43075.
2025-06-04 23:33:46.939 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:43075
2025-06-04 23:33:46.940 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-04 23:33:46.945 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 43075, None)
2025-06-04 23:33:46.948 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:43075 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 43075, None)
2025-06-04 23:33:46.950 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 43075, None)
2025-06-04 23:33:46.951 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 43075, None)
2025-06-04 23:33:47.035 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@35835e65{/,null,STOPPED,@Spark}
2025-06-04 23:33:47.036 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/jobs,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.036 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/jobs/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.037 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cc9385{/jobs/job,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.037 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7915bca3{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.038 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/stages,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.038 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67b4ec{/stages/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.039 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/stages/stage,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.039 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.040 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/stages/pool,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.040 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.040 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/storage,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.041 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/storage/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.041 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.042 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.042 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/environment,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.043 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/environment/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.043 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/executors,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.043 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397ef2{/executors/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.044 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e93c1f{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.045 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9b21bd3{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.049 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7661b5a{/static,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.050 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b6d92e{/,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.051 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7899de11{/api,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.051 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f951a7f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.051 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c777e7b{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.054 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62db3891{/metrics/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.133 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-04 23:33:47.137 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-04 23:33:47.144 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6732726{/SQL,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.144 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d64c581{/SQL/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.145 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d64c100{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.145 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fdf17dc{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:47.151 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ff8a9dc{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:33:48.268 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-04 23:33:48.279 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77476fcf{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-04 23:33:48.280 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e7d0db2{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:48.281 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56a09a5c{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-04 23:33:48.281 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f480c6{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-04 23:33:48.282 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ce24a1a{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:33:48.287 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ce0b1f03-5334-41f8-8f1c-db0fd5988f77. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-04 23:33:48.302 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-ce0b1f03-5334-41f8-8f1c-db0fd5988f77 resolved to file:/tmp/temporary-ce0b1f03-5334-41f8-8f1c-db0fd5988f77.
2025-06-04 23:33:48.302 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-04 23:33:48.349 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-ce0b1f03-5334-41f8-8f1c-db0fd5988f77/metadata using temp file file:/tmp/temporary-ce0b1f03-5334-41f8-8f1c-db0fd5988f77/.metadata.3f6f7d1b-2de8-4e67-b4e5-1791b35fe8c5.tmp
2025-06-04 23:33:48.402 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-ce0b1f03-5334-41f8-8f1c-db0fd5988f77/.metadata.3f6f7d1b-2de8-4e67-b4e5-1791b35fe8c5.tmp to file:/tmp/temporary-ce0b1f03-5334-41f8-8f1c-db0fd5988f77/metadata
2025-06-04 23:33:48.419 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = ff0e498c-41b9-445e-8f2d-a1ee8b085891, runId = cf48c1a7-e58d-4c72-bf7a-f8d6312818fc]. Use file:/tmp/temporary-ce0b1f03-5334-41f8-8f1c-db0fd5988f77 to store the query checkpoint.
2025-06-04 23:33:48.426 [stream execution thread for [id = ff0e498c-41b9-445e-8f2d-a1ee8b085891, runId = cf48c1a7-e58d-4c72-bf7a-f8d6312818fc] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1ae45306] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@62053345]
2025-06-04 23:33:48.426 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-04 23:33:48.427 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-04 23:33:48.432 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@4caff132{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:33:48.435 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-04 23:33:48.445 [stream execution thread for [id = ff0e498c-41b9-445e-8f2d-a1ee8b085891, runId = cf48c1a7-e58d-4c72-bf7a-f8d6312818fc] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-04 23:33:48.445 [dispatcher-event-loop-8 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-04 23:33:48.446 [stream execution thread for [id = ff0e498c-41b9-445e-8f2d-a1ee8b085891, runId = cf48c1a7-e58d-4c72-bf7a-f8d6312818fc] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-04 23:33:48.446 [stream execution thread for [id = ff0e498c-41b9-445e-8f2d-a1ee8b085891, runId = cf48c1a7-e58d-4c72-bf7a-f8d6312818fc] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-04 23:33:48.448 [stream execution thread for [id = ff0e498c-41b9-445e-8f2d-a1ee8b085891, runId = cf48c1a7-e58d-4c72-bf7a-f8d6312818fc] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-04 23:33:48.453 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-04 23:33:48.454 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-04 23:33:48.458 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-04 23:33:48.460 [dispatcher-event-loop-12 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-04 23:33:48.463 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-04 23:33:48.463 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-04 23:33:48.464 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-2f635e17-d392-41bb-9268-0968336dfec6
2025-06-04 23:33:48.465 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-ce0b1f03-5334-41f8-8f1c-db0fd5988f77
2025-06-04 23:35:27.374 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-04 23:35:27.461 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-04 23:35:27.504 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:35:27.504 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-04 23:35:27.504 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:35:27.504 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-04 23:35:27.515 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-04 23:35:27.522 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-04 23:35:27.522 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-04 23:35:27.550 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-04 23:35:27.550 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-04 23:35:27.550 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-04 23:35:27.550 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-04 23:35:27.551 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-04 23:35:27.669 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 44343.
2025-06-04 23:35:27.682 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-04 23:35:27.699 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-04 23:35:27.708 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-04 23:35:27.708 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-04 23:35:27.710 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-04 23:35:27.720 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-ab5d8bf1-b277-4d82-8b86-f2244bcd02f1
2025-06-04 23:35:27.735 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-04 23:35:27.743 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-04 23:35:27.762 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @949ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-04 23:35:27.804 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-04 23:35:27.809 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-04 23:35:27.817 [main INFO ] org.sparkproject.jetty.server.Server - Started @1004ms
2025-06-04 23:35:27.834 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@6f5e781d{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:35:27.834 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-04 23:35:27.844 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35835e65{/,null,AVAILABLE,@Spark}
2025-06-04 23:35:27.889 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-04 23:35:27.892 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-04 23:35:27.904 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44347.
2025-06-04 23:35:27.904 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:44347
2025-06-04 23:35:27.905 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-04 23:35:27.909 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 44347, None)
2025-06-04 23:35:27.911 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:44347 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 44347, None)
2025-06-04 23:35:27.913 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 44347, None)
2025-06-04 23:35:27.914 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 44347, None)
2025-06-04 23:35:27.990 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@35835e65{/,null,STOPPED,@Spark}
2025-06-04 23:35:27.991 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/jobs,null,AVAILABLE,@Spark}
2025-06-04 23:35:27.991 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/jobs/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:27.992 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cc9385{/jobs/job,null,AVAILABLE,@Spark}
2025-06-04 23:35:27.992 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7915bca3{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:27.993 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/stages,null,AVAILABLE,@Spark}
2025-06-04 23:35:27.993 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67b4ec{/stages/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:27.994 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/stages/stage,null,AVAILABLE,@Spark}
2025-06-04 23:35:27.994 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:27.995 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/stages/pool,null,AVAILABLE,@Spark}
2025-06-04 23:35:27.995 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:27.995 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/storage,null,AVAILABLE,@Spark}
2025-06-04 23:35:27.996 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/storage/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:27.996 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-04 23:35:27.997 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:27.997 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/environment,null,AVAILABLE,@Spark}
2025-06-04 23:35:27.997 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/environment/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:27.998 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/executors,null,AVAILABLE,@Spark}
2025-06-04 23:35:27.998 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397ef2{/executors/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:27.999 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e93c1f{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-04 23:35:27.999 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9b21bd3{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:28.003 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7661b5a{/static,null,AVAILABLE,@Spark}
2025-06-04 23:35:28.004 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b6d92e{/,null,AVAILABLE,@Spark}
2025-06-04 23:35:28.005 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7899de11{/api,null,AVAILABLE,@Spark}
2025-06-04 23:35:28.005 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f951a7f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-04 23:35:28.006 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c777e7b{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-04 23:35:28.008 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62db3891{/metrics/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:28.088 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-04 23:35:28.092 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-04 23:35:28.099 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c83ae01{/SQL,null,AVAILABLE,@Spark}
2025-06-04 23:35:28.100 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69d45cca{/SQL/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:28.100 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79c5460e{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-04 23:35:28.101 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f94e148{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:28.107 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f37b6d9{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:35:28.655 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 108.941693 ms
2025-06-04 23:35:28.682 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 19.608305 ms
2025-06-04 23:35:29.362 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-04 23:35:29.363 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-04 23:35:29.367 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@6f5e781d{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:35:29.369 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-04 23:35:29.379 [dispatcher-event-loop-7 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-04 23:35:29.386 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-04 23:35:29.386 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-04 23:35:29.390 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-04 23:35:29.391 [dispatcher-event-loop-11 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-04 23:35:29.395 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-04 23:35:29.395 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-04 23:35:29.395 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-c6292919-da11-49f9-9422-1f08ece854a7
2025-06-04 23:35:46.007 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-04 23:35:46.087 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-04 23:35:46.132 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:35:46.133 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-04 23:35:46.133 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:35:46.133 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-04 23:35:46.143 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-04 23:35:46.149 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-04 23:35:46.149 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-04 23:35:46.176 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-04 23:35:46.176 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-04 23:35:46.177 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-04 23:35:46.177 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-04 23:35:46.177 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-04 23:35:46.295 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 35883.
2025-06-04 23:35:46.309 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-04 23:35:46.325 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-04 23:35:46.334 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-04 23:35:46.334 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-04 23:35:46.336 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-04 23:35:46.346 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-70ac843a-dd8e-4481-adc0-a8235a3d7f03
2025-06-04 23:35:46.362 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-04 23:35:46.371 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-04 23:35:46.392 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @995ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-04 23:35:46.436 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-04 23:35:46.441 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-04 23:35:46.451 [main INFO ] org.sparkproject.jetty.server.Server - Started @1054ms
2025-06-04 23:35:46.467 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@9c79e48{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:35:46.468 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-04 23:35:46.480 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17740dae{/,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.528 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-04 23:35:46.532 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-04 23:35:46.544 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32785.
2025-06-04 23:35:46.544 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:32785
2025-06-04 23:35:46.545 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-04 23:35:46.549 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 32785, None)
2025-06-04 23:35:46.552 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:32785 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 32785, None)
2025-06-04 23:35:46.554 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 32785, None)
2025-06-04 23:35:46.555 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 32785, None)
2025-06-04 23:35:46.638 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@17740dae{/,null,STOPPED,@Spark}
2025-06-04 23:35:46.638 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21c815e4{/jobs,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.639 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a331b46{/jobs/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.640 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241f05b{/jobs/job,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.640 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.641 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/stages,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.641 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c9320c2{/stages/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.642 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/stages/stage,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.642 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67b4ec{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.643 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f91da5e{/stages/pool,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.643 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79fd6f95{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.644 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/storage,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.644 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/storage/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.644 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.645 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.645 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/environment,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.646 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/environment/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.646 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/executors,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.646 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/executors/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.647 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.648 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.653 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/static,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.654 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e0895f5{/,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.655 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fd9ebde{/api,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.655 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e9ef5b6{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.656 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4110765e{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.658 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53ec2968{/metrics/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.747 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-04 23:35:46.751 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-04 23:35:46.758 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c86da0c{/SQL,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.759 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6732726{/SQL/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.759 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47d023b7{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.760 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d64c100{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-04 23:35:46.766 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d904ff1{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:35:47.316 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 101.586117 ms
2025-06-04 23:35:47.336 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 14.235649 ms
2025-06-04 23:35:48.229 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-04 23:35:48.257 [main INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-04 23:35:48.258 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:35:48.258 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:35:48.258 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749054948258
2025-06-04 23:35:48.408 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-04 23:35:48.411 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 23:35:48.411 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 23:35:48.411 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 23:35:48.411 [main INFO ] o.a.spark.sql.kafka010.KafkaRelation - GetBatch generating RDD of offset range: KafkaOffsetRange(clickstream-events-0,-2,-1,None), KafkaOffsetRange(clickstream-events-1,-2,-1,None)
2025-06-04 23:35:48.595 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.387175 ms
2025-06-04 23:35:48.735 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 5.057052 ms
2025-06-04 23:35:48.743 [main INFO ] org.apache.spark.SparkContext - Starting job: show at SparkClickstreamProcessor.java:212
2025-06-04 23:35:48.750 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (show at SparkClickstreamProcessor.java:212) with 1 output partitions
2025-06-04 23:35:48.751 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (show at SparkClickstreamProcessor.java:212)
2025-06-04 23:35:48.751 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-04 23:35:48.752 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-04 23:35:48.754 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[8] at show at SparkClickstreamProcessor.java:212), which has no missing parents
2025-06-04 23:35:48.819 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 37.2 KiB, free 9.2 GiB)
2025-06-04 23:35:48.836 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 15.4 KiB, free 9.2 GiB)
2025-06-04 23:35:48.838 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:32785 (size: 15.4 KiB, free: 9.2 GiB)
2025-06-04 23:35:48.840 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-04 23:35:48.849 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at show at SparkClickstreamProcessor.java:212) (first 15 tasks are for partitions Vector(0))
2025-06-04 23:35:48.850 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
2025-06-04 23:35:48.885 [dispatcher-event-loop-6 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes) 
2025-06-04 23:35:48.892 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-04 23:35:48.984 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-04 23:35:49.003 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-04 23:35:49.031 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:35:49.031 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:35:49.031 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749054949031
2025-06-04 23:35:49.033 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor-1, groupId=spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor] Assigned to partition(s): clickstream-events-0
2025-06-04 23:35:49.035 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor-1, groupId=spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-04 23:35:49.043 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor-1, groupId=spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-04 23:35:49.050 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor-1, groupId=spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:35:49.050 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor-1, groupId=spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-04 23:35:49.051 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor-1, groupId=spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:35:49.083 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 17.803546 ms
2025-06-04 23:35:49.090 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor-1, groupId=spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-04 23:35:49.110 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor-1, groupId=spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-04 23:35:49.611 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor-1, groupId=spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:35:49.611 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor-1, groupId=spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-04 23:35:49.612 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor-1, groupId=spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:35:49.687 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 24.329473 ms
2025-06-04 23:35:49.700 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2497 bytes result sent to driver
2025-06-04 23:35:49.705 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 825 ms on phamviethoa (executor driver) (1/1)
2025-06-04 23:35:49.706 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-04 23:35:49.709 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (show at SparkClickstreamProcessor.java:212) finished in 0.948 s
2025-06-04 23:35:49.711 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-04 23:35:49.711 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-04 23:35:49.712 [main INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: show at SparkClickstreamProcessor.java:212, took 0.968720 s
2025-06-04 23:35:49.721 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.095283 ms
2025-06-04 23:35:49.731 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor-1, groupId=spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-04 23:35:49.731 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor-1, groupId=spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-04 23:35:49.735 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 23:35:49.735 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 23:35:49.735 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-04 23:35:49.735 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 23:35:49.737 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-relation-39a3465a-4d0d-402b-9697-5fdf60b55ff7-executor-1 unregistered
2025-06-04 23:35:49.738 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-04 23:35:49.738 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-04 23:35:49.743 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@9c79e48{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:35:49.746 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-04 23:35:49.756 [dispatcher-event-loop-10 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-04 23:35:49.763 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-04 23:35:49.764 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-04 23:35:49.768 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-04 23:35:49.770 [dispatcher-event-loop-14 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-04 23:35:49.773 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-04 23:35:49.774 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-04 23:35:49.774 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-8810ab13-0918-4770-8087-de3f6c10aa21
2025-06-04 23:36:06.489 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-04 23:36:06.570 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-04 23:36:06.614 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:36:06.615 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-04 23:36:06.615 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:36:06.615 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-04 23:36:06.626 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-04 23:36:06.632 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-04 23:36:06.632 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-04 23:36:06.660 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-04 23:36:06.660 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-04 23:36:06.661 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-04 23:36:06.661 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-04 23:36:06.661 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-04 23:36:06.779 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 42167.
2025-06-04 23:36:06.792 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-04 23:36:06.808 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-04 23:36:06.817 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-04 23:36:06.817 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-04 23:36:06.819 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-04 23:36:06.828 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-e39f99ef-66e8-4ddb-bff9-fde599bd7de3
2025-06-04 23:36:06.844 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-04 23:36:06.852 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-04 23:36:06.876 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1020ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-04 23:36:06.918 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-04 23:36:06.923 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-04 23:36:06.932 [main INFO ] org.sparkproject.jetty.server.Server - Started @1075ms
2025-06-04 23:36:06.947 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@748e9b20{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:36:06.947 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-04 23:36:06.961 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@289fdb08{/,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.007 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-04 23:36:07.011 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-04 23:36:07.021 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46791.
2025-06-04 23:36:07.021 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:46791
2025-06-04 23:36:07.022 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-04 23:36:07.025 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 46791, None)
2025-06-04 23:36:07.028 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:46791 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 46791, None)
2025-06-04 23:36:07.030 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 46791, None)
2025-06-04 23:36:07.030 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 46791, None)
2025-06-04 23:36:07.108 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@289fdb08{/,null,STOPPED,@Spark}
2025-06-04 23:36:07.109 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@304d0259{/jobs,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.110 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133661d{/jobs/json,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.110 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cf518cf{/jobs/job,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.111 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68d651f2{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.111 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e43e323{/stages,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.111 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10643593{/stages/json,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.112 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14823f76{/stages/stage,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.113 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ed16657{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.114 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@113e13f9{/stages/pool,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.114 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7979b8b7{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.115 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bc49bc5{/storage,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.115 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f66ffc8{/storage/json,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.116 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2def7a7a{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.116 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c080ef3{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.117 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ee6291f{/environment,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.117 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37e0292a{/environment/json,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.117 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35267fd4{/executors,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.118 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36a6bea6{/executors/json,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.119 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42373389{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.119 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a62c7cd{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.123 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c36db44{/static,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.124 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33899f7a{/,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.125 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@290d10ef{/api,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.125 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20cece0b{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.126 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f038248{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.128 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61cd1c71{/metrics/json,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.209 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-04 23:36:07.213 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-04 23:36:07.220 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d64c100{/SQL,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.221 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fdf17dc{/SQL/json,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.222 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d904ff1{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.222 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ff8a9dc{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.229 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71e35c4{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:36:07.798 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 115.584644 ms
2025-06-04 23:36:07.823 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 18.59259 ms
2025-06-04 23:36:08.759 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-04 23:36:08.787 [main INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-04 23:36:08.788 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:36:08.788 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:36:08.788 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749054968787
2025-06-04 23:36:08.942 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-04 23:36:08.944 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 23:36:08.944 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 23:36:08.944 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 23:36:08.945 [main INFO ] o.a.spark.sql.kafka010.KafkaRelation - GetBatch generating RDD of offset range: KafkaOffsetRange(clickstream-events-0,-2,-1,None), KafkaOffsetRange(clickstream-events-1,-2,-1,None)
2025-06-04 23:36:09.128 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 12.360152 ms
2025-06-04 23:36:09.229 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 4.737333 ms
2025-06-04 23:36:09.237 [main INFO ] org.apache.spark.SparkContext - Starting job: show at SparkClickstreamProcessor.java:212
2025-06-04 23:36:09.244 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (show at SparkClickstreamProcessor.java:212) with 1 output partitions
2025-06-04 23:36:09.245 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (show at SparkClickstreamProcessor.java:212)
2025-06-04 23:36:09.245 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-04 23:36:09.246 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-04 23:36:09.248 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[8] at show at SparkClickstreamProcessor.java:212), which has no missing parents
2025-06-04 23:36:09.307 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 37.2 KiB, free 9.2 GiB)
2025-06-04 23:36:09.323 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 15.4 KiB, free 9.2 GiB)
2025-06-04 23:36:09.325 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:46791 (size: 15.4 KiB, free: 9.2 GiB)
2025-06-04 23:36:09.328 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-04 23:36:09.338 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at show at SparkClickstreamProcessor.java:212) (first 15 tasks are for partitions Vector(0))
2025-06-04 23:36:09.339 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
2025-06-04 23:36:09.376 [dispatcher-event-loop-6 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes) 
2025-06-04 23:36:09.383 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-04 23:36:09.487 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-04 23:36:09.508 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-04 23:36:09.536 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:36:09.536 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:36:09.536 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749054969536
2025-06-04 23:36:09.537 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor-1, groupId=spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor] Assigned to partition(s): clickstream-events-0
2025-06-04 23:36:09.540 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor-1, groupId=spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-04 23:36:09.547 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor-1, groupId=spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-04 23:36:09.554 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor-1, groupId=spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:36:09.554 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor-1, groupId=spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-04 23:36:09.556 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor-1, groupId=spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:36:09.581 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 13.302534 ms
2025-06-04 23:36:09.590 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor-1, groupId=spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-04 23:36:09.610 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor-1, groupId=spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-04 23:36:10.110 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor-1, groupId=spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:36:10.110 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor-1, groupId=spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-04 23:36:10.111 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor-1, groupId=spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:36:10.185 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 19.943809 ms
2025-06-04 23:36:10.195 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2497 bytes result sent to driver
2025-06-04 23:36:10.201 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 833 ms on phamviethoa (executor driver) (1/1)
2025-06-04 23:36:10.202 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-04 23:36:10.206 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (show at SparkClickstreamProcessor.java:212) finished in 0.952 s
2025-06-04 23:36:10.208 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-04 23:36:10.209 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-04 23:36:10.210 [main INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: show at SparkClickstreamProcessor.java:212, took 0.972699 s
2025-06-04 23:36:10.217 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 3.627424 ms
2025-06-04 23:36:10.227 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor-1, groupId=spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-04 23:36:10.227 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor-1, groupId=spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-04 23:36:10.231 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 23:36:10.231 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 23:36:10.231 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-04 23:36:10.231 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 23:36:10.233 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-relation-0bc935de-81bc-44c5-933e-ac2def544483-executor-1 unregistered
2025-06-04 23:36:10.234 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-04 23:36:10.234 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-04 23:36:10.237 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@748e9b20{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:36:10.239 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-04 23:36:10.246 [dispatcher-event-loop-10 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-04 23:36:10.250 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-04 23:36:10.251 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-04 23:36:10.254 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-04 23:36:10.255 [dispatcher-event-loop-14 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-04 23:36:10.258 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-04 23:36:10.259 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-04 23:36:10.259 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-bee2eae3-94be-4ca9-a854-8fb83a21717c
2025-06-04 23:38:10.321 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-04 23:38:10.400 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-04 23:38:10.444 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:38:10.444 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-04 23:38:10.445 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:38:10.445 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-04 23:38:10.456 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-04 23:38:10.462 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-04 23:38:10.462 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-04 23:38:10.490 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-04 23:38:10.490 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-04 23:38:10.490 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-04 23:38:10.491 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-04 23:38:10.491 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-04 23:38:10.615 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 33657.
2025-06-04 23:38:10.628 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-04 23:38:10.644 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-04 23:38:10.653 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-04 23:38:10.654 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-04 23:38:10.656 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-04 23:38:10.667 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-a1805841-2d0a-4999-980b-2005a0308e7f
2025-06-04 23:38:10.687 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-04 23:38:10.695 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-04 23:38:10.715 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @956ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-04 23:38:10.761 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-04 23:38:10.767 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-04 23:38:10.775 [main INFO ] org.sparkproject.jetty.server.Server - Started @1016ms
2025-06-04 23:38:10.792 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@dd929b0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:38:10.792 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-04 23:38:10.803 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35835e65{/,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.852 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-04 23:38:10.856 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-04 23:38:10.867 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36857.
2025-06-04 23:38:10.868 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:36857
2025-06-04 23:38:10.869 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-04 23:38:10.872 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 36857, None)
2025-06-04 23:38:10.876 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:36857 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 36857, None)
2025-06-04 23:38:10.878 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 36857, None)
2025-06-04 23:38:10.878 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 36857, None)
2025-06-04 23:38:10.957 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@35835e65{/,null,STOPPED,@Spark}
2025-06-04 23:38:10.958 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/jobs,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.959 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/jobs/json,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.959 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cc9385{/jobs/job,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.960 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7915bca3{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.960 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/stages,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.960 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67b4ec{/stages/json,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.961 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/stages/stage,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.962 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.962 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/stages/pool,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.962 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.963 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/storage,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.963 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/storage/json,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.964 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.964 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.965 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/environment,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.965 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/environment/json,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.965 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/executors,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.966 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397ef2{/executors/json,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.966 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e93c1f{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.967 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9b21bd3{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.971 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7661b5a{/static,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.972 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b6d92e{/,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.973 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7899de11{/api,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.973 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f951a7f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.974 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c777e7b{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-04 23:38:10.976 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62db3891{/metrics/json,null,AVAILABLE,@Spark}
2025-06-04 23:38:11.060 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-04 23:38:11.064 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-04 23:38:11.071 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c83ae01{/SQL,null,AVAILABLE,@Spark}
2025-06-04 23:38:11.071 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69d45cca{/SQL/json,null,AVAILABLE,@Spark}
2025-06-04 23:38:11.072 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79c5460e{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-04 23:38:11.072 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f94e148{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-04 23:38:11.078 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f37b6d9{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:38:11.636 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 112.548414 ms
2025-06-04 23:38:11.660 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 17.894902 ms
2025-06-04 23:38:12.545 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-04 23:38:12.570 [main INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-04 23:38:12.571 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:38:12.571 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:38:12.571 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749055092570
2025-06-04 23:38:12.726 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-04 23:38:12.729 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 23:38:12.729 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 23:38:12.729 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 23:38:12.729 [main INFO ] o.a.spark.sql.kafka010.KafkaRelation - GetBatch generating RDD of offset range: KafkaOffsetRange(clickstream-events-0,-2,-1,None), KafkaOffsetRange(clickstream-events-1,-2,-1,None)
2025-06-04 23:38:12.912 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 11.015209 ms
2025-06-04 23:38:13.017 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.345016 ms
2025-06-04 23:38:13.027 [main INFO ] org.apache.spark.SparkContext - Starting job: show at SparkClickstreamProcessor.java:214
2025-06-04 23:38:13.035 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (show at SparkClickstreamProcessor.java:214) with 1 output partitions
2025-06-04 23:38:13.035 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (show at SparkClickstreamProcessor.java:214)
2025-06-04 23:38:13.035 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-04 23:38:13.036 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-04 23:38:13.038 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[9] at show at SparkClickstreamProcessor.java:214), which has no missing parents
2025-06-04 23:38:13.125 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 43.5 KiB, free 9.2 GiB)
2025-06-04 23:38:13.141 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 18.1 KiB, free 9.2 GiB)
2025-06-04 23:38:13.143 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:36857 (size: 18.1 KiB, free: 9.2 GiB)
2025-06-04 23:38:13.145 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-04 23:38:13.153 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[9] at show at SparkClickstreamProcessor.java:214) (first 15 tasks are for partitions Vector(0))
2025-06-04 23:38:13.154 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
2025-06-04 23:38:13.184 [dispatcher-event-loop-6 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes) 
2025-06-04 23:38:13.192 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-04 23:38:13.294 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-04 23:38:13.315 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-04 23:38:13.344 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:38:13.344 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:38:13.344 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749055093344
2025-06-04 23:38:13.346 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor-1, groupId=spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor] Assigned to partition(s): clickstream-events-0
2025-06-04 23:38:13.349 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor-1, groupId=spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-04 23:38:13.355 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor-1, groupId=spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-04 23:38:13.360 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor-1, groupId=spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:38:13.361 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor-1, groupId=spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-04 23:38:13.362 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor-1, groupId=spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:38:13.390 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 14.626925 ms
2025-06-04 23:38:13.414 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 13.138925 ms
2025-06-04 23:38:13.418 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor-1, groupId=spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-04 23:38:13.442 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor-1, groupId=spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-04 23:38:13.944 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor-1, groupId=spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:38:13.944 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor-1, groupId=spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-04 23:38:13.945 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor-1, groupId=spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:38:14.012 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 19.962362 ms
2025-06-04 23:38:14.040 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1612 bytes result sent to driver
2025-06-04 23:38:14.047 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 868 ms on phamviethoa (executor driver) (1/1)
2025-06-04 23:38:14.048 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-04 23:38:14.051 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (show at SparkClickstreamProcessor.java:214) finished in 1.004 s
2025-06-04 23:38:14.052 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-04 23:38:14.052 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-04 23:38:14.053 [main INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: show at SparkClickstreamProcessor.java:214, took 1.025799 s
2025-06-04 23:38:14.061 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 4.811863 ms
2025-06-04 23:38:14.068 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor-1, groupId=spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-04 23:38:14.068 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor-1, groupId=spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-04 23:38:14.071 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 23:38:14.071 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 23:38:14.071 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-04 23:38:14.071 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 23:38:14.074 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-relation-cd2f9014-84fe-4038-a68a-cbdc7629c68b-executor-1 unregistered
2025-06-04 23:38:14.074 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-04 23:38:14.074 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-04 23:38:14.078 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@dd929b0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:38:14.080 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-04 23:38:14.086 [dispatcher-event-loop-10 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-04 23:38:14.092 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-04 23:38:14.092 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-04 23:38:14.095 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-04 23:38:14.096 [dispatcher-event-loop-14 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-04 23:38:14.099 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-04 23:38:14.099 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-04 23:38:14.100 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-02e3046a-1b40-4214-a223-7e26dc7cbd36
2025-06-04 23:40:12.043 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-04 23:40:12.129 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-04 23:40:12.173 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:40:12.173 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-04 23:40:12.173 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:40:12.173 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-04 23:40:12.184 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-04 23:40:12.190 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-04 23:40:12.190 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-04 23:40:12.217 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-04 23:40:12.218 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-04 23:40:12.218 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-04 23:40:12.218 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-04 23:40:12.218 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-04 23:40:12.336 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 38815.
2025-06-04 23:40:12.350 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-04 23:40:12.366 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-04 23:40:12.375 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-04 23:40:12.376 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-04 23:40:12.378 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-04 23:40:12.389 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-b622beee-c250-4900-ac8e-062326599e78
2025-06-04 23:40:12.407 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-04 23:40:12.417 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-04 23:40:12.436 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @958ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-04 23:40:12.478 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-04 23:40:12.483 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-04 23:40:12.492 [main INFO ] org.sparkproject.jetty.server.Server - Started @1015ms
2025-06-04 23:40:12.507 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@1234ed04{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:40:12.507 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-04 23:40:12.518 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1fde4f40{/,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.563 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-04 23:40:12.567 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-04 23:40:12.578 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40841.
2025-06-04 23:40:12.578 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:40841
2025-06-04 23:40:12.579 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-04 23:40:12.583 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 40841, None)
2025-06-04 23:40:12.586 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:40841 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 40841, None)
2025-06-04 23:40:12.587 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 40841, None)
2025-06-04 23:40:12.588 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 40841, None)
2025-06-04 23:40:12.663 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1fde4f40{/,null,STOPPED,@Spark}
2025-06-04 23:40:12.664 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c9320c2{/jobs,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.664 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cc9385{/jobs/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.665 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/jobs/job,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.665 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67b4ec{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.666 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f91da5e{/stages,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.666 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79fd6f95{/stages/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.667 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/stages/stage,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.668 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.668 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/stages/pool,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.669 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.669 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/storage,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.670 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/storage/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.670 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.671 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.672 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/environment,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.672 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397ef2{/environment/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.672 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e93c1f{/executors,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.673 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9b21bd3{/executors/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.673 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7661b5a{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.674 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65c33b92{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.678 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e08acf9{/static,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.679 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bc0d349{/,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.679 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5292ceca{/api,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.680 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78e22d35{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.680 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59f93db8{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.682 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52e04737{/metrics/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.762 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-04 23:40:12.765 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-04 23:40:12.772 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e6d4780{/SQL,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.773 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e73d5eb{/SQL/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.773 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2cde651b{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.774 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3bb87d36{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:12.780 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@503df2d0{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:40:13.320 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 105.887491 ms
2025-06-04 23:40:13.340 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 14.155194 ms
2025-06-04 23:40:14.229 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-04 23:40:14.253 [main INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-04 23:40:14.254 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:40:14.254 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:40:14.254 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749055214253
2025-06-04 23:40:14.390 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-04 23:40:14.393 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 23:40:14.393 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 23:40:14.393 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 23:40:14.394 [main INFO ] o.a.spark.sql.kafka010.KafkaRelation - GetBatch generating RDD of offset range: KafkaOffsetRange(clickstream-events-0,-2,-1,None), KafkaOffsetRange(clickstream-events-1,-2,-1,None)
2025-06-04 23:40:14.583 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.95608 ms
2025-06-04 23:40:14.682 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 4.585708 ms
2025-06-04 23:40:14.689 [main INFO ] org.apache.spark.SparkContext - Starting job: show at SparkClickstreamProcessor.java:212
2025-06-04 23:40:14.697 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (show at SparkClickstreamProcessor.java:212) with 1 output partitions
2025-06-04 23:40:14.697 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (show at SparkClickstreamProcessor.java:212)
2025-06-04 23:40:14.697 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-04 23:40:14.698 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-04 23:40:14.700 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[8] at show at SparkClickstreamProcessor.java:212), which has no missing parents
2025-06-04 23:40:14.755 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 37.2 KiB, free 9.2 GiB)
2025-06-04 23:40:14.768 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 15.4 KiB, free 9.2 GiB)
2025-06-04 23:40:14.770 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:40841 (size: 15.4 KiB, free: 9.2 GiB)
2025-06-04 23:40:14.772 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-04 23:40:14.780 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at show at SparkClickstreamProcessor.java:212) (first 15 tasks are for partitions Vector(0))
2025-06-04 23:40:14.780 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
2025-06-04 23:40:14.807 [dispatcher-event-loop-6 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes) 
2025-06-04 23:40:14.814 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-04 23:40:14.904 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-04 23:40:14.922 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-04 23:40:14.947 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:40:14.948 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:40:14.948 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749055214947
2025-06-04 23:40:14.949 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor-1, groupId=spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor] Assigned to partition(s): clickstream-events-0
2025-06-04 23:40:14.951 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor-1, groupId=spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-04 23:40:14.958 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor-1, groupId=spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-04 23:40:14.962 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor-1, groupId=spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:40:14.963 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor-1, groupId=spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-04 23:40:14.964 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor-1, groupId=spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:40:14.986 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.806458 ms
2025-06-04 23:40:14.992 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor-1, groupId=spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-04 23:40:15.011 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor-1, groupId=spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-04 23:40:15.511 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor-1, groupId=spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:40:15.511 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor-1, groupId=spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-04 23:40:15.512 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor-1, groupId=spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:40:15.580 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 20.501708 ms
2025-06-04 23:40:15.591 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2497 bytes result sent to driver
2025-06-04 23:40:15.596 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 794 ms on phamviethoa (executor driver) (1/1)
2025-06-04 23:40:15.597 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-04 23:40:15.600 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (show at SparkClickstreamProcessor.java:212) finished in 0.895 s
2025-06-04 23:40:15.603 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-04 23:40:15.603 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-04 23:40:15.604 [main INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: show at SparkClickstreamProcessor.java:212, took 0.914339 s
2025-06-04 23:40:15.611 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 3.920625 ms
2025-06-04 23:40:15.621 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor-1, groupId=spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-04 23:40:15.621 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor-1, groupId=spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-04 23:40:15.625 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 23:40:15.625 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 23:40:15.625 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-04 23:40:15.626 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 23:40:15.629 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-relation-d6ecb068-7404-4e66-bb1b-92ef9d1948a8-executor-1 unregistered
2025-06-04 23:40:15.630 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-04 23:40:15.630 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-04 23:40:15.634 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@1234ed04{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:40:15.636 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-04 23:40:15.644 [dispatcher-event-loop-10 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-04 23:40:15.652 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-04 23:40:15.652 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-04 23:40:15.657 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-04 23:40:15.658 [dispatcher-event-loop-14 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-04 23:40:15.662 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-04 23:40:15.663 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-04 23:40:15.663 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-c0ea572d-ba05-444c-ae61-772392e92e93
2025-06-04 23:40:51.268 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-04 23:40:51.348 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-04 23:40:51.392 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:40:51.392 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-04 23:40:51.392 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:40:51.392 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-04 23:40:51.403 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-04 23:40:51.409 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-04 23:40:51.409 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-04 23:40:51.437 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-04 23:40:51.437 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-04 23:40:51.437 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-04 23:40:51.437 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-04 23:40:51.437 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-04 23:40:51.559 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 34363.
2025-06-04 23:40:51.575 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-04 23:40:51.598 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-04 23:40:51.613 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-04 23:40:51.614 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-04 23:40:51.616 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-04 23:40:51.626 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-188efa21-ecc9-4d04-966f-780add1e4202
2025-06-04 23:40:51.643 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-04 23:40:51.651 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-04 23:40:51.671 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1000ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-04 23:40:51.714 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-04 23:40:51.719 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-04 23:40:51.728 [main INFO ] org.sparkproject.jetty.server.Server - Started @1057ms
2025-06-04 23:40:51.745 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@d884a8{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:40:51.745 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-04 23:40:51.757 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@289fdb08{/,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.811 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-04 23:40:51.817 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-04 23:40:51.831 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41329.
2025-06-04 23:40:51.832 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:41329
2025-06-04 23:40:51.833 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-04 23:40:51.837 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 41329, None)
2025-06-04 23:40:51.840 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:41329 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 41329, None)
2025-06-04 23:40:51.842 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 41329, None)
2025-06-04 23:40:51.844 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 41329, None)
2025-06-04 23:40:51.931 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@289fdb08{/,null,STOPPED,@Spark}
2025-06-04 23:40:51.932 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@304d0259{/jobs,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.933 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133661d{/jobs/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.934 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cf518cf{/jobs/job,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.935 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68d651f2{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.935 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e43e323{/stages,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.936 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10643593{/stages/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.937 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14823f76{/stages/stage,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.937 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ed16657{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.938 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@113e13f9{/stages/pool,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.938 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7979b8b7{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.938 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bc49bc5{/storage,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.939 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f66ffc8{/storage/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.939 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2def7a7a{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.940 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c080ef3{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.940 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ee6291f{/environment,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.941 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37e0292a{/environment/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.941 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35267fd4{/executors,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.942 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36a6bea6{/executors/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.942 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42373389{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.943 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a62c7cd{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.947 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c36db44{/static,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.948 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33899f7a{/,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.949 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@290d10ef{/api,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.950 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20cece0b{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.950 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f038248{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-04 23:40:51.952 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61cd1c71{/metrics/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:52.037 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-04 23:40:52.041 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-04 23:40:52.048 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d64c100{/SQL,null,AVAILABLE,@Spark}
2025-06-04 23:40:52.049 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fdf17dc{/SQL/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:52.049 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d904ff1{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-04 23:40:52.050 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ff8a9dc{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-04 23:40:52.056 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71e35c4{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:40:52.617 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 111.805889 ms
2025-06-04 23:40:52.645 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 19.122539 ms
2025-06-04 23:40:53.562 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-04 23:40:53.585 [main INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-04 23:40:53.586 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:40:53.586 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:40:53.586 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749055253585
2025-06-04 23:40:53.726 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-04 23:40:53.729 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 23:40:53.729 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 23:40:53.729 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 23:40:53.729 [main INFO ] o.a.spark.sql.kafka010.KafkaRelation - GetBatch generating RDD of offset range: KafkaOffsetRange(clickstream-events-0,-2,-1,None), KafkaOffsetRange(clickstream-events-1,-2,-1,None)
2025-06-04 23:40:53.920 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.25901 ms
2025-06-04 23:40:54.023 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 4.85079 ms
2025-06-04 23:40:54.031 [main INFO ] org.apache.spark.SparkContext - Starting job: show at SparkClickstreamProcessor.java:212
2025-06-04 23:40:54.040 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (show at SparkClickstreamProcessor.java:212) with 1 output partitions
2025-06-04 23:40:54.040 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (show at SparkClickstreamProcessor.java:212)
2025-06-04 23:40:54.040 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-04 23:40:54.041 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-04 23:40:54.043 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[8] at show at SparkClickstreamProcessor.java:212), which has no missing parents
2025-06-04 23:40:54.110 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 37.2 KiB, free 9.2 GiB)
2025-06-04 23:40:54.125 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 15.4 KiB, free 9.2 GiB)
2025-06-04 23:40:54.127 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:41329 (size: 15.4 KiB, free: 9.2 GiB)
2025-06-04 23:40:54.130 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-04 23:40:54.138 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at show at SparkClickstreamProcessor.java:212) (first 15 tasks are for partitions Vector(0))
2025-06-04 23:40:54.139 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
2025-06-04 23:40:54.170 [dispatcher-event-loop-6 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes) 
2025-06-04 23:40:54.178 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-04 23:40:54.274 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-04 23:40:54.295 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-04 23:40:54.332 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:40:54.332 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:40:54.332 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749055254332
2025-06-04 23:40:54.334 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor-1, groupId=spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor] Assigned to partition(s): clickstream-events-0
2025-06-04 23:40:54.337 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor-1, groupId=spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-04 23:40:54.344 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor-1, groupId=spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-04 23:40:54.350 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor-1, groupId=spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:40:54.350 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor-1, groupId=spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-04 23:40:54.352 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor-1, groupId=spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:40:54.380 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 14.136199 ms
2025-06-04 23:40:54.389 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor-1, groupId=spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-04 23:40:54.413 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor-1, groupId=spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-04 23:40:54.914 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor-1, groupId=spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:40:54.915 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor-1, groupId=spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-04 23:40:54.915 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor-1, groupId=spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:40:54.989 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 21.409491 ms
2025-06-04 23:40:54.999 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2986 bytes result sent to driver
2025-06-04 23:40:55.005 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 841 ms on phamviethoa (executor driver) (1/1)
2025-06-04 23:40:55.006 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-04 23:40:55.009 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (show at SparkClickstreamProcessor.java:212) finished in 0.959 s
2025-06-04 23:40:55.010 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-04 23:40:55.011 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-04 23:40:55.011 [main INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: show at SparkClickstreamProcessor.java:212, took 0.980207 s
2025-06-04 23:40:55.019 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 3.720572 ms
2025-06-04 23:40:55.028 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor-1, groupId=spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-04 23:40:55.028 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor-1, groupId=spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-04 23:40:55.031 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 23:40:55.031 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 23:40:55.031 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-04 23:40:55.031 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 23:40:55.034 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-relation-42603203-42f2-4a94-8497-326cff06e0f8-executor-1 unregistered
2025-06-04 23:40:55.034 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-04 23:40:55.034 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-04 23:40:55.038 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@d884a8{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:40:55.040 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-04 23:40:55.046 [dispatcher-event-loop-10 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-04 23:40:55.052 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-04 23:40:55.052 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-04 23:40:55.054 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-04 23:40:55.056 [dispatcher-event-loop-14 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-04 23:40:55.059 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-04 23:40:55.059 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-04 23:40:55.060 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-e1e7fc5d-5e35-464b-9bae-8308d569b64f
2025-06-04 23:43:30.398 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-04 23:43:30.489 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-04 23:43:30.531 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:43:30.532 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-04 23:43:30.532 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:43:30.532 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-04 23:43:30.543 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-04 23:43:30.549 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-04 23:43:30.550 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-04 23:43:30.578 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-04 23:43:30.578 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-04 23:43:30.578 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-04 23:43:30.578 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-04 23:43:30.578 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-04 23:43:30.700 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 42611.
2025-06-04 23:43:30.714 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-04 23:43:30.731 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-04 23:43:30.739 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-04 23:43:30.740 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-04 23:43:30.742 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-04 23:43:30.751 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-feeb3fb2-f099-4cd8-9e2f-fdf543caae81
2025-06-04 23:43:30.767 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-04 23:43:30.776 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-04 23:43:30.795 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @955ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-04 23:43:30.838 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-04 23:43:30.844 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-04 23:43:30.852 [main INFO ] org.sparkproject.jetty.server.Server - Started @1012ms
2025-06-04 23:43:30.867 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@50f40653{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:43:30.867 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-04 23:43:30.878 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1fde4f40{/,null,AVAILABLE,@Spark}
2025-06-04 23:43:30.922 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-04 23:43:30.927 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-04 23:43:30.940 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36015.
2025-06-04 23:43:30.940 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:36015
2025-06-04 23:43:30.941 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-04 23:43:30.945 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 36015, None)
2025-06-04 23:43:30.948 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:36015 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 36015, None)
2025-06-04 23:43:30.950 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 36015, None)
2025-06-04 23:43:30.951 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 36015, None)
2025-06-04 23:43:31.030 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1fde4f40{/,null,STOPPED,@Spark}
2025-06-04 23:43:31.031 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c9320c2{/jobs,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.032 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cc9385{/jobs/json,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.032 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/jobs/job,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.032 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67b4ec{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.033 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f91da5e{/stages,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.033 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79fd6f95{/stages/json,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.034 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/stages/stage,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.035 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.035 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/stages/pool,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.035 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.036 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/storage,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.036 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/storage/json,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.037 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.037 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.038 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/environment,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.039 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397ef2{/environment/json,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.039 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e93c1f{/executors,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.040 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9b21bd3{/executors/json,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.040 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7661b5a{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.041 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65c33b92{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.046 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e08acf9{/static,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.046 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bc0d349{/,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.047 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5292ceca{/api,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.047 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78e22d35{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.048 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59f93db8{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.050 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52e04737{/metrics/json,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.130 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-04 23:43:31.134 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-04 23:43:31.141 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e6d4780{/SQL,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.141 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e73d5eb{/SQL/json,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.142 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2cde651b{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.142 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3bb87d36{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.148 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@503df2d0{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:43:31.687 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 100.578676 ms
2025-06-04 23:43:31.707 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 14.348735 ms
2025-06-04 23:43:32.594 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-04 23:43:32.621 [main INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-04 23:43:32.622 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:43:32.622 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:43:32.622 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749055412621
2025-06-04 23:43:32.767 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-04 23:43:32.769 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 23:43:32.769 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 23:43:32.769 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 23:43:32.770 [main INFO ] o.a.spark.sql.kafka010.KafkaRelation - GetBatch generating RDD of offset range: KafkaOffsetRange(clickstream-events-0,-2,-1,None), KafkaOffsetRange(clickstream-events-1,-2,-1,None)
2025-06-04 23:43:32.964 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.288573 ms
2025-06-04 23:43:33.068 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 5.073351 ms
2025-06-04 23:43:33.076 [main INFO ] org.apache.spark.SparkContext - Starting job: show at SparkClickstreamProcessor.java:232
2025-06-04 23:43:33.084 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (show at SparkClickstreamProcessor.java:232) with 1 output partitions
2025-06-04 23:43:33.084 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (show at SparkClickstreamProcessor.java:232)
2025-06-04 23:43:33.084 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-04 23:43:33.085 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-04 23:43:33.087 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[8] at show at SparkClickstreamProcessor.java:232), which has no missing parents
2025-06-04 23:43:33.146 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 37.2 KiB, free 9.2 GiB)
2025-06-04 23:43:33.162 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 15.4 KiB, free 9.2 GiB)
2025-06-04 23:43:33.164 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:36015 (size: 15.4 KiB, free: 9.2 GiB)
2025-06-04 23:43:33.166 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-04 23:43:33.175 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at show at SparkClickstreamProcessor.java:232) (first 15 tasks are for partitions Vector(0))
2025-06-04 23:43:33.176 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
2025-06-04 23:43:33.214 [dispatcher-event-loop-6 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes) 
2025-06-04 23:43:33.223 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-04 23:43:33.342 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-04 23:43:33.366 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-04 23:43:33.397 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:43:33.397 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:43:33.397 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749055413397
2025-06-04 23:43:33.399 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor-1, groupId=spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor] Assigned to partition(s): clickstream-events-0
2025-06-04 23:43:33.402 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor-1, groupId=spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-04 23:43:33.410 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor-1, groupId=spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-04 23:43:33.416 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor-1, groupId=spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:43:33.417 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor-1, groupId=spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-04 23:43:33.418 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor-1, groupId=spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:43:33.450 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 17.44895 ms
2025-06-04 23:43:33.456 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor-1, groupId=spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-04 23:43:33.476 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor-1, groupId=spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-04 23:43:33.976 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor-1, groupId=spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:43:33.977 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor-1, groupId=spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-04 23:43:33.977 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor-1, groupId=spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:43:34.057 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 23.787104 ms
2025-06-04 23:43:34.069 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2986 bytes result sent to driver
2025-06-04 23:43:34.075 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 867 ms on phamviethoa (executor driver) (1/1)
2025-06-04 23:43:34.076 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-04 23:43:34.079 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (show at SparkClickstreamProcessor.java:232) finished in 0.987 s
2025-06-04 23:43:34.081 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-04 23:43:34.081 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-04 23:43:34.082 [main INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: show at SparkClickstreamProcessor.java:232, took 1.006144 s
2025-06-04 23:43:34.093 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.707153 ms
2025-06-04 23:43:34.103 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor-1, groupId=spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-04 23:43:34.103 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor-1, groupId=spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-04 23:43:34.108 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 23:43:34.108 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 23:43:34.108 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-04 23:43:34.108 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 23:43:34.110 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-relation-473ce1cc-ed09-4d30-b9db-c8be62c8751c-executor-1 unregistered
2025-06-04 23:43:34.111 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-04 23:43:34.111 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-04 23:43:34.115 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@50f40653{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:43:34.118 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-04 23:43:34.126 [dispatcher-event-loop-10 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-04 23:43:34.132 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-04 23:43:34.132 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-04 23:43:34.135 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-04 23:43:34.136 [dispatcher-event-loop-14 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-04 23:43:34.140 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-04 23:43:34.140 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-04 23:43:34.140 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-72739da5-1403-442e-a4b5-f24c4f323bf5
2025-06-04 23:44:08.899 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-04 23:44:08.981 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-04 23:44:09.027 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:44:09.027 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-04 23:44:09.027 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-04 23:44:09.028 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-04 23:44:09.040 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-04 23:44:09.046 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-04 23:44:09.047 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-04 23:44:09.076 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-04 23:44:09.076 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-04 23:44:09.077 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-04 23:44:09.077 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-04 23:44:09.077 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-04 23:44:09.198 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 39889.
2025-06-04 23:44:09.215 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-04 23:44:09.235 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-04 23:44:09.245 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-04 23:44:09.246 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-04 23:44:09.249 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-04 23:44:09.259 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-19b922ab-027c-4e95-8465-0978a9b546be
2025-06-04 23:44:09.277 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-04 23:44:09.286 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-04 23:44:09.305 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @977ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-04 23:44:09.349 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-04 23:44:09.354 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-04 23:44:09.364 [main INFO ] org.sparkproject.jetty.server.Server - Started @1036ms
2025-06-04 23:44:09.380 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@47ae103b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:44:09.380 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-04 23:44:09.393 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7918c7f8{/,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.439 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-04 23:44:09.443 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-04 23:44:09.460 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37095.
2025-06-04 23:44:09.460 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:37095
2025-06-04 23:44:09.462 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-04 23:44:09.465 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 37095, None)
2025-06-04 23:44:09.468 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:37095 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 37095, None)
2025-06-04 23:44:09.470 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 37095, None)
2025-06-04 23:44:09.471 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 37095, None)
2025-06-04 23:44:09.548 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7918c7f8{/,null,STOPPED,@Spark}
2025-06-04 23:44:09.548 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1894e40d{/jobs,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.549 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7342e05d{/jobs/json,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.550 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@109a2025{/jobs/job,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.551 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@761956ac{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.551 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@304d0259{/stages,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.552 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133661d{/stages/json,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.553 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68d651f2{/stages/stage,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.553 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e43e323{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.554 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10643593{/stages/pool,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.554 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@eca6a74{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.555 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48840594{/storage,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.555 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14823f76{/storage/json,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.556 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ed16657{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.557 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@113e13f9{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.557 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7979b8b7{/environment,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.558 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bc49bc5{/environment/json,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.558 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f66ffc8{/executors,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.558 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2def7a7a{/executors/json,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.559 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c080ef3{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.560 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ee6291f{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.564 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37e0292a{/static,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.565 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60cf62ad{/,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.566 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ac4ccad{/api,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.566 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13d9261f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.566 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5300cac{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.569 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@292158f8{/metrics/json,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.655 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-04 23:44:09.659 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-04 23:44:09.666 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@201c3cda{/SQL,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.667 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d97caa4{/SQL/json,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.667 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4190bc8a{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.668 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c83ae01{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-04 23:44:09.674 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79c5460e{/static/sql,null,AVAILABLE,@Spark}
2025-06-04 23:44:10.271 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 122.890773 ms
2025-06-04 23:44:10.294 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 16.694443 ms
2025-06-04 23:44:11.209 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-04 23:44:11.234 [main INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-04 23:44:11.235 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:44:11.235 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:44:11.235 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749055451234
2025-06-04 23:44:11.385 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-04 23:44:11.388 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 23:44:11.388 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 23:44:11.388 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 23:44:11.388 [main INFO ] o.a.spark.sql.kafka010.KafkaRelation - GetBatch generating RDD of offset range: KafkaOffsetRange(clickstream-events-0,-2,-1,None), KafkaOffsetRange(clickstream-events-1,-2,-1,None)
2025-06-04 23:44:11.566 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.914846 ms
2025-06-04 23:44:11.665 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.449712 ms
2025-06-04 23:44:11.674 [main INFO ] org.apache.spark.SparkContext - Starting job: show at SparkClickstreamProcessor.java:235
2025-06-04 23:44:11.685 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (show at SparkClickstreamProcessor.java:235) with 1 output partitions
2025-06-04 23:44:11.686 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (show at SparkClickstreamProcessor.java:235)
2025-06-04 23:44:11.686 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-04 23:44:11.687 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-04 23:44:11.690 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[9] at show at SparkClickstreamProcessor.java:235), which has no missing parents
2025-06-04 23:44:11.763 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 45.1 KiB, free 9.2 GiB)
2025-06-04 23:44:11.779 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 18.4 KiB, free 9.2 GiB)
2025-06-04 23:44:11.781 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:37095 (size: 18.4 KiB, free: 9.2 GiB)
2025-06-04 23:44:11.784 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-04 23:44:11.793 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[9] at show at SparkClickstreamProcessor.java:235) (first 15 tasks are for partitions Vector(0))
2025-06-04 23:44:11.793 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
2025-06-04 23:44:11.827 [dispatcher-event-loop-6 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes) 
2025-06-04 23:44:11.835 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-04 23:44:11.925 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-04 23:44:11.944 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-04 23:44:11.970 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-04 23:44:11.970 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-04 23:44:11.970 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749055451970
2025-06-04 23:44:11.971 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor-1, groupId=spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor] Assigned to partition(s): clickstream-events-0
2025-06-04 23:44:11.973 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor-1, groupId=spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-04 23:44:11.980 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor-1, groupId=spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-04 23:44:11.986 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor-1, groupId=spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:44:11.986 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor-1, groupId=spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-04 23:44:11.987 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor-1, groupId=spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:44:12.016 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 16.19612 ms
2025-06-04 23:44:12.055 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 24.874961 ms
2025-06-04 23:44:12.060 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor-1, groupId=spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-04 23:44:12.085 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor-1, groupId=spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-04 23:44:12.586 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor-1, groupId=spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:44:12.587 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor-1, groupId=spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-04 23:44:12.587 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor-1, groupId=spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-04 23:44:12.661 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 24.70693 ms
2025-06-04 23:44:12.699 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2473 bytes result sent to driver
2025-06-04 23:44:12.705 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 884 ms on phamviethoa (executor driver) (1/1)
2025-06-04 23:44:12.706 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-04 23:44:12.709 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (show at SparkClickstreamProcessor.java:235) finished in 1.012 s
2025-06-04 23:44:12.710 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-04 23:44:12.711 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-04 23:44:12.712 [main INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: show at SparkClickstreamProcessor.java:235, took 1.037460 s
2025-06-04 23:44:12.721 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 4.768358 ms
2025-06-04 23:44:12.730 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor-1, groupId=spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-04 23:44:12.730 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor-1, groupId=spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-04 23:44:12.733 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-04 23:44:12.733 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-04 23:44:12.733 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-04 23:44:12.733 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-04 23:44:12.736 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-relation-29c12aca-3b20-4abc-82be-f782c6e82302-executor-1 unregistered
2025-06-04 23:44:12.736 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-04 23:44:12.736 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-04 23:44:12.741 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@47ae103b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-04 23:44:12.743 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-04 23:44:12.750 [dispatcher-event-loop-10 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-04 23:44:12.755 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-04 23:44:12.756 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-04 23:44:12.758 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-04 23:44:12.759 [dispatcher-event-loop-14 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-04 23:44:12.762 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-04 23:44:12.762 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-04 23:44:12.762 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-2396a3bf-7823-40ae-990d-2adbf023bd0d
