2025-06-12 08:31:06.308 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 66980 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-12 08:31:06.310 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-12 08:31:06.925 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-12 08:31:06.929 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-12 08:31:06.930 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-12 08:31:06.930 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-12 08:31:06.981 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-12 08:31:06.981 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 647 ms
2025-06-12 08:31:07.148 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-12 08:31:07.302 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-12 08:31:07.320 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-12 08:31:07.344 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:31:07.345 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:31:07.345 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749691867344
2025-06-12 08:31:07.595 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-12 08:31:07.597 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:31:07.597 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:31:07.597 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:31:07.602 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-12 08:31:07.606 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-12 08:31:07.615 [main INFO ] com.example.Application - Started Application in 1.539 seconds (JVM running for 1.907)
2025-06-12 08:31:07.684 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-12 08:31:07.684 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-12 08:31:07.685 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-12 08:31:22.357 [http-nio-8080-exec-1 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=d83533d5-f786-4074-b050-9b4e5262c7f1, event_name=page_view, event_time=2025-06-12T01:30:46.909Z, user_id=user_pej5ild, session_id=session_ohf3q0q, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/}, {event_id=1d0a45e4-0d9f-4223-8853-1ba2efda2854, event_name=scroll, event_time=2025-06-12T01:30:56.238Z, user_id=user_pej5ild, session_id=session_ohf3q0q, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=1}, {event_id=089d26bb-2870-4f76-929d-70e6a1da4de4, event_name=scroll, event_time=2025-06-12T01:30:57.239Z, user_id=user_pej5ild, session_id=session_ohf3q0q, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=39}, {event_id=21acf071-01b1-4ef6-8447-d2caa317bca8, event_name=click, event_time=2025-06-12T01:31:22.312Z, user_id=user_pej5ild, session_id=session_ohf3q0q, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 3
                        Customer favorit, element_type=div, element_name=null, track=product_click, productId=3}]}
2025-06-12 08:31:22.358 [http-nio-8080-exec-1 INFO ] c.e.controller.ClickstreamController - Received 4 events
2025-06-12 08:31:22.365 [http-nio-8080-exec-1 INFO ] o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 1000
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-06-12 08:31:22.372 [http-nio-8080-exec-1 INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-12 08:31:22.375 [http-nio-8080-exec-1 INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-06-12 08:31:22.385 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:31:22.385 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:31:22.385 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749691882385
2025-06-12 08:31:22.401 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {clickstream-events=LEADER_NOT_AVAILABLE}
2025-06-12 08:31:22.402 [kafka-producer-network-thread | producer-1 INFO ] org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: 8UN1o2zlTiO_QDl5CHzccg
2025-06-12 08:31:22.421 [kafka-producer-network-thread | producer-1 INFO ] o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 0 with epoch 0
2025-06-12 08:31:23.411 [http-nio-8080-exec-1 INFO ] c.e.controller.ClickstreamController - Successfully processed 4 events
2025-06-12 08:31:25.247 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=8c400fbc-ad0c-42e2-a60e-f6bfd30feab4, event_name=click, event_time=2025-06-12T01:31:23.136Z, user_id=user_pej5ild, session_id=session_ohf3q0q, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 2
                        Durable and styl, element_type=div, element_name=null, track=product_click, productId=2}, {event_id=c6250292-4c07-41f9-bbbf-4dfedad3a1b3, event_name=click, event_time=2025-06-12T01:31:24.320Z, user_id=user_pej5ild, session_id=session_ohf3q0q, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 1
                        High-quality ite, element_type=div, element_name=null, track=product_click, productId=1}, {event_id=c33c9a6c-3937-420f-b0cd-48a160546bf6, event_name=click, event_time=2025-06-12T01:31:25.243Z, user_id=user_pej5ild, session_id=session_ohf3q0q, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 4
                        Reliable and aff, element_type=div, element_name=null, track=product_click, productId=4}]}
2025-06-12 08:31:25.247 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 08:31:25.248 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 08:31:27.522 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=a7654607-760c-4a57-8073-f2cc7c83a629, event_name=click, event_time=2025-06-12T01:31:25.810Z, user_id=user_pej5ild, session_id=session_ohf3q0q, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 5
                        Bestseller with , element_type=div, element_name=null, track=product_click, productId=5}, {event_id=6aeb8401-2b8b-4b80-bb2d-77f55dc7f6e5, event_name=click, event_time=2025-06-12T01:31:26.308Z, user_id=user_pej5ild, session_id=session_ohf3q0q, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 6
                        Sleek design and, element_type=div, element_name=null, track=product_click, productId=6}, {event_id=fde595ad-cb79-4827-bd57-6d5db442a622, event_name=click, event_time=2025-06-12T01:31:27.519Z, user_id=user_pej5ild, session_id=session_ohf3q0q, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 9
                        Top-rated with e, element_type=div, element_name=null, track=product_click, productId=9}]}
2025-06-12 08:31:27.523 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 08:31:27.523 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 08:31:28.677 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=21cf4d6f-a5a5-4244-9cc7-a623b693305c, event_name=scroll, event_time=2025-06-12T01:31:27.806Z, user_id=user_pej5ild, session_id=session_ohf3q0q, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=100}, {event_id=5461fa56-01da-4f48-8784-bcc2b4f84ce6, event_name=click, event_time=2025-06-12T01:31:28.142Z, user_id=user_pej5ild, session_id=session_ohf3q0q, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 8
                        Eco-friendly and, element_type=div, element_name=null, track=product_click, productId=8}, {event_id=61109000-1674-497e-a90f-07e037791cc0, event_name=click, event_time=2025-06-12T01:31:28.673Z, user_id=user_pej5ild, session_id=session_ohf3q0q, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 7
                        Compact and effi, element_type=div, element_name=null, track=product_click, productId=7}]}
2025-06-12 08:31:28.678 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 08:31:28.679 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 08:31:31.409 [http-nio-8080-exec-5 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=167fddee-d2eb-4893-8e20-dbfb2227b358, event_name=scroll, event_time=2025-06-12T01:31:29.406Z, user_id=user_pej5ild, session_id=session_ohf3q0q, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=99}, {event_id=5910f0c0-c5f5-4403-b49d-14042d94d167, event_name=scroll, event_time=2025-06-12T01:31:30.406Z, user_id=user_pej5ild, session_id=session_ohf3q0q, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=47}, {event_id=ffbb5820-00e0-4038-ae53-92b039c52921, event_name=scroll, event_time=2025-06-12T01:31:31.406Z, user_id=user_pej5ild, session_id=session_ohf3q0q, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=0}]}
2025-06-12 08:31:31.410 [http-nio-8080-exec-5 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 08:31:31.411 [http-nio-8080-exec-5 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 08:38:57.188 [SpringApplicationShutdownHook INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-06-12 08:38:57.191 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:38:57.191 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:38:57.191 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-12 08:38:57.191 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:38:57.191 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-1 unregistered
2025-06-12 08:39:00.560 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 73359 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-12 08:39:00.562 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-12 08:39:01.044 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-12 08:39:01.047 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-12 08:39:01.047 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-12 08:39:01.048 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-12 08:39:01.092 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-12 08:39:01.092 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 510 ms
2025-06-12 08:39:01.237 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-12 08:39:01.390 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-12 08:39:01.403 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-12 08:39:01.417 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:39:01.417 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:39:01.417 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749692341416
2025-06-12 08:39:01.501 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-12 08:39:01.503 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:39:01.503 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:39:01.503 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:39:01.508 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-12 08:39:01.512 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-12 08:39:01.521 [main INFO ] com.example.Application - Started Application in 1.126 seconds (JVM running for 1.415)
2025-06-12 08:39:01.725 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-12 08:39:01.726 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-12 08:39:01.726 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 0 ms
2025-06-12 08:39:01.758 [Thread-1 INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-12 08:39:01.839 [Thread-1 WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-12 08:39:01.883 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-12 08:39:01.884 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-12 08:39:01.884 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-12 08:39:01.884 [Thread-1 INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-12 08:39:01.897 [Thread-1 INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-12 08:39:01.903 [Thread-1 INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-12 08:39:01.903 [Thread-1 INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-12 08:39:01.930 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-12 08:39:01.930 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-12 08:39:01.930 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-12 08:39:01.930 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-12 08:39:01.930 [Thread-1 INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-12 08:39:02.042 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 43521.
2025-06-12 08:39:02.090 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-12 08:39:02.105 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-12 08:39:02.115 [Thread-1 INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-12 08:39:02.115 [Thread-1 INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-12 08:39:02.117 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-12 08:39:02.124 [Thread-1 INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-289dbf95-d96f-4440-b20a-607816ca2a97
2025-06-12 08:39:02.151 [Thread-1 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-12 08:39:02.158 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-12 08:39:02.169 [Thread-1 INFO ] org.sparkproject.jetty.util.log - Logging initialized @2064ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-12 08:39:02.212 [Thread-1 INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-12 08:39:02.218 [Thread-1 INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-12 08:39:02.226 [Thread-1 INFO ] org.sparkproject.jetty.server.Server - Started @2121ms
2025-06-12 08:39:02.238 [Thread-1 INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@55a6989{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-12 08:39:02.238 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-12 08:39:02.246 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6296f1e7{/,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.276 [Thread-1 INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-12 08:39:02.279 [Thread-1 INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-12 08:39:02.288 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42365.
2025-06-12 08:39:02.288 [Thread-1 INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:42365
2025-06-12 08:39:02.289 [Thread-1 INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-12 08:39:02.293 [Thread-1 INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 42365, None)
2025-06-12 08:39:02.295 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:42365 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 42365, None)
2025-06-12 08:39:02.297 [Thread-1 INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 42365, None)
2025-06-12 08:39:02.297 [Thread-1 INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 42365, None)
2025-06-12 08:39:02.310 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6296f1e7{/,null,STOPPED,@Spark}
2025-06-12 08:39:02.311 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38d55d49{/jobs,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.311 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ac4c778{/jobs/json,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.311 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4074a890{/jobs/job,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.312 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3be789de{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.312 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48681213{/stages,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.312 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@107af2ab{/stages/json,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.313 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27fd4b41{/stages/stage,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.313 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22130a80{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.314 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39231bd{/stages/pool,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.314 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37ec5b0e{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.314 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2df2cbd4{/storage,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.315 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7dbecbc5{/storage/json,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.315 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1434b3a{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.315 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5cae0bbe{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.316 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@593be0e1{/environment,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.316 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a4db3a3{/environment/json,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.316 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d94d6d0{/executors,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.317 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49fbcf08{/executors/json,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.317 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@192af42a{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.317 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45b7ca6c{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.320 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35d83209{/static,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.320 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@210cb17f{/,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.321 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7db1d098{/api,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.321 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29332900{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.322 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f92ce5e{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.323 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e8b65de{/metrics/json,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.396 [Thread-1 WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-12 08:39:02.396 [Thread-1 INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-12 08:39:02.401 [Thread-1 INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-12 08:39:02.409 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8508825{/SQL,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.409 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1de8d804{/SQL/json,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.410 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@713e719a{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.410 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e21bf11{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-12 08:39:02.411 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14e9fc5f{/static/sql,null,AVAILABLE,@Spark}
2025-06-12 08:39:03.340 [Thread-1 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-12 08:39:03.347 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@673264fc{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-12 08:39:03.348 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@aad241d{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-12 08:39:03.348 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ff3442a{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-12 08:39:03.348 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70faed21{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-12 08:39:03.349 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54a30df6{/static/sql,null,AVAILABLE,@Spark}
2025-06-12 08:39:03.351 [Thread-1 WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-e8cbd003-ab45-45b2-bb1c-ec4889f94c53. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-12 08:39:03.360 [Thread-1 INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-e8cbd003-ab45-45b2-bb1c-ec4889f94c53 resolved to file:/tmp/temporary-e8cbd003-ab45-45b2-bb1c-ec4889f94c53.
2025-06-12 08:39:03.360 [Thread-1 WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-12 08:39:03.400 [Thread-1 INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-e8cbd003-ab45-45b2-bb1c-ec4889f94c53/metadata using temp file file:/tmp/temporary-e8cbd003-ab45-45b2-bb1c-ec4889f94c53/.metadata.c8a687d4-8c9f-4518-999f-79bd8243fb85.tmp
2025-06-12 08:39:03.436 [Thread-1 INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-e8cbd003-ab45-45b2-bb1c-ec4889f94c53/.metadata.c8a687d4-8c9f-4518-999f-79bd8243fb85.tmp to file:/tmp/temporary-e8cbd003-ab45-45b2-bb1c-ec4889f94c53/metadata
2025-06-12 08:39:03.448 [Thread-1 INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d]. Use file:/tmp/temporary-e8cbd003-ab45-45b2-bb1c-ec4889f94c53 to store the query checkpoint.
2025-06-12 08:39:03.452 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@6cbbfcc4] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@24e192e3]
2025-06-12 08:39:03.465 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-12 08:39:03.465 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-12 08:39:03.465 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-12 08:39:03.466 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-12 08:39:03.574 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-12 08:39:03.576 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-12 08:39:03.577 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:39:03.577 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:39:03.577 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749692343576
2025-06-12 08:39:03.600 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-e8cbd003-ab45-45b2-bb1c-ec4889f94c53/sources/0/0 using temp file file:/tmp/temporary-e8cbd003-ab45-45b2-bb1c-ec4889f94c53/sources/0/.0.19a40835-d227-478c-90b7-6a17de64a8ad.tmp
2025-06-12 08:39:03.612 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-e8cbd003-ab45-45b2-bb1c-ec4889f94c53/sources/0/.0.19a40835-d227-478c-90b7-6a17de64a8ad.tmp to file:/tmp/temporary-e8cbd003-ab45-45b2-bb1c-ec4889f94c53/sources/0/0
2025-06-12 08:39:03.612 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-12 08:39:03.624 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-e8cbd003-ab45-45b2-bb1c-ec4889f94c53/offsets/0 using temp file file:/tmp/temporary-e8cbd003-ab45-45b2-bb1c-ec4889f94c53/offsets/.0.2214c0cc-ed15-4a34-b204-a5936b38abd5.tmp
2025-06-12 08:39:03.640 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-e8cbd003-ab45-45b2-bb1c-ec4889f94c53/offsets/.0.2214c0cc-ed15-4a34-b204-a5936b38abd5.tmp to file:/tmp/temporary-e8cbd003-ab45-45b2-bb1c-ec4889f94c53/offsets/0
2025-06-12 08:39:03.641 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749692343619,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-06-12 08:39:03.773 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:39:03.793 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:39:03.828 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:39:03.829 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:39:03.995 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 107.296003 ms
2025-06-12 08:39:04.145 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.486138 ms
2025-06-12 08:39:04.189 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] org.apache.spark.SparkContext - Starting job: start at Processor.java:92
2025-06-12 08:39:04.197 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at Processor.java:92) with 2 output partitions
2025-06-12 08:39:04.197 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (start at Processor.java:92)
2025-06-12 08:39:04.197 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-12 08:39:04.198 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-12 08:39:04.199 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[8] at start at Processor.java:92), which has no missing parents
2025-06-12 08:39:04.258 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 38.4 KiB, free 9.2 GiB)
2025-06-12 08:39:04.266 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 9.2 GiB)
2025-06-12 08:39:04.267 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:42365 (size: 16.6 KiB, free: 9.2 GiB)
2025-06-12 08:39:04.269 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-12 08:39:04.273 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at start at Processor.java:92) (first 15 tasks are for partitions Vector(0, 1))
2025-06-12 08:39:04.273 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-12 08:39:04.293 [dispatcher-event-loop-3 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 08:39:04.295 [dispatcher-event-loop-3 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 08:39:04.298 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-12 08:39:04.298 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-12 08:39:04.382 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 15.686162 ms
2025-06-12 08:39:04.400 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.486835 ms
2025-06-12 08:39:04.419 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.102711 ms
2025-06-12 08:39:04.431 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.023241 ms
2025-06-12 08:39:04.438 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-12 08:39:04.438 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-12 08:39:04.452 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-12 08:39:04.452 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-12 08:39:04.469 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:39:04.469 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:39:04.469 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749692344469
2025-06-12 08:39:04.470 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:39:04.470 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:39:04.470 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749692344469
2025-06-12 08:39:04.470 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor-1, groupId=spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor] Assigned to partition(s): clickstream-events-1
2025-06-12 08:39:04.470 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor-2, groupId=spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor] Assigned to partition(s): clickstream-events-0
2025-06-12 08:39:04.474 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor-1, groupId=spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-12 08:39:04.474 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor-2, groupId=spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-12 08:39:04.478 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor-2, groupId=spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor] Cluster ID: 8UN1o2zlTiO_QDl5CHzccg
2025-06-12 08:39:04.478 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor-1, groupId=spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor] Cluster ID: 8UN1o2zlTiO_QDl5CHzccg
2025-06-12 08:39:04.518 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor-1, groupId=spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-12 08:39:04.518 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor-2, groupId=spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-12 08:39:05.025 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor-1, groupId=spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:39:05.025 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor-2, groupId=spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:39:05.025 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor-1, groupId=spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-12 08:39:05.025 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor-2, groupId=spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-12 08:39:05.026 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor-1, groupId=spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:39:05.026 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor-2, groupId=spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:39:05.069 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:39:05.069 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:39:05.074 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:39:05.074 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:39:05.074 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [af00e299-a85b-48fe-b583-3ca522ad3c24] (1 queries & 0 savepoints) is rolled back.
2025-06-12 08:39:05.074 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [39c62ac0-1246-44c1-845c-ece129cb3ba1] (1 queries & 0 savepoints) is rolled back.
2025-06-12 08:39:05.074 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [a50e5b10-f32e-4687-926c-07f7de789b35] (0 queries & 0 savepoints) is committed.
2025-06-12 08:39:05.074 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [9836feaf-c870-4537-aa14-2f5b9ff406e4] (0 queries & 0 savepoints) is committed.
2025-06-12 08:39:05.082 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) ERROR] org.apache.spark.executor.Executor - Exception in task 1.0 in stage 0.0 (TID 1)
java.time.format.DateTimeParseException: Text '2025-06-12T01:30:46.909Z' could not be parsed at index 10
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2046)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1948)
	at java.base/java.time.LocalDateTime.parse(LocalDateTime.java:492)
	at com.clickhouse.data.value.ClickHouseOffsetDateTimeValue.update(ClickHouseOffsetDateTimeValue.java:371)
	at com.clickhouse.data.value.ClickHouseOffsetDateTimeValue.update(ClickHouseOffsetDateTimeValue.java:21)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.setString(InputBasedPreparedStatement.java:277)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeSetter$8(JdbcUtils.scala:592)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeSetter$8$adapted(JdbcUtils.scala:591)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:727)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-12 08:39:05.082 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) ERROR] org.apache.spark.executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.time.format.DateTimeParseException: Text '2025-06-12T01:30:56.238Z' could not be parsed at index 10
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2046)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1948)
	at java.base/java.time.LocalDateTime.parse(LocalDateTime.java:492)
	at com.clickhouse.data.value.ClickHouseOffsetDateTimeValue.update(ClickHouseOffsetDateTimeValue.java:371)
	at com.clickhouse.data.value.ClickHouseOffsetDateTimeValue.update(ClickHouseOffsetDateTimeValue.java:21)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.setString(InputBasedPreparedStatement.java:277)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeSetter$8(JdbcUtils.scala:592)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeSetter$8$adapted(JdbcUtils.scala:591)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:727)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-12 08:39:05.090 [task-result-getter-1 WARN ] o.a.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0) (phamviethoa executor driver): java.time.format.DateTimeParseException: Text '2025-06-12T01:30:56.238Z' could not be parsed at index 10
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2046)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1948)
	at java.base/java.time.LocalDateTime.parse(LocalDateTime.java:492)
	at com.clickhouse.data.value.ClickHouseOffsetDateTimeValue.update(ClickHouseOffsetDateTimeValue.java:371)
	at com.clickhouse.data.value.ClickHouseOffsetDateTimeValue.update(ClickHouseOffsetDateTimeValue.java:21)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.setString(InputBasedPreparedStatement.java:277)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeSetter$8(JdbcUtils.scala:592)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeSetter$8$adapted(JdbcUtils.scala:591)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:727)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

2025-06-12 08:39:05.090 [task-result-getter-1 ERROR] o.a.spark.scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
2025-06-12 08:39:05.091 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-12 08:39:05.091 [task-result-getter-0 WARN ] o.a.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 0.0 (TID 1) (phamviethoa executor driver): java.time.format.DateTimeParseException: Text '2025-06-12T01:30:46.909Z' could not be parsed at index 10
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2046)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1948)
	at java.base/java.time.LocalDateTime.parse(LocalDateTime.java:492)
	at com.clickhouse.data.value.ClickHouseOffsetDateTimeValue.update(ClickHouseOffsetDateTimeValue.java:371)
	at com.clickhouse.data.value.ClickHouseOffsetDateTimeValue.update(ClickHouseOffsetDateTimeValue.java:21)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.setString(InputBasedPreparedStatement.java:277)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeSetter$8(JdbcUtils.scala:592)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeSetter$8$adapted(JdbcUtils.scala:591)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:727)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

2025-06-12 08:39:05.091 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-12 08:39:05.093 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Cancelling stage 0
2025-06-12 08:39:05.093 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage cancelled
2025-06-12 08:39:05.094 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (start at Processor.java:92) failed in 0.890 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (phamviethoa executor driver): java.time.format.DateTimeParseException: Text '2025-06-12T01:30:56.238Z' could not be parsed at index 10
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2046)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1948)
	at java.base/java.time.LocalDateTime.parse(LocalDateTime.java:492)
	at com.clickhouse.data.value.ClickHouseOffsetDateTimeValue.update(ClickHouseOffsetDateTimeValue.java:371)
	at com.clickhouse.data.value.ClickHouseOffsetDateTimeValue.update(ClickHouseOffsetDateTimeValue.java:21)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.setString(InputBasedPreparedStatement.java:277)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeSetter$8(JdbcUtils.scala:592)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeSetter$8$adapted(JdbcUtils.scala:591)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:727)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Driver stacktrace:
2025-06-12 08:39:05.096 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 failed: start at Processor.java:92, took 0.906026 s
2025-06-12 08:39:05.104 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] terminated with error
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (phamviethoa executor driver): java.time.format.DateTimeParseException: Text '2025-06-12T01:30:56.238Z' could not be parsed at index 10
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2046)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1948)
	at java.base/java.time.LocalDateTime.parse(LocalDateTime.java:492)
	at com.clickhouse.data.value.ClickHouseOffsetDateTimeValue.update(ClickHouseOffsetDateTimeValue.java:371)
	at com.clickhouse.data.value.ClickHouseOffsetDateTimeValue.update(ClickHouseOffsetDateTimeValue.java:21)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.setString(InputBasedPreparedStatement.java:277)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeSetter$8(JdbcUtils.scala:592)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeSetter$8$adapted(JdbcUtils.scala:591)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:727)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1009)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1007)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:890)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.Processor.lambda$main$a450ce84$1(Processor.java:89)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
Caused by: java.time.format.DateTimeParseException: Text '2025-06-12T01:30:56.238Z' could not be parsed at index 10
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2046)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1948)
	at java.base/java.time.LocalDateTime.parse(LocalDateTime.java:492)
	at com.clickhouse.data.value.ClickHouseOffsetDateTimeValue.update(ClickHouseOffsetDateTimeValue.java:371)
	at com.clickhouse.data.value.ClickHouseOffsetDateTimeValue.update(ClickHouseOffsetDateTimeValue.java:21)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.setString(InputBasedPreparedStatement.java:277)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeSetter$8(JdbcUtils.scala:592)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeSetter$8$adapted(JdbcUtils.scala:591)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:727)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-12 08:39:05.105 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-2 unregistered
2025-06-12 08:39:05.105 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:39:05.105 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:39:05.105 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:39:05.106 [stream execution thread for [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = d0051d89-1919-44bb-b5fb-865b52accd4c, runId = 0c8c4701-ba03-405b-9961-9a5a30ec845d] has been shutdown
2025-06-12 08:41:10.228 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor-2, groupId=spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-12 08:41:10.229 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor-2, groupId=spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-12 08:41:10.234 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:41:10.234 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:41:10.235 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-12 08:41:10.235 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:41:10.236 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor-2 unregistered
2025-06-12 08:41:10.236 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor-1, groupId=spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-12 08:41:10.236 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor-1, groupId=spark-kafka-source-a3046aa3-a5df-4f79-960d-e841c438f021-470863856-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-12 08:41:10.238 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:41:10.238 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:41:12.951 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 75213 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-12 08:41:12.952 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-12 08:41:13.446 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-12 08:41:13.449 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-12 08:41:13.450 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-12 08:41:13.450 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-12 08:41:13.496 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-12 08:41:13.496 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 522 ms
2025-06-12 08:41:13.652 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-12 08:41:13.807 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-12 08:41:13.822 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-12 08:41:13.835 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:41:13.835 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:41:13.835 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749692473835
2025-06-12 08:41:13.932 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-12 08:41:13.935 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:41:13.935 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:41:13.935 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:41:13.940 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-12 08:41:13.944 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-12 08:41:13.953 [main INFO ] com.example.Application - Started Application in 1.162 seconds (JVM running for 1.46)
2025-06-12 08:41:14.137 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-12 08:41:14.138 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-12 08:41:14.138 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 0 ms
2025-06-12 08:41:14.153 [Thread-1 INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-12 08:41:14.209 [Thread-1 WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-12 08:41:14.240 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-12 08:41:14.240 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-12 08:41:14.240 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-12 08:41:14.241 [Thread-1 INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-12 08:41:14.249 [Thread-1 INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-12 08:41:14.254 [Thread-1 INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-12 08:41:14.254 [Thread-1 INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-12 08:41:14.272 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-12 08:41:14.272 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-12 08:41:14.272 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-12 08:41:14.272 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-12 08:41:14.273 [Thread-1 INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-12 08:41:14.359 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 38605.
2025-06-12 08:41:14.377 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-12 08:41:14.389 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-12 08:41:14.395 [Thread-1 INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-12 08:41:14.395 [Thread-1 INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-12 08:41:14.397 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-12 08:41:14.401 [Thread-1 INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-a7810000-eb86-487a-a282-dc6a743e6511
2025-06-12 08:41:14.415 [Thread-1 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-12 08:41:14.421 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-12 08:41:14.429 [Thread-1 INFO ] org.sparkproject.jetty.util.log - Logging initialized @1937ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-12 08:41:14.464 [Thread-1 INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-12 08:41:14.469 [Thread-1 INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-12 08:41:14.480 [Thread-1 INFO ] org.sparkproject.jetty.server.Server - Started @1987ms
2025-06-12 08:41:14.496 [Thread-1 INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@32da0c6d{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-12 08:41:14.496 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-12 08:41:14.504 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33fcb89c{/,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.533 [Thread-1 INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-12 08:41:14.536 [Thread-1 INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-12 08:41:14.543 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36137.
2025-06-12 08:41:14.543 [Thread-1 INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:36137
2025-06-12 08:41:14.544 [Thread-1 INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-12 08:41:14.547 [Thread-1 INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 36137, None)
2025-06-12 08:41:14.548 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:36137 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 36137, None)
2025-06-12 08:41:14.549 [Thread-1 INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 36137, None)
2025-06-12 08:41:14.550 [Thread-1 INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 36137, None)
2025-06-12 08:41:14.563 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@33fcb89c{/,null,STOPPED,@Spark}
2025-06-12 08:41:14.564 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@191c3827{/jobs,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.564 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@797464e0{/jobs/json,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.565 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45c3c6f1{/jobs/job,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.565 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@135f6bec{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.565 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6694df{/stages,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.566 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bd53909{/stages/json,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.566 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4437c3da{/stages/stage,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.566 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d15c56e{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.567 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c020a2b{/stages/pool,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.567 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11f94db7{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.567 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72791d61{/storage,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.568 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d071b15{/storage/json,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.568 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9d9de10{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.568 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@291a8c1d{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.569 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5744ea60{/environment,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.569 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41c12567{/environment/json,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.569 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7531c493{/executors,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.570 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@303c9c56{/executors/json,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.570 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ffdf90e{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.571 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2043b9a1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.573 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@296e2fec{/static,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.574 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4cadf666{/,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.575 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64263073{/api,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.575 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@100556f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.575 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@277287c4{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.578 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bd94ca3{/metrics/json,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.641 [Thread-1 WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-12 08:41:14.641 [Thread-1 INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-12 08:41:14.645 [Thread-1 INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-12 08:41:14.649 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e303d09{/SQL,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.649 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3da513f8{/SQL/json,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.650 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7efc15cc{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.650 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69748c9f{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-12 08:41:14.651 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70a44193{/static/sql,null,AVAILABLE,@Spark}
2025-06-12 08:41:15.484 [Thread-1 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-12 08:41:15.490 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3cc1f0ff{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-12 08:41:15.490 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64a6d1b5{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-12 08:41:15.490 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18dd0a41{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-12 08:41:15.491 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28dcaceb{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-12 08:41:15.491 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b0371c4{/static/sql,null,AVAILABLE,@Spark}
2025-06-12 08:41:15.494 [Thread-1 WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-002c9d94-f93b-4296-ba40-6a3dd51d6402. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-12 08:41:15.501 [Thread-1 INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-002c9d94-f93b-4296-ba40-6a3dd51d6402 resolved to file:/tmp/temporary-002c9d94-f93b-4296-ba40-6a3dd51d6402.
2025-06-12 08:41:15.502 [Thread-1 WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-12 08:41:15.537 [Thread-1 INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-002c9d94-f93b-4296-ba40-6a3dd51d6402/metadata using temp file file:/tmp/temporary-002c9d94-f93b-4296-ba40-6a3dd51d6402/.metadata.e14b6bac-7758-4a10-bf99-7939f8c91347.tmp
2025-06-12 08:41:15.573 [Thread-1 INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-002c9d94-f93b-4296-ba40-6a3dd51d6402/.metadata.e14b6bac-7758-4a10-bf99-7939f8c91347.tmp to file:/tmp/temporary-002c9d94-f93b-4296-ba40-6a3dd51d6402/metadata
2025-06-12 08:41:15.588 [Thread-1 INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3]. Use file:/tmp/temporary-002c9d94-f93b-4296-ba40-6a3dd51d6402 to store the query checkpoint.
2025-06-12 08:41:15.592 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@3b9511b6] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@77055db9]
2025-06-12 08:41:15.604 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-12 08:41:15.605 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-12 08:41:15.605 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-12 08:41:15.606 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-12 08:41:15.705 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-12 08:41:15.707 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-12 08:41:15.707 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:41:15.707 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:41:15.707 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749692475707
2025-06-12 08:41:15.730 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-002c9d94-f93b-4296-ba40-6a3dd51d6402/sources/0/0 using temp file file:/tmp/temporary-002c9d94-f93b-4296-ba40-6a3dd51d6402/sources/0/.0.ea178711-0517-4dad-8041-37f023b1f429.tmp
2025-06-12 08:41:15.741 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-002c9d94-f93b-4296-ba40-6a3dd51d6402/sources/0/.0.ea178711-0517-4dad-8041-37f023b1f429.tmp to file:/tmp/temporary-002c9d94-f93b-4296-ba40-6a3dd51d6402/sources/0/0
2025-06-12 08:41:15.741 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-12 08:41:15.753 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-002c9d94-f93b-4296-ba40-6a3dd51d6402/offsets/0 using temp file file:/tmp/temporary-002c9d94-f93b-4296-ba40-6a3dd51d6402/offsets/.0.edfe0233-6da5-443b-a070-242ad09dd03e.tmp
2025-06-12 08:41:15.769 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-002c9d94-f93b-4296-ba40-6a3dd51d6402/offsets/.0.edfe0233-6da5-443b-a070-242ad09dd03e.tmp to file:/tmp/temporary-002c9d94-f93b-4296-ba40-6a3dd51d6402/offsets/0
2025-06-12 08:41:15.769 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749692475748,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-06-12 08:41:15.899 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:41:15.919 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:41:15.953 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:41:15.953 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:41:16.112 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 103.375407 ms
2025-06-12 08:41:16.253 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.513985 ms
2025-06-12 08:41:16.296 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] org.apache.spark.SparkContext - Starting job: start at Processor.java:92
2025-06-12 08:41:16.302 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at Processor.java:92) with 2 output partitions
2025-06-12 08:41:16.302 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (start at Processor.java:92)
2025-06-12 08:41:16.303 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-12 08:41:16.303 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-12 08:41:16.304 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[8] at start at Processor.java:92), which has no missing parents
2025-06-12 08:41:16.361 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 39.5 KiB, free 9.2 GiB)
2025-06-12 08:41:16.368 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 9.2 GiB)
2025-06-12 08:41:16.370 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:36137 (size: 17.1 KiB, free: 9.2 GiB)
2025-06-12 08:41:16.371 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-12 08:41:16.375 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at start at Processor.java:92) (first 15 tasks are for partitions Vector(0, 1))
2025-06-12 08:41:16.376 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-12 08:41:16.393 [dispatcher-event-loop-3 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 08:41:16.395 [dispatcher-event-loop-3 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 08:41:16.398 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-12 08:41:16.398 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-12 08:41:16.483 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 15.526717 ms
2025-06-12 08:41:16.503 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.891214 ms
2025-06-12 08:41:16.520 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 5.296641 ms
2025-06-12 08:41:16.531 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.937218 ms
2025-06-12 08:41:16.538 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-12 08:41:16.538 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-12 08:41:16.552 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-12 08:41:16.552 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-12 08:41:16.569 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:41:16.569 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:41:16.569 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749692476569
2025-06-12 08:41:16.569 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:41:16.569 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:41:16.569 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749692476569
2025-06-12 08:41:16.569 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor-1, groupId=spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor] Assigned to partition(s): clickstream-events-0
2025-06-12 08:41:16.569 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor-2, groupId=spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor] Assigned to partition(s): clickstream-events-1
2025-06-12 08:41:16.573 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor-2, groupId=spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-12 08:41:16.573 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor-1, groupId=spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-12 08:41:16.577 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor-2, groupId=spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor] Cluster ID: 8UN1o2zlTiO_QDl5CHzccg
2025-06-12 08:41:16.577 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor-1, groupId=spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor] Cluster ID: 8UN1o2zlTiO_QDl5CHzccg
2025-06-12 08:41:16.595 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor-2, groupId=spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-12 08:41:16.595 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor-1, groupId=spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-12 08:41:17.097 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor-1, groupId=spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:41:17.097 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor-2, groupId=spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:41:17.097 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor-2, groupId=spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-12 08:41:17.097 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor-1, groupId=spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-12 08:41:17.098 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor-1, groupId=spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:41:17.098 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor-2, groupId=spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:41:17.143 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:41:17.143 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:41:17.150 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:41:17.150 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:41:17.150 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [c8e22fe6-52d6-4adc-a645-d7ca7170e085] (1 queries & 0 savepoints) is rolled back.
2025-06-12 08:41:17.150 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [d6ec1278-e420-4cd7-b0fe-1d199d76a1c4] (1 queries & 0 savepoints) is rolled back.
2025-06-12 08:41:17.150 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [f6939b07-47a2-4da8-a9c8-988f97c1ade5] (0 queries & 0 savepoints) is committed.
2025-06-12 08:41:17.150 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [714f5a13-7a4e-4231-96d6-7cb976923571] (0 queries & 0 savepoints) is committed.
2025-06-12 08:41:17.156 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) ERROR] org.apache.spark.executor.Executor - Exception in task 1.0 in stage 0.0 (TID 1)
java.sql.SQLException: Cannot set null to non-nullable column #6 [app_id String]
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:340)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-12 08:41:17.156 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) ERROR] org.apache.spark.executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.sql.SQLException: Cannot set null to non-nullable column #6 [app_id String]
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:340)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-12 08:41:17.165 [task-result-getter-0 WARN ] o.a.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 0.0 (TID 1) (phamviethoa executor driver): java.sql.SQLException: Cannot set null to non-nullable column #6 [app_id String]
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:340)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

2025-06-12 08:41:17.166 [task-result-getter-0 ERROR] o.a.spark.scheduler.TaskSetManager - Task 1 in stage 0.0 failed 1 times; aborting job
2025-06-12 08:41:17.167 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-12 08:41:17.167 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0) on phamviethoa, executor driver: java.sql.SQLException (Cannot set null to non-nullable column #6 [app_id String]) [duplicate 1]
2025-06-12 08:41:17.167 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-12 08:41:17.168 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Cancelling stage 0
2025-06-12 08:41:17.169 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage cancelled
2025-06-12 08:41:17.169 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (start at Processor.java:92) failed in 0.860 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1) (phamviethoa executor driver): java.sql.SQLException: Cannot set null to non-nullable column #6 [app_id String]
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:340)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Driver stacktrace:
2025-06-12 08:41:17.171 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 failed: start at Processor.java:92, took 0.874596 s
2025-06-12 08:41:17.181 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] terminated with error
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1) (phamviethoa executor driver): java.sql.SQLException: Cannot set null to non-nullable column #6 [app_id String]
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:340)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1009)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1007)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:890)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.Processor.lambda$main$a450ce84$1(Processor.java:89)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
Caused by: java.sql.SQLException: Cannot set null to non-nullable column #6 [app_id String]
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:340)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-12 08:41:17.181 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-2 unregistered
2025-06-12 08:41:17.182 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:41:17.182 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:41:17.182 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:41:17.183 [stream execution thread for [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = a742ccb4-f12f-410e-9676-97ccdb8d161a, runId = da3d0fc3-f946-4e97-ba46-da97e3fd61f3] has been shutdown
2025-06-12 08:43:35.532 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor-2, groupId=spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-12 08:43:35.532 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor-2, groupId=spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-12 08:43:35.536 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:43:35.536 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:43:35.536 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-12 08:43:35.537 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:43:35.538 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor-2 unregistered
2025-06-12 08:43:35.538 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor-1, groupId=spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-12 08:43:35.538 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor-1, groupId=spark-kafka-source-17771891-f3c0-4283-b047-3f183a797a94-794306180-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-12 08:43:40.556 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 77224 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-12 08:43:40.556 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-12 08:43:41.060 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-12 08:43:41.062 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-12 08:43:41.063 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-12 08:43:41.063 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-12 08:43:41.102 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-12 08:43:41.102 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 524 ms
2025-06-12 08:43:41.239 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-12 08:43:41.402 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-12 08:43:41.421 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-12 08:43:41.438 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:43:41.438 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:43:41.438 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749692621438
2025-06-12 08:43:41.525 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-12 08:43:41.528 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:43:41.528 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:43:41.528 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:43:41.535 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-12 08:43:41.540 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-12 08:43:41.549 [main INFO ] com.example.Application - Started Application in 1.168 seconds (JVM running for 1.529)
2025-06-12 08:43:41.768 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-12 08:43:41.768 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-12 08:43:41.769 [RMI TCP Connection(4)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-12 08:43:41.811 [Thread-1 INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-12 08:43:41.863 [Thread-1 WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-12 08:43:41.894 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-12 08:43:41.895 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-12 08:43:41.895 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-12 08:43:41.895 [Thread-1 INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-12 08:43:41.903 [Thread-1 INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-12 08:43:41.909 [Thread-1 INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-12 08:43:41.909 [Thread-1 INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-12 08:43:41.927 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-12 08:43:41.927 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-12 08:43:41.927 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-12 08:43:41.927 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-12 08:43:41.927 [Thread-1 INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-12 08:43:42.020 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 38453.
2025-06-12 08:43:42.028 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-12 08:43:42.046 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-12 08:43:42.057 [Thread-1 INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-12 08:43:42.057 [Thread-1 INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-12 08:43:42.059 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-12 08:43:42.063 [Thread-1 INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-8031f33f-ceca-42f0-835f-4da72cb1d250
2025-06-12 08:43:42.077 [Thread-1 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-12 08:43:42.083 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-12 08:43:42.091 [Thread-1 INFO ] org.sparkproject.jetty.util.log - Logging initialized @2071ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-12 08:43:42.130 [Thread-1 INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-12 08:43:42.134 [Thread-1 INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-12 08:43:42.140 [Thread-1 INFO ] org.sparkproject.jetty.server.Server - Started @2120ms
2025-06-12 08:43:42.151 [Thread-1 INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@72a377a8{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-12 08:43:42.151 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-12 08:43:42.158 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b373903{/,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.188 [Thread-1 INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-12 08:43:42.191 [Thread-1 INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-12 08:43:42.199 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43679.
2025-06-12 08:43:42.199 [Thread-1 INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:43679
2025-06-12 08:43:42.199 [Thread-1 INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-12 08:43:42.202 [Thread-1 INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 43679, None)
2025-06-12 08:43:42.204 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:43679 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 43679, None)
2025-06-12 08:43:42.206 [Thread-1 INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 43679, None)
2025-06-12 08:43:42.206 [Thread-1 INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 43679, None)
2025-06-12 08:43:42.220 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1b373903{/,null,STOPPED,@Spark}
2025-06-12 08:43:42.221 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@289772a4{/jobs,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.221 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21207794{/jobs/json,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.222 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e14630d{/jobs/job,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.222 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57ee074f{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.223 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@736fec8{/stages,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.223 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ea2a52d{/stages/json,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.224 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42269dd1{/stages/stage,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.224 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b5253d2{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.225 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7daefd60{/stages/pool,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.225 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1664d9d6{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.226 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16bf7f4e{/storage,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.226 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@575c9bc0{/storage/json,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.226 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@203c3cb9{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.227 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@463a21af{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.227 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69225057{/environment,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.228 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52179ce8{/environment/json,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.228 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52cd212a{/executors,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.228 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46cbca5f{/executors/json,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.229 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@675d4baa{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.230 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7895d5a4{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.232 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3eda93d5{/static,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.233 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25b36918{/,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.233 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d2df581{/api,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.234 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ca0c039{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.234 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@416bc0cd{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.236 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8586137{/metrics/json,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.296 [Thread-1 WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-12 08:43:42.296 [Thread-1 INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-12 08:43:42.300 [Thread-1 INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-12 08:43:42.304 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63591662{/SQL,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.304 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38f8aa8b{/SQL/json,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.305 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@714e57e{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.305 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67f1ccf2{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-12 08:43:42.306 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74dac887{/static/sql,null,AVAILABLE,@Spark}
2025-06-12 08:43:43.151 [Thread-1 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-12 08:43:43.155 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@111c28b3{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-12 08:43:43.156 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d721f5d{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-12 08:43:43.156 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18a9507c{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-12 08:43:43.156 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3df6a279{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-12 08:43:43.157 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@473f8065{/static/sql,null,AVAILABLE,@Spark}
2025-06-12 08:43:43.159 [Thread-1 WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-065e95a4-4076-4764-83d1-98b70f39b590. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-12 08:43:43.167 [Thread-1 INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-065e95a4-4076-4764-83d1-98b70f39b590 resolved to file:/tmp/temporary-065e95a4-4076-4764-83d1-98b70f39b590.
2025-06-12 08:43:43.167 [Thread-1 WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-12 08:43:43.200 [Thread-1 INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-065e95a4-4076-4764-83d1-98b70f39b590/metadata using temp file file:/tmp/temporary-065e95a4-4076-4764-83d1-98b70f39b590/.metadata.62eeb185-47a7-44e2-85d7-ae65f06c3c88.tmp
2025-06-12 08:43:43.237 [Thread-1 INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-065e95a4-4076-4764-83d1-98b70f39b590/.metadata.62eeb185-47a7-44e2-85d7-ae65f06c3c88.tmp to file:/tmp/temporary-065e95a4-4076-4764-83d1-98b70f39b590/metadata
2025-06-12 08:43:43.249 [Thread-1 INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b]. Use file:/tmp/temporary-065e95a4-4076-4764-83d1-98b70f39b590 to store the query checkpoint.
2025-06-12 08:43:43.254 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@691a54a] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@6368996b]
2025-06-12 08:43:43.264 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-12 08:43:43.265 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-12 08:43:43.265 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-12 08:43:43.266 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-12 08:43:43.364 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-12 08:43:43.366 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-12 08:43:43.366 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:43:43.366 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:43:43.366 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749692623366
2025-06-12 08:43:43.388 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-065e95a4-4076-4764-83d1-98b70f39b590/sources/0/0 using temp file file:/tmp/temporary-065e95a4-4076-4764-83d1-98b70f39b590/sources/0/.0.bf99c4ed-447e-4b19-b48a-dd4d549d57f0.tmp
2025-06-12 08:43:43.400 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-065e95a4-4076-4764-83d1-98b70f39b590/sources/0/.0.bf99c4ed-447e-4b19-b48a-dd4d549d57f0.tmp to file:/tmp/temporary-065e95a4-4076-4764-83d1-98b70f39b590/sources/0/0
2025-06-12 08:43:43.400 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-12 08:43:43.410 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-065e95a4-4076-4764-83d1-98b70f39b590/offsets/0 using temp file file:/tmp/temporary-065e95a4-4076-4764-83d1-98b70f39b590/offsets/.0.242b901b-0d19-4d65-a631-04225290cb5e.tmp
2025-06-12 08:43:43.425 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-065e95a4-4076-4764-83d1-98b70f39b590/offsets/.0.242b901b-0d19-4d65-a631-04225290cb5e.tmp to file:/tmp/temporary-065e95a4-4076-4764-83d1-98b70f39b590/offsets/0
2025-06-12 08:43:43.425 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749692623406,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-06-12 08:43:43.559 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:43:43.579 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:43:43.611 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:43:43.611 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:43:43.771 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 104.12923 ms
2025-06-12 08:43:43.913 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.950864 ms
2025-06-12 08:43:43.955 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] org.apache.spark.SparkContext - Starting job: start at Processor.java:92
2025-06-12 08:43:43.961 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at Processor.java:92) with 2 output partitions
2025-06-12 08:43:43.961 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (start at Processor.java:92)
2025-06-12 08:43:43.962 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-12 08:43:43.962 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-12 08:43:43.963 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[8] at start at Processor.java:92), which has no missing parents
2025-06-12 08:43:44.018 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 38.7 KiB, free 9.2 GiB)
2025-06-12 08:43:44.026 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 9.2 GiB)
2025-06-12 08:43:44.028 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:43679 (size: 17.0 KiB, free: 9.2 GiB)
2025-06-12 08:43:44.029 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-12 08:43:44.033 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at start at Processor.java:92) (first 15 tasks are for partitions Vector(0, 1))
2025-06-12 08:43:44.033 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-12 08:43:44.051 [dispatcher-event-loop-3 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-12 08:43:44.052 [dispatcher-event-loop-3 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8313 bytes) 
2025-06-12 08:43:44.055 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-12 08:43:44.055 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-12 08:43:44.143 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 16.295398 ms
2025-06-12 08:43:44.161 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.8377 ms
2025-06-12 08:43:44.180 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 5.377486 ms
2025-06-12 08:43:44.190 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.99007 ms
2025-06-12 08:43:44.197 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-12 08:43:44.197 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-12 08:43:44.210 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-12 08:43:44.210 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-12 08:43:44.227 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:43:44.228 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:43:44.228 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749692624227
2025-06-12 08:43:44.228 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:43:44.228 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:43:44.228 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749692624227
2025-06-12 08:43:44.228 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor-1, groupId=spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor] Assigned to partition(s): clickstream-events-0
2025-06-12 08:43:44.228 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor-2, groupId=spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor] Assigned to partition(s): clickstream-events-1
2025-06-12 08:43:44.232 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor-1, groupId=spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-12 08:43:44.232 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor-2, groupId=spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-12 08:43:44.236 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor-2, groupId=spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor] Cluster ID: 8UN1o2zlTiO_QDl5CHzccg
2025-06-12 08:43:44.236 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor-1, groupId=spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor] Cluster ID: 8UN1o2zlTiO_QDl5CHzccg
2025-06-12 08:43:44.253 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor-2, groupId=spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-12 08:43:44.253 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor-1, groupId=spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-12 08:43:44.756 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor-1, groupId=spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:43:44.756 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor-2, groupId=spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:43:44.756 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor-1, groupId=spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-12 08:43:44.757 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor-2, groupId=spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-12 08:43:44.757 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor-1, groupId=spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:43:44.757 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor-2, groupId=spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:43:44.799 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:43:44.799 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:43:44.813 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:43:44.813 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:43:44.813 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [bd63182a-9a1c-4157-b355-5d290b8de222] (2 queries & 0 savepoints) is committed.
2025-06-12 08:43:44.813 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [f4aa7eec-2b4a-4032-819b-9a08ccb1db00] (2 queries & 0 savepoints) is committed.
2025-06-12 08:43:44.813 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [90555e43-5622-452a-bfed-6a206a226c4e] (0 queries & 0 savepoints) is committed.
2025-06-12 08:43:44.813 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [f513497d-bd98-4da0-b7e5-9e39f6f9835b] (0 queries & 0 savepoints) is committed.
2025-06-12 08:43:44.821 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1688 bytes result sent to driver
2025-06-12 08:43:44.821 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1688 bytes result sent to driver
2025-06-12 08:43:44.826 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 773 ms on phamviethoa (executor driver) (1/2)
2025-06-12 08:43:44.826 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 779 ms on phamviethoa (executor driver) (2/2)
2025-06-12 08:43:44.827 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-12 08:43:44.829 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (start at Processor.java:92) finished in 0.861 s
2025-06-12 08:43:44.830 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-12 08:43:44.830 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-12 08:43:44.831 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at Processor.java:92, took 0.875513 s
2025-06-12 08:43:44.851 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] terminated with error
org.apache.spark.SparkSQLException: Unsupported type TIMESTAMP_WITH_TIMEZONE.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedJdbcTypeError(QueryExecutionErrors.scala:1019)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getCatalystType(JdbcUtils.scala:239)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getSchema$1(JdbcUtils.scala:321)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:321)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:71)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:88)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.Processor.lambda$main$a450ce84$1(Processor.java:89)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
2025-06-12 08:43:44.852 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-2 unregistered
2025-06-12 08:43:44.852 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:43:44.852 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:43:44.852 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:43:44.853 [stream execution thread for [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = 4ad33eaa-2aeb-41c2-853f-a4a4f9fb7f30, runId = ba58b9eb-3bd5-4464-b626-dbc4b2a7219b] has been shutdown
2025-06-12 08:44:43.644 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor-1, groupId=spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-12 08:44:43.644 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor-1, groupId=spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-12 08:44:43.648 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:44:43.649 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:44:43.649 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-12 08:44:43.649 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:44:43.650 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor-1 unregistered
2025-06-12 08:44:43.651 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor-2, groupId=spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-12 08:44:43.651 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor-2, groupId=spark-kafka-source-66455747-c16c-4a1a-b965-9a5f910da8a5--263549108-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-12 08:44:46.718 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 78282 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-12 08:44:46.719 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-12 08:44:47.236 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-12 08:44:47.239 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-12 08:44:47.239 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-12 08:44:47.239 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-12 08:44:47.290 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-12 08:44:47.290 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 542 ms
2025-06-12 08:44:47.457 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-12 08:44:47.620 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-12 08:44:47.635 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-12 08:44:47.651 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:44:47.651 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:44:47.651 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749692687650
2025-06-12 08:44:47.745 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-12 08:44:47.750 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:44:47.750 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:44:47.750 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:44:47.755 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-12 08:44:47.759 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-12 08:44:47.769 [main INFO ] com.example.Application - Started Application in 1.255 seconds (JVM running for 1.614)
2025-06-12 08:44:48.060 [Thread-1 INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-12 08:44:48.107 [Thread-1 WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-12 08:44:48.140 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-12 08:44:48.140 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-12 08:44:48.140 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-12 08:44:48.140 [Thread-1 INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-12 08:44:48.149 [Thread-1 INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-12 08:44:48.154 [Thread-1 INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-12 08:44:48.154 [Thread-1 INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-12 08:44:48.173 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-12 08:44:48.173 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-12 08:44:48.173 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-12 08:44:48.173 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-12 08:44:48.173 [Thread-1 INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-12 08:44:48.256 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 37571.
2025-06-12 08:44:48.265 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-12 08:44:48.279 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-12 08:44:48.292 [Thread-1 INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-12 08:44:48.292 [Thread-1 INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-12 08:44:48.295 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-12 08:44:48.296 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-12 08:44:48.297 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-12 08:44:48.297 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 0 ms
2025-06-12 08:44:48.322 [Thread-1 INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-6b1d05b3-63f7-460e-90df-cd07384b26bb
2025-06-12 08:44:48.346 [Thread-1 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-12 08:44:48.354 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-12 08:44:48.364 [Thread-1 INFO ] org.sparkproject.jetty.util.log - Logging initialized @2210ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-12 08:44:48.400 [Thread-1 INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-12 08:44:48.403 [Thread-1 INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-12 08:44:48.409 [Thread-1 INFO ] org.sparkproject.jetty.server.Server - Started @2255ms
2025-06-12 08:44:48.419 [Thread-1 INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@2c96aea2{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-12 08:44:48.420 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-12 08:44:48.426 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52eb18ea{/,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.454 [Thread-1 INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-12 08:44:48.457 [Thread-1 INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-12 08:44:48.465 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37771.
2025-06-12 08:44:48.465 [Thread-1 INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:37771
2025-06-12 08:44:48.466 [Thread-1 INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-12 08:44:48.470 [Thread-1 INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 37771, None)
2025-06-12 08:44:48.472 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:37771 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 37771, None)
2025-06-12 08:44:48.474 [Thread-1 INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 37771, None)
2025-06-12 08:44:48.474 [Thread-1 INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 37771, None)
2025-06-12 08:44:48.490 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@52eb18ea{/,null,STOPPED,@Spark}
2025-06-12 08:44:48.491 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3dda8f6d{/jobs,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.491 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25e56699{/jobs/json,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.492 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f627e60{/jobs/job,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.492 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14987a7e{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.492 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@617a0f2c{/stages,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.493 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7665d550{/stages/json,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.493 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@265b93c1{/stages/stage,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.494 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42a498f2{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.494 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20a59468{/stages/pool,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.494 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a0b2ef1{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.495 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a1c35f6{/storage,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.495 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67479e7d{/storage/json,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.495 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d249386{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.496 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30e07835{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.496 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad7af17{/environment,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.496 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f66a6b6{/environment/json,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.497 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@eac2866{/executors,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.497 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a39a4fc{/executors/json,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.498 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@681a79d1{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.498 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8ae2895{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.501 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60fa91e0{/static,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.502 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@eaffa01{/,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.503 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d52bb17{/api,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.503 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1992dfe{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.504 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46785fa7{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.506 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e012ecc{/metrics/json,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.571 [Thread-1 WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-12 08:44:48.572 [Thread-1 INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-12 08:44:48.575 [Thread-1 INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-12 08:44:48.580 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7596cac7{/SQL,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.580 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c60ce66{/SQL/json,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.580 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36850a22{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.581 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f1871d8{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-12 08:44:48.581 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f736613{/static/sql,null,AVAILABLE,@Spark}
2025-06-12 08:44:49.484 [Thread-1 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-12 08:44:49.489 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@163e0850{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-12 08:44:49.489 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@154ba7b4{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-12 08:44:49.490 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f43b2ce{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-12 08:44:49.490 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@726eb73b{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-12 08:44:49.491 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e89193{/static/sql,null,AVAILABLE,@Spark}
2025-06-12 08:44:49.494 [Thread-1 WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-80156994-5e0f-4f86-98e0-a1cd763a1636. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-12 08:44:49.502 [Thread-1 INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-80156994-5e0f-4f86-98e0-a1cd763a1636 resolved to file:/tmp/temporary-80156994-5e0f-4f86-98e0-a1cd763a1636.
2025-06-12 08:44:49.502 [Thread-1 WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-12 08:44:49.540 [Thread-1 INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-80156994-5e0f-4f86-98e0-a1cd763a1636/metadata using temp file file:/tmp/temporary-80156994-5e0f-4f86-98e0-a1cd763a1636/.metadata.98f0bbae-5da5-48ff-aeed-28283e1607b6.tmp
2025-06-12 08:44:49.578 [Thread-1 INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-80156994-5e0f-4f86-98e0-a1cd763a1636/.metadata.98f0bbae-5da5-48ff-aeed-28283e1607b6.tmp to file:/tmp/temporary-80156994-5e0f-4f86-98e0-a1cd763a1636/metadata
2025-06-12 08:44:49.591 [Thread-1 INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed]. Use file:/tmp/temporary-80156994-5e0f-4f86-98e0-a1cd763a1636 to store the query checkpoint.
2025-06-12 08:44:49.595 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@242b0ad1] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@384e2f3a]
2025-06-12 08:44:49.606 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-12 08:44:49.607 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-12 08:44:49.607 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-12 08:44:49.608 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-12 08:44:49.714 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-12 08:44:49.716 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-12 08:44:49.716 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:44:49.716 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:44:49.716 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749692689716
2025-06-12 08:44:49.741 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-80156994-5e0f-4f86-98e0-a1cd763a1636/sources/0/0 using temp file file:/tmp/temporary-80156994-5e0f-4f86-98e0-a1cd763a1636/sources/0/.0.c2201d35-4469-4513-bd71-b1395865c455.tmp
2025-06-12 08:44:49.754 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-80156994-5e0f-4f86-98e0-a1cd763a1636/sources/0/.0.c2201d35-4469-4513-bd71-b1395865c455.tmp to file:/tmp/temporary-80156994-5e0f-4f86-98e0-a1cd763a1636/sources/0/0
2025-06-12 08:44:49.754 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-12 08:44:49.766 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-80156994-5e0f-4f86-98e0-a1cd763a1636/offsets/0 using temp file file:/tmp/temporary-80156994-5e0f-4f86-98e0-a1cd763a1636/offsets/.0.7696a8b6-99a0-460d-82f1-55303de50fdd.tmp
2025-06-12 08:44:49.782 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-80156994-5e0f-4f86-98e0-a1cd763a1636/offsets/.0.7696a8b6-99a0-460d-82f1-55303de50fdd.tmp to file:/tmp/temporary-80156994-5e0f-4f86-98e0-a1cd763a1636/offsets/0
2025-06-12 08:44:49.783 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749692689761,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-06-12 08:44:49.919 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:44:49.940 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:44:49.974 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:44:49.975 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:44:50.152 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 108.39686 ms
2025-06-12 08:44:50.303 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.830434 ms
2025-06-12 08:44:50.346 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] org.apache.spark.SparkContext - Starting job: start at Processor.java:92
2025-06-12 08:44:50.355 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at Processor.java:92) with 2 output partitions
2025-06-12 08:44:50.355 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (start at Processor.java:92)
2025-06-12 08:44:50.355 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-12 08:44:50.356 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-12 08:44:50.357 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[8] at start at Processor.java:92), which has no missing parents
2025-06-12 08:44:50.417 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 39.1 KiB, free 9.2 GiB)
2025-06-12 08:44:50.425 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 9.2 GiB)
2025-06-12 08:44:50.426 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:37771 (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 08:44:50.427 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-12 08:44:50.432 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at start at Processor.java:92) (first 15 tasks are for partitions Vector(0, 1))
2025-06-12 08:44:50.432 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-12 08:44:50.452 [dispatcher-event-loop-3 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-12 08:44:50.453 [dispatcher-event-loop-3 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8313 bytes) 
2025-06-12 08:44:50.457 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-12 08:44:50.457 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-12 08:44:50.546 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 15.72037 ms
2025-06-12 08:44:50.562 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.535252 ms
2025-06-12 08:44:50.582 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 5.858274 ms
2025-06-12 08:44:50.594 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.744312 ms
2025-06-12 08:44:50.602 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-12 08:44:50.602 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-12 08:44:50.616 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-12 08:44:50.616 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-12 08:44:50.634 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:44:50.634 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:44:50.634 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749692690634
2025-06-12 08:44:50.635 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:44:50.635 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:44:50.635 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749692690634
2025-06-12 08:44:50.635 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor-1, groupId=spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor] Assigned to partition(s): clickstream-events-1
2025-06-12 08:44:50.635 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor-2, groupId=spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor] Assigned to partition(s): clickstream-events-0
2025-06-12 08:44:50.639 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor-2, groupId=spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-12 08:44:50.639 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor-1, groupId=spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-12 08:44:50.643 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor-2, groupId=spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor] Cluster ID: 8UN1o2zlTiO_QDl5CHzccg
2025-06-12 08:44:50.643 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor-1, groupId=spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor] Cluster ID: 8UN1o2zlTiO_QDl5CHzccg
2025-06-12 08:44:50.662 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor-1, groupId=spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-12 08:44:50.662 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor-2, groupId=spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-12 08:44:51.165 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor-1, groupId=spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:44:51.165 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor-2, groupId=spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:44:51.165 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor-1, groupId=spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-12 08:44:51.165 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor-2, groupId=spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-12 08:44:51.166 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor-1, groupId=spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:44:51.166 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor-2, groupId=spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:44:51.215 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:44:51.215 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:44:51.229 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:44:51.229 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:44:51.229 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [d0fd2c97-4c5d-474f-aa2a-ab1302b2c838] (2 queries & 0 savepoints) is committed.
2025-06-12 08:44:51.229 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [98fe4b8d-fe45-4448-8c00-15121cccf2f0] (2 queries & 0 savepoints) is committed.
2025-06-12 08:44:51.229 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [63dfba0a-6be1-4891-8daf-2cad3a927d5a] (0 queries & 0 savepoints) is committed.
2025-06-12 08:44:51.230 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [017e1f7d-d736-46ee-ac93-4d839800cab0] (0 queries & 0 savepoints) is committed.
2025-06-12 08:44:51.238 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1731 bytes result sent to driver
2025-06-12 08:44:51.238 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1731 bytes result sent to driver
2025-06-12 08:44:51.242 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 788 ms on phamviethoa (executor driver) (1/2)
2025-06-12 08:44:51.242 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 795 ms on phamviethoa (executor driver) (2/2)
2025-06-12 08:44:51.243 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-12 08:44:51.245 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (start at Processor.java:92) finished in 0.883 s
2025-06-12 08:44:51.246 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-12 08:44:51.246 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-12 08:44:51.247 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at Processor.java:92, took 0.900593 s
2025-06-12 08:44:51.267 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] terminated with error
org.apache.spark.SparkSQLException: Unsupported type TIMESTAMP_WITH_TIMEZONE.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedJdbcTypeError(QueryExecutionErrors.scala:1019)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getCatalystType(JdbcUtils.scala:239)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getSchema$1(JdbcUtils.scala:321)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:321)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:71)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:88)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.Processor.lambda$main$a450ce84$1(Processor.java:89)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
2025-06-12 08:44:51.268 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-2 unregistered
2025-06-12 08:44:51.268 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:44:51.268 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:44:51.268 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:44:51.269 [stream execution thread for [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = 7de9c503-07df-4c88-aed1-d3077f4186ad, runId = aaf1ecd3-9a9f-4be6-95d9-15f1deec1fed] has been shutdown
2025-06-12 08:48:42.546 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor-1, groupId=spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-12 08:48:42.546 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor-1, groupId=spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-12 08:48:42.550 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:48:42.550 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:48:42.551 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-12 08:48:42.551 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:48:42.553 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-5a031292-b267-4caf-8384-a2839089588d-1545807503-executor-1 unregistered
2025-06-12 08:48:44.764 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 81927 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-12 08:48:44.765 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-12 08:48:45.250 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-12 08:48:45.252 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-12 08:48:45.253 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-12 08:48:45.253 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-12 08:48:45.297 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-12 08:48:45.298 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 513 ms
2025-06-12 08:48:45.437 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-12 08:48:45.596 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-12 08:48:45.610 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-12 08:48:45.624 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:48:45.625 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:48:45.625 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749692925624
2025-06-12 08:48:45.708 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-12 08:48:45.710 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:48:45.710 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:48:45.710 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:48:45.714 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-12 08:48:45.718 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-12 08:48:45.727 [main INFO ] com.example.Application - Started Application in 1.116 seconds (JVM running for 1.404)
2025-06-12 08:48:45.938 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-12 08:48:45.938 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-12 08:48:45.939 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-12 08:48:45.945 [Thread-1 INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-12 08:48:45.996 [Thread-1 WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-12 08:48:46.026 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-12 08:48:46.027 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-12 08:48:46.027 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-12 08:48:46.027 [Thread-1 INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-12 08:48:46.035 [Thread-1 INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-12 08:48:46.040 [Thread-1 INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-12 08:48:46.040 [Thread-1 INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-12 08:48:46.057 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-12 08:48:46.057 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-12 08:48:46.057 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-12 08:48:46.057 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-12 08:48:46.057 [Thread-1 INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-12 08:48:46.138 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 44999.
2025-06-12 08:48:46.154 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-12 08:48:46.168 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-12 08:48:46.174 [Thread-1 INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-12 08:48:46.174 [Thread-1 INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-12 08:48:46.175 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-12 08:48:46.180 [Thread-1 INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-0e7db446-2624-4c41-a430-8bf74bd88e50
2025-06-12 08:48:46.194 [Thread-1 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-12 08:48:46.200 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-12 08:48:46.209 [Thread-1 INFO ] org.sparkproject.jetty.util.log - Logging initialized @1887ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-12 08:48:46.246 [Thread-1 INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-12 08:48:46.249 [Thread-1 INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-12 08:48:46.255 [Thread-1 INFO ] org.sparkproject.jetty.server.Server - Started @1932ms
2025-06-12 08:48:46.266 [Thread-1 INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@7d3b6a8b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-12 08:48:46.266 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-12 08:48:46.279 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b3bf094{/,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.312 [Thread-1 INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-12 08:48:46.315 [Thread-1 INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-12 08:48:46.322 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37789.
2025-06-12 08:48:46.322 [Thread-1 INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:37789
2025-06-12 08:48:46.323 [Thread-1 INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-12 08:48:46.326 [Thread-1 INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 37789, None)
2025-06-12 08:48:46.327 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:37789 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 37789, None)
2025-06-12 08:48:46.328 [Thread-1 INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 37789, None)
2025-06-12 08:48:46.329 [Thread-1 INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 37789, None)
2025-06-12 08:48:46.342 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7b3bf094{/,null,STOPPED,@Spark}
2025-06-12 08:48:46.342 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69512fe3{/jobs,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.343 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74de1d0{/jobs/json,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.343 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36158977{/jobs/job,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.344 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2651c00f{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.344 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1780ac02{/stages,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.344 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3102965f{/stages/json,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.345 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f4fe2a8{/stages/stage,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.345 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64eaddfe{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.345 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@332b9fe2{/stages/pool,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.346 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64c543bb{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.346 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23aba30b{/storage,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.346 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@288756fb{/storage/json,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.347 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d1e49d4{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.347 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@618b623f{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.347 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75f60059{/environment,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.347 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72531549{/environment/json,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.348 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@602d69de{/executors,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.348 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a7acfbc{/executors/json,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.349 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16460f5c{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.349 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61b2e9d0{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.351 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2dd92a15{/static,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.351 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38cdb510{/,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.352 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5458eec{/api,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.352 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5595c616{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.353 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38dec7c8{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.354 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65b07bcf{/metrics/json,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.416 [Thread-1 WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-12 08:48:46.416 [Thread-1 INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-12 08:48:46.419 [Thread-1 INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-12 08:48:46.424 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72744342{/SQL,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.425 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@637e85f7{/SQL/json,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.425 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@467c2be7{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.426 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b369b00{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-12 08:48:46.426 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@bb0b7e2{/static/sql,null,AVAILABLE,@Spark}
2025-06-12 08:48:47.281 [Thread-1 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-12 08:48:47.287 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ee3f1ae{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-12 08:48:47.287 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27f0ea61{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-12 08:48:47.288 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5fb549b2{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-12 08:48:47.288 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f6fa9a1{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-12 08:48:47.288 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721524ed{/static/sql,null,AVAILABLE,@Spark}
2025-06-12 08:48:47.291 [Thread-1 WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-94c673cf-aa34-428b-a08f-dee2018cf623. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-12 08:48:47.300 [Thread-1 INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-94c673cf-aa34-428b-a08f-dee2018cf623 resolved to file:/tmp/temporary-94c673cf-aa34-428b-a08f-dee2018cf623.
2025-06-12 08:48:47.300 [Thread-1 WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-12 08:48:47.336 [Thread-1 INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-94c673cf-aa34-428b-a08f-dee2018cf623/metadata using temp file file:/tmp/temporary-94c673cf-aa34-428b-a08f-dee2018cf623/.metadata.0a61d611-68cc-45b6-94d5-55a4b4c7139e.tmp
2025-06-12 08:48:47.372 [Thread-1 INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-94c673cf-aa34-428b-a08f-dee2018cf623/.metadata.0a61d611-68cc-45b6-94d5-55a4b4c7139e.tmp to file:/tmp/temporary-94c673cf-aa34-428b-a08f-dee2018cf623/metadata
2025-06-12 08:48:47.385 [Thread-1 INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3]. Use file:/tmp/temporary-94c673cf-aa34-428b-a08f-dee2018cf623 to store the query checkpoint.
2025-06-12 08:48:47.389 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@6efe58d5] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@2c5be492]
2025-06-12 08:48:47.400 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-12 08:48:47.401 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-12 08:48:47.401 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-12 08:48:47.402 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-12 08:48:47.503 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-12 08:48:47.506 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-12 08:48:47.506 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:48:47.506 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:48:47.506 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749692927506
2025-06-12 08:48:47.528 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-94c673cf-aa34-428b-a08f-dee2018cf623/sources/0/0 using temp file file:/tmp/temporary-94c673cf-aa34-428b-a08f-dee2018cf623/sources/0/.0.1f82236e-9f56-4409-8535-53a03baf97d1.tmp
2025-06-12 08:48:47.540 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-94c673cf-aa34-428b-a08f-dee2018cf623/sources/0/.0.1f82236e-9f56-4409-8535-53a03baf97d1.tmp to file:/tmp/temporary-94c673cf-aa34-428b-a08f-dee2018cf623/sources/0/0
2025-06-12 08:48:47.540 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-12 08:48:47.551 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-94c673cf-aa34-428b-a08f-dee2018cf623/offsets/0 using temp file file:/tmp/temporary-94c673cf-aa34-428b-a08f-dee2018cf623/offsets/.0.24381738-a311-4c1a-a94e-dd01619affc2.tmp
2025-06-12 08:48:47.566 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-94c673cf-aa34-428b-a08f-dee2018cf623/offsets/.0.24381738-a311-4c1a-a94e-dd01619affc2.tmp to file:/tmp/temporary-94c673cf-aa34-428b-a08f-dee2018cf623/offsets/0
2025-06-12 08:48:47.566 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749692927546,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-06-12 08:48:47.703 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:48:47.724 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:48:47.758 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:48:47.759 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:48:47.939 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 111.748704 ms
2025-06-12 08:48:48.088 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.752654 ms
2025-06-12 08:48:48.135 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] org.apache.spark.SparkContext - Starting job: start at Processor.java:92
2025-06-12 08:48:48.143 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at Processor.java:92) with 2 output partitions
2025-06-12 08:48:48.144 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (start at Processor.java:92)
2025-06-12 08:48:48.144 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-12 08:48:48.144 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-12 08:48:48.146 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[8] at start at Processor.java:92), which has no missing parents
2025-06-12 08:48:48.214 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 39.1 KiB, free 9.2 GiB)
2025-06-12 08:48:48.223 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 9.2 GiB)
2025-06-12 08:48:48.224 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:37789 (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 08:48:48.226 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-12 08:48:48.230 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at start at Processor.java:92) (first 15 tasks are for partitions Vector(0, 1))
2025-06-12 08:48:48.231 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-12 08:48:48.252 [dispatcher-event-loop-3 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 08:48:48.255 [dispatcher-event-loop-3 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 08:48:48.259 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-12 08:48:48.259 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-12 08:48:48.365 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 20.643162 ms
2025-06-12 08:48:48.384 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.828563 ms
2025-06-12 08:48:48.404 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.103118 ms
2025-06-12 08:48:48.417 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.617824 ms
2025-06-12 08:48:48.425 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-12 08:48:48.425 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-12 08:48:48.439 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-12 08:48:48.440 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-12 08:48:48.458 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:48:48.458 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:48:48.458 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749692928458
2025-06-12 08:48:48.458 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:48:48.458 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:48:48.459 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749692928458
2025-06-12 08:48:48.459 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor-1, groupId=spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor] Assigned to partition(s): clickstream-events-1
2025-06-12 08:48:48.459 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor-2, groupId=spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor] Assigned to partition(s): clickstream-events-0
2025-06-12 08:48:48.463 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor-1, groupId=spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-12 08:48:48.463 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor-2, groupId=spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-12 08:48:48.468 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor-2, groupId=spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor] Cluster ID: 8UN1o2zlTiO_QDl5CHzccg
2025-06-12 08:48:48.468 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor-1, groupId=spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor] Cluster ID: 8UN1o2zlTiO_QDl5CHzccg
2025-06-12 08:48:48.487 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor-2, groupId=spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-12 08:48:48.487 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor-1, groupId=spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-12 08:48:48.989 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor-1, groupId=spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:48:48.989 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor-2, groupId=spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:48:48.989 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor-1, groupId=spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-12 08:48:48.989 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor-2, groupId=spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-12 08:48:48.989 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor-2, groupId=spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:48:48.990 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor-1, groupId=spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:48:49.033 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:48:49.033 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:48:49.047 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:48:49.047 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:48:49.047 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [0792e7ae-ca4a-466f-b9ae-fc31aedf33e9] (2 queries & 0 savepoints) is committed.
2025-06-12 08:48:49.047 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [f756e445-271c-4876-bc93-c62a4764dbcd] (2 queries & 0 savepoints) is committed.
2025-06-12 08:48:49.047 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [c4c2c4f1-31f2-427f-94b5-dda7f6065ffd] (0 queries & 0 savepoints) is committed.
2025-06-12 08:48:49.047 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [9dbfc4ef-769f-47e9-a0ef-415e9f8da55f] (0 queries & 0 savepoints) is committed.
2025-06-12 08:48:49.056 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1731 bytes result sent to driver
2025-06-12 08:48:49.056 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1731 bytes result sent to driver
2025-06-12 08:48:49.060 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 806 ms on phamviethoa (executor driver) (1/2)
2025-06-12 08:48:49.061 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 815 ms on phamviethoa (executor driver) (2/2)
2025-06-12 08:48:49.061 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-12 08:48:49.064 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (start at Processor.java:92) finished in 0.912 s
2025-06-12 08:48:49.065 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-12 08:48:49.065 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-12 08:48:49.066 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at Processor.java:92, took 0.930478 s
2025-06-12 08:48:49.086 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] terminated with error
org.apache.spark.SparkSQLException: Unsupported type TIMESTAMP_WITH_TIMEZONE.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedJdbcTypeError(QueryExecutionErrors.scala:1019)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getCatalystType(JdbcUtils.scala:239)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getSchema$1(JdbcUtils.scala:321)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:321)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:71)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:88)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.Processor.lambda$main$a450ce84$1(Processor.java:89)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
2025-06-12 08:48:49.087 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-2 unregistered
2025-06-12 08:48:49.088 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:48:49.088 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:48:49.088 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:48:49.088 [stream execution thread for [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = f20f2400-58e8-43e5-a5de-22ab7e3138f0, runId = 25d7a796-bb95-4700-aa76-25671698e2d3] has been shutdown
2025-06-12 08:49:27.023 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor-2, groupId=spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-12 08:49:27.024 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor-2, groupId=spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-12 08:49:27.028 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:49:27.028 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:49:27.028 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-12 08:49:27.028 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:49:27.031 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor-2 unregistered
2025-06-12 08:49:27.032 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor-1, groupId=spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-12 08:49:27.032 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor-1, groupId=spark-kafka-source-68faf6b3-3564-4d19-bc82-d1f16e534800-739290673-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-12 08:49:27.034 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:49:27.034 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:49:27.034 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-12 08:49:27.034 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:49:28.970 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 82790 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-12 08:49:28.971 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-12 08:49:29.459 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-12 08:49:29.462 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-12 08:49:29.463 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-12 08:49:29.463 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-12 08:49:29.510 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-12 08:49:29.510 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 519 ms
2025-06-12 08:49:29.658 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-12 08:49:29.812 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-12 08:49:29.826 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-12 08:49:29.841 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:49:29.841 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:49:29.841 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749692969840
2025-06-12 08:49:29.928 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-12 08:49:29.932 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:49:29.932 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:49:29.932 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:49:29.938 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-12 08:49:29.942 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-12 08:49:29.951 [main INFO ] com.example.Application - Started Application in 1.139 seconds (JVM running for 1.43)
2025-06-12 08:49:30.129 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-12 08:49:30.129 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-12 08:49:30.130 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-12 08:49:30.178 [Thread-1 INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-12 08:49:30.228 [Thread-1 WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-12 08:49:30.259 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-12 08:49:30.259 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-12 08:49:30.259 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-12 08:49:30.260 [Thread-1 INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-12 08:49:30.268 [Thread-1 INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-12 08:49:30.273 [Thread-1 INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-12 08:49:30.273 [Thread-1 INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-12 08:49:30.290 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-12 08:49:30.290 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-12 08:49:30.290 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-12 08:49:30.290 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-12 08:49:30.290 [Thread-1 INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-12 08:49:30.379 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 35033.
2025-06-12 08:49:30.390 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-12 08:49:30.401 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-12 08:49:30.406 [Thread-1 INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-12 08:49:30.407 [Thread-1 INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-12 08:49:30.408 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-12 08:49:30.413 [Thread-1 INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-6fb9d30d-845f-43be-bced-8e3f2826be9c
2025-06-12 08:49:30.428 [Thread-1 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-12 08:49:30.434 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-12 08:49:30.443 [Thread-1 INFO ] org.sparkproject.jetty.util.log - Logging initialized @1921ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-12 08:49:30.489 [Thread-1 INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-12 08:49:30.493 [Thread-1 INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-12 08:49:30.499 [Thread-1 INFO ] org.sparkproject.jetty.server.Server - Started @1977ms
2025-06-12 08:49:30.509 [Thread-1 INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@2a33dc97{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-12 08:49:30.510 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-12 08:49:30.516 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5499a6de{/,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.543 [Thread-1 INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-12 08:49:30.546 [Thread-1 INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-12 08:49:30.553 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32783.
2025-06-12 08:49:30.553 [Thread-1 INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:32783
2025-06-12 08:49:30.554 [Thread-1 INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-12 08:49:30.557 [Thread-1 INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 32783, None)
2025-06-12 08:49:30.559 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:32783 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 32783, None)
2025-06-12 08:49:30.560 [Thread-1 INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 32783, None)
2025-06-12 08:49:30.561 [Thread-1 INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 32783, None)
2025-06-12 08:49:30.575 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@5499a6de{/,null,STOPPED,@Spark}
2025-06-12 08:49:30.575 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c09dc32{/jobs,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.576 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@adc8624{/jobs/json,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.576 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@364230ce{/jobs/job,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.576 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@719bff20{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.577 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@568e2c8{/stages,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.577 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@612c720d{/stages/json,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.577 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34360535{/stages/stage,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.578 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@43dd6463{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.578 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5319a78a{/stages/pool,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.578 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29572ae6{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.579 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70d60281{/storage,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.579 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c8d3ecd{/storage/json,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.579 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c0ba722{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.580 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c20630a{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.580 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6adcd872{/environment,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.580 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e034093{/environment/json,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.581 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@739a1e62{/executors,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.581 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74622643{/executors/json,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.582 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6221f5b9{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.582 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b29ae3{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.584 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8f2ce71{/static,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.585 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2425a468{/,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.586 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c9de6c7{/api,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.586 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c2c6fd2{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.586 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f44d37{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.588 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1584844e{/metrics/json,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.650 [Thread-1 WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-12 08:49:30.651 [Thread-1 INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-12 08:49:30.654 [Thread-1 INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-12 08:49:30.658 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49241439{/SQL,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.659 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f7ac8df{/SQL/json,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.659 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51a95d3b{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.660 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22cbf812{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-12 08:49:30.660 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c0eaedf{/static/sql,null,AVAILABLE,@Spark}
2025-06-12 08:49:31.530 [Thread-1 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-12 08:49:31.536 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ea4af72{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-12 08:49:31.537 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e8c2cb3{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-12 08:49:31.537 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@527a27c0{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-12 08:49:31.538 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a30ec2{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-12 08:49:31.538 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4bbb8d9b{/static/sql,null,AVAILABLE,@Spark}
2025-06-12 08:49:31.541 [Thread-1 WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-35ac5244-95d6-4962-a6c9-e5b5135c10d2. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-12 08:49:31.549 [Thread-1 INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-35ac5244-95d6-4962-a6c9-e5b5135c10d2 resolved to file:/tmp/temporary-35ac5244-95d6-4962-a6c9-e5b5135c10d2.
2025-06-12 08:49:31.549 [Thread-1 WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-12 08:49:31.585 [Thread-1 INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-35ac5244-95d6-4962-a6c9-e5b5135c10d2/metadata using temp file file:/tmp/temporary-35ac5244-95d6-4962-a6c9-e5b5135c10d2/.metadata.1ed82cbc-666b-48c4-8ab0-34f94a44db87.tmp
2025-06-12 08:49:31.622 [Thread-1 INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-35ac5244-95d6-4962-a6c9-e5b5135c10d2/.metadata.1ed82cbc-666b-48c4-8ab0-34f94a44db87.tmp to file:/tmp/temporary-35ac5244-95d6-4962-a6c9-e5b5135c10d2/metadata
2025-06-12 08:49:31.634 [Thread-1 INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf]. Use file:/tmp/temporary-35ac5244-95d6-4962-a6c9-e5b5135c10d2 to store the query checkpoint.
2025-06-12 08:49:31.640 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1308d207] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@63e706f8]
2025-06-12 08:49:31.650 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-12 08:49:31.651 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-12 08:49:31.651 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-12 08:49:31.652 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-12 08:49:31.751 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-12 08:49:31.753 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-12 08:49:31.753 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:49:31.753 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:49:31.753 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749692971753
2025-06-12 08:49:31.774 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-35ac5244-95d6-4962-a6c9-e5b5135c10d2/sources/0/0 using temp file file:/tmp/temporary-35ac5244-95d6-4962-a6c9-e5b5135c10d2/sources/0/.0.1ab868cd-73ff-4263-a7f9-c98f1faa1061.tmp
2025-06-12 08:49:31.786 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-35ac5244-95d6-4962-a6c9-e5b5135c10d2/sources/0/.0.1ab868cd-73ff-4263-a7f9-c98f1faa1061.tmp to file:/tmp/temporary-35ac5244-95d6-4962-a6c9-e5b5135c10d2/sources/0/0
2025-06-12 08:49:31.787 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-12 08:49:31.797 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-35ac5244-95d6-4962-a6c9-e5b5135c10d2/offsets/0 using temp file file:/tmp/temporary-35ac5244-95d6-4962-a6c9-e5b5135c10d2/offsets/.0.954bb11e-05c4-48d7-b847-da97878a2d8f.tmp
2025-06-12 08:49:31.813 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-35ac5244-95d6-4962-a6c9-e5b5135c10d2/offsets/.0.954bb11e-05c4-48d7-b847-da97878a2d8f.tmp to file:/tmp/temporary-35ac5244-95d6-4962-a6c9-e5b5135c10d2/offsets/0
2025-06-12 08:49:31.813 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749692971793,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-06-12 08:49:31.942 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:49:31.961 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:49:31.995 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:49:31.995 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:49:32.171 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 111.118912 ms
2025-06-12 08:49:32.325 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.082168 ms
2025-06-12 08:49:32.369 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] org.apache.spark.SparkContext - Starting job: start at Processor.java:92
2025-06-12 08:49:32.377 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at Processor.java:92) with 2 output partitions
2025-06-12 08:49:32.377 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (start at Processor.java:92)
2025-06-12 08:49:32.377 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-12 08:49:32.378 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-12 08:49:32.380 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[8] at start at Processor.java:92), which has no missing parents
2025-06-12 08:49:32.445 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 39.1 KiB, free 9.2 GiB)
2025-06-12 08:49:32.453 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 9.2 GiB)
2025-06-12 08:49:32.455 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:32783 (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 08:49:32.457 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-12 08:49:32.461 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at start at Processor.java:92) (first 15 tasks are for partitions Vector(0, 1))
2025-06-12 08:49:32.462 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-12 08:49:32.481 [dispatcher-event-loop-3 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-12 08:49:32.483 [dispatcher-event-loop-3 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8313 bytes) 
2025-06-12 08:49:32.487 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-12 08:49:32.487 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-12 08:49:32.593 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 21.969554 ms
2025-06-12 08:49:32.615 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 11.442447 ms
2025-06-12 08:49:32.637 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.244177 ms
2025-06-12 08:49:32.648 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.510238 ms
2025-06-12 08:49:32.657 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-12 08:49:32.657 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-12 08:49:32.670 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-12 08:49:32.670 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-12 08:49:32.688 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:49:32.688 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:49:32.688 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749692972688
2025-06-12 08:49:32.688 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:49:32.688 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:49:32.688 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749692972688
2025-06-12 08:49:32.688 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor-1, groupId=spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor] Assigned to partition(s): clickstream-events-0
2025-06-12 08:49:32.688 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor-2, groupId=spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor] Assigned to partition(s): clickstream-events-1
2025-06-12 08:49:32.692 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor-1, groupId=spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-12 08:49:32.692 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor-2, groupId=spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-12 08:49:32.696 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor-2, groupId=spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor] Cluster ID: 8UN1o2zlTiO_QDl5CHzccg
2025-06-12 08:49:32.697 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor-1, groupId=spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor] Cluster ID: 8UN1o2zlTiO_QDl5CHzccg
2025-06-12 08:49:32.715 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor-2, groupId=spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-12 08:49:32.715 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor-1, groupId=spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-12 08:49:33.217 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor-2, groupId=spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:49:33.217 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor-1, groupId=spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:49:33.217 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor-2, groupId=spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-12 08:49:33.217 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor-1, groupId=spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-12 08:49:33.218 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor-1, groupId=spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:49:33.218 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor-2, groupId=spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:49:33.260 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:49:33.260 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:49:33.274 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:49:33.274 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:49:33.274 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [a0bfed79-9f52-4eb8-9ecb-feabe23a09b2] (2 queries & 0 savepoints) is committed.
2025-06-12 08:49:33.274 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [cf1a1fe5-0d20-4ab5-bb57-1a58e5dde3cc] (2 queries & 0 savepoints) is committed.
2025-06-12 08:49:33.274 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [33c64853-e951-4a6d-af40-757de9239282] (0 queries & 0 savepoints) is committed.
2025-06-12 08:49:33.274 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [6a2bcab0-0aa5-4fc7-87d7-4ab8c846397b] (0 queries & 0 savepoints) is committed.
2025-06-12 08:49:33.283 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1731 bytes result sent to driver
2025-06-12 08:49:33.283 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1731 bytes result sent to driver
2025-06-12 08:49:33.287 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 810 ms on phamviethoa (executor driver) (1/2)
2025-06-12 08:49:33.288 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 805 ms on phamviethoa (executor driver) (2/2)
2025-06-12 08:49:33.288 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-12 08:49:33.291 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (start at Processor.java:92) finished in 0.903 s
2025-06-12 08:49:33.292 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-12 08:49:33.292 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-12 08:49:33.293 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at Processor.java:92, took 0.923533 s
2025-06-12 08:49:33.312 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] terminated with error
org.apache.spark.SparkSQLException: Unsupported type TIMESTAMP_WITH_TIMEZONE.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedJdbcTypeError(QueryExecutionErrors.scala:1019)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getCatalystType(JdbcUtils.scala:239)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getSchema$1(JdbcUtils.scala:321)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:321)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:71)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:88)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.Processor.lambda$main$a450ce84$1(Processor.java:89)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
2025-06-12 08:49:33.313 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-2 unregistered
2025-06-12 08:49:33.314 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:49:33.314 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:49:33.314 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:49:33.315 [stream execution thread for [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = e905ba40-2fb6-41dd-991e-e8af41a4bd86, runId = a43d7514-101b-4cae-83e2-7bdc6acafeaf] has been shutdown
2025-06-12 08:51:13.564 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor-1, groupId=spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-12 08:51:13.564 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor-1, groupId=spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-12 08:51:13.568 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:51:13.568 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:51:13.568 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-12 08:51:13.568 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:51:13.569 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor-1 unregistered
2025-06-12 08:51:13.569 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor-2, groupId=spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-12 08:51:13.569 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor-2, groupId=spark-kafka-source-e43ecf62-d701-4743-a9fe-5f9131d63f9e-1761691640-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-12 08:51:15.564 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 84401 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-12 08:51:15.565 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-12 08:51:16.057 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-12 08:51:16.060 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-12 08:51:16.060 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-12 08:51:16.060 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-12 08:51:16.107 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-12 08:51:16.107 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 519 ms
2025-06-12 08:51:16.257 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-12 08:51:16.409 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-12 08:51:16.423 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-12 08:51:16.438 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:51:16.439 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:51:16.439 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749693076438
2025-06-12 08:51:16.524 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-12 08:51:16.527 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:51:16.527 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:51:16.527 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:51:16.531 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-12 08:51:16.536 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-12 08:51:16.545 [main INFO ] com.example.Application - Started Application in 1.149 seconds (JVM running for 1.441)
2025-06-12 08:51:16.711 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-12 08:51:16.711 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-12 08:51:16.712 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-12 08:51:16.776 [Thread-1 INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-12 08:51:16.825 [Thread-1 WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-12 08:51:16.857 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-12 08:51:16.857 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-12 08:51:16.857 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-12 08:51:16.857 [Thread-1 INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-12 08:51:16.865 [Thread-1 INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-12 08:51:16.870 [Thread-1 INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-12 08:51:16.871 [Thread-1 INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-12 08:51:16.888 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-12 08:51:16.888 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-12 08:51:16.888 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-12 08:51:16.888 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-12 08:51:16.888 [Thread-1 INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-12 08:51:16.979 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 35319.
2025-06-12 08:51:16.990 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-12 08:51:17.000 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-12 08:51:17.006 [Thread-1 INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-12 08:51:17.006 [Thread-1 INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-12 08:51:17.008 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-12 08:51:17.012 [Thread-1 INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-a3755495-9354-4ea8-a254-75cd2868d118
2025-06-12 08:51:17.027 [Thread-1 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-12 08:51:17.033 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-12 08:51:17.048 [Thread-1 INFO ] org.sparkproject.jetty.util.log - Logging initialized @1943ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-12 08:51:17.092 [Thread-1 INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-12 08:51:17.096 [Thread-1 INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-12 08:51:17.102 [Thread-1 INFO ] org.sparkproject.jetty.server.Server - Started @1997ms
2025-06-12 08:51:17.112 [Thread-1 INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@5ab2a4f0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-12 08:51:17.112 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-12 08:51:17.119 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24f2387a{/,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.145 [Thread-1 INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-12 08:51:17.148 [Thread-1 INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-12 08:51:17.155 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46087.
2025-06-12 08:51:17.155 [Thread-1 INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:46087
2025-06-12 08:51:17.156 [Thread-1 INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-12 08:51:17.159 [Thread-1 INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 46087, None)
2025-06-12 08:51:17.160 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:46087 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 46087, None)
2025-06-12 08:51:17.161 [Thread-1 INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 46087, None)
2025-06-12 08:51:17.162 [Thread-1 INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 46087, None)
2025-06-12 08:51:17.175 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@24f2387a{/,null,STOPPED,@Spark}
2025-06-12 08:51:17.176 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25f9daac{/jobs,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.177 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d3b455d{/jobs/json,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.177 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4407b74e{/jobs/job,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.177 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@137cfb89{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.178 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31ec7445{/stages,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.178 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5534cd25{/stages/json,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.178 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69ed6185{/stages/stage,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.179 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f2b39a8{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.179 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@767749d4{/stages/pool,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.179 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d75c2d0{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.180 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@623bdf2f{/storage,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.180 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@506c8808{/storage/json,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.180 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@32d4acd1{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.181 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60b9e60f{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.181 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2faa3dc8{/environment,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.181 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5cb392d5{/environment/json,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.182 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67a281b5{/executors,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.182 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7854f2a5{/executors/json,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.183 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57150dba{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.183 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fdfe979{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.186 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@288482cd{/static,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.186 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1897aa40{/,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.187 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29ba8c79{/api,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.187 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d21d001{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.188 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@776ec23d{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.189 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@446ac230{/metrics/json,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.252 [Thread-1 WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-12 08:51:17.252 [Thread-1 INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-12 08:51:17.255 [Thread-1 INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-12 08:51:17.259 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4872489f{/SQL,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.260 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72663581{/SQL/json,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.260 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25697f16{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.260 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25d20db{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-12 08:51:17.261 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e37334b{/static/sql,null,AVAILABLE,@Spark}
2025-06-12 08:51:18.096 [Thread-1 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-12 08:51:18.101 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3010dd93{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-12 08:51:18.101 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d626ddd{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-12 08:51:18.102 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4bd0b8f5{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-12 08:51:18.102 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d32e170{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-12 08:51:18.103 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78e19523{/static/sql,null,AVAILABLE,@Spark}
2025-06-12 08:51:18.105 [Thread-1 WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a62eefdb-30ca-4592-98bd-1356843b4da2. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-12 08:51:18.112 [Thread-1 INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-a62eefdb-30ca-4592-98bd-1356843b4da2 resolved to file:/tmp/temporary-a62eefdb-30ca-4592-98bd-1356843b4da2.
2025-06-12 08:51:18.113 [Thread-1 WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-12 08:51:18.146 [Thread-1 INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-a62eefdb-30ca-4592-98bd-1356843b4da2/metadata using temp file file:/tmp/temporary-a62eefdb-30ca-4592-98bd-1356843b4da2/.metadata.ede3cc8d-0fc7-42c8-9215-ed504181fa5f.tmp
2025-06-12 08:51:18.180 [Thread-1 INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-a62eefdb-30ca-4592-98bd-1356843b4da2/.metadata.ede3cc8d-0fc7-42c8-9215-ed504181fa5f.tmp to file:/tmp/temporary-a62eefdb-30ca-4592-98bd-1356843b4da2/metadata
2025-06-12 08:51:18.193 [Thread-1 INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc]. Use file:/tmp/temporary-a62eefdb-30ca-4592-98bd-1356843b4da2 to store the query checkpoint.
2025-06-12 08:51:18.198 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@c1a81a3] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@498d3630]
2025-06-12 08:51:18.210 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-12 08:51:18.210 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-12 08:51:18.210 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-12 08:51:18.211 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-12 08:51:18.310 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-12 08:51:18.312 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-12 08:51:18.312 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:51:18.312 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:51:18.312 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749693078312
2025-06-12 08:51:18.333 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-a62eefdb-30ca-4592-98bd-1356843b4da2/sources/0/0 using temp file file:/tmp/temporary-a62eefdb-30ca-4592-98bd-1356843b4da2/sources/0/.0.83c3adc5-db85-462f-ba09-139952785a64.tmp
2025-06-12 08:51:18.345 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-a62eefdb-30ca-4592-98bd-1356843b4da2/sources/0/.0.83c3adc5-db85-462f-ba09-139952785a64.tmp to file:/tmp/temporary-a62eefdb-30ca-4592-98bd-1356843b4da2/sources/0/0
2025-06-12 08:51:18.346 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-12 08:51:18.356 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-a62eefdb-30ca-4592-98bd-1356843b4da2/offsets/0 using temp file file:/tmp/temporary-a62eefdb-30ca-4592-98bd-1356843b4da2/offsets/.0.6083704b-08ee-4786-9a2f-2ace1fcf8cf4.tmp
2025-06-12 08:51:18.371 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-a62eefdb-30ca-4592-98bd-1356843b4da2/offsets/.0.6083704b-08ee-4786-9a2f-2ace1fcf8cf4.tmp to file:/tmp/temporary-a62eefdb-30ca-4592-98bd-1356843b4da2/offsets/0
2025-06-12 08:51:18.372 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749693078352,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-06-12 08:51:18.509 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:51:18.531 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:51:18.564 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:51:18.564 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:51:18.736 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 106.46242 ms
2025-06-12 08:51:18.877 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.893558 ms
2025-06-12 08:51:18.919 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] org.apache.spark.SparkContext - Starting job: start at Processor.java:94
2025-06-12 08:51:18.925 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at Processor.java:94) with 2 output partitions
2025-06-12 08:51:18.925 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (start at Processor.java:94)
2025-06-12 08:51:18.925 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-12 08:51:18.925 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-12 08:51:18.927 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[8] at start at Processor.java:94), which has no missing parents
2025-06-12 08:51:18.984 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 39.1 KiB, free 9.2 GiB)
2025-06-12 08:51:18.992 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 9.2 GiB)
2025-06-12 08:51:18.993 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:46087 (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 08:51:18.995 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-12 08:51:18.999 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at start at Processor.java:94) (first 15 tasks are for partitions Vector(0, 1))
2025-06-12 08:51:18.999 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-12 08:51:19.026 [dispatcher-event-loop-3 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8314 bytes) 
2025-06-12 08:51:19.028 [dispatcher-event-loop-3 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8314 bytes) 
2025-06-12 08:51:19.032 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-12 08:51:19.032 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-12 08:51:19.117 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 15.328124 ms
2025-06-12 08:51:19.133 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.946475 ms
2025-06-12 08:51:19.150 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 5.215982 ms
2025-06-12 08:51:19.160 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.041016 ms
2025-06-12 08:51:19.168 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-12 08:51:19.168 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-12 08:51:19.180 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-12 08:51:19.180 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-12 08:51:19.198 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:51:19.198 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:51:19.198 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749693079198
2025-06-12 08:51:19.198 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:51:19.198 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:51:19.198 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749693079198
2025-06-12 08:51:19.199 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor-2, groupId=spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor] Assigned to partition(s): clickstream-events-1
2025-06-12 08:51:19.199 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor-1, groupId=spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor] Assigned to partition(s): clickstream-events-0
2025-06-12 08:51:19.203 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor-1, groupId=spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-12 08:51:19.203 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor-2, groupId=spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-12 08:51:19.207 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor-2, groupId=spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor] Cluster ID: 8UN1o2zlTiO_QDl5CHzccg
2025-06-12 08:51:19.207 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor-1, groupId=spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor] Cluster ID: 8UN1o2zlTiO_QDl5CHzccg
2025-06-12 08:51:19.225 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor-2, groupId=spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-12 08:51:19.225 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor-1, groupId=spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-12 08:51:19.727 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor-2, groupId=spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:51:19.727 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor-1, groupId=spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:51:19.727 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor-2, groupId=spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-12 08:51:19.727 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor-1, groupId=spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-12 08:51:19.728 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor-2, groupId=spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:51:19.728 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor-1, groupId=spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:51:19.772 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:51:19.772 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:51:19.786 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:51:19.786 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:51:19.786 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [6899431f-ff33-401a-80c4-886d1fc4eb11] (2 queries & 0 savepoints) is committed.
2025-06-12 08:51:19.786 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [1e513ad3-d09c-494b-a7e0-01e8d2c72c07] (2 queries & 0 savepoints) is committed.
2025-06-12 08:51:19.786 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [0ddfaddf-0adc-4511-b4cd-e068c4ece37e] (0 queries & 0 savepoints) is committed.
2025-06-12 08:51:19.786 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [7acd38ca-ef73-41cb-9c57-314e14fc8b26] (0 queries & 0 savepoints) is committed.
2025-06-12 08:51:19.794 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1688 bytes result sent to driver
2025-06-12 08:51:19.794 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1688 bytes result sent to driver
2025-06-12 08:51:19.798 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 770 ms on phamviethoa (executor driver) (1/2)
2025-06-12 08:51:19.799 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 780 ms on phamviethoa (executor driver) (2/2)
2025-06-12 08:51:19.800 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-12 08:51:19.802 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (start at Processor.java:94) finished in 0.870 s
2025-06-12 08:51:19.803 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-12 08:51:19.803 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-12 08:51:19.804 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at Processor.java:94, took 0.884638 s
2025-06-12 08:51:19.823 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] terminated with error
org.apache.spark.SparkSQLException: Unsupported type TIMESTAMP_WITH_TIMEZONE.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedJdbcTypeError(QueryExecutionErrors.scala:1019)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getCatalystType(JdbcUtils.scala:239)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getSchema$1(JdbcUtils.scala:321)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:321)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:71)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:88)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.Processor.lambda$main$a450ce84$1(Processor.java:91)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
2025-06-12 08:51:19.824 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-2 unregistered
2025-06-12 08:51:19.825 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:51:19.825 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:51:19.825 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:51:19.825 [stream execution thread for [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = e277b63e-895f-40f8-b924-3b4b5ef82a86, runId = 5fd732e7-6455-4965-8168-08bad50db1dc] has been shutdown
2025-06-12 08:53:44.315 [http-nio-8080-exec-1 WARN ] o.s.w.s.m.s.DefaultHandlerExceptionResolver - Resolved [org.springframework.web.HttpMediaTypeNotSupportedException: Content type 'text/plain;charset=UTF-8' not supported]
2025-06-12 08:53:51.105 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=079cfff3-c3d4-4fff-9612-17babb6a1b28, event_name=page_view, event_time=2025-06-12T01:53:48.090Z, user_id=user_pej5ild, session_id=session_lyohdih, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/}, {event_id=0935c083-669d-4494-bf69-ad896ddf3436, event_name=scroll, event_time=2025-06-12T01:53:50.098Z, user_id=user_pej5ild, session_id=session_lyohdih, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=1}, {event_id=10828f28-2a62-4500-a735-fcb10aac6f3d, event_name=scroll, event_time=2025-06-12T01:53:51.098Z, user_id=user_pej5ild, session_id=session_lyohdih, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=99}]}
2025-06-12 08:53:51.105 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 08:53:51.110 [http-nio-8080-exec-2 INFO ] o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 1000
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-06-12 08:53:51.110 [http-nio-8080-exec-2 INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-12 08:53:51.113 [http-nio-8080-exec-2 INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-06-12 08:53:51.118 [http-nio-8080-exec-2 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:53:51.118 [http-nio-8080-exec-2 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:53:51.118 [http-nio-8080-exec-2 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749693231118
2025-06-12 08:53:51.122 [kafka-producer-network-thread | producer-1 INFO ] org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: 8UN1o2zlTiO_QDl5CHzccg
2025-06-12 08:53:51.122 [kafka-producer-network-thread | producer-1 INFO ] o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 1 with epoch 0
2025-06-12 08:53:51.125 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 08:53:57.198 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=3a6ebb3b-a1f6-4a3d-8219-cc7d60d70fc2, event_name=scroll, event_time=2025-06-12T01:53:52.098Z, user_id=user_pej5ild, session_id=session_lyohdih, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=100}, {event_id=a2505dca-e283-4b97-9794-d53d15c1d532, event_name=click, event_time=2025-06-12T01:53:56.321Z, user_id=user_pej5ild, session_id=session_lyohdih, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 3
                        Customer favorit, element_type=div, element_name=null, track=product_click, productId=3}, {event_id=31317af1-c9c7-487a-a75a-b0f588be95ca, event_name=click, event_time=2025-06-12T01:53:57.194Z, user_id=user_pej5ild, session_id=session_lyohdih, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 2
                        Durable and styl, element_type=div, element_name=null, track=product_click, productId=2}]}
2025-06-12 08:53:57.198 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 08:53:57.198 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 08:53:58.571 [http-nio-8080-exec-5 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=c20bbd88-42a8-4bb6-a98a-551b601016e2, event_name=click, event_time=2025-06-12T01:53:57.715Z, user_id=user_pej5ild, session_id=session_lyohdih, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 1
                        High-quality ite, element_type=div, element_name=null, track=product_click, productId=1}, {event_id=bd32b9d8-cfd1-467c-bc42-060fe44d8f66, event_name=click, event_time=2025-06-12T01:53:58.102Z, user_id=user_pej5ild, session_id=session_lyohdih, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 4
                        Reliable and aff, element_type=div, element_name=null, track=product_click, productId=4}, {event_id=686ffd55-b203-4e7a-af08-86000e332c6b, event_name=click, event_time=2025-06-12T01:53:58.568Z, user_id=user_pej5ild, session_id=session_lyohdih, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 7
                        Compact and effi, element_type=div, element_name=null, track=product_click, productId=7}]}
2025-06-12 08:53:58.571 [http-nio-8080-exec-5 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 08:53:58.572 [http-nio-8080-exec-5 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 08:54:01.126 [http-nio-8080-exec-6 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=67fd585d-dbca-4694-86eb-53a7602d0bec, event_name=click, event_time=2025-06-12T01:53:59.073Z, user_id=user_pej5ild, session_id=session_lyohdih, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 8
                        Eco-friendly and, element_type=div, element_name=null, track=product_click, productId=8}, {event_id=ee248f63-5a18-43cc-949d-c89d39b6a682, event_name=click, event_time=2025-06-12T01:54:00.281Z, user_id=user_pej5ild, session_id=session_lyohdih, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 5
                        Bestseller with , element_type=div, element_name=null, track=product_click, productId=5}, {event_id=f744beb4-52e7-4a17-aa04-6fac946f6b8f, event_name=click, event_time=2025-06-12T01:54:01.123Z, user_id=user_pej5ild, session_id=session_lyohdih, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 9
                        Top-rated with e, element_type=div, element_name=null, track=product_click, productId=9}]}
2025-06-12 08:54:01.126 [http-nio-8080-exec-6 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 08:54:01.127 [http-nio-8080-exec-6 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 08:54:35.906 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor-1, groupId=spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-12 08:54:35.906 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor-1, groupId=spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-12 08:54:35.912 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:54:35.912 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:54:35.912 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-12 08:54:35.912 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:54:35.914 [SpringApplicationShutdownHook INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-06-12 08:54:35.914 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor-1 unregistered
2025-06-12 08:54:35.915 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor-2, groupId=spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-12 08:54:35.915 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor-2, groupId=spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-12 08:54:35.916 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:54:35.916 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:54:35.916 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-12 08:54:35.916 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:54:35.917 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:54:35.917 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:54:35.917 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-12 08:54:35.917 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:54:35.917 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-1 unregistered
2025-06-12 08:54:35.918 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-3a1918ec-d1a6-4618-bd60-82e6460152a5--1376088029-executor-2 unregistered
2025-06-12 08:54:35.918 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-12 08:54:35.918 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-12 08:54:37.073 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 87251 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-12 08:54:37.074 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-12 08:54:37.552 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-12 08:54:37.554 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-12 08:54:37.555 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-12 08:54:37.555 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-12 08:54:37.598 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-12 08:54:37.598 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 505 ms
2025-06-12 08:54:37.742 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-12 08:54:37.893 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-12 08:54:37.906 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-12 08:54:37.919 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:54:37.920 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:54:37.920 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749693277919
2025-06-12 08:54:38.002 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-12 08:54:38.004 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:54:38.004 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:54:38.004 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:54:38.009 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-12 08:54:38.013 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-12 08:54:38.021 [main INFO ] com.example.Application - Started Application in 1.109 seconds (JVM running for 1.402)
2025-06-12 08:54:38.292 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-12 08:54:38.292 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-12 08:54:38.293 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-12 08:54:38.315 [Thread-1 INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-12 08:54:38.368 [Thread-1 WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-12 08:54:38.399 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-12 08:54:38.399 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-12 08:54:38.399 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-12 08:54:38.399 [Thread-1 INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-12 08:54:38.407 [Thread-1 INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-12 08:54:38.413 [Thread-1 INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-12 08:54:38.413 [Thread-1 INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-12 08:54:38.430 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-12 08:54:38.431 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-12 08:54:38.431 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-12 08:54:38.431 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-12 08:54:38.431 [Thread-1 INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-12 08:54:38.523 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 33421.
2025-06-12 08:54:38.534 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-12 08:54:38.545 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-12 08:54:38.551 [Thread-1 INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-12 08:54:38.552 [Thread-1 INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-12 08:54:38.553 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-12 08:54:38.558 [Thread-1 INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-174eab2e-0326-4c84-9e4a-9c4a56e0aecc
2025-06-12 08:54:38.572 [Thread-1 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-12 08:54:38.578 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-12 08:54:38.587 [Thread-1 INFO ] org.sparkproject.jetty.util.log - Logging initialized @1967ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-12 08:54:38.622 [Thread-1 INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-12 08:54:38.626 [Thread-1 INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-12 08:54:38.632 [Thread-1 INFO ] org.sparkproject.jetty.server.Server - Started @2013ms
2025-06-12 08:54:38.643 [Thread-1 INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@19c58191{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-12 08:54:38.643 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-12 08:54:38.649 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5499a6de{/,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.677 [Thread-1 INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-12 08:54:38.679 [Thread-1 INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-12 08:54:38.687 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35057.
2025-06-12 08:54:38.687 [Thread-1 INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:35057
2025-06-12 08:54:38.687 [Thread-1 INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-12 08:54:38.690 [Thread-1 INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 35057, None)
2025-06-12 08:54:38.692 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:35057 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 35057, None)
2025-06-12 08:54:38.693 [Thread-1 INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 35057, None)
2025-06-12 08:54:38.693 [Thread-1 INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 35057, None)
2025-06-12 08:54:38.706 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@5499a6de{/,null,STOPPED,@Spark}
2025-06-12 08:54:38.707 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c09dc32{/jobs,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.707 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@adc8624{/jobs/json,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.708 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@364230ce{/jobs/job,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.708 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@719bff20{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.709 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@568e2c8{/stages,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.709 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@612c720d{/stages/json,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.709 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34360535{/stages/stage,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.710 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@43dd6463{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.710 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5319a78a{/stages/pool,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.710 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29572ae6{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.711 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70d60281{/storage,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.711 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c8d3ecd{/storage/json,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.711 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c0ba722{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.712 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c20630a{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.712 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6adcd872{/environment,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.712 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e034093{/environment/json,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.713 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@739a1e62{/executors,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.713 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74622643{/executors/json,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.714 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6221f5b9{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.714 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b29ae3{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.716 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8f2ce71{/static,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.717 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2425a468{/,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.718 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c9de6c7{/api,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.718 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c2c6fd2{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.718 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f44d37{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.720 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1584844e{/metrics/json,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.784 [Thread-1 WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-12 08:54:38.784 [Thread-1 INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-12 08:54:38.787 [Thread-1 INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-12 08:54:38.792 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49241439{/SQL,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.792 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f7ac8df{/SQL/json,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.792 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51a95d3b{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.793 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22cbf812{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-12 08:54:38.793 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c0eaedf{/static/sql,null,AVAILABLE,@Spark}
2025-06-12 08:54:39.643 [Thread-1 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-12 08:54:39.648 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33a8a458{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-12 08:54:39.649 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@433f7c98{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-12 08:54:39.649 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@459c3852{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-12 08:54:39.649 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a6fb1f2{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-12 08:54:39.650 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35a41440{/static/sql,null,AVAILABLE,@Spark}
2025-06-12 08:54:39.652 [Thread-1 WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-cb48b128-c3fd-4da8-88f1-d727ca498b7f. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-12 08:54:39.660 [Thread-1 INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-cb48b128-c3fd-4da8-88f1-d727ca498b7f resolved to file:/tmp/temporary-cb48b128-c3fd-4da8-88f1-d727ca498b7f.
2025-06-12 08:54:39.660 [Thread-1 WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-12 08:54:39.693 [Thread-1 INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-cb48b128-c3fd-4da8-88f1-d727ca498b7f/metadata using temp file file:/tmp/temporary-cb48b128-c3fd-4da8-88f1-d727ca498b7f/.metadata.c8b08e37-6c5c-4482-a1e5-a5e66fefec84.tmp
2025-06-12 08:54:39.729 [Thread-1 INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-cb48b128-c3fd-4da8-88f1-d727ca498b7f/.metadata.c8b08e37-6c5c-4482-a1e5-a5e66fefec84.tmp to file:/tmp/temporary-cb48b128-c3fd-4da8-88f1-d727ca498b7f/metadata
2025-06-12 08:54:39.741 [Thread-1 INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db]. Use file:/tmp/temporary-cb48b128-c3fd-4da8-88f1-d727ca498b7f to store the query checkpoint.
2025-06-12 08:54:39.746 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@11113be3] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@2be67070]
2025-06-12 08:54:39.758 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-12 08:54:39.758 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-12 08:54:39.758 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-12 08:54:39.759 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-12 08:54:39.859 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-12 08:54:39.861 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-12 08:54:39.861 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:54:39.861 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:54:39.861 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749693279861
2025-06-12 08:54:39.884 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-cb48b128-c3fd-4da8-88f1-d727ca498b7f/sources/0/0 using temp file file:/tmp/temporary-cb48b128-c3fd-4da8-88f1-d727ca498b7f/sources/0/.0.c1e6d48f-3b90-47f4-bdc4-d6f165318c04.tmp
2025-06-12 08:54:39.895 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-cb48b128-c3fd-4da8-88f1-d727ca498b7f/sources/0/.0.c1e6d48f-3b90-47f4-bdc4-d6f165318c04.tmp to file:/tmp/temporary-cb48b128-c3fd-4da8-88f1-d727ca498b7f/sources/0/0
2025-06-12 08:54:39.896 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":8,"0":8}}
2025-06-12 08:54:39.906 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-cb48b128-c3fd-4da8-88f1-d727ca498b7f/offsets/0 using temp file file:/tmp/temporary-cb48b128-c3fd-4da8-88f1-d727ca498b7f/offsets/.0.1e18d8da-acf3-4613-8dd2-2f49b1943c57.tmp
2025-06-12 08:54:39.921 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-cb48b128-c3fd-4da8-88f1-d727ca498b7f/offsets/.0.1e18d8da-acf3-4613-8dd2-2f49b1943c57.tmp to file:/tmp/temporary-cb48b128-c3fd-4da8-88f1-d727ca498b7f/offsets/0
2025-06-12 08:54:39.922 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749693279901,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-06-12 08:54:40.054 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:54:40.076 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:54:40.110 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:54:40.111 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:54:40.301 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 121.90224 ms
2025-06-12 08:54:40.461 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.920275 ms
2025-06-12 08:54:40.506 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] org.apache.spark.SparkContext - Starting job: start at Processor.java:94
2025-06-12 08:54:40.513 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at Processor.java:94) with 2 output partitions
2025-06-12 08:54:40.513 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (start at Processor.java:94)
2025-06-12 08:54:40.513 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-12 08:54:40.514 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-12 08:54:40.516 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[8] at start at Processor.java:94), which has no missing parents
2025-06-12 08:54:40.575 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 39.1 KiB, free 9.2 GiB)
2025-06-12 08:54:40.583 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 9.2 GiB)
2025-06-12 08:54:40.584 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:35057 (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 08:54:40.586 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-12 08:54:40.590 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at start at Processor.java:94) (first 15 tasks are for partitions Vector(0, 1))
2025-06-12 08:54:40.590 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-12 08:54:40.609 [dispatcher-event-loop-3 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8314 bytes) 
2025-06-12 08:54:40.610 [dispatcher-event-loop-3 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8314 bytes) 
2025-06-12 08:54:40.614 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-12 08:54:40.614 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-12 08:54:40.710 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 16.314723 ms
2025-06-12 08:54:40.726 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.135045 ms
2025-06-12 08:54:40.745 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 5.687998 ms
2025-06-12 08:54:40.755 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.92767 ms
2025-06-12 08:54:40.762 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-12 08:54:40.762 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-12 08:54:40.776 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-12 08:54:40.776 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-12 08:54:40.794 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:54:40.794 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:54:40.794 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749693280793
2025-06-12 08:54:40.794 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:54:40.794 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:54:40.794 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749693280793
2025-06-12 08:54:40.794 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor-2, groupId=spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor] Assigned to partition(s): clickstream-events-1
2025-06-12 08:54:40.794 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor-1, groupId=spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor] Assigned to partition(s): clickstream-events-0
2025-06-12 08:54:40.798 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor-2, groupId=spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor] Seeking to offset 8 for partition clickstream-events-1
2025-06-12 08:54:40.798 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor-1, groupId=spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor] Seeking to offset 8 for partition clickstream-events-0
2025-06-12 08:54:40.802 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor-2, groupId=spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor] Cluster ID: 8UN1o2zlTiO_QDl5CHzccg
2025-06-12 08:54:40.802 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor-1, groupId=spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor] Cluster ID: 8UN1o2zlTiO_QDl5CHzccg
2025-06-12 08:54:40.820 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor-1, groupId=spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-12 08:54:40.820 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor-2, groupId=spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-12 08:54:41.322 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor-1, groupId=spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:54:41.322 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor-2, groupId=spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:54:41.322 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor-1, groupId=spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-12 08:54:41.322 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor-2, groupId=spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-12 08:54:41.323 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor-1, groupId=spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=17, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:54:41.323 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor-2, groupId=spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=11, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:54:41.365 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:54:41.365 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:54:41.378 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:54:41.378 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:54:41.378 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [3d67ea14-9384-4d2c-8256-dd6b67e9a2fb] (2 queries & 0 savepoints) is committed.
2025-06-12 08:54:41.378 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [5578c129-1037-4c74-a70a-fcf89c3dc0c1] (2 queries & 0 savepoints) is committed.
2025-06-12 08:54:41.378 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [b6790adf-4396-4161-bf71-e84d206ec751] (0 queries & 0 savepoints) is committed.
2025-06-12 08:54:41.378 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [389f29a8-6c9e-4935-a044-41e934e7b1de] (0 queries & 0 savepoints) is committed.
2025-06-12 08:54:41.386 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1688 bytes result sent to driver
2025-06-12 08:54:41.386 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1688 bytes result sent to driver
2025-06-12 08:54:41.390 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 780 ms on phamviethoa (executor driver) (1/2)
2025-06-12 08:54:41.391 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 786 ms on phamviethoa (executor driver) (2/2)
2025-06-12 08:54:41.391 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-12 08:54:41.394 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (start at Processor.java:94) finished in 0.872 s
2025-06-12 08:54:41.395 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-12 08:54:41.395 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-12 08:54:41.396 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at Processor.java:94, took 0.889664 s
2025-06-12 08:54:41.415 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] terminated with error
org.apache.spark.SparkSQLException: Unsupported type TIMESTAMP_WITH_TIMEZONE.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedJdbcTypeError(QueryExecutionErrors.scala:1019)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getCatalystType(JdbcUtils.scala:239)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getSchema$1(JdbcUtils.scala:321)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:321)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:71)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:88)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.Processor.lambda$main$a450ce84$1(Processor.java:91)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
2025-06-12 08:54:41.416 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-2 unregistered
2025-06-12 08:54:41.417 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:54:41.417 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:54:41.417 [kafka-admin-client-thread | adminclient-2 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:54:41.417 [stream execution thread for [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = c425e023-f9f7-47af-8a05-a7f0b98b1872, runId = 297e6d04-a693-4d77-981f-7a9b660c37db] has been shutdown
2025-06-12 08:55:24.637 [http-nio-8080-exec-1 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=45dbc9c8-a793-46e0-8f2b-4865056f9149, event_name=click, event_time=2025-06-12T01:54:01.522Z, user_id=user_pej5ild, session_id=session_lyohdih, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 6
                        Sleek design and, element_type=div, element_name=null, track=product_click, productId=6}, {event_id=8ba3d9c7-25ac-43db-bbd6-ecf2360a69a6, event_name=scroll, event_time=2025-06-12T01:54:08.251Z, user_id=user_pej5ild, session_id=session_lyohdih, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=0}, {event_id=4ba96673-eeb5-4ff5-b09f-d15697f4c8c5, event_name=tab_change, event_time=2025-06-12T01:55:18.991Z, user_id=user_pej5ild, session_id=session_lyohdih, platform=web, screen_resolution=1920x1080, viewport_size=1287x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, is_visible=false, time_visible=0}]}
2025-06-12 08:55:24.637 [http-nio-8080-exec-1 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 08:55:24.641 [http-nio-8080-exec-1 INFO ] o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 1000
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-06-12 08:55:24.642 [http-nio-8080-exec-1 INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-12 08:55:24.644 [http-nio-8080-exec-1 INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-06-12 08:55:24.650 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:55:24.650 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:55:24.650 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749693324650
2025-06-12 08:55:24.653 [kafka-producer-network-thread | producer-1 INFO ] org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: 8UN1o2zlTiO_QDl5CHzccg
2025-06-12 08:55:24.654 [kafka-producer-network-thread | producer-1 INFO ] o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 2 with epoch 0
2025-06-12 08:55:24.656 [http-nio-8080-exec-1 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 08:57:21.067 [http-nio-8080-exec-2 WARN ] o.s.w.s.m.s.DefaultHandlerExceptionResolver - Resolved [org.springframework.web.HttpMediaTypeNotSupportedException: Content type 'text/plain;charset=UTF-8' not supported]
2025-06-12 08:57:26.567 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=c677a1c1-7fb2-4362-8824-85703982a3c3, event_name=page_view, event_time=2025-06-12T01:57:22.965Z, user_id=user_pej5ild, session_id=session_foelo66, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/}, {event_id=211c41a0-1c5b-439b-add0-bfa6c649d714, event_name=scroll, event_time=2025-06-12T01:57:25.563Z, user_id=user_pej5ild, session_id=session_foelo66, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=1}, {event_id=0155fb38-d19a-4be1-af7d-8fd767b4c63a, event_name=scroll, event_time=2025-06-12T01:57:26.564Z, user_id=user_pej5ild, session_id=session_foelo66, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=77}]}
2025-06-12 08:57:26.567 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 08:57:26.568 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 08:57:27.931 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=ff92dbf5-16e7-41a4-adb8-cf010f6921cf, event_name=click, event_time=2025-06-12T01:57:27.454Z, user_id=user_pej5ild, session_id=session_foelo66, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 3
                        Customer favorit, element_type=div, element_name=null, track=product_click, productId=3}, {event_id=309aa63e-c2db-4f57-8533-2191d1902ce2, event_name=scroll, event_time=2025-06-12T01:57:27.565Z, user_id=user_pej5ild, session_id=session_foelo66, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=100}, {event_id=ba07b64b-55eb-46d9-a2f8-ed52a38cf980, event_name=click, event_time=2025-06-12T01:57:27.928Z, user_id=user_pej5ild, session_id=session_foelo66, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 2
                        Durable and styl, element_type=div, element_name=null, track=product_click, productId=2}]}
2025-06-12 08:57:27.931 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 08:57:27.932 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 08:57:29.142 [http-nio-8080-exec-5 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=b5570d22-cda1-4127-9840-5d9b29d7ad96, event_name=click, event_time=2025-06-12T01:57:28.377Z, user_id=user_pej5ild, session_id=session_foelo66, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 1
                        High-quality ite, element_type=div, element_name=null, track=product_click, productId=1}, {event_id=6fa81473-1644-4d70-9d9e-7e957db1462b, event_name=click, event_time=2025-06-12T01:57:28.762Z, user_id=user_pej5ild, session_id=session_foelo66, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 4
                        Reliable and aff, element_type=div, element_name=null, track=product_click, productId=4}, {event_id=97d77e0a-1f6f-457b-972f-a3d4acaa9c8b, event_name=click, event_time=2025-06-12T01:57:29.139Z, user_id=user_pej5ild, session_id=session_foelo66, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 5
                        Bestseller with , element_type=div, element_name=null, track=product_click, productId=5}]}
2025-06-12 08:57:29.142 [http-nio-8080-exec-5 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 08:57:29.142 [http-nio-8080-exec-5 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 08:57:30.157 [http-nio-8080-exec-6 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=91a17065-ade1-4d85-88ef-ca203402107d, event_name=click, event_time=2025-06-12T01:57:29.565Z, user_id=user_pej5ild, session_id=session_foelo66, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 6
                        Sleek design and, element_type=div, element_name=null, track=product_click, productId=6}, {event_id=decf6938-b86f-401f-9867-40452f1f87c2, event_name=click, event_time=2025-06-12T01:57:29.882Z, user_id=user_pej5ild, session_id=session_foelo66, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 9
                        Top-rated with e, element_type=div, element_name=null, track=product_click, productId=9}, {event_id=2a4fcef6-d039-421b-a062-f2bcad0df66a, event_name=click, event_time=2025-06-12T01:57:30.154Z, user_id=user_pej5ild, session_id=session_foelo66, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 8
                        Eco-friendly and, element_type=div, element_name=null, track=product_click, productId=8}]}
2025-06-12 08:57:30.157 [http-nio-8080-exec-6 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 08:57:30.158 [http-nio-8080-exec-6 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 08:57:54.598 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor-1, groupId=spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-12 08:57:54.598 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor-1, groupId=spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-12 08:57:54.601 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:57:54.601 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:57:54.602 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-12 08:57:54.602 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:57:54.604 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor-1 unregistered
2025-06-12 08:57:54.605 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor-2, groupId=spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-12 08:57:54.605 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor-2, groupId=spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-12 08:57:54.605 [SpringApplicationShutdownHook INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-06-12 08:57:54.606 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:57:54.606 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:57:54.606 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-12 08:57:54.606 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:57:54.607 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:57:54.607 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:57:54.607 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-12 08:57:54.607 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:57:54.607 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-1 unregistered
2025-06-12 08:57:54.608 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-63e2533b-915a-4d6f-8106-767f91d230ba--1397239959-executor-2 unregistered
2025-06-12 08:57:54.608 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-12 08:57:54.608 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-12 08:57:55.773 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 90008 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-12 08:57:55.774 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-12 08:57:56.280 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-12 08:57:56.284 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-12 08:57:56.284 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-12 08:57:56.284 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-12 08:57:56.331 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-12 08:57:56.331 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 537 ms
2025-06-12 08:57:56.481 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-12 08:57:56.629 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-12 08:57:56.645 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-12 08:57:56.660 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:57:56.660 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:57:56.660 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749693476659
2025-06-12 08:57:56.751 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-12 08:57:56.753 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 08:57:56.754 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 08:57:56.754 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 08:57:56.758 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-12 08:57:56.763 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-12 08:57:56.773 [main INFO ] com.example.Application - Started Application in 1.16 seconds (JVM running for 1.461)
2025-06-12 08:57:56.993 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-12 08:57:56.993 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-12 08:57:56.994 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-12 08:57:57.068 [Thread-1 INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-12 08:57:57.117 [Thread-1 WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-12 08:57:57.151 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-12 08:57:57.151 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-12 08:57:57.151 [Thread-1 INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-12 08:57:57.152 [Thread-1 INFO ] org.apache.spark.SparkContext - Submitted application: Processor
2025-06-12 08:57:57.160 [Thread-1 INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-12 08:57:57.166 [Thread-1 INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-12 08:57:57.166 [Thread-1 INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-12 08:57:57.189 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-12 08:57:57.189 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-12 08:57:57.189 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-12 08:57:57.189 [Thread-1 INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-12 08:57:57.189 [Thread-1 INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-12 08:57:57.284 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 41111.
2025-06-12 08:57:57.298 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-12 08:57:57.313 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-12 08:57:57.320 [Thread-1 INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-12 08:57:57.320 [Thread-1 INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-12 08:57:57.322 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-12 08:57:57.327 [Thread-1 INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-b48d8019-988b-4d06-9307-6cc7cbc88216
2025-06-12 08:57:57.341 [Thread-1 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-12 08:57:57.347 [Thread-1 INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-12 08:57:57.355 [Thread-1 INFO ] org.sparkproject.jetty.util.log - Logging initialized @2043ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-12 08:57:57.388 [Thread-1 INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-12 08:57:57.392 [Thread-1 INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-12 08:57:57.397 [Thread-1 INFO ] org.sparkproject.jetty.server.Server - Started @2086ms
2025-06-12 08:57:57.407 [Thread-1 INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@63984d14{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-12 08:57:57.407 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-12 08:57:57.414 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75d1945e{/,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.441 [Thread-1 INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-12 08:57:57.443 [Thread-1 INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-12 08:57:57.451 [Thread-1 INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38963.
2025-06-12 08:57:57.451 [Thread-1 INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:38963
2025-06-12 08:57:57.451 [Thread-1 INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-12 08:57:57.454 [Thread-1 INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 38963, None)
2025-06-12 08:57:57.456 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:38963 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 38963, None)
2025-06-12 08:57:57.457 [Thread-1 INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 38963, None)
2025-06-12 08:57:57.457 [Thread-1 INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 38963, None)
2025-06-12 08:57:57.471 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@75d1945e{/,null,STOPPED,@Spark}
2025-06-12 08:57:57.471 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@be694d2{/jobs,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.472 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1633a298{/jobs/json,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.472 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79cbde3{/jobs/job,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.472 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42568052{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.473 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2bb03a74{/stages,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.473 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@171e51c{/stages/json,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.474 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c40c6ea{/stages/stage,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.474 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58c44120{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.474 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@32b141a6{/stages/pool,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.475 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a64f0c7{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.475 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5faf1bb5{/storage,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.475 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@199da86b{/storage/json,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.476 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78ef3a0{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.476 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@32188e1{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.476 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a5547f3{/environment,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.477 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fd99b35{/environment/json,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.477 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@402d5e00{/executors,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.477 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@176ad6{/executors/json,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.478 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7561db3f{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.478 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f03a41b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.481 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1927e3e3{/static,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.481 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c80662d{/,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.482 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54bb58d6{/api,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.482 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61b1c6dc{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.482 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38490819{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.484 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fee7d23{/metrics/json,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.548 [Thread-1 WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-12 08:57:57.548 [Thread-1 INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-12 08:57:57.551 [Thread-1 INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-12 08:57:57.555 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@163b1511{/SQL,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.556 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e1adffb{/SQL/json,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.556 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79f6ac17{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.557 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61ec8c82{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-12 08:57:57.557 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36b9bfd1{/static/sql,null,AVAILABLE,@Spark}
2025-06-12 08:57:58.472 [Thread-1 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-12 08:57:58.478 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c0ab783{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-12 08:57:58.478 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e00c605{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-12 08:57:58.478 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78bee0d0{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-12 08:57:58.479 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e40cffc{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-12 08:57:58.479 [Thread-1 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@496c2840{/static/sql,null,AVAILABLE,@Spark}
2025-06-12 08:57:58.482 [Thread-1 WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-12 08:57:58.491 [Thread-1 INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103 resolved to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103.
2025-06-12 08:57:58.491 [Thread-1 WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-12 08:57:58.528 [Thread-1 INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/metadata using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/.metadata.359ab6cb-10c0-4a98-ba8b-3a44c3c5d8a6.tmp
2025-06-12 08:57:58.563 [Thread-1 INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/.metadata.359ab6cb-10c0-4a98-ba8b-3a44c3c5d8a6.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/metadata
2025-06-12 08:57:58.576 [Thread-1 INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2]. Use file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103 to store the query checkpoint.
2025-06-12 08:57:58.580 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@224a8738] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@5e8fe21a]
2025-06-12 08:57:58.590 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-12 08:57:58.591 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-12 08:57:58.591 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-12 08:57:58.592 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-12 08:57:58.690 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-12 08:57:58.692 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-12 08:57:58.692 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:57:58.692 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:57:58.692 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749693478692
2025-06-12 08:57:58.714 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/sources/0/0 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/sources/0/.0.6365cbf6-f8a1-4049-86d7-0a770ea0bec8.tmp
2025-06-12 08:57:58.726 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/sources/0/.0.6365cbf6-f8a1-4049-86d7-0a770ea0bec8.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/sources/0/0
2025-06-12 08:57:58.726 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":11,"0":20}}
2025-06-12 08:57:58.736 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/0 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.0.b50a8ad3-4a4d-4ae5-82dc-4110e284e037.tmp
2025-06-12 08:57:58.752 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.0.b50a8ad3-4a4d-4ae5-82dc-4110e284e037.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/0
2025-06-12 08:57:58.752 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749693478732,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-06-12 08:57:58.881 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:57:58.900 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:57:58.933 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:57:58.933 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:57:59.104 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 105.089195 ms
2025-06-12 08:57:59.244 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.412833 ms
2025-06-12 08:57:59.285 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] org.apache.spark.SparkContext - Starting job: start at Processor.java:94
2025-06-12 08:57:59.291 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at Processor.java:94) with 2 output partitions
2025-06-12 08:57:59.291 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (start at Processor.java:94)
2025-06-12 08:57:59.291 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-12 08:57:59.291 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-12 08:57:59.293 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[8] at start at Processor.java:94), which has no missing parents
2025-06-12 08:57:59.351 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 39.1 KiB, free 9.2 GiB)
2025-06-12 08:57:59.359 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 9.2 GiB)
2025-06-12 08:57:59.361 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:38963 (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 08:57:59.362 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-12 08:57:59.367 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at start at Processor.java:94) (first 15 tasks are for partitions Vector(0, 1))
2025-06-12 08:57:59.367 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-12 08:57:59.387 [dispatcher-event-loop-3 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 08:57:59.389 [dispatcher-event-loop-3 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 08:57:59.392 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-12 08:57:59.392 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-12 08:57:59.478 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 15.340527 ms
2025-06-12 08:57:59.493 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.882768 ms
2025-06-12 08:57:59.511 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 5.399056 ms
2025-06-12 08:57:59.521 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.137155 ms
2025-06-12 08:57:59.528 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-12 08:57:59.528 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-12 08:57:59.541 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-12 08:57:59.541 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-12 08:57:59.558 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:57:59.558 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:57:59.558 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749693479558
2025-06-12 08:57:59.558 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:57:59.558 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:57:59.558 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749693479558
2025-06-12 08:57:59.559 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Assigned to partition(s): clickstream-events-1
2025-06-12 08:57:59.559 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Assigned to partition(s): clickstream-events-0
2025-06-12 08:57:59.562 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to offset 11 for partition clickstream-events-1
2025-06-12 08:57:59.562 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to offset 20 for partition clickstream-events-0
2025-06-12 08:57:59.567 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Cluster ID: 8UN1o2zlTiO_QDl5CHzccg
2025-06-12 08:57:59.567 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Cluster ID: 8UN1o2zlTiO_QDl5CHzccg
2025-06-12 08:57:59.585 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-12 08:57:59.585 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-12 08:58:00.088 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=20, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:58:00.088 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-12 08:58:00.088 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=11, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:58:00.088 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-12 08:58:00.088 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=25, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:58:00.089 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=18, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:58:00.131 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:58:00.131 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:58:00.143 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:58:00.143 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:58:00.143 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [03372ded-9578-4928-8cca-6adfdecef9e5] (2 queries & 0 savepoints) is committed.
2025-06-12 08:58:00.143 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [ea86588d-6c99-4103-86b4-0e7ca47794af] (2 queries & 0 savepoints) is committed.
2025-06-12 08:58:00.143 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [e9f71f70-df4b-4c67-bda9-bcc7cbe7d4d8] (0 queries & 0 savepoints) is committed.
2025-06-12 08:58:00.143 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [6d955b62-086a-4405-8399-e11ce9f31657] (0 queries & 0 savepoints) is committed.
2025-06-12 08:58:00.151 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1688 bytes result sent to driver
2025-06-12 08:58:00.151 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1688 bytes result sent to driver
2025-06-12 08:58:00.155 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 772 ms on phamviethoa (executor driver) (1/2)
2025-06-12 08:58:00.156 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 767 ms on phamviethoa (executor driver) (2/2)
2025-06-12 08:58:00.156 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-12 08:58:00.158 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (start at Processor.java:94) finished in 0.861 s
2025-06-12 08:58:00.160 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-12 08:58:00.160 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-12 08:58:00.161 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at Processor.java:94, took 0.875668 s
2025-06-12 08:58:00.184 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/0 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.0.fa5750cc-f96b-455e-ad6d-0186c5c9d398.tmp
2025-06-12 08:58:00.196 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.0.fa5750cc-f96b-455e-ad6d-0186c5c9d398.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/0
2025-06-12 08:58:00.209 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T01:57:58.589Z",
  "batchId" : 0,
  "numInputRows" : 12,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 7.462686567164178,
  "durationMs" : {
    "addBatch" : 1258,
    "commitOffsets" : 16,
    "getBatch" : 10,
    "latestOffset" : 138,
    "queryPlanning" : 154,
    "triggerExecution" : 1608,
    "walCommit" : 19
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : null,
    "endOffset" : {
      "clickstream-events" : {
        "1" : 18,
        "0" : 25
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 18,
        "0" : 25
      }
    },
    "numInputRows" : 12,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 7.462686567164178,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 08:58:10.206 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T01:58:10.205Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 18,
        "0" : 25
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 18,
        "0" : 25
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 18,
        "0" : 25
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 08:58:20.216 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T01:58:20.215Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 18,
        "0" : 25
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 18,
        "0" : 25
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 18,
        "0" : 25
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 08:58:30.216 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T01:58:30.215Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 18,
        "0" : 25
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 18,
        "0" : 25
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 18,
        "0" : 25
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 08:58:31.675 [http-nio-8080-exec-1 WARN ] o.s.w.s.m.s.DefaultHandlerExceptionResolver - Resolved [org.springframework.web.HttpMediaTypeNotSupportedException: Content type 'text/plain;charset=UTF-8' not supported]
2025-06-12 08:58:36.411 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=2204351d-5618-4ec9-b488-8f3ee8f644d6, event_name=page_view, event_time=2025-06-12T01:58:33.377Z, user_id=user_pej5ild, session_id=session_u34vnqj, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/}, {event_id=8331d0f9-ef35-417a-a414-d7f637ac1f84, event_name=scroll, event_time=2025-06-12T01:58:35.402Z, user_id=user_pej5ild, session_id=session_u34vnqj, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=1}, {event_id=5dc77616-8bf7-4ea3-b49a-cc92160b56ad, event_name=scroll, event_time=2025-06-12T01:58:36.402Z, user_id=user_pej5ild, session_id=session_u34vnqj, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=77}]}
2025-06-12 08:58:36.411 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 08:58:36.415 [http-nio-8080-exec-2 INFO ] o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 1000
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-06-12 08:58:36.416 [http-nio-8080-exec-2 INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-12 08:58:36.419 [http-nio-8080-exec-2 INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-06-12 08:58:36.425 [http-nio-8080-exec-2 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 08:58:36.425 [http-nio-8080-exec-2 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 08:58:36.425 [http-nio-8080-exec-2 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749693516425
2025-06-12 08:58:36.428 [kafka-producer-network-thread | producer-1 INFO ] org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: 8UN1o2zlTiO_QDl5CHzccg
2025-06-12 08:58:36.430 [kafka-producer-network-thread | producer-1 INFO ] o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 3 with epoch 0
2025-06-12 08:58:36.432 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 08:58:36.447 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/1 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.1.093030c3-6368-4b30-a8b7-c60fce240bc5.tmp
2025-06-12 08:58:36.456 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.1.093030c3-6368-4b30-a8b7-c60fce240bc5.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/1
2025-06-12 08:58:36.456 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1749693516444,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-06-12 08:58:36.469 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:58:36.469 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:58:36.478 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:58:36.478 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:58:36.519 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] org.apache.spark.SparkContext - Starting job: start at Processor.java:94
2025-06-12 08:58:36.519 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 1 (start at Processor.java:94) with 2 output partitions
2025-06-12 08:58:36.520 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at Processor.java:94)
2025-06-12 08:58:36.520 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-12 08:58:36.520 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-12 08:58:36.520 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[17] at start at Processor.java:94), which has no missing parents
2025-06-12 08:58:36.524 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 39.1 KiB, free 9.2 GiB)
2025-06-12 08:58:36.526 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 9.2 GiB)
2025-06-12 08:58:36.527 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:38963 (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 08:58:36.527 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-12 08:58:36.527 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[17] at start at Processor.java:94) (first 15 tasks are for partitions Vector(0, 1))
2025-06-12 08:58:36.527 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 2 tasks resource profile 0
2025-06-12 08:58:36.528 [dispatcher-event-loop-2 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 08:58:36.528 [dispatcher-event-loop-2 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 1.0 (TID 3) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 08:58:36.528 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-12 08:58:36.528 [Executor task launch worker for task 1.0 in stage 1.0 (TID 3) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 1.0 (TID 3)
2025-06-12 08:58:36.538 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to offset 18 for partition clickstream-events-1
2025-06-12 08:58:36.539 [Executor task launch worker for task 1.0 in stage 1.0 (TID 3) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to offset 25 for partition clickstream-events-0
2025-06-12 08:58:36.541 [Executor task launch worker for task 1.0 in stage 1.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-12 08:58:36.541 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-12 08:58:37.042 [Executor task launch worker for task 1.0 in stage 1.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=25, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:58:37.042 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=18, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:58:37.042 [Executor task launch worker for task 1.0 in stage 1.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-12 08:58:37.042 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-12 08:58:37.043 [Executor task launch worker for task 1.0 in stage 1.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=26, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:58:37.043 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=20, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:58:37.052 [Executor task launch worker for task 1.0 in stage 1.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:58:37.053 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:58:37.057 [Executor task launch worker for task 1.0 in stage 1.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:58:37.057 [Executor task launch worker for task 1.0 in stage 1.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [5b5280cc-5768-4708-a2bf-61f91cb3b19d] (2 queries & 0 savepoints) is committed.
2025-06-12 08:58:37.058 [Executor task launch worker for task 1.0 in stage 1.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [88121fa6-9ca9-45e5-b6e7-5828bc637ccf] (0 queries & 0 savepoints) is committed.
2025-06-12 08:58:37.059 [Executor task launch worker for task 1.0 in stage 1.0 (TID 3) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 1.0 (TID 3). 1688 bytes result sent to driver
2025-06-12 08:58:37.060 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 1.0 (TID 3) in 532 ms on phamviethoa (executor driver) (1/2)
2025-06-12 08:58:37.060 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:58:37.060 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [0b48f05f-0c83-4191-81e5-4e79be75ecab] (2 queries & 0 savepoints) is committed.
2025-06-12 08:58:37.060 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [5b9c9c17-cf80-4836-b36d-602c9d459bc3] (0 queries & 0 savepoints) is committed.
2025-06-12 08:58:37.061 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 1688 bytes result sent to driver
2025-06-12 08:58:37.062 [task-result-getter-3 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 534 ms on phamviethoa (executor driver) (2/2)
2025-06-12 08:58:37.062 [task-result-getter-3 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-12 08:58:37.063 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at Processor.java:94) finished in 0.542 s
2025-06-12 08:58:37.063 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-12 08:58:37.063 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-12 08:58:37.063 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.spark.scheduler.DAGScheduler - Job 1 finished: start at Processor.java:94, took 0.543817 s
2025-06-12 08:58:37.077 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/1 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.1.dfd2d8e0-3dad-4818-8fc3-73dbc8598e8c.tmp
2025-06-12 08:58:37.086 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.1.dfd2d8e0-3dad-4818-8fc3-73dbc8598e8c.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/1
2025-06-12 08:58:37.087 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T01:58:36.443Z",
  "batchId" : 1,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 272.72727272727275,
  "processedRowsPerSecond" : 4.665629860031104,
  "durationMs" : {
    "addBatch" : 603,
    "commitOffsets" : 12,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 13,
    "triggerExecution" : 643,
    "walCommit" : 13
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 18,
        "0" : 25
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 20,
        "0" : 26
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 20,
        "0" : 26
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 272.72727272727275,
    "processedRowsPerSecond" : 4.665629860031104,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 08:58:37.994 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=f0bc4214-8735-44b8-9f4d-d0f92332a478, event_name=click, event_time=2025-06-12T01:58:37.305Z, user_id=user_pej5ild, session_id=session_u34vnqj, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 1
                        High-quality ite, element_type=div, element_name=null, track=product_click, productId=1}, {event_id=e3584db5-b8ba-4688-8864-94ff9a8fba7c, event_name=scroll, event_time=2025-06-12T01:58:37.404Z, user_id=user_pej5ild, session_id=session_u34vnqj, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=100}, {event_id=a5f91bcf-6fb6-4722-a99f-052b42025de2, event_name=click, event_time=2025-06-12T01:58:37.992Z, user_id=user_pej5ild, session_id=session_u34vnqj, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 2
                        Durable and styl, element_type=div, element_name=null, track=product_click, productId=2}]}
2025-06-12 08:58:37.994 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 08:58:37.995 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 08:58:38.001 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/2 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.2.fcb1e1f9-b7fc-4a71-a6a5-56590967f2a0.tmp
2025-06-12 08:58:38.010 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.2.fcb1e1f9-b7fc-4a71-a6a5-56590967f2a0.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/2
2025-06-12 08:58:38.010 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1749693517998,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-06-12 08:58:38.020 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:58:38.020 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:58:38.026 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:58:38.027 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:58:38.064 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] org.apache.spark.SparkContext - Starting job: start at Processor.java:94
2025-06-12 08:58:38.065 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 2 (start at Processor.java:94) with 2 output partitions
2025-06-12 08:58:38.065 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (start at Processor.java:94)
2025-06-12 08:58:38.065 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-12 08:58:38.065 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-12 08:58:38.065 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[26] at start at Processor.java:94), which has no missing parents
2025-06-12 08:58:38.069 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 39.1 KiB, free 9.2 GiB)
2025-06-12 08:58:38.071 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 9.2 GiB)
2025-06-12 08:58:38.072 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on phamviethoa:38963 (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 08:58:38.072 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1535
2025-06-12 08:58:38.072 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[26] at start at Processor.java:94) (first 15 tasks are for partitions Vector(0, 1))
2025-06-12 08:58:38.072 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 2 tasks resource profile 0
2025-06-12 08:58:38.073 [dispatcher-event-loop-0 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 4) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 08:58:38.073 [dispatcher-event-loop-0 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 5) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 08:58:38.073 [Executor task launch worker for task 0.0 in stage 2.0 (TID 4) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 4)
2025-06-12 08:58:38.073 [Executor task launch worker for task 1.0 in stage 2.0 (TID 5) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 5)
2025-06-12 08:58:38.083 [Executor task launch worker for task 0.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to offset 20 for partition clickstream-events-1
2025-06-12 08:58:38.083 [Executor task launch worker for task 1.0 in stage 2.0 (TID 5) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to offset 26 for partition clickstream-events-0
2025-06-12 08:58:38.084 [Executor task launch worker for task 0.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-12 08:58:38.084 [Executor task launch worker for task 1.0 in stage 2.0 (TID 5) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-12 08:58:38.584 [Executor task launch worker for task 0.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=18, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:58:38.585 [Executor task launch worker for task 0.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-12 08:58:38.585 [Executor task launch worker for task 0.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=21, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:58:38.585 [Executor task launch worker for task 1.0 in stage 2.0 (TID 5) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=25, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:58:38.585 [Executor task launch worker for task 1.0 in stage 2.0 (TID 5) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-12 08:58:38.585 [Executor task launch worker for task 1.0 in stage 2.0 (TID 5) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=28, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:58:38.592 [Executor task launch worker for task 1.0 in stage 2.0 (TID 5) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:58:38.594 [Executor task launch worker for task 0.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:58:38.598 [Executor task launch worker for task 1.0 in stage 2.0 (TID 5) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:58:38.598 [Executor task launch worker for task 1.0 in stage 2.0 (TID 5) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [343ac36f-0c18-43e9-b755-9ad016d76579] (2 queries & 0 savepoints) is committed.
2025-06-12 08:58:38.598 [Executor task launch worker for task 1.0 in stage 2.0 (TID 5) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [f4154bd2-8857-4580-808a-eb01a415b78c] (0 queries & 0 savepoints) is committed.
2025-06-12 08:58:38.599 [Executor task launch worker for task 1.0 in stage 2.0 (TID 5) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 5). 1688 bytes result sent to driver
2025-06-12 08:58:38.599 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 5) in 526 ms on phamviethoa (executor driver) (1/2)
2025-06-12 08:58:38.600 [Executor task launch worker for task 0.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:58:38.600 [Executor task launch worker for task 0.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [0725c972-925f-421d-b01c-0159bf894451] (2 queries & 0 savepoints) is committed.
2025-06-12 08:58:38.600 [Executor task launch worker for task 0.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [bfc7705d-ed87-4696-b2fb-c6f3c3245e4c] (0 queries & 0 savepoints) is committed.
2025-06-12 08:58:38.602 [Executor task launch worker for task 0.0 in stage 2.0 (TID 4) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 4). 1688 bytes result sent to driver
2025-06-12 08:58:38.602 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 4) in 529 ms on phamviethoa (executor driver) (2/2)
2025-06-12 08:58:38.602 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2025-06-12 08:58:38.603 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 2 (start at Processor.java:94) finished in 0.537 s
2025-06-12 08:58:38.603 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-12 08:58:38.603 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage finished
2025-06-12 08:58:38.603 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.spark.scheduler.DAGScheduler - Job 2 finished: start at Processor.java:94, took 0.538402 s
2025-06-12 08:58:38.614 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/2 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.2.c5e4ee92-782d-4351-8ec8-2ce438280f0c.tmp
2025-06-12 08:58:38.623 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.2.c5e4ee92-782d-4351-8ec8-2ce438280f0c.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/2
2025-06-12 08:58:38.624 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T01:58:37.997Z",
  "batchId" : 2,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 272.72727272727275,
  "processedRowsPerSecond" : 4.792332268370607,
  "durationMs" : {
    "addBatch" : 590,
    "commitOffsets" : 12,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 11,
    "triggerExecution" : 626,
    "walCommit" : 12
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 20,
        "0" : 26
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 21,
        "0" : 28
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 21,
        "0" : 28
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 272.72727272727275,
    "processedRowsPerSecond" : 4.792332268370607,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 08:58:39.041 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=df83c23c-c5c2-4d91-9f69-f3a2bc71e1ac, event_name=click, event_time=2025-06-12T01:58:38.356Z, user_id=user_pej5ild, session_id=session_u34vnqj, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 3
                        Customer favorit, element_type=div, element_name=null, track=product_click, productId=3}, {event_id=a531a688-9fdf-4cdb-b1dd-243381559c79, event_name=click, event_time=2025-06-12T01:58:38.756Z, user_id=user_pej5ild, session_id=session_u34vnqj, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 6
                        Sleek design and, element_type=div, element_name=null, track=product_click, productId=6}, {event_id=4d16909c-e054-440b-aae2-42eb8ea17edb, event_name=click, event_time=2025-06-12T01:58:39.038Z, user_id=user_pej5ild, session_id=session_u34vnqj, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 5
                        Bestseller with , element_type=div, element_name=null, track=product_click, productId=5}]}
2025-06-12 08:58:39.041 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 08:58:39.042 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 08:58:39.052 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/3 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.3.cd77467a-0c7c-4558-a92f-192c5910c9c5.tmp
2025-06-12 08:58:39.060 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.3.cd77467a-0c7c-4558-a92f-192c5910c9c5.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/3
2025-06-12 08:58:39.060 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1749693519049,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-06-12 08:58:39.069 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:58:39.070 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:58:39.076 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:58:39.077 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:58:39.113 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] org.apache.spark.SparkContext - Starting job: start at Processor.java:94
2025-06-12 08:58:39.113 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 3 (start at Processor.java:94) with 2 output partitions
2025-06-12 08:58:39.113 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (start at Processor.java:94)
2025-06-12 08:58:39.113 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-12 08:58:39.113 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-12 08:58:39.114 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[35] at start at Processor.java:94), which has no missing parents
2025-06-12 08:58:39.119 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 39.1 KiB, free 9.2 GiB)
2025-06-12 08:58:39.121 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 9.2 GiB)
2025-06-12 08:58:39.121 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on phamviethoa:38963 (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 08:58:39.121 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1535
2025-06-12 08:58:39.121 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[35] at start at Processor.java:94) (first 15 tasks are for partitions Vector(0, 1))
2025-06-12 08:58:39.121 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 2 tasks resource profile 0
2025-06-12 08:58:39.122 [dispatcher-event-loop-1 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 6) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 08:58:39.122 [dispatcher-event-loop-1 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 7) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 08:58:39.123 [Executor task launch worker for task 1.0 in stage 3.0 (TID 7) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 7)
2025-06-12 08:58:39.123 [Executor task launch worker for task 0.0 in stage 3.0 (TID 6) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 6)
2025-06-12 08:58:39.132 [Executor task launch worker for task 1.0 in stage 3.0 (TID 7) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to offset 28 for partition clickstream-events-0
2025-06-12 08:58:39.133 [Executor task launch worker for task 0.0 in stage 3.0 (TID 6) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to offset 21 for partition clickstream-events-1
2025-06-12 08:58:39.133 [Executor task launch worker for task 1.0 in stage 3.0 (TID 7) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-12 08:58:39.134 [Executor task launch worker for task 0.0 in stage 3.0 (TID 6) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-12 08:58:39.634 [Executor task launch worker for task 1.0 in stage 3.0 (TID 7) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=25, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:58:39.635 [Executor task launch worker for task 1.0 in stage 3.0 (TID 7) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-12 08:58:39.635 [Executor task launch worker for task 0.0 in stage 3.0 (TID 6) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=18, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:58:39.635 [Executor task launch worker for task 0.0 in stage 3.0 (TID 6) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-12 08:58:39.635 [Executor task launch worker for task 1.0 in stage 3.0 (TID 7) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=30, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:58:39.635 [Executor task launch worker for task 0.0 in stage 3.0 (TID 6) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=22, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:58:39.642 [Executor task launch worker for task 0.0 in stage 3.0 (TID 6) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:58:39.642 [Executor task launch worker for task 1.0 in stage 3.0 (TID 7) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:58:39.647 [Executor task launch worker for task 0.0 in stage 3.0 (TID 6) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:58:39.647 [Executor task launch worker for task 0.0 in stage 3.0 (TID 6) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [2ad9131e-8c3f-485e-a0df-ae92c03f5e63] (2 queries & 0 savepoints) is committed.
2025-06-12 08:58:39.647 [Executor task launch worker for task 0.0 in stage 3.0 (TID 6) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [2a3d7bd2-09b2-44c0-b237-0ad1c1f9180a] (0 queries & 0 savepoints) is committed.
2025-06-12 08:58:39.647 [Executor task launch worker for task 1.0 in stage 3.0 (TID 7) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:58:39.647 [Executor task launch worker for task 1.0 in stage 3.0 (TID 7) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [5b872ffe-0647-43bf-a14b-b89c92ed7f39] (2 queries & 0 savepoints) is committed.
2025-06-12 08:58:39.647 [Executor task launch worker for task 1.0 in stage 3.0 (TID 7) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [7e6330bb-0a27-4212-b0a1-e42e5ef36ed9] (0 queries & 0 savepoints) is committed.
2025-06-12 08:58:39.649 [Executor task launch worker for task 0.0 in stage 3.0 (TID 6) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 6). 1688 bytes result sent to driver
2025-06-12 08:58:39.649 [Executor task launch worker for task 1.0 in stage 3.0 (TID 7) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 7). 1688 bytes result sent to driver
2025-06-12 08:58:39.649 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 6) in 527 ms on phamviethoa (executor driver) (1/2)
2025-06-12 08:58:39.649 [task-result-getter-3 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 7) in 527 ms on phamviethoa (executor driver) (2/2)
2025-06-12 08:58:39.649 [task-result-getter-3 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2025-06-12 08:58:39.650 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 3 (start at Processor.java:94) finished in 0.535 s
2025-06-12 08:58:39.650 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-12 08:58:39.650 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 3: Stage finished
2025-06-12 08:58:39.650 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.spark.scheduler.DAGScheduler - Job 3 finished: start at Processor.java:94, took 0.536929 s
2025-06-12 08:58:39.663 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/3 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.3.8a6d42e7-8935-4b49-83e0-c467d8a96296.tmp
2025-06-12 08:58:39.673 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.3.8a6d42e7-8935-4b49-83e0-c467d8a96296.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/3
2025-06-12 08:58:39.673 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T01:58:39.048Z",
  "batchId" : 3,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 272.72727272727275,
  "processedRowsPerSecond" : 4.8,
  "durationMs" : {
    "addBatch" : 589,
    "commitOffsets" : 12,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 9,
    "triggerExecution" : 625,
    "walCommit" : 11
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 21,
        "0" : 28
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 22,
        "0" : 30
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 22,
        "0" : 30
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 272.72727272727275,
    "processedRowsPerSecond" : 4.8,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 08:58:40.086 [http-nio-8080-exec-5 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=ae479fa9-e327-4299-a60f-dc81b3a8bb93, event_name=click, event_time=2025-06-12T01:58:39.412Z, user_id=user_pej5ild, session_id=session_u34vnqj, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 4
                        Reliable and aff, element_type=div, element_name=null, track=product_click, productId=4}, {event_id=f3f2ba81-133d-495c-bc78-9a6aa38ec405, event_name=click, event_time=2025-06-12T01:58:39.712Z, user_id=user_pej5ild, session_id=session_u34vnqj, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 7
                        Compact and effi, element_type=div, element_name=null, track=product_click, productId=7}, {event_id=c66b778c-dbd1-4b0f-86b0-7d0c6814676a, event_name=click, event_time=2025-06-12T01:58:40.083Z, user_id=user_pej5ild, session_id=session_u34vnqj, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 8
                        Eco-friendly and, element_type=div, element_name=null, track=product_click, productId=8}]}
2025-06-12 08:58:40.086 [http-nio-8080-exec-5 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 08:58:40.087 [http-nio-8080-exec-5 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 08:58:40.093 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/4 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.4.1a4c10b1-3b88-47cc-8371-db945a1c05ff.tmp
2025-06-12 08:58:40.103 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.4.1a4c10b1-3b88-47cc-8371-db945a1c05ff.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/4
2025-06-12 08:58:40.103 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1749693520090,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-06-12 08:58:40.111 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:58:40.111 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:58:40.119 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:58:40.119 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:58:40.174 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] org.apache.spark.SparkContext - Starting job: start at Processor.java:94
2025-06-12 08:58:40.179 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 4 (start at Processor.java:94) with 1 output partitions
2025-06-12 08:58:40.179 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 4 (start at Processor.java:94)
2025-06-12 08:58:40.179 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-12 08:58:40.179 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-12 08:58:40.180 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 4 (MapPartitionsRDD[44] at start at Processor.java:94), which has no missing parents
2025-06-12 08:58:40.182 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on phamviethoa:38963 in memory (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 08:58:40.185 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 39.0 KiB, free 9.2 GiB)
2025-06-12 08:58:40.186 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 9.2 GiB)
2025-06-12 08:58:40.186 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on phamviethoa:38963 (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 08:58:40.187 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1535
2025-06-12 08:58:40.187 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on phamviethoa:38963 in memory (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 08:58:40.187 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[44] at start at Processor.java:94) (first 15 tasks are for partitions Vector(0))
2025-06-12 08:58:40.187 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks resource profile 0
2025-06-12 08:58:40.187 [dispatcher-event-loop-2 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 8) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 08:58:40.188 [Executor task launch worker for task 0.0 in stage 4.0 (TID 8) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 8)
2025-06-12 08:58:40.189 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on phamviethoa:38963 in memory (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 08:58:40.190 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on phamviethoa:38963 in memory (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 08:58:40.196 [Executor task launch worker for task 0.0 in stage 4.0 (TID 8) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to offset 22 for partition clickstream-events-1
2025-06-12 08:58:40.198 [Executor task launch worker for task 0.0 in stage 4.0 (TID 8) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-12 08:58:40.699 [Executor task launch worker for task 0.0 in stage 4.0 (TID 8) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=18, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:58:40.699 [Executor task launch worker for task 0.0 in stage 4.0 (TID 8) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-12 08:58:40.700 [Executor task launch worker for task 0.0 in stage 4.0 (TID 8) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=25, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:58:40.707 [Executor task launch worker for task 0.0 in stage 4.0 (TID 8) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:58:40.712 [Executor task launch worker for task 0.0 in stage 4.0 (TID 8) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:58:40.712 [Executor task launch worker for task 0.0 in stage 4.0 (TID 8) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [3af87b30-c574-40ef-9bb9-b9db1c043a78] (2 queries & 0 savepoints) is committed.
2025-06-12 08:58:40.712 [Executor task launch worker for task 0.0 in stage 4.0 (TID 8) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [552916ff-20a5-4f18-b940-414bcb0d23d0] (0 queries & 0 savepoints) is committed.
2025-06-12 08:58:40.713 [Executor task launch worker for task 0.0 in stage 4.0 (TID 8) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 8). 1688 bytes result sent to driver
2025-06-12 08:58:40.713 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 8) in 526 ms on phamviethoa (executor driver) (1/1)
2025-06-12 08:58:40.713 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2025-06-12 08:58:40.714 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 4 (start at Processor.java:94) finished in 0.532 s
2025-06-12 08:58:40.714 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-12 08:58:40.714 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 4: Stage finished
2025-06-12 08:58:40.714 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.spark.scheduler.DAGScheduler - Job 4 finished: start at Processor.java:94, took 0.535378 s
2025-06-12 08:58:40.725 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/4 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.4.5e0aff30-c85c-40e8-8de7-91403a2d5605.tmp
2025-06-12 08:58:40.734 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.4.5e0aff30-c85c-40e8-8de7-91403a2d5605.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/4
2025-06-12 08:58:40.734 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T01:58:40.089Z",
  "batchId" : 4,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 272.72727272727275,
  "processedRowsPerSecond" : 4.651162790697675,
  "durationMs" : {
    "addBatch" : 609,
    "commitOffsets" : 12,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 9,
    "triggerExecution" : 645,
    "walCommit" : 13
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 22,
        "0" : 30
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 25,
        "0" : 30
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 25,
        "0" : 30
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 272.72727272727275,
    "processedRowsPerSecond" : 4.651162790697675,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 08:58:42.955 [http-nio-8080-exec-6 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=16aafe8c-8564-4df7-97dc-fc5a98969978, event_name=click, event_time=2025-06-12T01:58:40.471Z, user_id=user_pej5ild, session_id=session_u34vnqj, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 9
                        Top-rated with e, element_type=div, element_name=null, track=product_click, productId=9}, {event_id=52016f35-4886-4064-8869-8d89ff78f274, event_name=scroll, event_time=2025-06-12T01:58:40.952Z, user_id=user_pej5ild, session_id=session_u34vnqj, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=99}, {event_id=f42d3ba8-0734-44c5-beb1-5b4a84d2019e, event_name=scroll, event_time=2025-06-12T01:58:42.952Z, user_id=user_pej5ild, session_id=session_u34vnqj, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=0}]}
2025-06-12 08:58:42.955 [http-nio-8080-exec-6 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 08:58:42.956 [http-nio-8080-exec-6 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 08:58:42.968 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/5 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.5.e136d4fd-8504-44f6-a9e0-be328e0144e1.tmp
2025-06-12 08:58:42.977 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.5.e136d4fd-8504-44f6-a9e0-be328e0144e1.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/5
2025-06-12 08:58:42.977 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1749693522965,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-06-12 08:58:42.984 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:58:42.985 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:58:42.990 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:58:42.991 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:58:43.022 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] org.apache.spark.SparkContext - Starting job: start at Processor.java:94
2025-06-12 08:58:43.023 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 5 (start at Processor.java:94) with 2 output partitions
2025-06-12 08:58:43.023 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (start at Processor.java:94)
2025-06-12 08:58:43.023 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-12 08:58:43.023 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-12 08:58:43.024 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[53] at start at Processor.java:94), which has no missing parents
2025-06-12 08:58:43.028 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 39.1 KiB, free 9.2 GiB)
2025-06-12 08:58:43.029 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 9.2 GiB)
2025-06-12 08:58:43.029 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on phamviethoa:38963 (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 08:58:43.029 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1535
2025-06-12 08:58:43.030 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[53] at start at Processor.java:94) (first 15 tasks are for partitions Vector(0, 1))
2025-06-12 08:58:43.030 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 2 tasks resource profile 0
2025-06-12 08:58:43.030 [dispatcher-event-loop-2 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 9) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 08:58:43.030 [dispatcher-event-loop-2 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 5.0 (TID 10) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 08:58:43.031 [Executor task launch worker for task 1.0 in stage 5.0 (TID 10) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 5.0 (TID 10)
2025-06-12 08:58:43.031 [Executor task launch worker for task 0.0 in stage 5.0 (TID 9) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 9)
2025-06-12 08:58:43.038 [Executor task launch worker for task 0.0 in stage 5.0 (TID 9) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to offset 25 for partition clickstream-events-1
2025-06-12 08:58:43.038 [Executor task launch worker for task 1.0 in stage 5.0 (TID 10) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to offset 30 for partition clickstream-events-0
2025-06-12 08:58:43.040 [Executor task launch worker for task 0.0 in stage 5.0 (TID 9) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-12 08:58:43.040 [Executor task launch worker for task 1.0 in stage 5.0 (TID 10) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-12 08:58:43.540 [Executor task launch worker for task 1.0 in stage 5.0 (TID 10) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=25, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:58:43.541 [Executor task launch worker for task 0.0 in stage 5.0 (TID 9) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=18, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:58:43.541 [Executor task launch worker for task 0.0 in stage 5.0 (TID 9) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-12 08:58:43.541 [Executor task launch worker for task 1.0 in stage 5.0 (TID 10) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-12 08:58:43.541 [Executor task launch worker for task 1.0 in stage 5.0 (TID 10) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=31, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:58:43.541 [Executor task launch worker for task 0.0 in stage 5.0 (TID 9) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=27, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:58:43.548 [Executor task launch worker for task 1.0 in stage 5.0 (TID 10) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:58:43.548 [Executor task launch worker for task 0.0 in stage 5.0 (TID 9) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:58:43.554 [Executor task launch worker for task 1.0 in stage 5.0 (TID 10) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:58:43.554 [Executor task launch worker for task 1.0 in stage 5.0 (TID 10) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [704bd71a-ccb8-4567-adca-bb3a3e18664b] (2 queries & 0 savepoints) is committed.
2025-06-12 08:58:43.554 [Executor task launch worker for task 0.0 in stage 5.0 (TID 9) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:58:43.554 [Executor task launch worker for task 0.0 in stage 5.0 (TID 9) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [84e3b284-c142-470b-af15-370145c470d2] (2 queries & 0 savepoints) is committed.
2025-06-12 08:58:43.554 [Executor task launch worker for task 1.0 in stage 5.0 (TID 10) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [f8131d36-f150-4493-92f9-4891d073cad2] (0 queries & 0 savepoints) is committed.
2025-06-12 08:58:43.554 [Executor task launch worker for task 0.0 in stage 5.0 (TID 9) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [428f462d-cf54-4612-aeab-5b10cc1c6fe5] (0 queries & 0 savepoints) is committed.
2025-06-12 08:58:43.555 [Executor task launch worker for task 1.0 in stage 5.0 (TID 10) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 5.0 (TID 10). 1688 bytes result sent to driver
2025-06-12 08:58:43.555 [Executor task launch worker for task 0.0 in stage 5.0 (TID 9) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 9). 1688 bytes result sent to driver
2025-06-12 08:58:43.555 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 5.0 (TID 10) in 525 ms on phamviethoa (executor driver) (1/2)
2025-06-12 08:58:43.555 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 9) in 525 ms on phamviethoa (executor driver) (2/2)
2025-06-12 08:58:43.555 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2025-06-12 08:58:43.556 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 5 (start at Processor.java:94) finished in 0.532 s
2025-06-12 08:58:43.556 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-12 08:58:43.556 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 5: Stage finished
2025-06-12 08:58:43.556 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.spark.scheduler.DAGScheduler - Job 5 finished: start at Processor.java:94, took 0.534156 s
2025-06-12 08:58:43.568 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/5 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.5.c7cf3ebe-3171-49d0-bfe6-fec22b0edda2.tmp
2025-06-12 08:58:43.577 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.5.c7cf3ebe-3171-49d0-bfe6-fec22b0edda2.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/5
2025-06-12 08:58:43.578 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T01:58:42.964Z",
  "batchId" : 5,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 272.72727272727275,
  "processedRowsPerSecond" : 4.88599348534202,
  "durationMs" : {
    "addBatch" : 580,
    "commitOffsets" : 12,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 8,
    "triggerExecution" : 614,
    "walCommit" : 12
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 25,
        "0" : 30
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 27,
        "0" : 31
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 27,
        "0" : 31
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 272.72727272727275,
    "processedRowsPerSecond" : 4.88599348534202,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 08:58:53.587 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T01:58:53.586Z",
  "batchId" : 6,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 0,
    "triggerExecution" : 0
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 27,
        "0" : 31
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 27,
        "0" : 31
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 27,
        "0" : 31
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 08:59:03.593 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T01:59:03.592Z",
  "batchId" : 6,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 27,
        "0" : 31
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 27,
        "0" : 31
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 27,
        "0" : 31
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 08:59:13.593 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T01:59:13.591Z",
  "batchId" : 6,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 27,
        "0" : 31
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 27,
        "0" : 31
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 27,
        "0" : 31
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 08:59:23.600 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T01:59:23.598Z",
  "batchId" : 6,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 27,
        "0" : 31
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 27,
        "0" : 31
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 27,
        "0" : 31
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 08:59:33.600 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T01:59:33.599Z",
  "batchId" : 6,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 27,
        "0" : 31
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 27,
        "0" : 31
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 27,
        "0" : 31
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 08:59:43.608 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T01:59:43.607Z",
  "batchId" : 6,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 0,
    "triggerExecution" : 0
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 27,
        "0" : 31
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 27,
        "0" : 31
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 27,
        "0" : 31
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 08:59:52.555 [http-nio-8080-exec-7 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=b9e3a636-9efb-4fb5-ba02-61ab6b23e42b, event_name=tab_change, event_time=2025-06-12T01:58:42.968Z, user_id=user_pej5ild, session_id=session_u34vnqj, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, is_visible=false, time_visible=0}, {event_id=4d77b70b-f884-43cb-9974-598c369219b2, event_name=tab_change, event_time=2025-06-12T01:58:43.586Z, user_id=user_pej5ild, session_id=session_u34vnqj, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, is_visible=true, time_visible=0}, {event_id=3395c5da-b0ee-4a7f-aff2-314608881ef3, event_name=scroll, event_time=2025-06-12T01:59:52.552Z, user_id=user_pej5ild, session_id=session_u34vnqj, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=61}]}
2025-06-12 08:59:52.555 [http-nio-8080-exec-7 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 08:59:52.557 [http-nio-8080-exec-7 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 08:59:52.571 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/6 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.6.9f1055bf-f997-4dcb-a923-560491560165.tmp
2025-06-12 08:59:52.580 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.6.9f1055bf-f997-4dcb-a923-560491560165.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/6
2025-06-12 08:59:52.580 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1749693592568,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-06-12 08:59:52.587 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:59:52.588 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:59:52.592 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:59:52.593 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:59:52.625 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] org.apache.spark.SparkContext - Starting job: start at Processor.java:94
2025-06-12 08:59:52.626 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 6 (start at Processor.java:94) with 1 output partitions
2025-06-12 08:59:52.626 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (start at Processor.java:94)
2025-06-12 08:59:52.626 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-12 08:59:52.626 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-12 08:59:52.626 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (MapPartitionsRDD[62] at start at Processor.java:94), which has no missing parents
2025-06-12 08:59:52.629 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 39.0 KiB, free 9.2 GiB)
2025-06-12 08:59:52.630 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 9.2 GiB)
2025-06-12 08:59:52.630 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on phamviethoa:38963 (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 08:59:52.630 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1535
2025-06-12 08:59:52.631 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[62] at start at Processor.java:94) (first 15 tasks are for partitions Vector(0))
2025-06-12 08:59:52.631 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks resource profile 0
2025-06-12 08:59:52.631 [dispatcher-event-loop-0 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 11) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 08:59:52.632 [Executor task launch worker for task 0.0 in stage 6.0 (TID 11) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 11)
2025-06-12 08:59:52.640 [Executor task launch worker for task 0.0 in stage 6.0 (TID 11) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to offset 31 for partition clickstream-events-0
2025-06-12 08:59:52.642 [Executor task launch worker for task 0.0 in stage 6.0 (TID 11) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-12 08:59:53.143 [Executor task launch worker for task 0.0 in stage 6.0 (TID 11) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=25, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:59:53.143 [Executor task launch worker for task 0.0 in stage 6.0 (TID 11) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-12 08:59:53.144 [Executor task launch worker for task 0.0 in stage 6.0 (TID 11) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=34, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:59:53.151 [Executor task launch worker for task 0.0 in stage 6.0 (TID 11) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:59:53.156 [Executor task launch worker for task 0.0 in stage 6.0 (TID 11) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:59:53.156 [Executor task launch worker for task 0.0 in stage 6.0 (TID 11) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [6f5a2630-33ba-4a98-b9cc-dceccfb99c23] (2 queries & 0 savepoints) is committed.
2025-06-12 08:59:53.156 [Executor task launch worker for task 0.0 in stage 6.0 (TID 11) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [97d99624-b73f-4363-ac80-3be2451b07df] (0 queries & 0 savepoints) is committed.
2025-06-12 08:59:53.157 [Executor task launch worker for task 0.0 in stage 6.0 (TID 11) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 11). 1688 bytes result sent to driver
2025-06-12 08:59:53.157 [task-result-getter-3 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 11) in 526 ms on phamviethoa (executor driver) (1/1)
2025-06-12 08:59:53.157 [task-result-getter-3 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
2025-06-12 08:59:53.158 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 6 (start at Processor.java:94) finished in 0.531 s
2025-06-12 08:59:53.158 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-12 08:59:53.158 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 6: Stage finished
2025-06-12 08:59:53.158 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.spark.scheduler.DAGScheduler - Job 6 finished: start at Processor.java:94, took 0.532402 s
2025-06-12 08:59:53.169 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/6 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.6.06dc3871-7081-4ff0-9147-4913bfbb61b9.tmp
2025-06-12 08:59:53.178 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.6.06dc3871-7081-4ff0-9147-4913bfbb61b9.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/6
2025-06-12 08:59:53.178 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T01:59:52.567Z",
  "batchId" : 6,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 272.72727272727275,
  "processedRowsPerSecond" : 4.909983633387889,
  "durationMs" : {
    "addBatch" : 577,
    "commitOffsets" : 12,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 7,
    "triggerExecution" : 611,
    "walCommit" : 13
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 27,
        "0" : 31
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 27,
        "0" : 34
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 27,
        "0" : 34
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 272.72727272727275,
    "processedRowsPerSecond" : 4.909983633387889,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 08:59:57.458 [http-nio-8080-exec-8 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=b68f5fcd-6126-4d5f-9715-1fc4d8091b93, event_name=scroll, event_time=2025-06-12T01:59:54.455Z, user_id=user_pej5ild, session_id=session_u34vnqj, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=62}, {event_id=e9a4b443-8201-419b-89c8-1a31c292b1b3, event_name=scroll, event_time=2025-06-12T01:59:56.455Z, user_id=user_pej5ild, session_id=session_u34vnqj, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=23}, {event_id=9567d32a-3586-430f-a3ba-6981ec1fc20c, event_name=scroll, event_time=2025-06-12T01:59:57.455Z, user_id=user_pej5ild, session_id=session_u34vnqj, platform=web, screen_resolution=1920x1080, viewport_size=1838x935, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0, language=vi, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=34}]}
2025-06-12 08:59:57.458 [http-nio-8080-exec-8 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 08:59:57.458 [http-nio-8080-exec-8 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 08:59:57.465 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/7 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.7.b780dc69-3b2d-4173-8d0c-5d53aa42af89.tmp
2025-06-12 08:59:57.474 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.7.b780dc69-3b2d-4173-8d0c-5d53aa42af89.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/7
2025-06-12 08:59:57.474 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 7. Metadata OffsetSeqMetadata(0,1749693597462,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-06-12 08:59:57.480 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:59:57.481 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:59:57.487 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:59:57.487 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 08:59:57.518 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] org.apache.spark.SparkContext - Starting job: start at Processor.java:94
2025-06-12 08:59:57.518 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 7 (start at Processor.java:94) with 2 output partitions
2025-06-12 08:59:57.518 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (start at Processor.java:94)
2025-06-12 08:59:57.518 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-12 08:59:57.518 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-12 08:59:57.518 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (MapPartitionsRDD[71] at start at Processor.java:94), which has no missing parents
2025-06-12 08:59:57.521 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 39.1 KiB, free 9.2 GiB)
2025-06-12 08:59:57.522 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 9.2 GiB)
2025-06-12 08:59:57.522 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on phamviethoa:38963 (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 08:59:57.522 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1535
2025-06-12 08:59:57.523 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[71] at start at Processor.java:94) (first 15 tasks are for partitions Vector(0, 1))
2025-06-12 08:59:57.523 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 2 tasks resource profile 0
2025-06-12 08:59:57.523 [dispatcher-event-loop-2 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 12) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 08:59:57.523 [dispatcher-event-loop-2 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 7.0 (TID 13) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 08:59:57.523 [Executor task launch worker for task 0.0 in stage 7.0 (TID 12) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 12)
2025-06-12 08:59:57.524 [Executor task launch worker for task 1.0 in stage 7.0 (TID 13) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 7.0 (TID 13)
2025-06-12 08:59:57.532 [Executor task launch worker for task 1.0 in stage 7.0 (TID 13) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to offset 34 for partition clickstream-events-0
2025-06-12 08:59:57.533 [Executor task launch worker for task 0.0 in stage 7.0 (TID 12) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to offset 27 for partition clickstream-events-1
2025-06-12 08:59:57.534 [Executor task launch worker for task 1.0 in stage 7.0 (TID 13) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-12 08:59:57.534 [Executor task launch worker for task 0.0 in stage 7.0 (TID 12) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-12 08:59:58.036 [Executor task launch worker for task 0.0 in stage 7.0 (TID 12) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=18, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:59:58.036 [Executor task launch worker for task 0.0 in stage 7.0 (TID 12) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-12 08:59:58.036 [Executor task launch worker for task 1.0 in stage 7.0 (TID 13) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=25, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:59:58.036 [Executor task launch worker for task 1.0 in stage 7.0 (TID 13) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-12 08:59:58.036 [Executor task launch worker for task 0.0 in stage 7.0 (TID 12) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=28, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:59:58.036 [Executor task launch worker for task 1.0 in stage 7.0 (TID 13) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=36, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 08:59:58.043 [Executor task launch worker for task 0.0 in stage 7.0 (TID 12) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:59:58.043 [Executor task launch worker for task 1.0 in stage 7.0 (TID 13) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:59:58.047 [Executor task launch worker for task 0.0 in stage 7.0 (TID 12) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:59:58.047 [Executor task launch worker for task 0.0 in stage 7.0 (TID 12) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [10487b5d-0f11-4a8f-90ba-a979ca6207ee] (2 queries & 0 savepoints) is committed.
2025-06-12 08:59:58.047 [Executor task launch worker for task 0.0 in stage 7.0 (TID 12) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [ee255467-84ac-4f34-9f6e-931a02c09248] (0 queries & 0 savepoints) is committed.
2025-06-12 08:59:58.048 [Executor task launch worker for task 0.0 in stage 7.0 (TID 12) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 12). 1688 bytes result sent to driver
2025-06-12 08:59:58.048 [Executor task launch worker for task 1.0 in stage 7.0 (TID 13) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 08:59:58.048 [Executor task launch worker for task 1.0 in stage 7.0 (TID 13) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [28a03168-a160-4289-8bae-5ac78324d4cc] (2 queries & 0 savepoints) is committed.
2025-06-12 08:59:58.048 [Executor task launch worker for task 1.0 in stage 7.0 (TID 13) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [508d4744-877f-46af-a251-30bc609e01a5] (0 queries & 0 savepoints) is committed.
2025-06-12 08:59:58.048 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 12) in 525 ms on phamviethoa (executor driver) (1/2)
2025-06-12 08:59:58.049 [Executor task launch worker for task 1.0 in stage 7.0 (TID 13) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 7.0 (TID 13). 1688 bytes result sent to driver
2025-06-12 08:59:58.049 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 7.0 (TID 13) in 526 ms on phamviethoa (executor driver) (2/2)
2025-06-12 08:59:58.049 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2025-06-12 08:59:58.050 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 7 (start at Processor.java:94) finished in 0.531 s
2025-06-12 08:59:58.050 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-12 08:59:58.050 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 7: Stage finished
2025-06-12 08:59:58.050 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.spark.scheduler.DAGScheduler - Job 7 finished: start at Processor.java:94, took 0.532323 s
2025-06-12 08:59:58.061 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/7 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.7.63b292f5-dfde-4a2f-88df-93a3ac76d467.tmp
2025-06-12 08:59:58.070 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.7.63b292f5-dfde-4a2f-88df-93a3ac76d467.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/7
2025-06-12 08:59:58.070 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T01:59:57.462Z",
  "batchId" : 7,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 272.72727272727275,
  "processedRowsPerSecond" : 4.934210526315789,
  "durationMs" : {
    "addBatch" : 576,
    "commitOffsets" : 12,
    "getBatch" : 0,
    "latestOffset" : 0,
    "queryPlanning" : 7,
    "triggerExecution" : 608,
    "walCommit" : 12
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 27,
        "0" : 34
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 28,
        "0" : 36
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 28,
        "0" : 36
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 272.72727272727275,
    "processedRowsPerSecond" : 4.934210526315789,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 09:00:03.711 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on phamviethoa:38963 in memory (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 09:00:03.712 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on phamviethoa:38963 in memory (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 09:00:03.714 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on phamviethoa:38963 in memory (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 09:00:03.715 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on phamviethoa:38963 in memory (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 09:00:08.077 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T02:00:08.076Z",
  "batchId" : 8,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 0,
    "triggerExecution" : 0
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 28,
        "0" : 36
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 28,
        "0" : 36
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 28,
        "0" : 36
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 09:00:18.088 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T02:00:18.086Z",
  "batchId" : 8,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 28,
        "0" : 36
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 28,
        "0" : 36
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 28,
        "0" : 36
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 09:00:22.626 [http-nio-8080-exec-9 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=6f170228-ca31-40bd-8422-a889ea69e425, event_name=page_view, event_time=2025-06-12T02:00:20.073Z, user_id=user_4q6eh9x, session_id=session_9lqote5, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/}, {event_id=6c1ff3cf-f466-4b7b-a564-02e001d8c25d, event_name=scroll, event_time=2025-06-12T02:00:21.624Z, user_id=user_4q6eh9x, session_id=session_9lqote5, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=1}, {event_id=cf45012f-af86-4489-ac0f-d8733797c396, event_name=scroll, event_time=2025-06-12T02:00:22.624Z, user_id=user_4q6eh9x, session_id=session_9lqote5, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=100}]}
2025-06-12 09:00:22.627 [http-nio-8080-exec-9 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 09:00:22.627 [http-nio-8080-exec-9 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 09:00:22.633 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/8 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.8.66564d30-0f45-46b0-bc5f-688ba5d32789.tmp
2025-06-12 09:00:22.641 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.8.66564d30-0f45-46b0-bc5f-688ba5d32789.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/8
2025-06-12 09:00:22.641 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 8. Metadata OffsetSeqMetadata(0,1749693622629,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-06-12 09:00:22.648 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 09:00:22.649 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 09:00:22.654 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 09:00:22.654 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 09:00:22.687 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] org.apache.spark.SparkContext - Starting job: start at Processor.java:94
2025-06-12 09:00:22.687 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 8 (start at Processor.java:94) with 2 output partitions
2025-06-12 09:00:22.687 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 (start at Processor.java:94)
2025-06-12 09:00:22.687 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-12 09:00:22.687 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-12 09:00:22.688 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 8 (MapPartitionsRDD[80] at start at Processor.java:94), which has no missing parents
2025-06-12 09:00:22.690 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 39.1 KiB, free 9.2 GiB)
2025-06-12 09:00:22.691 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 9.2 GiB)
2025-06-12 09:00:22.691 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on phamviethoa:38963 (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 09:00:22.691 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1535
2025-06-12 09:00:22.691 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 8 (MapPartitionsRDD[80] at start at Processor.java:94) (first 15 tasks are for partitions Vector(0, 1))
2025-06-12 09:00:22.691 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 2 tasks resource profile 0
2025-06-12 09:00:22.692 [dispatcher-event-loop-0 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 14) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 09:00:22.692 [dispatcher-event-loop-0 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 8.0 (TID 15) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 09:00:22.692 [Executor task launch worker for task 1.0 in stage 8.0 (TID 15) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 8.0 (TID 15)
2025-06-12 09:00:22.692 [Executor task launch worker for task 0.0 in stage 8.0 (TID 14) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 14)
2025-06-12 09:00:22.702 [Executor task launch worker for task 0.0 in stage 8.0 (TID 14) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to offset 28 for partition clickstream-events-1
2025-06-12 09:00:22.703 [Executor task launch worker for task 1.0 in stage 8.0 (TID 15) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to offset 36 for partition clickstream-events-0
2025-06-12 09:00:22.704 [Executor task launch worker for task 0.0 in stage 8.0 (TID 14) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-12 09:00:22.705 [Executor task launch worker for task 1.0 in stage 8.0 (TID 15) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-12 09:00:23.206 [Executor task launch worker for task 1.0 in stage 8.0 (TID 15) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=25, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 09:00:23.206 [Executor task launch worker for task 0.0 in stage 8.0 (TID 14) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=18, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 09:00:23.206 [Executor task launch worker for task 0.0 in stage 8.0 (TID 14) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-12 09:00:23.206 [Executor task launch worker for task 1.0 in stage 8.0 (TID 15) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-12 09:00:23.207 [Executor task launch worker for task 0.0 in stage 8.0 (TID 14) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=29, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 09:00:23.207 [Executor task launch worker for task 1.0 in stage 8.0 (TID 15) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=38, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 09:00:23.214 [Executor task launch worker for task 0.0 in stage 8.0 (TID 14) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 09:00:23.214 [Executor task launch worker for task 1.0 in stage 8.0 (TID 15) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 09:00:23.219 [Executor task launch worker for task 0.0 in stage 8.0 (TID 14) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 09:00:23.219 [Executor task launch worker for task 0.0 in stage 8.0 (TID 14) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [e072b9b5-7bd6-4597-832c-35b715f866c5] (2 queries & 0 savepoints) is committed.
2025-06-12 09:00:23.219 [Executor task launch worker for task 0.0 in stage 8.0 (TID 14) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [d6c23d5b-74d6-4faf-b372-bba837d6909c] (0 queries & 0 savepoints) is committed.
2025-06-12 09:00:23.220 [Executor task launch worker for task 1.0 in stage 8.0 (TID 15) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 09:00:23.220 [Executor task launch worker for task 1.0 in stage 8.0 (TID 15) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [a9929ad0-045e-4ab3-84f0-e0263a96778f] (2 queries & 0 savepoints) is committed.
2025-06-12 09:00:23.220 [Executor task launch worker for task 1.0 in stage 8.0 (TID 15) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [eec36bae-3c4c-442c-9254-22c3fe90705d] (0 queries & 0 savepoints) is committed.
2025-06-12 09:00:23.220 [Executor task launch worker for task 0.0 in stage 8.0 (TID 14) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 14). 1688 bytes result sent to driver
2025-06-12 09:00:23.221 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 14) in 529 ms on phamviethoa (executor driver) (1/2)
2025-06-12 09:00:23.221 [Executor task launch worker for task 1.0 in stage 8.0 (TID 15) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 8.0 (TID 15). 1688 bytes result sent to driver
2025-06-12 09:00:23.221 [task-result-getter-3 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 8.0 (TID 15) in 529 ms on phamviethoa (executor driver) (2/2)
2025-06-12 09:00:23.221 [task-result-getter-3 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool 
2025-06-12 09:00:23.222 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 8 (start at Processor.java:94) finished in 0.534 s
2025-06-12 09:00:23.222 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-12 09:00:23.222 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 8: Stage finished
2025-06-12 09:00:23.222 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.spark.scheduler.DAGScheduler - Job 8 finished: start at Processor.java:94, took 0.534936 s
2025-06-12 09:00:23.233 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/8 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.8.b9071279-b58d-4fef-a247-2d95a86fdf63.tmp
2025-06-12 09:00:23.242 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.8.b9071279-b58d-4fef-a247-2d95a86fdf63.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/8
2025-06-12 09:00:23.242 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T02:00:22.628Z",
  "batchId" : 8,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 272.72727272727275,
  "processedRowsPerSecond" : 4.88599348534202,
  "durationMs" : {
    "addBatch" : 580,
    "commitOffsets" : 12,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 7,
    "triggerExecution" : 614,
    "walCommit" : 12
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 28,
        "0" : 36
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 29,
        "0" : 38
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 29,
        "0" : 38
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 272.72727272727275,
    "processedRowsPerSecond" : 4.88599348534202,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 09:00:23.993 [http-nio-8080-exec-10 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=a42525b7-766e-4656-bb87-7075570da0bb, event_name=click, event_time=2025-06-12T02:00:22.911Z, user_id=user_4q6eh9x, session_id=session_9lqote5, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 2
                        Durable and styl, element_type=div, element_name=null, track=product_click, productId=2}, {event_id=3e40a0b9-a84b-4cdd-9e1a-c5c43bba319b, event_name=click, event_time=2025-06-12T02:00:23.478Z, user_id=user_4q6eh9x, session_id=session_9lqote5, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 3
                        Customer favorit, element_type=div, element_name=null, track=product_click, productId=3}, {event_id=43054ee6-0a51-4f81-a38b-3727c140860a, event_name=click, event_time=2025-06-12T02:00:23.991Z, user_id=user_4q6eh9x, session_id=session_9lqote5, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 6
                        Sleek design and, element_type=div, element_name=null, track=product_click, productId=6}]}
2025-06-12 09:00:23.993 [http-nio-8080-exec-10 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 09:00:23.993 [http-nio-8080-exec-10 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 09:00:24.001 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/9 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.9.9c6a3979-ca6b-4686-8eaf-1c86803db8f2.tmp
2025-06-12 09:00:24.010 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.9.9c6a3979-ca6b-4686-8eaf-1c86803db8f2.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/9
2025-06-12 09:00:24.010 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 9. Metadata OffsetSeqMetadata(0,1749693623998,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-06-12 09:00:24.018 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 09:00:24.018 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 09:00:24.024 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 09:00:24.025 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 09:00:24.055 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] org.apache.spark.SparkContext - Starting job: start at Processor.java:94
2025-06-12 09:00:24.055 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 9 (start at Processor.java:94) with 2 output partitions
2025-06-12 09:00:24.055 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 9 (start at Processor.java:94)
2025-06-12 09:00:24.055 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-12 09:00:24.055 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-12 09:00:24.056 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 9 (MapPartitionsRDD[89] at start at Processor.java:94), which has no missing parents
2025-06-12 09:00:24.058 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 39.1 KiB, free 9.2 GiB)
2025-06-12 09:00:24.059 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 9.2 GiB)
2025-06-12 09:00:24.060 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on phamviethoa:38963 (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 09:00:24.060 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1535
2025-06-12 09:00:24.060 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 9 (MapPartitionsRDD[89] at start at Processor.java:94) (first 15 tasks are for partitions Vector(0, 1))
2025-06-12 09:00:24.060 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 2 tasks resource profile 0
2025-06-12 09:00:24.061 [dispatcher-event-loop-2 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 16) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 09:00:24.061 [dispatcher-event-loop-2 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 9.0 (TID 17) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 09:00:24.061 [Executor task launch worker for task 0.0 in stage 9.0 (TID 16) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 16)
2025-06-12 09:00:24.061 [Executor task launch worker for task 1.0 in stage 9.0 (TID 17) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 9.0 (TID 17)
2025-06-12 09:00:24.072 [Executor task launch worker for task 0.0 in stage 9.0 (TID 16) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to offset 29 for partition clickstream-events-1
2025-06-12 09:00:24.074 [Executor task launch worker for task 0.0 in stage 9.0 (TID 16) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-12 09:00:24.075 [Executor task launch worker for task 1.0 in stage 9.0 (TID 17) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to offset 38 for partition clickstream-events-0
2025-06-12 09:00:24.076 [Executor task launch worker for task 1.0 in stage 9.0 (TID 17) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-12 09:00:24.576 [Executor task launch worker for task 0.0 in stage 9.0 (TID 16) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=18, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 09:00:24.576 [Executor task launch worker for task 0.0 in stage 9.0 (TID 16) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-12 09:00:24.577 [Executor task launch worker for task 0.0 in stage 9.0 (TID 16) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=31, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 09:00:24.578 [Executor task launch worker for task 1.0 in stage 9.0 (TID 17) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=25, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 09:00:24.578 [Executor task launch worker for task 1.0 in stage 9.0 (TID 17) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-12 09:00:24.579 [Executor task launch worker for task 1.0 in stage 9.0 (TID 17) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=39, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 09:00:24.584 [Executor task launch worker for task 0.0 in stage 9.0 (TID 16) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 09:00:24.585 [Executor task launch worker for task 1.0 in stage 9.0 (TID 17) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 09:00:24.589 [Executor task launch worker for task 0.0 in stage 9.0 (TID 16) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 09:00:24.589 [Executor task launch worker for task 0.0 in stage 9.0 (TID 16) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [4e050aa0-869a-4dde-823b-84c863f114c2] (2 queries & 0 savepoints) is committed.
2025-06-12 09:00:24.589 [Executor task launch worker for task 0.0 in stage 9.0 (TID 16) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [65f947bc-faf2-46a8-b9fc-7ab54ecd1416] (0 queries & 0 savepoints) is committed.
2025-06-12 09:00:24.590 [Executor task launch worker for task 0.0 in stage 9.0 (TID 16) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 16). 1688 bytes result sent to driver
2025-06-12 09:00:24.590 [Executor task launch worker for task 1.0 in stage 9.0 (TID 17) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 09:00:24.590 [Executor task launch worker for task 1.0 in stage 9.0 (TID 17) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [f9b7b83b-7905-47e8-bd48-2f95d1e5b30a] (2 queries & 0 savepoints) is committed.
2025-06-12 09:00:24.590 [Executor task launch worker for task 1.0 in stage 9.0 (TID 17) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [d80c7eec-ee53-49b1-bc01-a0e93c30ab0b] (0 queries & 0 savepoints) is committed.
2025-06-12 09:00:24.590 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 16) in 529 ms on phamviethoa (executor driver) (1/2)
2025-06-12 09:00:24.591 [Executor task launch worker for task 1.0 in stage 9.0 (TID 17) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 9.0 (TID 17). 1688 bytes result sent to driver
2025-06-12 09:00:24.592 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 9.0 (TID 17) in 531 ms on phamviethoa (executor driver) (2/2)
2025-06-12 09:00:24.592 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
2025-06-12 09:00:24.592 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 9 (start at Processor.java:94) finished in 0.536 s
2025-06-12 09:00:24.592 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-12 09:00:24.592 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 9: Stage finished
2025-06-12 09:00:24.592 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.spark.scheduler.DAGScheduler - Job 9 finished: start at Processor.java:94, took 0.537267 s
2025-06-12 09:00:24.603 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/9 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.9.f4fcec38-5d91-4cf8-90e3-7b6255fd2937.tmp
2025-06-12 09:00:24.612 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.9.f4fcec38-5d91-4cf8-90e3-7b6255fd2937.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/9
2025-06-12 09:00:24.612 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T02:00:23.997Z",
  "batchId" : 9,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 272.72727272727275,
  "processedRowsPerSecond" : 4.878048780487805,
  "durationMs" : {
    "addBatch" : 581,
    "commitOffsets" : 12,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 9,
    "triggerExecution" : 615,
    "walCommit" : 12
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 29,
        "0" : 38
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 31,
        "0" : 39
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 31,
        "0" : 39
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 272.72727272727275,
    "processedRowsPerSecond" : 4.878048780487805,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 09:00:25.109 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=952a0895-03d6-435f-8a16-924050b353eb, event_name=click, event_time=2025-06-12T02:00:24.333Z, user_id=user_4q6eh9x, session_id=session_9lqote5, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 5
                        Bestseller with , element_type=div, element_name=null, track=product_click, productId=5}, {event_id=b48b3dbb-3ece-4c6d-a5ca-0d5fdcc9a39e, event_name=click, event_time=2025-06-12T02:00:24.781Z, user_id=user_4q6eh9x, session_id=session_9lqote5, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 4
                        Reliable and aff, element_type=div, element_name=null, track=product_click, productId=4}, {event_id=89463fcd-6fe1-4490-9aae-9445cf514e95, event_name=click, event_time=2025-06-12T02:00:25.107Z, user_id=user_4q6eh9x, session_id=session_9lqote5, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 1
                        High-quality ite, element_type=div, element_name=null, track=product_click, productId=1}]}
2025-06-12 09:00:25.109 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 09:00:25.109 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 09:00:25.119 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/10 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.10.ebaf2c5d-95d4-417f-88d5-e0e31e47fe39.tmp
2025-06-12 09:00:25.129 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.10.ebaf2c5d-95d4-417f-88d5-e0e31e47fe39.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/10
2025-06-12 09:00:25.129 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 10. Metadata OffsetSeqMetadata(0,1749693625117,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-06-12 09:00:25.135 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 09:00:25.136 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 09:00:25.141 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 09:00:25.142 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 09:00:25.173 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] org.apache.spark.SparkContext - Starting job: start at Processor.java:94
2025-06-12 09:00:25.174 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 10 (start at Processor.java:94) with 2 output partitions
2025-06-12 09:00:25.174 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 10 (start at Processor.java:94)
2025-06-12 09:00:25.174 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-12 09:00:25.174 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-12 09:00:25.174 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 10 (MapPartitionsRDD[98] at start at Processor.java:94), which has no missing parents
2025-06-12 09:00:25.177 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 39.1 KiB, free 9.2 GiB)
2025-06-12 09:00:25.177 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 9.2 GiB)
2025-06-12 09:00:25.178 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on phamviethoa:38963 (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 09:00:25.178 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 10 from broadcast at DAGScheduler.scala:1535
2025-06-12 09:00:25.178 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 10 (MapPartitionsRDD[98] at start at Processor.java:94) (first 15 tasks are for partitions Vector(0, 1))
2025-06-12 09:00:25.178 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 10.0 with 2 tasks resource profile 0
2025-06-12 09:00:25.179 [dispatcher-event-loop-3 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 10.0 (TID 18) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 09:00:25.179 [dispatcher-event-loop-3 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 10.0 (TID 19) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 09:00:25.179 [Executor task launch worker for task 1.0 in stage 10.0 (TID 19) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 10.0 (TID 19)
2025-06-12 09:00:25.179 [Executor task launch worker for task 0.0 in stage 10.0 (TID 18) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 10.0 (TID 18)
2025-06-12 09:00:25.187 [Executor task launch worker for task 1.0 in stage 10.0 (TID 19) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to offset 39 for partition clickstream-events-0
2025-06-12 09:00:25.189 [Executor task launch worker for task 1.0 in stage 10.0 (TID 19) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-12 09:00:25.189 [Executor task launch worker for task 0.0 in stage 10.0 (TID 18) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to offset 31 for partition clickstream-events-1
2025-06-12 09:00:25.190 [Executor task launch worker for task 0.0 in stage 10.0 (TID 18) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-12 09:00:25.690 [Executor task launch worker for task 1.0 in stage 10.0 (TID 19) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=25, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 09:00:25.690 [Executor task launch worker for task 1.0 in stage 10.0 (TID 19) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-12 09:00:25.691 [Executor task launch worker for task 1.0 in stage 10.0 (TID 19) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=40, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 09:00:25.691 [Executor task launch worker for task 0.0 in stage 10.0 (TID 18) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=18, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 09:00:25.691 [Executor task launch worker for task 0.0 in stage 10.0 (TID 18) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-12 09:00:25.691 [Executor task launch worker for task 0.0 in stage 10.0 (TID 18) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=33, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 09:00:25.697 [Executor task launch worker for task 0.0 in stage 10.0 (TID 18) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 09:00:25.698 [Executor task launch worker for task 1.0 in stage 10.0 (TID 19) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 09:00:25.702 [Executor task launch worker for task 1.0 in stage 10.0 (TID 19) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 09:00:25.702 [Executor task launch worker for task 1.0 in stage 10.0 (TID 19) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [3e75400f-4272-4f8a-acce-7730d6b0f1a3] (2 queries & 0 savepoints) is committed.
2025-06-12 09:00:25.702 [Executor task launch worker for task 1.0 in stage 10.0 (TID 19) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [afb90a2b-fcc8-49ec-a94c-4b98175287a7] (0 queries & 0 savepoints) is committed.
2025-06-12 09:00:25.703 [Executor task launch worker for task 0.0 in stage 10.0 (TID 18) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 09:00:25.703 [Executor task launch worker for task 0.0 in stage 10.0 (TID 18) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [d1552615-2007-4369-a799-218fb3656d8e] (2 queries & 0 savepoints) is committed.
2025-06-12 09:00:25.703 [Executor task launch worker for task 0.0 in stage 10.0 (TID 18) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [a557aabb-b0a2-4c86-818b-7f4aa349bf1a] (0 queries & 0 savepoints) is committed.
2025-06-12 09:00:25.703 [Executor task launch worker for task 1.0 in stage 10.0 (TID 19) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 10.0 (TID 19). 1688 bytes result sent to driver
2025-06-12 09:00:25.703 [Executor task launch worker for task 0.0 in stage 10.0 (TID 18) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 10.0 (TID 18). 1688 bytes result sent to driver
2025-06-12 09:00:25.703 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 10.0 (TID 19) in 524 ms on phamviethoa (executor driver) (1/2)
2025-06-12 09:00:25.705 [task-result-getter-3 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 10.0 (TID 18) in 526 ms on phamviethoa (executor driver) (2/2)
2025-06-12 09:00:25.705 [task-result-getter-3 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 10.0, whose tasks have all completed, from pool 
2025-06-12 09:00:25.705 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 10 (start at Processor.java:94) finished in 0.530 s
2025-06-12 09:00:25.705 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-12 09:00:25.705 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 10: Stage finished
2025-06-12 09:00:25.705 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.spark.scheduler.DAGScheduler - Job 10 finished: start at Processor.java:94, took 0.531820 s
2025-06-12 09:00:25.716 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/10 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.10.d69ba12b-4823-4e63-879d-6ae1201771f6.tmp
2025-06-12 09:00:25.725 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.10.d69ba12b-4823-4e63-879d-6ae1201771f6.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/10
2025-06-12 09:00:25.726 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T02:00:25.116Z",
  "batchId" : 10,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 272.72727272727275,
  "processedRowsPerSecond" : 4.926108374384237,
  "durationMs" : {
    "addBatch" : 577,
    "commitOffsets" : 11,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 7,
    "triggerExecution" : 609,
    "walCommit" : 12
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 31,
        "0" : 39
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 33,
        "0" : 40
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 33,
        "0" : 40
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 272.72727272727275,
    "processedRowsPerSecond" : 4.926108374384237,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 09:00:26.471 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=1eac5b94-a557-4b57-9981-cdbab4fbdf49, event_name=click, event_time=2025-06-12T02:00:25.692Z, user_id=user_4q6eh9x, session_id=session_9lqote5, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 7
                        Compact and effi, element_type=div, element_name=null, track=product_click, productId=7}, {event_id=d29c8bf0-eee8-49ec-a844-551641b642b0, event_name=click, event_time=2025-06-12T02:00:26.085Z, user_id=user_4q6eh9x, session_id=session_9lqote5, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 8
                        Eco-friendly and, element_type=div, element_name=null, track=product_click, productId=8}, {event_id=1bf65ca3-f28d-453a-8e0b-472d15960b64, event_name=click, event_time=2025-06-12T02:00:26.469Z, user_id=user_4q6eh9x, session_id=session_9lqote5, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 9
                        Top-rated with e, element_type=div, element_name=null, track=product_click, productId=9}]}
2025-06-12 09:00:26.471 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 09:00:26.471 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 09:00:26.484 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/11 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.11.d7509875-73be-4176-a81b-3a4e9a5e0a50.tmp
2025-06-12 09:00:26.492 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.11.d7509875-73be-4176-a81b-3a4e9a5e0a50.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/11
2025-06-12 09:00:26.492 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 11. Metadata OffsetSeqMetadata(0,1749693626481,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-06-12 09:00:26.499 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 09:00:26.499 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 09:00:26.504 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 09:00:26.504 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 09:00:26.532 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] org.apache.spark.SparkContext - Starting job: start at Processor.java:94
2025-06-12 09:00:26.533 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 11 (start at Processor.java:94) with 2 output partitions
2025-06-12 09:00:26.533 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (start at Processor.java:94)
2025-06-12 09:00:26.533 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-12 09:00:26.533 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-12 09:00:26.533 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[107] at start at Processor.java:94), which has no missing parents
2025-06-12 09:00:26.535 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_11 stored as values in memory (estimated size 39.1 KiB, free 9.2 GiB)
2025-06-12 09:00:26.536 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_11_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 9.2 GiB)
2025-06-12 09:00:26.536 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on phamviethoa:38963 (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 09:00:26.537 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 11 from broadcast at DAGScheduler.scala:1535
2025-06-12 09:00:26.537 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 11 (MapPartitionsRDD[107] at start at Processor.java:94) (first 15 tasks are for partitions Vector(0, 1))
2025-06-12 09:00:26.537 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 2 tasks resource profile 0
2025-06-12 09:00:26.537 [dispatcher-event-loop-1 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 20) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 09:00:26.538 [dispatcher-event-loop-1 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 11.0 (TID 21) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 09:00:26.538 [Executor task launch worker for task 0.0 in stage 11.0 (TID 20) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 20)
2025-06-12 09:00:26.538 [Executor task launch worker for task 1.0 in stage 11.0 (TID 21) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 11.0 (TID 21)
2025-06-12 09:00:26.545 [Executor task launch worker for task 1.0 in stage 11.0 (TID 21) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to offset 40 for partition clickstream-events-0
2025-06-12 09:00:26.545 [Executor task launch worker for task 0.0 in stage 11.0 (TID 20) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to offset 33 for partition clickstream-events-1
2025-06-12 09:00:26.546 [Executor task launch worker for task 1.0 in stage 11.0 (TID 21) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-12 09:00:26.546 [Executor task launch worker for task 0.0 in stage 11.0 (TID 20) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-12 09:00:27.047 [Executor task launch worker for task 1.0 in stage 11.0 (TID 21) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=25, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 09:00:27.047 [Executor task launch worker for task 1.0 in stage 11.0 (TID 21) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-12 09:00:27.048 [Executor task launch worker for task 1.0 in stage 11.0 (TID 21) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=42, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 09:00:27.048 [Executor task launch worker for task 0.0 in stage 11.0 (TID 20) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=18, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 09:00:27.048 [Executor task launch worker for task 0.0 in stage 11.0 (TID 20) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-12 09:00:27.048 [Executor task launch worker for task 0.0 in stage 11.0 (TID 20) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=34, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 09:00:27.054 [Executor task launch worker for task 1.0 in stage 11.0 (TID 21) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 09:00:27.054 [Executor task launch worker for task 0.0 in stage 11.0 (TID 20) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 09:00:27.059 [Executor task launch worker for task 0.0 in stage 11.0 (TID 20) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 09:00:27.059 [Executor task launch worker for task 0.0 in stage 11.0 (TID 20) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [846dc32f-0264-47d1-a998-3b58158269e0] (2 queries & 0 savepoints) is committed.
2025-06-12 09:00:27.059 [Executor task launch worker for task 0.0 in stage 11.0 (TID 20) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [fd4d8ab1-ca02-464c-95a4-a9ba6e26f675] (0 queries & 0 savepoints) is committed.
2025-06-12 09:00:27.059 [Executor task launch worker for task 1.0 in stage 11.0 (TID 21) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 09:00:27.059 [Executor task launch worker for task 1.0 in stage 11.0 (TID 21) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [06b95481-0c98-439e-91db-2aa7b9fd63ed] (2 queries & 0 savepoints) is committed.
2025-06-12 09:00:27.059 [Executor task launch worker for task 1.0 in stage 11.0 (TID 21) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [c81f3fe2-cf9d-46e3-aed8-f66868150020] (0 queries & 0 savepoints) is committed.
2025-06-12 09:00:27.059 [Executor task launch worker for task 0.0 in stage 11.0 (TID 20) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 20). 1688 bytes result sent to driver
2025-06-12 09:00:27.060 [Executor task launch worker for task 1.0 in stage 11.0 (TID 21) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 11.0 (TID 21). 1688 bytes result sent to driver
2025-06-12 09:00:27.060 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 11.0 (TID 21) in 523 ms on phamviethoa (executor driver) (1/2)
2025-06-12 09:00:27.060 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 20) in 523 ms on phamviethoa (executor driver) (2/2)
2025-06-12 09:00:27.060 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool 
2025-06-12 09:00:27.060 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 11 (start at Processor.java:94) finished in 0.527 s
2025-06-12 09:00:27.060 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-12 09:00:27.060 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 11: Stage finished
2025-06-12 09:00:27.060 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.spark.scheduler.DAGScheduler - Job 11 finished: start at Processor.java:94, took 0.527909 s
2025-06-12 09:00:27.073 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/11 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.11.f9a4a984-96c4-48db-b8fb-3b4c8be96be7.tmp
2025-06-12 09:00:27.081 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.11.f9a4a984-96c4-48db-b8fb-3b4c8be96be7.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/11
2025-06-12 09:00:27.082 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T02:00:26.481Z",
  "batchId" : 11,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 250.0,
  "processedRowsPerSecond" : 5.0,
  "durationMs" : {
    "addBatch" : 569,
    "commitOffsets" : 12,
    "getBatch" : 0,
    "latestOffset" : 0,
    "queryPlanning" : 6,
    "triggerExecution" : 600,
    "walCommit" : 12
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 33,
        "0" : 40
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 34,
        "0" : 42
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 34,
        "0" : 42
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 250.0,
    "processedRowsPerSecond" : 5.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 09:00:29.411 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=9355f6d6-8afa-4386-bb2d-c904456cb15b, event_name=scroll, event_time=2025-06-12T02:00:27.408Z, user_id=user_4q6eh9x, session_id=session_9lqote5, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=99}, {event_id=c65b7a52-68be-43ed-8b83-14c5c4631bcb, event_name=scroll, event_time=2025-06-12T02:00:28.409Z, user_id=user_4q6eh9x, session_id=session_9lqote5, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=24}, {event_id=aa0515f3-8ce8-403a-b59d-df33aebe450c, event_name=scroll, event_time=2025-06-12T02:00:29.410Z, user_id=user_4q6eh9x, session_id=session_9lqote5, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=66}]}
2025-06-12 09:00:29.412 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-12 09:00:29.412 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-12 09:00:29.420 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/12 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.12.1c4ec885-62f0-408a-a0d4-18e1b9970e6e.tmp
2025-06-12 09:00:29.429 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/.12.1c4ec885-62f0-408a-a0d4-18e1b9970e6e.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/offsets/12
2025-06-12 09:00:29.429 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 12. Metadata OffsetSeqMetadata(0,1749693629417,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
2025-06-12 09:00:29.435 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 09:00:29.436 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 09:00:29.440 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 09:00:29.441 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-12 09:00:29.474 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] org.apache.spark.SparkContext - Starting job: start at Processor.java:94
2025-06-12 09:00:29.474 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 12 (start at Processor.java:94) with 1 output partitions
2025-06-12 09:00:29.474 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 12 (start at Processor.java:94)
2025-06-12 09:00:29.474 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-12 09:00:29.474 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-12 09:00:29.474 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 12 (MapPartitionsRDD[116] at start at Processor.java:94), which has no missing parents
2025-06-12 09:00:29.477 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_12 stored as values in memory (estimated size 39.0 KiB, free 9.2 GiB)
2025-06-12 09:00:29.478 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_12_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 9.2 GiB)
2025-06-12 09:00:29.478 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on phamviethoa:38963 (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 09:00:29.479 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 12 from broadcast at DAGScheduler.scala:1535
2025-06-12 09:00:29.479 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[116] at start at Processor.java:94) (first 15 tasks are for partitions Vector(0))
2025-06-12 09:00:29.479 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 12.0 with 1 tasks resource profile 0
2025-06-12 09:00:29.479 [dispatcher-event-loop-0 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 12.0 (TID 22) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8312 bytes) 
2025-06-12 09:00:29.480 [Executor task launch worker for task 0.0 in stage 12.0 (TID 22) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 12.0 (TID 22)
2025-06-12 09:00:29.486 [Executor task launch worker for task 0.0 in stage 12.0 (TID 22) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to offset 34 for partition clickstream-events-1
2025-06-12 09:00:29.488 [Executor task launch worker for task 0.0 in stage 12.0 (TID 22) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-12 09:00:29.988 [Executor task launch worker for task 0.0 in stage 12.0 (TID 22) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=18, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 09:00:29.988 [Executor task launch worker for task 0.0 in stage 12.0 (TID 22) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-12 09:00:29.989 [Executor task launch worker for task 0.0 in stage 12.0 (TID 22) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=37, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-12 09:00:29.996 [Executor task launch worker for task 0.0 in stage 12.0 (TID 22) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 09:00:30.001 [Executor task launch worker for task 0.0 in stage 12.0 (TID 22) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-12 09:00:30.001 [Executor task launch worker for task 0.0 in stage 12.0 (TID 22) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [73168b17-2a7e-4c12-a972-90168d9af2c4] (2 queries & 0 savepoints) is committed.
2025-06-12 09:00:30.001 [Executor task launch worker for task 0.0 in stage 12.0 (TID 22) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [a960a560-a416-4ac8-a5de-034c48ecf539] (0 queries & 0 savepoints) is committed.
2025-06-12 09:00:30.002 [Executor task launch worker for task 0.0 in stage 12.0 (TID 22) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 12.0 (TID 22). 1688 bytes result sent to driver
2025-06-12 09:00:30.002 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 12.0 (TID 22) in 523 ms on phamviethoa (executor driver) (1/1)
2025-06-12 09:00:30.002 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 12.0, whose tasks have all completed, from pool 
2025-06-12 09:00:30.002 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 12 (start at Processor.java:94) finished in 0.527 s
2025-06-12 09:00:30.002 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-12 09:00:30.002 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 12: Stage finished
2025-06-12 09:00:30.003 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.spark.scheduler.DAGScheduler - Job 12 finished: start at Processor.java:94, took 0.528841 s
2025-06-12 09:00:30.013 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/12 using temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.12.4ae94eba-90df-4dc0-befc-23a9135677c1.tmp
2025-06-12 09:00:30.021 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/.12.4ae94eba-90df-4dc0-befc-23a9135677c1.tmp to file:/tmp/temporary-bc6ce491-a07e-4db9-8b1e-89c7b420c103/commits/12
2025-06-12 09:00:30.022 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T02:00:29.416Z",
  "batchId" : 12,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 272.72727272727275,
  "processedRowsPerSecond" : 4.958677685950414,
  "durationMs" : {
    "addBatch" : 573,
    "commitOffsets" : 11,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 7,
    "triggerExecution" : 605,
    "walCommit" : 12
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 34,
        "0" : 42
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 37,
        "0" : 42
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 37,
        "0" : 42
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 272.72727272727275,
    "processedRowsPerSecond" : 4.958677685950414,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 09:00:40.022 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T02:00:40.021Z",
  "batchId" : 13,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 37,
        "0" : 42
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 37,
        "0" : 42
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 37,
        "0" : 42
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 09:00:50.032 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T02:00:50.031Z",
  "batchId" : 13,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 0,
    "triggerExecution" : 0
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 37,
        "0" : 42
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 37,
        "0" : 42
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 37,
        "0" : 42
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 09:01:00.041 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T02:01:00.040Z",
  "batchId" : 13,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 0,
    "triggerExecution" : 0
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 37,
        "0" : 42
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 37,
        "0" : 42
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 37,
        "0" : 42
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 09:01:10.049 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T02:01:10.049Z",
  "batchId" : 13,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 0,
    "triggerExecution" : 0
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 37,
        "0" : 42
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 37,
        "0" : 42
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 37,
        "0" : 42
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 09:01:20.050 [stream execution thread for [id = b6d1780c-a1f5-4a11-9cdb-949bcd34c696, runId = ce9f44a4-b26c-4c00-93ce-f5e1b036efb2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Streaming query made progress: {
  "id" : "b6d1780c-a1f5-4a11-9cdb-949bcd34c696",
  "runId" : "ce9f44a4-b26c-4c00-93ce-f5e1b036efb2",
  "name" : null,
  "timestamp" : "2025-06-12T02:01:20.049Z",
  "batchId" : 13,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 0,
    "triggerExecution" : 0
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[clickstream-events]]",
    "startOffset" : {
      "clickstream-events" : {
        "1" : 37,
        "0" : 42
      }
    },
    "endOffset" : {
      "clickstream-events" : {
        "1" : 37,
        "0" : 42
      }
    },
    "latestOffset" : {
      "clickstream-events" : {
        "1" : 37,
        "0" : 42
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
2025-06-12 09:01:22.503 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_11_piece0 on phamviethoa:38963 in memory (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 09:01:22.504 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_12_piece0 on phamviethoa:38963 in memory (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 09:01:22.505 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_9_piece0 on phamviethoa:38963 in memory (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 09:01:22.507 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on phamviethoa:38963 in memory (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 09:01:22.508 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_10_piece0 on phamviethoa:38963 in memory (size: 17.3 KiB, free: 9.2 GiB)
2025-06-12 09:01:24.328 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-12 09:01:24.329 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-12 09:01:24.332 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 09:01:24.332 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 09:01:24.332 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-12 09:01:24.332 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 09:01:24.335 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-2 unregistered
2025-06-12 09:01:24.335 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-12 09:01:24.335 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1, groupId=spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-12 09:01:24.336 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 09:01:24.336 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 09:01:24.336 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-12 09:01:24.336 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 09:01:24.337 [SpringApplicationShutdownHook INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-06-12 09:01:24.338 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-a8f93a05-a918-46e9-9378-ecbda8e88d6d-829941894-executor-1 unregistered
2025-06-12 09:01:24.338 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-12 09:01:24.338 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-12 09:01:24.339 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 09:01:24.339 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 09:01:24.339 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-12 09:01:24.339 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 09:01:24.339 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-1 unregistered
2025-06-12 14:31:44.005 [main INFO ] com.example.clickstream.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 341008 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-12 14:31:44.007 [main INFO ] com.example.clickstream.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-12 14:31:44.519 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-12 14:31:44.521 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-12 14:31:44.522 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-12 14:31:44.522 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-12 14:31:44.566 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-12 14:31:44.566 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 541 ms
2025-06-12 14:31:44.599 [main ERROR] o.s.b.w.e.tomcat.TomcatStarter - Error starting Tomcat context. Exception: org.springframework.beans.factory.UnsatisfiedDependencyException. Message: Error creating bean with name 'webMvcMetricsFilter' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/web/servlet/WebMvcMetricsAutoConfiguration.class]: Unsatisfied dependency expressed through method 'webMvcMetricsFilter' parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'prometheusMeterRegistry' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/export/prometheus/PrometheusMetricsExportAutoConfiguration.class]: Initialization of bean failed; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataSourcePoolMetadataMeterBinder' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/jdbc/DataSourcePoolMetricsAutoConfiguration$DataSourcePoolMetadataMetricsConfiguration.class]: Unsatisfied dependency expressed through method 'dataSourcePoolMetadataMeterBinder' parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'clickHouseConfig' defined in file [/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes/com/example/clickstream/config/ClickHouseConfig.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'appConfig': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_APP_NAME' in value "${SPARK_APP_NAME}"
2025-06-12 14:31:44.606 [main INFO ] o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-06-12 14:31:44.615 [main WARN ] o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.context.ApplicationContextException: Unable to start web server; nested exception is org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
2025-06-12 14:31:44.622 [main INFO ] o.s.b.a.l.ConditionEvaluationReportLoggingListener - 

Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
2025-06-12 14:31:44.630 [main ERROR] o.s.boot.SpringApplication - Application run failed
org.springframework.context.ApplicationContextException: Unable to start web server; nested exception is org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:165)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:585)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:147)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:732)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:409)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:308)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1300)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1289)
	at com.example.clickstream.Application.main(Application.java:12)
Caused by: org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:142)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.<init>(TomcatWebServer.java:104)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getTomcatWebServer(TomcatServletWebServerFactory.java:481)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getWebServer(TomcatServletWebServerFactory.java:211)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.createWebServer(ServletWebServerApplicationContext.java:184)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:162)
	... 8 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'webMvcMetricsFilter' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/web/servlet/WebMvcMetricsAutoConfiguration.class]: Unsatisfied dependency expressed through method 'webMvcMetricsFilter' parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'prometheusMeterRegistry' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/export/prometheus/PrometheusMetricsExportAutoConfiguration.class]: Initialization of bean failed; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataSourcePoolMetadataMeterBinder' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/jdbc/DataSourcePoolMetricsAutoConfiguration$DataSourcePoolMetadataMetricsConfiguration.class]: Unsatisfied dependency expressed through method 'dataSourcePoolMetadataMeterBinder' parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'clickHouseConfig' defined in file [/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes/com/example/clickstream/config/ClickHouseConfig.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'appConfig': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_APP_NAME' in value "${SPARK_APP_NAME}"
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:801)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:536)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1352)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1195)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:582)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:213)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:213)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:204)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addServletContextInitializerBeans(ServletContextInitializerBeans.java:98)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.<init>(ServletContextInitializerBeans.java:86)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.getServletContextInitializerBeans(ServletWebServerApplicationContext.java:262)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.selfInitialize(ServletWebServerApplicationContext.java:236)
	at org.springframework.boot.web.embedded.tomcat.TomcatStarter.onStartup(TomcatStarter.java:53)
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:4904)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:171)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1332)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1322)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:140)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:866)
	at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:794)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:171)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1332)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1322)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:140)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:866)
	at org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:248)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:171)
	at org.apache.catalina.core.StandardService.startInternal(StandardService.java:433)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:171)
	at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:921)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:171)
	at org.apache.catalina.startup.Tomcat.start(Tomcat.java:489)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:123)
	... 13 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'prometheusMeterRegistry' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/export/prometheus/PrometheusMetricsExportAutoConfiguration.class]: Initialization of bean failed; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataSourcePoolMetadataMeterBinder' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/jdbc/DataSourcePoolMetricsAutoConfiguration$DataSourcePoolMetadataMetricsConfiguration.class]: Unsatisfied dependency expressed through method 'dataSourcePoolMetadataMeterBinder' parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'clickHouseConfig' defined in file [/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes/com/example/clickstream/config/ClickHouseConfig.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'appConfig': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_APP_NAME' in value "${SPARK_APP_NAME}"
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:628)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:276)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1391)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1311)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:788)
	... 53 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataSourcePoolMetadataMeterBinder' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/jdbc/DataSourcePoolMetricsAutoConfiguration$DataSourcePoolMetadataMetricsConfiguration.class]: Unsatisfied dependency expressed through method 'dataSourcePoolMetadataMeterBinder' parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'clickHouseConfig' defined in file [/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes/com/example/clickstream/config/ClickHouseConfig.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'appConfig': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_APP_NAME' in value "${SPARK_APP_NAME}"
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:801)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:536)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1352)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1195)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:582)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:276)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.addCandidateEntry(DefaultListableBeanFactory.java:1616)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:1573)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveMultipleBeans(DefaultListableBeanFactory.java:1417)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1349)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory$DependencyObjectProvider.resolveStream(DefaultListableBeanFactory.java:2119)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory$DependencyObjectProvider.orderedStream(DefaultListableBeanFactory.java:2113)
	at org.springframework.boot.actuate.autoconfigure.metrics.MeterRegistryConfigurer.addBinders(MeterRegistryConfigurer.java:87)
	at org.springframework.boot.actuate.autoconfigure.metrics.MeterRegistryConfigurer.configure(MeterRegistryConfigurer.java:68)
	at org.springframework.boot.actuate.autoconfigure.metrics.MeterRegistryPostProcessor.postProcessAfterInitialization(MeterRegistryPostProcessor.java:64)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsAfterInitialization(AbstractAutowireCapableBeanFactory.java:455)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1808)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:620)
	... 63 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'clickHouseConfig' defined in file [/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes/com/example/clickstream/config/ClickHouseConfig.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'appConfig': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_APP_NAME' in value "${SPARK_APP_NAME}"
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:801)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:224)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1372)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1222)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:582)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:405)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1352)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1195)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:582)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:276)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.addCandidateEntry(DefaultListableBeanFactory.java:1609)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:1573)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveMultipleBeans(DefaultListableBeanFactory.java:1492)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1349)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1311)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:788)
	... 85 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'appConfig': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_APP_NAME' in value "${SPARK_APP_NAME}"
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:414)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1431)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:619)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:276)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1391)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1311)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:788)
	... 111 common frames omitted
Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_APP_NAME' in value "${SPARK_APP_NAME}"
	at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:180)
	at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:126)
	at org.springframework.core.env.AbstractPropertyResolver.doResolvePlaceholders(AbstractPropertyResolver.java:239)
	at org.springframework.core.env.AbstractPropertyResolver.resolveRequiredPlaceholders(AbstractPropertyResolver.java:210)
	at org.springframework.core.env.AbstractPropertyResolver.resolveNestedPlaceholders(AbstractPropertyResolver.java:230)
	at org.springframework.boot.context.properties.source.ConfigurationPropertySourcesPropertyResolver.getProperty(ConfigurationPropertySourcesPropertyResolver.java:79)
	at org.springframework.boot.context.properties.source.ConfigurationPropertySourcesPropertyResolver.getProperty(ConfigurationPropertySourcesPropertyResolver.java:60)
	at org.springframework.core.env.AbstractEnvironment.getProperty(AbstractEnvironment.java:594)
	at org.springframework.context.support.PropertySourcesPlaceholderConfigurer$1.getProperty(PropertySourcesPlaceholderConfigurer.java:153)
	at org.springframework.context.support.PropertySourcesPlaceholderConfigurer$1.getProperty(PropertySourcesPlaceholderConfigurer.java:149)
	at org.springframework.core.env.PropertySourcesPropertyResolver.getProperty(PropertySourcesPropertyResolver.java:85)
	at org.springframework.core.env.PropertySourcesPropertyResolver.getPropertyAsRawString(PropertySourcesPropertyResolver.java:74)
	at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:153)
	at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:126)
	at org.springframework.core.env.AbstractPropertyResolver.doResolvePlaceholders(AbstractPropertyResolver.java:239)
	at org.springframework.core.env.AbstractPropertyResolver.resolveRequiredPlaceholders(AbstractPropertyResolver.java:210)
	at org.springframework.context.support.PropertySourcesPlaceholderConfigurer.lambda$processProperties$0(PropertySourcesPlaceholderConfigurer.java:191)
	at org.springframework.beans.factory.support.AbstractBeanFactory.resolveEmbeddedValue(AbstractBeanFactory.java:936)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1332)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1311)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:710)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:693)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:119)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:408)
	... 123 common frames omitted
2025-06-12 14:33:05.705 [main INFO ] com.example.clickstream.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 341173 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-12 14:33:05.706 [main INFO ] com.example.clickstream.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-12 14:33:06.213 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-12 14:33:06.216 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-12 14:33:06.216 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-12 14:33:06.216 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-12 14:33:06.257 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-12 14:33:06.257 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 533 ms
2025-06-12 14:33:06.284 [main ERROR] o.s.b.w.e.tomcat.TomcatStarter - Error starting Tomcat context. Exception: org.springframework.beans.factory.UnsatisfiedDependencyException. Message: Error creating bean with name 'webMvcMetricsFilter' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/web/servlet/WebMvcMetricsAutoConfiguration.class]: Unsatisfied dependency expressed through method 'webMvcMetricsFilter' parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'prometheusMeterRegistry' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/export/prometheus/PrometheusMetricsExportAutoConfiguration.class]: Initialization of bean failed; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataSourcePoolMetadataMeterBinder' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/jdbc/DataSourcePoolMetricsAutoConfiguration$DataSourcePoolMetadataMetricsConfiguration.class]: Unsatisfied dependency expressed through method 'dataSourcePoolMetadataMeterBinder' parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'clickHouseConfig' defined in file [/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes/com/example/clickstream/config/ClickHouseConfig.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'appConfig': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_MASTER' in value "${SPARK_MASTER}"
2025-06-12 14:33:06.291 [main INFO ] o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-06-12 14:33:06.298 [main WARN ] o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.context.ApplicationContextException: Unable to start web server; nested exception is org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
2025-06-12 14:33:06.306 [main INFO ] o.s.b.a.l.ConditionEvaluationReportLoggingListener - 

Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
2025-06-12 14:33:06.313 [main ERROR] o.s.boot.SpringApplication - Application run failed
org.springframework.context.ApplicationContextException: Unable to start web server; nested exception is org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:165)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:585)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:147)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:732)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:409)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:308)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1300)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1289)
	at com.example.clickstream.Application.main(Application.java:12)
Caused by: org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:142)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.<init>(TomcatWebServer.java:104)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getTomcatWebServer(TomcatServletWebServerFactory.java:481)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getWebServer(TomcatServletWebServerFactory.java:211)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.createWebServer(ServletWebServerApplicationContext.java:184)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:162)
	... 8 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'webMvcMetricsFilter' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/web/servlet/WebMvcMetricsAutoConfiguration.class]: Unsatisfied dependency expressed through method 'webMvcMetricsFilter' parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'prometheusMeterRegistry' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/export/prometheus/PrometheusMetricsExportAutoConfiguration.class]: Initialization of bean failed; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataSourcePoolMetadataMeterBinder' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/jdbc/DataSourcePoolMetricsAutoConfiguration$DataSourcePoolMetadataMetricsConfiguration.class]: Unsatisfied dependency expressed through method 'dataSourcePoolMetadataMeterBinder' parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'clickHouseConfig' defined in file [/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes/com/example/clickstream/config/ClickHouseConfig.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'appConfig': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_MASTER' in value "${SPARK_MASTER}"
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:801)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:536)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1352)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1195)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:582)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:213)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:213)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:204)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addServletContextInitializerBeans(ServletContextInitializerBeans.java:98)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.<init>(ServletContextInitializerBeans.java:86)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.getServletContextInitializerBeans(ServletWebServerApplicationContext.java:262)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.selfInitialize(ServletWebServerApplicationContext.java:236)
	at org.springframework.boot.web.embedded.tomcat.TomcatStarter.onStartup(TomcatStarter.java:53)
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:4904)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:171)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1332)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1322)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:140)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:866)
	at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:794)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:171)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1332)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1322)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:140)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:866)
	at org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:248)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:171)
	at org.apache.catalina.core.StandardService.startInternal(StandardService.java:433)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:171)
	at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:921)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:171)
	at org.apache.catalina.startup.Tomcat.start(Tomcat.java:489)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:123)
	... 13 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'prometheusMeterRegistry' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/export/prometheus/PrometheusMetricsExportAutoConfiguration.class]: Initialization of bean failed; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataSourcePoolMetadataMeterBinder' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/jdbc/DataSourcePoolMetricsAutoConfiguration$DataSourcePoolMetadataMetricsConfiguration.class]: Unsatisfied dependency expressed through method 'dataSourcePoolMetadataMeterBinder' parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'clickHouseConfig' defined in file [/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes/com/example/clickstream/config/ClickHouseConfig.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'appConfig': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_MASTER' in value "${SPARK_MASTER}"
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:628)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:276)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1391)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1311)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:788)
	... 53 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataSourcePoolMetadataMeterBinder' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/jdbc/DataSourcePoolMetricsAutoConfiguration$DataSourcePoolMetadataMetricsConfiguration.class]: Unsatisfied dependency expressed through method 'dataSourcePoolMetadataMeterBinder' parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'clickHouseConfig' defined in file [/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes/com/example/clickstream/config/ClickHouseConfig.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'appConfig': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_MASTER' in value "${SPARK_MASTER}"
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:801)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:536)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1352)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1195)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:582)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:276)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.addCandidateEntry(DefaultListableBeanFactory.java:1616)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:1573)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveMultipleBeans(DefaultListableBeanFactory.java:1417)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1349)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory$DependencyObjectProvider.resolveStream(DefaultListableBeanFactory.java:2119)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory$DependencyObjectProvider.orderedStream(DefaultListableBeanFactory.java:2113)
	at org.springframework.boot.actuate.autoconfigure.metrics.MeterRegistryConfigurer.addBinders(MeterRegistryConfigurer.java:87)
	at org.springframework.boot.actuate.autoconfigure.metrics.MeterRegistryConfigurer.configure(MeterRegistryConfigurer.java:68)
	at org.springframework.boot.actuate.autoconfigure.metrics.MeterRegistryPostProcessor.postProcessAfterInitialization(MeterRegistryPostProcessor.java:64)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsAfterInitialization(AbstractAutowireCapableBeanFactory.java:455)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1808)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:620)
	... 63 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'clickHouseConfig' defined in file [/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes/com/example/clickstream/config/ClickHouseConfig.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'appConfig': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_MASTER' in value "${SPARK_MASTER}"
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:801)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:224)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1372)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1222)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:582)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:405)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1352)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1195)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:582)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:276)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.addCandidateEntry(DefaultListableBeanFactory.java:1609)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:1573)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveMultipleBeans(DefaultListableBeanFactory.java:1492)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1349)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1311)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:788)
	... 85 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'appConfig': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_MASTER' in value "${SPARK_MASTER}"
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:414)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1431)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:619)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:276)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1391)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1311)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:788)
	... 111 common frames omitted
Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_MASTER' in value "${SPARK_MASTER}"
	at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:180)
	at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:126)
	at org.springframework.core.env.AbstractPropertyResolver.doResolvePlaceholders(AbstractPropertyResolver.java:239)
	at org.springframework.core.env.AbstractPropertyResolver.resolveRequiredPlaceholders(AbstractPropertyResolver.java:210)
	at org.springframework.core.env.AbstractPropertyResolver.resolveNestedPlaceholders(AbstractPropertyResolver.java:230)
	at org.springframework.boot.context.properties.source.ConfigurationPropertySourcesPropertyResolver.getProperty(ConfigurationPropertySourcesPropertyResolver.java:79)
	at org.springframework.boot.context.properties.source.ConfigurationPropertySourcesPropertyResolver.getProperty(ConfigurationPropertySourcesPropertyResolver.java:60)
	at org.springframework.core.env.AbstractEnvironment.getProperty(AbstractEnvironment.java:594)
	at org.springframework.context.support.PropertySourcesPlaceholderConfigurer$1.getProperty(PropertySourcesPlaceholderConfigurer.java:153)
	at org.springframework.context.support.PropertySourcesPlaceholderConfigurer$1.getProperty(PropertySourcesPlaceholderConfigurer.java:149)
	at org.springframework.core.env.PropertySourcesPropertyResolver.getProperty(PropertySourcesPropertyResolver.java:85)
	at org.springframework.core.env.PropertySourcesPropertyResolver.getPropertyAsRawString(PropertySourcesPropertyResolver.java:74)
	at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:153)
	at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:126)
	at org.springframework.core.env.AbstractPropertyResolver.doResolvePlaceholders(AbstractPropertyResolver.java:239)
	at org.springframework.core.env.AbstractPropertyResolver.resolveRequiredPlaceholders(AbstractPropertyResolver.java:210)
	at org.springframework.context.support.PropertySourcesPlaceholderConfigurer.lambda$processProperties$0(PropertySourcesPlaceholderConfigurer.java:191)
	at org.springframework.beans.factory.support.AbstractBeanFactory.resolveEmbeddedValue(AbstractBeanFactory.java:936)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1332)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1311)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:710)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:693)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:119)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:408)
	... 123 common frames omitted
2025-06-12 14:35:49.743 [main INFO ] com.example.clickstream.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 341359 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-12 14:35:49.744 [main INFO ] com.example.clickstream.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-12 14:35:50.256 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-12 14:35:50.258 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-12 14:35:50.259 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-12 14:35:50.259 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-12 14:35:50.302 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-12 14:35:50.302 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 540 ms
2025-06-12 14:35:50.335 [main ERROR] o.s.b.w.e.tomcat.TomcatStarter - Error starting Tomcat context. Exception: org.springframework.beans.factory.UnsatisfiedDependencyException. Message: Error creating bean with name 'webMvcMetricsFilter' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/web/servlet/WebMvcMetricsAutoConfiguration.class]: Unsatisfied dependency expressed through method 'webMvcMetricsFilter' parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'prometheusMeterRegistry' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/export/prometheus/PrometheusMetricsExportAutoConfiguration.class]: Initialization of bean failed; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataSourcePoolMetadataMeterBinder' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/jdbc/DataSourcePoolMetricsAutoConfiguration$DataSourcePoolMetadataMetricsConfiguration.class]: Unsatisfied dependency expressed through method 'dataSourcePoolMetadataMeterBinder' parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'clickHouseConfig' defined in file [/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes/com/example/clickstream/config/ClickHouseConfig.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'appConfig': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_APP_NAME' in value "${SPARK_APP_NAME}"
2025-06-12 14:35:50.342 [main INFO ] o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-06-12 14:35:50.350 [main WARN ] o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.context.ApplicationContextException: Unable to start web server; nested exception is org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
2025-06-12 14:35:50.358 [main INFO ] o.s.b.a.l.ConditionEvaluationReportLoggingListener - 

Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
2025-06-12 14:35:50.369 [main ERROR] o.s.boot.SpringApplication - Application run failed
org.springframework.context.ApplicationContextException: Unable to start web server; nested exception is org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:165)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:585)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:147)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:732)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:409)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:308)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1300)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1289)
	at com.example.clickstream.Application.main(Application.java:12)
Caused by: org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:142)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.<init>(TomcatWebServer.java:104)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getTomcatWebServer(TomcatServletWebServerFactory.java:481)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getWebServer(TomcatServletWebServerFactory.java:211)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.createWebServer(ServletWebServerApplicationContext.java:184)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:162)
	... 8 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'webMvcMetricsFilter' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/web/servlet/WebMvcMetricsAutoConfiguration.class]: Unsatisfied dependency expressed through method 'webMvcMetricsFilter' parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'prometheusMeterRegistry' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/export/prometheus/PrometheusMetricsExportAutoConfiguration.class]: Initialization of bean failed; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataSourcePoolMetadataMeterBinder' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/jdbc/DataSourcePoolMetricsAutoConfiguration$DataSourcePoolMetadataMetricsConfiguration.class]: Unsatisfied dependency expressed through method 'dataSourcePoolMetadataMeterBinder' parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'clickHouseConfig' defined in file [/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes/com/example/clickstream/config/ClickHouseConfig.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'appConfig': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_APP_NAME' in value "${SPARK_APP_NAME}"
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:801)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:536)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1352)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1195)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:582)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:213)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:213)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:204)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addServletContextInitializerBeans(ServletContextInitializerBeans.java:98)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.<init>(ServletContextInitializerBeans.java:86)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.getServletContextInitializerBeans(ServletWebServerApplicationContext.java:262)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.selfInitialize(ServletWebServerApplicationContext.java:236)
	at org.springframework.boot.web.embedded.tomcat.TomcatStarter.onStartup(TomcatStarter.java:53)
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:4904)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:171)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1332)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1322)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:140)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:866)
	at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:794)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:171)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1332)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1322)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:140)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:866)
	at org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:248)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:171)
	at org.apache.catalina.core.StandardService.startInternal(StandardService.java:433)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:171)
	at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:921)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:171)
	at org.apache.catalina.startup.Tomcat.start(Tomcat.java:489)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:123)
	... 13 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'prometheusMeterRegistry' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/export/prometheus/PrometheusMetricsExportAutoConfiguration.class]: Initialization of bean failed; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataSourcePoolMetadataMeterBinder' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/jdbc/DataSourcePoolMetricsAutoConfiguration$DataSourcePoolMetadataMetricsConfiguration.class]: Unsatisfied dependency expressed through method 'dataSourcePoolMetadataMeterBinder' parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'clickHouseConfig' defined in file [/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes/com/example/clickstream/config/ClickHouseConfig.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'appConfig': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_APP_NAME' in value "${SPARK_APP_NAME}"
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:628)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:276)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1391)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1311)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:788)
	... 53 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataSourcePoolMetadataMeterBinder' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/jdbc/DataSourcePoolMetricsAutoConfiguration$DataSourcePoolMetadataMetricsConfiguration.class]: Unsatisfied dependency expressed through method 'dataSourcePoolMetadataMeterBinder' parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'clickHouseConfig' defined in file [/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes/com/example/clickstream/config/ClickHouseConfig.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'appConfig': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_APP_NAME' in value "${SPARK_APP_NAME}"
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:801)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:536)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1352)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1195)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:582)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:276)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.addCandidateEntry(DefaultListableBeanFactory.java:1616)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:1573)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveMultipleBeans(DefaultListableBeanFactory.java:1417)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1349)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory$DependencyObjectProvider.resolveStream(DefaultListableBeanFactory.java:2119)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory$DependencyObjectProvider.orderedStream(DefaultListableBeanFactory.java:2113)
	at org.springframework.boot.actuate.autoconfigure.metrics.MeterRegistryConfigurer.addBinders(MeterRegistryConfigurer.java:87)
	at org.springframework.boot.actuate.autoconfigure.metrics.MeterRegistryConfigurer.configure(MeterRegistryConfigurer.java:68)
	at org.springframework.boot.actuate.autoconfigure.metrics.MeterRegistryPostProcessor.postProcessAfterInitialization(MeterRegistryPostProcessor.java:64)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsAfterInitialization(AbstractAutowireCapableBeanFactory.java:455)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1808)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:620)
	... 63 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'clickHouseConfig' defined in file [/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes/com/example/clickstream/config/ClickHouseConfig.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'appConfig': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_APP_NAME' in value "${SPARK_APP_NAME}"
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:801)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:224)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1372)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1222)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:582)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:405)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1352)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1195)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:582)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:276)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.addCandidateEntry(DefaultListableBeanFactory.java:1609)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:1573)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveMultipleBeans(DefaultListableBeanFactory.java:1492)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1349)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1311)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:788)
	... 85 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'appConfig': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_APP_NAME' in value "${SPARK_APP_NAME}"
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:414)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1431)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:619)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:276)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1391)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1311)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:788)
	... 111 common frames omitted
Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_APP_NAME' in value "${SPARK_APP_NAME}"
	at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:180)
	at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:126)
	at org.springframework.core.env.AbstractPropertyResolver.doResolvePlaceholders(AbstractPropertyResolver.java:239)
	at org.springframework.core.env.AbstractPropertyResolver.resolveRequiredPlaceholders(AbstractPropertyResolver.java:210)
	at org.springframework.core.env.AbstractPropertyResolver.resolveNestedPlaceholders(AbstractPropertyResolver.java:230)
	at org.springframework.boot.context.properties.source.ConfigurationPropertySourcesPropertyResolver.getProperty(ConfigurationPropertySourcesPropertyResolver.java:79)
	at org.springframework.boot.context.properties.source.ConfigurationPropertySourcesPropertyResolver.getProperty(ConfigurationPropertySourcesPropertyResolver.java:60)
	at org.springframework.core.env.AbstractEnvironment.getProperty(AbstractEnvironment.java:594)
	at org.springframework.context.support.PropertySourcesPlaceholderConfigurer$1.getProperty(PropertySourcesPlaceholderConfigurer.java:153)
	at org.springframework.context.support.PropertySourcesPlaceholderConfigurer$1.getProperty(PropertySourcesPlaceholderConfigurer.java:149)
	at org.springframework.core.env.PropertySourcesPropertyResolver.getProperty(PropertySourcesPropertyResolver.java:85)
	at org.springframework.core.env.PropertySourcesPropertyResolver.getPropertyAsRawString(PropertySourcesPropertyResolver.java:74)
	at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:153)
	at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:126)
	at org.springframework.core.env.AbstractPropertyResolver.doResolvePlaceholders(AbstractPropertyResolver.java:239)
	at org.springframework.core.env.AbstractPropertyResolver.resolveRequiredPlaceholders(AbstractPropertyResolver.java:210)
	at org.springframework.context.support.PropertySourcesPlaceholderConfigurer.lambda$processProperties$0(PropertySourcesPlaceholderConfigurer.java:191)
	at org.springframework.beans.factory.support.AbstractBeanFactory.resolveEmbeddedValue(AbstractBeanFactory.java:936)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1332)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1311)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:710)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:693)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:119)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:408)
	... 123 common frames omitted
2025-06-12 14:38:42.029 [main INFO ] com.example.clickstream.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 341712 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-12 14:38:42.030 [main INFO ] com.example.clickstream.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-12 14:38:42.542 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-12 14:38:42.545 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-12 14:38:42.545 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-12 14:38:42.545 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-12 14:38:42.591 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-12 14:38:42.591 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 541 ms
2025-06-12 14:38:42.625 [main ERROR] o.s.b.w.e.tomcat.TomcatStarter - Error starting Tomcat context. Exception: org.springframework.beans.factory.UnsatisfiedDependencyException. Message: Error creating bean with name 'webMvcMetricsFilter' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/web/servlet/WebMvcMetricsAutoConfiguration.class]: Unsatisfied dependency expressed through method 'webMvcMetricsFilter' parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'prometheusMeterRegistry' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/export/prometheus/PrometheusMetricsExportAutoConfiguration.class]: Initialization of bean failed; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataSourcePoolMetadataMeterBinder' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/jdbc/DataSourcePoolMetricsAutoConfiguration$DataSourcePoolMetadataMetricsConfiguration.class]: Unsatisfied dependency expressed through method 'dataSourcePoolMetadataMeterBinder' parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'clickHouseConfig' defined in file [/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes/com/example/clickstream/config/ClickHouseConfig.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'appConfig': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_APP_NAME' in value "${SPARK_APP_NAME}"
2025-06-12 14:38:42.633 [main INFO ] o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-06-12 14:38:42.641 [main WARN ] o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.context.ApplicationContextException: Unable to start web server; nested exception is org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
2025-06-12 14:38:42.649 [main INFO ] o.s.b.a.l.ConditionEvaluationReportLoggingListener - 

Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
2025-06-12 14:38:42.657 [main ERROR] o.s.boot.SpringApplication - Application run failed
org.springframework.context.ApplicationContextException: Unable to start web server; nested exception is org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:165)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:585)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:147)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:732)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:409)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:308)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1300)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1289)
	at com.example.clickstream.Application.main(Application.java:12)
Caused by: org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:142)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.<init>(TomcatWebServer.java:104)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getTomcatWebServer(TomcatServletWebServerFactory.java:481)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getWebServer(TomcatServletWebServerFactory.java:211)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.createWebServer(ServletWebServerApplicationContext.java:184)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:162)
	... 8 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'webMvcMetricsFilter' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/web/servlet/WebMvcMetricsAutoConfiguration.class]: Unsatisfied dependency expressed through method 'webMvcMetricsFilter' parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'prometheusMeterRegistry' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/export/prometheus/PrometheusMetricsExportAutoConfiguration.class]: Initialization of bean failed; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataSourcePoolMetadataMeterBinder' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/jdbc/DataSourcePoolMetricsAutoConfiguration$DataSourcePoolMetadataMetricsConfiguration.class]: Unsatisfied dependency expressed through method 'dataSourcePoolMetadataMeterBinder' parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'clickHouseConfig' defined in file [/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes/com/example/clickstream/config/ClickHouseConfig.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'appConfig': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_APP_NAME' in value "${SPARK_APP_NAME}"
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:801)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:536)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1352)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1195)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:582)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:213)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:213)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:204)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addServletContextInitializerBeans(ServletContextInitializerBeans.java:98)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.<init>(ServletContextInitializerBeans.java:86)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.getServletContextInitializerBeans(ServletWebServerApplicationContext.java:262)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.selfInitialize(ServletWebServerApplicationContext.java:236)
	at org.springframework.boot.web.embedded.tomcat.TomcatStarter.onStartup(TomcatStarter.java:53)
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:4904)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:171)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1332)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1322)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:140)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:866)
	at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:794)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:171)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1332)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1322)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:140)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:866)
	at org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:248)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:171)
	at org.apache.catalina.core.StandardService.startInternal(StandardService.java:433)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:171)
	at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:921)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:171)
	at org.apache.catalina.startup.Tomcat.start(Tomcat.java:489)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:123)
	... 13 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'prometheusMeterRegistry' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/export/prometheus/PrometheusMetricsExportAutoConfiguration.class]: Initialization of bean failed; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataSourcePoolMetadataMeterBinder' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/jdbc/DataSourcePoolMetricsAutoConfiguration$DataSourcePoolMetadataMetricsConfiguration.class]: Unsatisfied dependency expressed through method 'dataSourcePoolMetadataMeterBinder' parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'clickHouseConfig' defined in file [/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes/com/example/clickstream/config/ClickHouseConfig.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'appConfig': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_APP_NAME' in value "${SPARK_APP_NAME}"
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:628)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:276)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1391)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1311)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:788)
	... 53 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataSourcePoolMetadataMeterBinder' defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/jdbc/DataSourcePoolMetricsAutoConfiguration$DataSourcePoolMetadataMetricsConfiguration.class]: Unsatisfied dependency expressed through method 'dataSourcePoolMetadataMeterBinder' parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'clickHouseConfig' defined in file [/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes/com/example/clickstream/config/ClickHouseConfig.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'appConfig': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_APP_NAME' in value "${SPARK_APP_NAME}"
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:801)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:536)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1352)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1195)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:582)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:276)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.addCandidateEntry(DefaultListableBeanFactory.java:1616)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:1573)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveMultipleBeans(DefaultListableBeanFactory.java:1417)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1349)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory$DependencyObjectProvider.resolveStream(DefaultListableBeanFactory.java:2119)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory$DependencyObjectProvider.orderedStream(DefaultListableBeanFactory.java:2113)
	at org.springframework.boot.actuate.autoconfigure.metrics.MeterRegistryConfigurer.addBinders(MeterRegistryConfigurer.java:87)
	at org.springframework.boot.actuate.autoconfigure.metrics.MeterRegistryConfigurer.configure(MeterRegistryConfigurer.java:68)
	at org.springframework.boot.actuate.autoconfigure.metrics.MeterRegistryPostProcessor.postProcessAfterInitialization(MeterRegistryPostProcessor.java:64)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsAfterInitialization(AbstractAutowireCapableBeanFactory.java:455)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1808)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:620)
	... 63 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'clickHouseConfig' defined in file [/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes/com/example/clickstream/config/ClickHouseConfig.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'appConfig': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_APP_NAME' in value "${SPARK_APP_NAME}"
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:801)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:224)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1372)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1222)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:582)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:405)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1352)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1195)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:582)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:276)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.addCandidateEntry(DefaultListableBeanFactory.java:1609)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:1573)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveMultipleBeans(DefaultListableBeanFactory.java:1492)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1349)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1311)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:788)
	... 85 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'appConfig': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_APP_NAME' in value "${SPARK_APP_NAME}"
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:414)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1431)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:619)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:276)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1391)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1311)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:788)
	... 111 common frames omitted
Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'SPARK_APP_NAME' in value "${SPARK_APP_NAME}"
	at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:180)
	at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:126)
	at org.springframework.core.env.AbstractPropertyResolver.doResolvePlaceholders(AbstractPropertyResolver.java:239)
	at org.springframework.core.env.AbstractPropertyResolver.resolveRequiredPlaceholders(AbstractPropertyResolver.java:210)
	at org.springframework.core.env.AbstractPropertyResolver.resolveNestedPlaceholders(AbstractPropertyResolver.java:230)
	at org.springframework.boot.context.properties.source.ConfigurationPropertySourcesPropertyResolver.getProperty(ConfigurationPropertySourcesPropertyResolver.java:79)
	at org.springframework.boot.context.properties.source.ConfigurationPropertySourcesPropertyResolver.getProperty(ConfigurationPropertySourcesPropertyResolver.java:60)
	at org.springframework.core.env.AbstractEnvironment.getProperty(AbstractEnvironment.java:594)
	at org.springframework.context.support.PropertySourcesPlaceholderConfigurer$1.getProperty(PropertySourcesPlaceholderConfigurer.java:153)
	at org.springframework.context.support.PropertySourcesPlaceholderConfigurer$1.getProperty(PropertySourcesPlaceholderConfigurer.java:149)
	at org.springframework.core.env.PropertySourcesPropertyResolver.getProperty(PropertySourcesPropertyResolver.java:85)
	at org.springframework.core.env.PropertySourcesPropertyResolver.getPropertyAsRawString(PropertySourcesPropertyResolver.java:74)
	at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:153)
	at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:126)
	at org.springframework.core.env.AbstractPropertyResolver.doResolvePlaceholders(AbstractPropertyResolver.java:239)
	at org.springframework.core.env.AbstractPropertyResolver.resolveRequiredPlaceholders(AbstractPropertyResolver.java:210)
	at org.springframework.context.support.PropertySourcesPlaceholderConfigurer.lambda$processProperties$0(PropertySourcesPlaceholderConfigurer.java:191)
	at org.springframework.beans.factory.support.AbstractBeanFactory.resolveEmbeddedValue(AbstractBeanFactory.java:936)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1332)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1311)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:710)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:693)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:119)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:408)
	... 123 common frames omitted
2025-06-12 14:44:34.258 [main INFO ] com.example.clickstream.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 342488 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-12 14:44:34.259 [main INFO ] com.example.clickstream.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-12 14:44:34.775 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-12 14:44:34.777 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-12 14:44:34.778 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-12 14:44:34.778 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-12 14:44:34.819 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-12 14:44:34.819 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 541 ms
2025-06-12 14:44:34.954 [main WARN ] o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'clickstreamController' defined in file [/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes/com/example/clickstream/controller/ClickstreamController.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'clickstreamService' defined in file [/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes/com/example/clickstream/service/ClickstreamService.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'clickstreamProducer' defined in file [/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes/com/example/clickstream/producer/ClickstreamProducer.class]: Unsatisfied dependency expressed through constructor parameter 3; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type 'com.example.monitoring.metrics.ProducerMetrics' available: expected at least 1 bean which qualifies as autowire candidate. Dependency annotations: {}
2025-06-12 14:44:34.955 [main INFO ] o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-06-12 14:44:34.964 [main INFO ] o.s.b.a.l.ConditionEvaluationReportLoggingListener - 

Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
2025-06-12 14:44:34.974 [main ERROR] o.s.b.d.LoggingFailureAnalysisReporter - 

***************************
APPLICATION FAILED TO START
***************************

Description:

Parameter 3 of constructor in com.example.clickstream.producer.ClickstreamProducer required a bean of type 'com.example.monitoring.metrics.ProducerMetrics' that could not be found.


Action:

Consider defining a bean of type 'com.example.monitoring.metrics.ProducerMetrics' in your configuration.

2025-06-12 14:46:18.588 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 342641 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-12 14:46:18.590 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-12 14:46:19.100 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-12 14:46:19.105 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-12 14:46:19.106 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-12 14:46:19.106 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-12 14:46:19.144 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-12 14:46:19.144 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 531 ms
2025-06-12 14:46:19.505 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-12 14:46:19.547 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-12 14:46:19.577 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-12 14:46:19.577 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-12 14:46:19.577 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-12 14:46:19.577 [main INFO ] org.apache.spark.SparkContext - Submitted application: ClickstreamProcessor
2025-06-12 14:46:19.585 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-12 14:46:19.591 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-12 14:46:19.591 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-12 14:46:19.609 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-12 14:46:19.609 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-12 14:46:19.610 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-12 14:46:19.610 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-12 14:46:19.610 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-12 14:46:19.690 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 37101.
2025-06-12 14:46:19.697 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-12 14:46:19.708 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-12 14:46:19.714 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-12 14:46:19.714 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-12 14:46:19.716 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-12 14:46:19.720 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-f5bf74b9-da4c-4193-a936-3550a464e41d
2025-06-12 14:46:19.735 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-12 14:46:19.742 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-12 14:46:19.752 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1605ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-12 14:46:19.788 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-12 14:46:19.792 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-12 14:46:19.798 [main INFO ] org.sparkproject.jetty.server.Server - Started @1652ms
2025-06-12 14:46:19.809 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@7e50256d{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-12 14:46:19.809 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-12 14:46:19.816 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3400d6fa{/,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.845 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-12 14:46:19.848 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-12 14:46:19.856 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39507.
2025-06-12 14:46:19.856 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:39507
2025-06-12 14:46:19.856 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-12 14:46:19.859 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 39507, None)
2025-06-12 14:46:19.861 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:39507 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 39507, None)
2025-06-12 14:46:19.863 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 39507, None)
2025-06-12 14:46:19.863 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 39507, None)
2025-06-12 14:46:19.877 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@3400d6fa{/,null,STOPPED,@Spark}
2025-06-12 14:46:19.878 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6fa7ce4{/jobs,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.878 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a8b42a3{/jobs/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.879 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5649f55{/jobs/job,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.879 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12270a01{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.879 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@646d58cd{/stages,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.880 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12532e37{/stages/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.880 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d2a8819{/stages/stage,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.881 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b64bf61{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.881 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7846913f{/stages/pool,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.881 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60b553f{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.882 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66abb2fa{/storage,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.882 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133b712{/storage/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.882 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70f91ae3{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.883 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c2a3f0c{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.883 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d93ff21{/environment,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.883 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ca4c88a{/environment/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.884 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55397d15{/executors,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.884 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24ac6fef{/executors/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.885 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@227b9277{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.885 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b56d8a7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.887 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6de5ad56{/static,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.887 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63a72cc6{/,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.888 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cef885d{/api,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.888 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c02899{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.889 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74231642{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.890 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e94de5f{/metrics/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.961 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-12 14:46:19.973 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-12 14:46:19.978 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-12 14:46:19.985 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@390a7532{/SQL,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.985 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d49a1a0{/SQL/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.986 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@520ee6b3{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.986 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16a499d1{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:19.987 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@720ffab4{/static/sql,null,AVAILABLE,@Spark}
2025-06-12 14:46:20.264 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-12 14:46:20.410 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-12 14:46:20.427 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-12 14:46:20.444 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 14:46:20.444 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 14:46:20.444 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749714380443
2025-06-12 14:46:20.452 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:20.452 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:20.553 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:20.553 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:20.654 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:20.654 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:20.955 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:20.956 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:21.457 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:21.457 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:22.260 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:22.260 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:23.163 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:23.163 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:24.167 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:24.167 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:25.004 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-12 14:46:25.004 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-12 14:46:25.007 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@7e50256d{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-12 14:46:25.009 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-12 14:46:25.017 [dispatcher-event-loop-7 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-12 14:46:25.023 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-12 14:46:25.024 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-12 14:46:25.026 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-12 14:46:25.028 [dispatcher-event-loop-11 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-12 14:46:25.030 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-12 14:46:25.031 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-12 14:46:25.031 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-8b73a681-a208-4b8f-bd94-11d2a6835bb8
2025-06-12 14:46:25.069 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:25.070 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:26.072 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:26.073 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:26.976 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:26.976 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:27.979 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:27.979 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:28.982 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:28.982 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:29.986 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:29.986 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:30.888 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:30.888 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:31.791 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:31.791 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:32.795 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:32.795 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:33.798 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:33.798 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:34.801 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:34.801 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:35.804 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:35.804 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:36.807 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:36.807 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:37.810 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:37.810 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:38.813 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:38.813 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:39.816 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:39.816 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:40.719 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:40.719 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:41.722 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:41.722 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:42.725 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:42.726 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:43.729 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:43.729 [kafka-admin-client-thread | adminclient-1 WARN ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-12 14:46:45.398 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:45.398 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Cancelled in-flight API_VERSIONS request with correlation id 0 due to node -1 being disconnected (elapsed time since creation: 666ms, elapsed time since send: 666ms, request timeout: 3600000ms)
2025-06-12 14:46:46.401 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:46.401 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Cancelled in-flight API_VERSIONS request with correlation id 1 due to node -1 being disconnected (elapsed time since creation: 0ms, elapsed time since send: 0ms, request timeout: 3600000ms)
2025-06-12 14:46:47.203 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Node -1 disconnected.
2025-06-12 14:46:47.204 [kafka-admin-client-thread | adminclient-1 INFO ] o.apache.kafka.clients.NetworkClient - [AdminClient clientId=adminclient-1] Cancelled in-flight API_VERSIONS request with correlation id 2 due to node -1 being disconnected (elapsed time since creation: 0ms, elapsed time since send: 0ms, request timeout: 3600000ms)
2025-06-12 14:46:48.658 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-12 14:46:48.660 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 14:46:48.660 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 14:46:48.660 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 14:46:48.673 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-12 14:46:48.684 [main INFO ] com.example.Application - Started Application in 30.231 seconds (JVM running for 30.538)
2025-06-12 14:46:48.697 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-12 14:46:48.697 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext already stopped.
2025-06-12 14:46:51.670 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 345557 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-12 14:46:51.671 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-12 14:46:52.186 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-12 14:46:52.192 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-12 14:46:52.193 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-12 14:46:52.193 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-12 14:46:52.229 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-12 14:46:52.229 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 534 ms
2025-06-12 14:46:52.579 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-12 14:46:52.624 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-12 14:46:52.654 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-12 14:46:52.654 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-12 14:46:52.654 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-12 14:46:52.655 [main INFO ] org.apache.spark.SparkContext - Submitted application: ClickstreamProcessor
2025-06-12 14:46:52.662 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-12 14:46:52.667 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-12 14:46:52.667 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-12 14:46:52.684 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-12 14:46:52.684 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-12 14:46:52.684 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-12 14:46:52.684 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-12 14:46:52.684 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-12 14:46:52.759 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 43033.
2025-06-12 14:46:52.767 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-12 14:46:52.779 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-12 14:46:52.785 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-12 14:46:52.785 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-12 14:46:52.786 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-12 14:46:52.791 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-69a4274e-b708-4977-b3bc-7d5aca2cadbc
2025-06-12 14:46:52.805 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-12 14:46:52.813 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-12 14:46:52.823 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1611ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-12 14:46:52.860 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-12 14:46:52.864 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-12 14:46:52.870 [main INFO ] org.sparkproject.jetty.server.Server - Started @1658ms
2025-06-12 14:46:52.881 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@68b6068f{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-12 14:46:52.881 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-12 14:46:52.888 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3cf06a01{/,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.917 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-12 14:46:52.921 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-12 14:46:52.928 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36171.
2025-06-12 14:46:52.928 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:36171
2025-06-12 14:46:52.929 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-12 14:46:52.932 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 36171, None)
2025-06-12 14:46:52.933 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:36171 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 36171, None)
2025-06-12 14:46:52.935 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 36171, None)
2025-06-12 14:46:52.936 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 36171, None)
2025-06-12 14:46:52.951 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@3cf06a01{/,null,STOPPED,@Spark}
2025-06-12 14:46:52.952 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b7e4d14{/jobs,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.952 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@601d9f3a{/jobs/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.952 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51fb5fe6{/jobs/job,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.953 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1791e231{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.953 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e360c3b{/stages,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.953 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3cb49121{/stages/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.954 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3155f190{/stages/stage,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.954 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ebd8d2{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.955 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a63fa71{/stages/pool,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.955 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5018b56b{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.955 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@737ff5c4{/storage,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.956 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ff64d{/storage/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.956 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79777da7{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.956 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e05a706{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.957 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a69014e{/environment,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.957 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@543ac221{/environment/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.957 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50e1f3fc{/executors,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.958 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56da8847{/executors/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.958 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c02a007{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.959 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61bd0845{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.961 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35d4035f{/static,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.961 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57b3d869{/,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.962 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@291cbe70{/api,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.962 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4efe014f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.963 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c5ac0{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-12 14:46:52.964 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45832b85{/metrics/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:53.050 [main WARN ] o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-06-12 14:46:53.051 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-12 14:46:53.054 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-12 14:46:53.058 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40aad17d{/SQL,null,AVAILABLE,@Spark}
2025-06-12 14:46:53.059 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bc14211{/SQL/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:53.059 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74f89bad{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-12 14:46:53.059 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@570299e3{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:53.060 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@745cf754{/static/sql,null,AVAILABLE,@Spark}
2025-06-12 14:46:53.340 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-12 14:46:53.488 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-12 14:46:53.505 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-12 14:46:53.523 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-12 14:46:53.523 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-12 14:46:53.523 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749714413523
2025-06-12 14:46:53.658 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-12 14:46:53.660 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-12 14:46:53.660 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-12 14:46:53.660 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-12 14:46:53.665 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-12 14:46:53.669 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-12 14:46:53.677 [main INFO ] com.example.Application - Started Application in 2.149 seconds (JVM running for 2.465)
2025-06-12 14:46:53.788 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-12 14:46:53.788 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-12 14:46:53.788 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 0 ms
2025-06-12 14:46:53.831 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-12 14:46:53.838 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52978db8{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-12 14:46:53.839 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69e5d0ef{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:53.839 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@253b1182{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-12 14:46:53.840 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a0acf2f{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-12 14:46:53.840 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f7b8f89{/static/sql,null,AVAILABLE,@Spark}
2025-06-12 14:47:26.743 [http-nio-8080-exec-1 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-12 14:47:27.501 [http-nio-8080-exec-1 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `event_id` cannot be resolved. Did you mean one of the following? [`eventId`, `eventName`, `eventParams`, `userId`, `appId`].;
'Filter ((((isnotnull('event_id) AND isnotnull('event_name)) AND isnotnull('event_time)) AND isnotnull('user_id)) AND isnotnull('session_id))
+- LocalRelation [appId#0, eventId#1, eventName#2, eventParams#3, eventTimestamp#4, pageUrl#5, platform#6, sessionId#7, userId#8]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:258)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:211)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:4201)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1712)
	at com.example.clickstream.validation.EventValidator.validateEvents(EventValidator.java:27)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:39)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-12 14:47:27.705 [http-nio-8080-exec-2 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-12 14:47:27.719 [http-nio-8080-exec-2 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `event_id` cannot be resolved. Did you mean one of the following? [`eventId`, `eventName`, `eventParams`, `userId`, `appId`].;
'Filter ((((isnotnull('event_id) AND isnotnull('event_name)) AND isnotnull('event_time)) AND isnotnull('user_id)) AND isnotnull('session_id))
+- LocalRelation [appId#18, eventId#19, eventName#20, eventParams#21, eventTimestamp#22, pageUrl#23, platform#24, sessionId#25, userId#26]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:258)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:211)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:4201)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1712)
	at com.example.clickstream.validation.EventValidator.validateEvents(EventValidator.java:27)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:39)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-12 14:47:28.706 [http-nio-8080-exec-3 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-12 14:47:28.716 [http-nio-8080-exec-3 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `event_id` cannot be resolved. Did you mean one of the following? [`eventId`, `eventName`, `eventParams`, `userId`, `appId`].;
'Filter ((((isnotnull('event_id) AND isnotnull('event_name)) AND isnotnull('event_time)) AND isnotnull('user_id)) AND isnotnull('session_id))
+- LocalRelation [appId#36, eventId#37, eventName#38, eventParams#39, eventTimestamp#40, pageUrl#41, platform#42, sessionId#43, userId#44]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:258)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:211)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:4201)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1712)
	at com.example.clickstream.validation.EventValidator.validateEvents(EventValidator.java:27)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:39)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-12 14:47:34.153 [http-nio-8080-exec-4 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-12 14:47:34.162 [http-nio-8080-exec-4 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `event_id` cannot be resolved. Did you mean one of the following? [`eventId`, `eventName`, `eventParams`, `userId`, `appId`].;
'Filter ((((isnotnull('event_id) AND isnotnull('event_name)) AND isnotnull('event_time)) AND isnotnull('user_id)) AND isnotnull('session_id))
+- LocalRelation [appId#54, eventId#55, eventName#56, eventParams#57, eventTimestamp#58, pageUrl#59, platform#60, sessionId#61, userId#62]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:258)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:211)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:4201)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1712)
	at com.example.clickstream.validation.EventValidator.validateEvents(EventValidator.java:27)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:39)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-12 14:47:35.185 [http-nio-8080-exec-5 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-12 14:47:35.196 [http-nio-8080-exec-5 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `event_id` cannot be resolved. Did you mean one of the following? [`eventId`, `eventName`, `eventParams`, `userId`, `appId`].;
'Filter ((((isnotnull('event_id) AND isnotnull('event_name)) AND isnotnull('event_time)) AND isnotnull('user_id)) AND isnotnull('session_id))
+- LocalRelation [appId#72, eventId#73, eventName#74, eventParams#75, eventTimestamp#76, pageUrl#77, platform#78, sessionId#79, userId#80]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:258)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:211)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:4201)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1712)
	at com.example.clickstream.validation.EventValidator.validateEvents(EventValidator.java:27)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:39)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-12 14:47:36.205 [http-nio-8080-exec-6 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-12 14:47:36.212 [http-nio-8080-exec-6 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `event_id` cannot be resolved. Did you mean one of the following? [`eventId`, `eventName`, `eventParams`, `userId`, `appId`].;
'Filter ((((isnotnull('event_id) AND isnotnull('event_name)) AND isnotnull('event_time)) AND isnotnull('user_id)) AND isnotnull('session_id))
+- LocalRelation [appId#90, eventId#91, eventName#92, eventParams#93, eventTimestamp#94, pageUrl#95, platform#96, sessionId#97, userId#98]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:258)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:211)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:4201)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1712)
	at com.example.clickstream.validation.EventValidator.validateEvents(EventValidator.java:27)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:39)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-12 14:47:36.608 [http-nio-8080-exec-7 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-12 14:47:36.615 [http-nio-8080-exec-7 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `event_id` cannot be resolved. Did you mean one of the following? [`eventId`, `eventName`, `eventParams`, `userId`, `appId`].;
'Filter ((((isnotnull('event_id) AND isnotnull('event_name)) AND isnotnull('event_time)) AND isnotnull('user_id)) AND isnotnull('session_id))
+- LocalRelation [appId#108, eventId#109, eventName#110, eventParams#111, eventTimestamp#112, pageUrl#113, platform#114, sessionId#115, userId#116]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:258)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:211)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:4201)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1712)
	at com.example.clickstream.validation.EventValidator.validateEvents(EventValidator.java:27)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:39)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-12 14:47:37.220 [http-nio-8080-exec-8 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-12 14:47:37.226 [http-nio-8080-exec-8 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `event_id` cannot be resolved. Did you mean one of the following? [`eventId`, `eventName`, `eventParams`, `userId`, `appId`].;
'Filter ((((isnotnull('event_id) AND isnotnull('event_name)) AND isnotnull('event_time)) AND isnotnull('user_id)) AND isnotnull('session_id))
+- LocalRelation [appId#126, eventId#127, eventName#128, eventParams#129, eventTimestamp#130, pageUrl#131, platform#132, sessionId#133, userId#134]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:258)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:211)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:4201)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1712)
	at com.example.clickstream.validation.EventValidator.validateEvents(EventValidator.java:27)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:39)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-12 14:47:38.222 [http-nio-8080-exec-9 INFO ] c.e.c.c.ClickstreamController - Received events batch [Request ID: null]
2025-06-12 14:47:38.227 [http-nio-8080-exec-9 ERROR] c.e.c.c.ClickstreamController - Error processing events
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `event_id` cannot be resolved. Did you mean one of the following? [`eventId`, `eventName`, `eventParams`, `userId`, `appId`].;
'Filter ((((isnotnull('event_id) AND isnotnull('event_name)) AND isnotnull('event_time)) AND isnotnull('user_id)) AND isnotnull('session_id))
+- LocalRelation [appId#144, eventId#145, eventName#146, eventParams#147, eventTimestamp#148, pageUrl#149, platform#150, sessionId#151, userId#152]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:258)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:211)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:4201)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1712)
	at com.example.clickstream.validation.EventValidator.validateEvents(EventValidator.java:27)
	at com.example.clickstream.service.ClickstreamService.processEvents(ClickstreamService.java:39)
	at com.example.clickstream.controller.ClickstreamController.ingestEvents(ClickstreamController.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-12 14:48:36.552 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-12 14:48:36.552 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-12 14:48:36.557 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@68b6068f{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-12 14:48:36.559 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-12 14:48:36.559 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-12 14:48:36.559 [SpringApplicationShutdownHook INFO ] org.apache.spark.SparkContext - SparkContext already stopped.
