2025-06-05 06:46:56.296 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 06:46:56.396 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 06:46:56.447 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 06:46:56.447 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 06:46:56.447 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 06:46:56.447 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 06:46:56.464 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 06:46:56.471 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 06:46:56.472 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 06:46:56.501 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 06:46:56.501 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 06:46:56.501 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 06:46:56.502 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 06:46:56.502 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 06:46:56.651 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 40493.
2025-06-05 06:46:56.665 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 06:46:56.681 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 06:46:56.690 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 06:46:56.691 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 06:46:56.693 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 06:46:56.702 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-8571ddba-2c27-4d73-b119-fb12b96dabc7
2025-06-05 06:46:56.723 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 06:46:56.735 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 06:46:56.769 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1063ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 06:46:56.828 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 06:46:56.834 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 06:46:56.844 [main INFO ] org.sparkproject.jetty.server.Server - Started @1139ms
2025-06-05 06:46:56.864 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@3ca94f35{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 06:46:56.864 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 06:46:56.877 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e2f86e6{/,null,AVAILABLE,@Spark}
2025-06-05 06:46:56.931 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 06:46:56.936 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 06:46:56.950 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40245.
2025-06-05 06:46:56.950 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:40245
2025-06-05 06:46:56.951 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 06:46:56.956 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 40245, None)
2025-06-05 06:46:56.959 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:40245 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 40245, None)
2025-06-05 06:46:56.962 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 40245, None)
2025-06-05 06:46:56.962 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 40245, None)
2025-06-05 06:46:57.047 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7e2f86e6{/,null,STOPPED,@Spark}
2025-06-05 06:46:57.048 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e43e323{/jobs,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.049 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10643593{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.049 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48840594{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.050 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14823f76{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.050 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ed16657{/stages,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.051 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@113e13f9{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.052 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f66ffc8{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.052 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2def7a7a{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.053 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c080ef3{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.054 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ee6291f{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.054 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37e0292a{/storage,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.055 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35267fd4{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.056 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36a6bea6{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.056 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42373389{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.057 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a62c7cd{/environment,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.057 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c36db44{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.057 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7903d448{/executors,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.058 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42ea287{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.058 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f0b3cfe{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.059 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65a48602{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.063 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@336206d8{/static,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.064 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ba359bd{/,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.065 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@673919a7{/api,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.065 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@582a764a{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.065 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@292158f8{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.067 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35d5ac51{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.150 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 06:46:57.154 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 06:46:57.161 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2cde651b{/SQL,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.162 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3bb87d36{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.162 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22a10ac6{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.163 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@503df2d0{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.169 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@655a01d8{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 06:46:57.737 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 107.558632 ms
2025-06-05 06:46:57.758 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 14.686781 ms
2025-06-05 06:46:58.755 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 06:46:58.756 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 06:46:58.760 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@3ca94f35{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 06:46:58.763 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 06:46:58.774 [dispatcher-event-loop-7 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 06:46:58.782 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 06:46:58.782 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 06:46:58.785 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 06:46:58.788 [dispatcher-event-loop-11 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 06:46:58.791 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 06:46:58.792 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 06:46:58.792 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-125d3fdd-3c1f-45cc-9c21-0e86493c113e
2025-06-05 06:47:54.141 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 06:47:54.229 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 06:47:54.278 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 06:47:54.278 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 06:47:54.279 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 06:47:54.279 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 06:47:54.290 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 06:47:54.296 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 06:47:54.297 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 06:47:54.328 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 06:47:54.329 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 06:47:54.329 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 06:47:54.329 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 06:47:54.329 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 06:47:54.453 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 37663.
2025-06-05 06:47:54.467 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 06:47:54.485 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 06:47:54.494 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 06:47:54.494 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 06:47:54.496 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 06:47:54.507 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-6ab40e0b-bfe7-4ed2-be9d-700043da3454
2025-06-05 06:47:54.524 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 06:47:54.533 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 06:47:54.556 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @976ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 06:47:54.605 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 06:47:54.611 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 06:47:54.620 [main INFO ] org.sparkproject.jetty.server.Server - Started @1040ms
2025-06-05 06:47:54.637 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@1ea67099{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 06:47:54.637 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 06:47:54.651 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30814f43{/,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.704 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 06:47:54.709 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 06:47:54.720 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34731.
2025-06-05 06:47:54.720 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:34731
2025-06-05 06:47:54.722 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 06:47:54.725 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 34731, None)
2025-06-05 06:47:54.729 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:34731 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 34731, None)
2025-06-05 06:47:54.731 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 34731, None)
2025-06-05 06:47:54.732 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 34731, None)
2025-06-05 06:47:54.816 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@30814f43{/,null,STOPPED,@Spark}
2025-06-05 06:47:54.817 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133661d{/jobs,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.818 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3414a8c3{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.818 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68d651f2{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.819 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e43e323{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.819 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10643593{/stages,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.819 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@eca6a74{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.820 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ed16657{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.821 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@113e13f9{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.822 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7979b8b7{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.822 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bc49bc5{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.823 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f66ffc8{/storage,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.824 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2def7a7a{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.825 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c080ef3{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.825 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ee6291f{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.826 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37e0292a{/environment,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.827 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35267fd4{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.828 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36a6bea6{/executors,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.828 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42373389{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.829 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a62c7cd{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.830 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c36db44{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.835 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7903d448{/static,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.836 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@290d10ef{/,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.837 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@644ded04{/api,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.837 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f038248{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.838 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e8a1ab4{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.840 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6944e53e{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.926 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 06:47:54.930 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 06:47:54.937 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fdf17dc{/SQL,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.938 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@650ae78c{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.938 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ff8a9dc{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.939 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@534e58b6{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 06:47:54.946 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12fe1f28{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 06:47:55.505 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 102.330973 ms
2025-06-05 06:47:55.524 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 13.290673 ms
2025-06-05 06:47:56.552 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 06:47:56.579 [main INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 06:47:56.580 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 06:47:56.580 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 06:47:56.580 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749080876580
2025-06-05 06:47:56.737 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 06:47:56.740 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 06:47:56.740 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 06:47:56.740 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 06:47:56.740 [main INFO ] o.a.spark.sql.kafka010.KafkaRelation - GetBatch generating RDD of offset range: KafkaOffsetRange(clickstream-events-0,-2,-1,None), KafkaOffsetRange(clickstream-events-1,-2,-1,None)
2025-06-05 06:47:56.916 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.89558 ms
2025-06-05 06:47:57.053 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 33.458287 ms
2025-06-05 06:47:57.067 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.734437 ms
2025-06-05 06:47:57.088 [main INFO ] org.apache.spark.SparkContext - Starting job: save at SparkClickstreamProcessor.java:231
2025-06-05 06:47:57.099 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (save at SparkClickstreamProcessor.java:231) with 1 output partitions
2025-06-05 06:47:57.100 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (save at SparkClickstreamProcessor.java:231)
2025-06-05 06:47:57.100 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-05 06:47:57.101 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-05 06:47:57.104 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[10] at save at SparkClickstreamProcessor.java:231), which has no missing parents
2025-06-05 06:47:57.196 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 62.6 KiB, free 9.2 GiB)
2025-06-05 06:47:57.215 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 9.2 GiB)
2025-06-05 06:47:57.217 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:34731 (size: 22.2 KiB, free: 9.2 GiB)
2025-06-05 06:47:57.219 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 06:47:57.227 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[10] at save at SparkClickstreamProcessor.java:231) (first 15 tasks are for partitions Vector(0))
2025-06-05 06:47:57.228 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
2025-06-05 06:47:57.260 [dispatcher-event-loop-6 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes) 
2025-06-05 06:47:57.267 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 06:47:57.352 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 06:47:57.375 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 06:47:57.407 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 06:47:57.407 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 06:47:57.407 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749080877407
2025-06-05 06:47:57.409 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor-1, groupId=spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 06:47:57.412 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor-1, groupId=spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 06:47:57.420 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor-1, groupId=spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 06:47:57.428 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor-1, groupId=spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 06:47:57.428 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor-1, groupId=spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 06:47:57.429 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor-1, groupId=spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 06:47:57.454 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 11.785381 ms
2025-06-05 06:47:57.472 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 12.225783 ms
2025-06-05 06:47:57.477 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor-1, groupId=spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 06:47:57.498 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor-1, groupId=spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 06:47:58.000 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor-1, groupId=spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 06:47:58.000 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor-1, groupId=spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 06:47:58.001 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor-1, groupId=spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 06:47:58.073 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 19.484723 ms
2025-06-05 06:47:58.115 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 4117 bytes result sent to driver
2025-06-05 06:47:58.120 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 864 ms on phamviethoa (executor driver) (1/1)
2025-06-05 06:47:58.121 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 06:47:58.125 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (save at SparkClickstreamProcessor.java:231) finished in 1.013 s
2025-06-05 06:47:58.128 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 06:47:58.128 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-05 06:47:58.129 [main INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: save at SparkClickstreamProcessor.java:231, took 1.040711 s
2025-06-05 06:47:58.151 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 13.102171 ms
2025-06-05 06:47:58.165 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor-1, groupId=spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 06:47:58.165 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor-1, groupId=spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 06:47:58.169 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 06:47:58.169 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 06:47:58.169 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 06:47:58.169 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 06:47:58.174 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-relation-3aac8a14-69c3-4d29-941f-e964aba75eea-executor-1 unregistered
2025-06-05 06:47:58.174 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 06:47:58.175 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 06:47:58.180 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@1ea67099{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 06:47:58.183 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 06:47:58.191 [dispatcher-event-loop-10 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 06:47:58.196 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 06:47:58.196 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 06:47:58.199 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 06:47:58.200 [dispatcher-event-loop-14 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 06:47:58.203 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 06:47:58.203 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 06:47:58.204 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-46777967-1454-4401-9d9f-588c9c976dde
2025-06-05 06:52:55.820 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 06:52:55.901 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 06:52:55.945 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 06:52:55.945 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 06:52:55.945 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 06:52:55.946 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 06:52:55.956 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 06:52:55.962 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 06:52:55.962 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 06:52:55.990 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 06:52:55.990 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 06:52:55.990 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 06:52:55.990 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 06:52:55.991 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 06:52:56.113 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 37689.
2025-06-05 06:52:56.127 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 06:52:56.143 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 06:52:56.151 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 06:52:56.152 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 06:52:56.154 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 06:52:56.165 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-884ae3ed-df4b-44fb-8219-350f86ae3718
2025-06-05 06:52:56.182 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 06:52:56.192 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 06:52:56.215 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1055ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 06:52:56.259 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 06:52:56.265 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 06:52:56.273 [main INFO ] org.sparkproject.jetty.server.Server - Started @1113ms
2025-06-05 06:52:56.288 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@5cd3efb7{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 06:52:56.288 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 06:52:56.300 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17740dae{/,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.350 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 06:52:56.354 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 06:52:56.367 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38877.
2025-06-05 06:52:56.367 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:38877
2025-06-05 06:52:56.369 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 06:52:56.373 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 38877, None)
2025-06-05 06:52:56.375 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:38877 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 38877, None)
2025-06-05 06:52:56.377 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 38877, None)
2025-06-05 06:52:56.378 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 38877, None)
2025-06-05 06:52:56.458 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@17740dae{/,null,STOPPED,@Spark}
2025-06-05 06:52:56.459 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21c815e4{/jobs,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.460 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a331b46{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.461 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241f05b{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.461 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.461 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/stages,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.462 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c9320c2{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.462 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.463 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67b4ec{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.463 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f91da5e{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.464 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79fd6f95{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.464 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/storage,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.464 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.465 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.465 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.466 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/environment,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.466 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.466 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/executors,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.467 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.467 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.468 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.472 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/static,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.472 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e0895f5{/,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.473 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fd9ebde{/api,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.474 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e9ef5b6{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.474 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4110765e{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.476 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53ec2968{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.557 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 06:52:56.561 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 06:52:56.569 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c86da0c{/SQL,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.569 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6732726{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.570 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47d023b7{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.570 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d64c100{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 06:52:56.577 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d904ff1{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 06:52:57.152 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 102.751768 ms
2025-06-05 06:52:57.176 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 17.4121 ms
2025-06-05 06:52:58.270 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 06:52:58.297 [main INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 06:52:58.298 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 06:52:58.298 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 06:52:58.298 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081178297
2025-06-05 06:52:58.452 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 06:52:58.455 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 06:52:58.455 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 06:52:58.456 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 06:52:58.456 [main INFO ] o.a.spark.sql.kafka010.KafkaRelation - GetBatch generating RDD of offset range: KafkaOffsetRange(clickstream-events-0,-2,-1,None), KafkaOffsetRange(clickstream-events-1,-2,-1,None)
2025-06-05 06:52:58.627 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.077965 ms
2025-06-05 06:52:58.755 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 25.196063 ms
2025-06-05 06:52:58.763 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.618518 ms
2025-06-05 06:52:58.776 [main INFO ] org.apache.spark.SparkContext - Starting job: save at SparkClickstreamProcessor.java:232
2025-06-05 06:52:58.787 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (save at SparkClickstreamProcessor.java:232) with 1 output partitions
2025-06-05 06:52:58.787 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (save at SparkClickstreamProcessor.java:232)
2025-06-05 06:52:58.787 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-05 06:52:58.788 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-05 06:52:58.791 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[10] at save at SparkClickstreamProcessor.java:232), which has no missing parents
2025-06-05 06:52:58.889 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 63.0 KiB, free 9.2 GiB)
2025-06-05 06:52:58.907 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.3 KiB, free 9.2 GiB)
2025-06-05 06:52:58.910 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:38877 (size: 22.3 KiB, free: 9.2 GiB)
2025-06-05 06:52:58.912 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 06:52:58.922 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[10] at save at SparkClickstreamProcessor.java:232) (first 15 tasks are for partitions Vector(0))
2025-06-05 06:52:58.923 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
2025-06-05 06:52:58.952 [dispatcher-event-loop-6 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes) 
2025-06-05 06:52:58.960 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 06:52:59.052 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 06:52:59.075 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 06:52:59.106 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 06:52:59.106 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 06:52:59.106 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081179106
2025-06-05 06:52:59.108 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor-1, groupId=spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 06:52:59.111 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor-1, groupId=spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 06:52:59.120 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor-1, groupId=spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 06:52:59.127 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor-1, groupId=spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 06:52:59.128 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor-1, groupId=spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 06:52:59.129 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor-1, groupId=spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 06:52:59.158 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 15.243415 ms
2025-06-05 06:52:59.178 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 14.367339 ms
2025-06-05 06:52:59.183 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor-1, groupId=spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 06:52:59.207 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor-1, groupId=spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 06:52:59.707 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor-1, groupId=spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 06:52:59.708 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor-1, groupId=spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 06:52:59.708 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor-1, groupId=spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 06:52:59.786 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 20.72133 ms
2025-06-05 06:52:59.826 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 4160 bytes result sent to driver
2025-06-05 06:52:59.833 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 887 ms on phamviethoa (executor driver) (1/1)
2025-06-05 06:52:59.834 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 06:52:59.838 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (save at SparkClickstreamProcessor.java:232) finished in 1.038 s
2025-06-05 06:52:59.839 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 06:52:59.839 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-05 06:52:59.840 [main INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: save at SparkClickstreamProcessor.java:232, took 1.063731 s
2025-06-05 06:52:59.856 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.681703 ms
2025-06-05 06:52:59.871 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor-1, groupId=spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 06:52:59.871 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor-1, groupId=spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 06:52:59.876 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 06:52:59.876 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 06:52:59.876 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 06:52:59.876 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 06:52:59.881 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-relation-72bf874e-9882-4bba-a790-fb758515688f-executor-1 unregistered
2025-06-05 06:52:59.881 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 06:52:59.882 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 06:52:59.887 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@5cd3efb7{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 06:52:59.890 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 06:52:59.900 [dispatcher-event-loop-10 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 06:52:59.906 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 06:52:59.906 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 06:52:59.910 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 06:52:59.912 [dispatcher-event-loop-14 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 06:52:59.916 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 06:52:59.916 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 06:52:59.917 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-c1b62e05-93d7-4a8f-b36b-7114759acbb2
2025-06-05 06:54:16.723 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 06:54:16.819 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 06:54:16.880 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 06:54:16.880 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 06:54:16.880 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 06:54:16.881 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 06:54:16.894 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 06:54:16.900 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 06:54:16.900 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 06:54:16.938 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 06:54:16.938 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 06:54:16.938 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 06:54:16.938 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 06:54:16.939 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 06:54:17.068 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 41407.
2025-06-05 06:54:17.081 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 06:54:17.097 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 06:54:17.106 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 06:54:17.107 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 06:54:17.108 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 06:54:17.118 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-715e26e1-a902-402d-9fb3-7aa7bcf02b3c
2025-06-05 06:54:17.135 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 06:54:17.144 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 06:54:17.165 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1060ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 06:54:17.214 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 06:54:17.219 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 06:54:17.229 [main INFO ] org.sparkproject.jetty.server.Server - Started @1124ms
2025-06-05 06:54:17.247 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@1ea67099{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 06:54:17.248 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 06:54:17.263 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7144655b{/,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.319 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 06:54:17.323 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 06:54:17.338 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41755.
2025-06-05 06:54:17.338 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:41755
2025-06-05 06:54:17.339 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 06:54:17.343 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 41755, None)
2025-06-05 06:54:17.345 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:41755 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 41755, None)
2025-06-05 06:54:17.348 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 41755, None)
2025-06-05 06:54:17.349 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 41755, None)
2025-06-05 06:54:17.440 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7144655b{/,null,STOPPED,@Spark}
2025-06-05 06:54:17.441 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241f05b{/jobs,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.442 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.443 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c9320c2{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.443 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cc9385{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.444 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7915bca3{/stages,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.444 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.445 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79fd6f95{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.446 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.446 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.447 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.447 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/storage,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.448 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.448 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.449 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.449 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/environment,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.450 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.450 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/executors,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.451 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.452 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397ef2{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.453 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e93c1f{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.457 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9b21bd3{/static,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.458 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@323f3c96{/,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.459 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b6d92e{/api,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.459 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25d93198{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.460 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f951a7f{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.462 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e7f0216{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.543 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 06:54:17.546 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 06:54:17.553 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4190bc8a{/SQL,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.554 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c83ae01{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.554 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@650ae78c{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.555 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79c5460e{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:17.561 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b495d4{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 06:54:18.112 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 101.231092 ms
2025-06-05 06:54:18.134 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 15.507894 ms
2025-06-05 06:54:19.210 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 06:54:19.236 [main INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 06:54:19.237 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 06:54:19.237 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 06:54:19.237 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081259236
2025-06-05 06:54:19.390 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 06:54:19.392 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 06:54:19.393 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 06:54:19.393 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 06:54:19.393 [main INFO ] o.a.spark.sql.kafka010.KafkaRelation - GetBatch generating RDD of offset range: KafkaOffsetRange(clickstream-events-0,-2,-1,None), KafkaOffsetRange(clickstream-events-1,-2,-1,None)
2025-06-05 06:54:19.563 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.452036 ms
2025-06-05 06:54:19.691 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 24.545747 ms
2025-06-05 06:54:19.699 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 5.579628 ms
2025-06-05 06:54:19.712 [main INFO ] org.apache.spark.SparkContext - Starting job: save at SparkClickstreamProcessor.java:232
2025-06-05 06:54:19.723 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (save at SparkClickstreamProcessor.java:232) with 1 output partitions
2025-06-05 06:54:19.723 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (save at SparkClickstreamProcessor.java:232)
2025-06-05 06:54:19.723 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-05 06:54:19.724 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-05 06:54:19.726 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[10] at save at SparkClickstreamProcessor.java:232), which has no missing parents
2025-06-05 06:54:19.813 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 63.0 KiB, free 9.2 GiB)
2025-06-05 06:54:19.830 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 9.2 GiB)
2025-06-05 06:54:19.833 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:41755 (size: 22.2 KiB, free: 9.2 GiB)
2025-06-05 06:54:19.835 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 06:54:19.845 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[10] at save at SparkClickstreamProcessor.java:232) (first 15 tasks are for partitions Vector(0))
2025-06-05 06:54:19.845 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
2025-06-05 06:54:19.875 [dispatcher-event-loop-6 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes) 
2025-06-05 06:54:19.881 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 06:54:19.980 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 06:54:19.999 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 06:54:20.028 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 06:54:20.028 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 06:54:20.028 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081260028
2025-06-05 06:54:20.030 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor-1, groupId=spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 06:54:20.032 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor-1, groupId=spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 06:54:20.039 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor-1, groupId=spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 06:54:20.046 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor-1, groupId=spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 06:54:20.046 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor-1, groupId=spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 06:54:20.047 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor-1, groupId=spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 06:54:20.070 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.332677 ms
2025-06-05 06:54:20.088 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 12.469429 ms
2025-06-05 06:54:20.094 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor-1, groupId=spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 06:54:20.118 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor-1, groupId=spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 06:54:20.619 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor-1, groupId=spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 06:54:20.619 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor-1, groupId=spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 06:54:20.620 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor-1, groupId=spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 06:54:20.695 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 16.880151 ms
2025-06-05 06:54:20.730 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3736 bytes result sent to driver
2025-06-05 06:54:20.737 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 867 ms on phamviethoa (executor driver) (1/1)
2025-06-05 06:54:20.738 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 06:54:20.740 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (save at SparkClickstreamProcessor.java:232) finished in 1.007 s
2025-06-05 06:54:20.742 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 06:54:20.742 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-05 06:54:20.743 [main INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: save at SparkClickstreamProcessor.java:232, took 1.029981 s
2025-06-05 06:54:20.756 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.991173 ms
2025-06-05 06:54:20.768 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor-1, groupId=spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 06:54:20.768 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor-1, groupId=spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 06:54:20.772 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 06:54:20.772 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 06:54:20.772 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 06:54:20.772 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 06:54:20.775 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-relation-a359eb57-b74b-4c27-bdba-525464e5031d-executor-1 unregistered
2025-06-05 06:54:20.776 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 06:54:20.776 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 06:54:20.781 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@1ea67099{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 06:54:20.783 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 06:54:20.790 [dispatcher-event-loop-10 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 06:54:20.795 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 06:54:20.795 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 06:54:20.798 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 06:54:20.799 [dispatcher-event-loop-14 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 06:54:20.802 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 06:54:20.802 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 06:54:20.802 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-d7f2d19c-7ef4-40fa-946b-8a7e74f3c65f
2025-06-05 06:54:55.731 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 06:54:55.818 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 06:54:55.862 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 06:54:55.862 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 06:54:55.862 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 06:54:55.863 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 06:54:55.874 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 06:54:55.880 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 06:54:55.880 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 06:54:55.908 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 06:54:55.908 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 06:54:55.908 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 06:54:55.908 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 06:54:55.908 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 06:54:56.033 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 36581.
2025-06-05 06:54:56.046 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 06:54:56.062 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 06:54:56.071 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 06:54:56.071 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 06:54:56.073 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 06:54:56.094 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-3e77aea6-99c8-496f-a736-5a18ad90b756
2025-06-05 06:54:56.110 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 06:54:56.118 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 06:54:56.138 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1009ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 06:54:56.184 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 06:54:56.189 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 06:54:56.199 [main INFO ] org.sparkproject.jetty.server.Server - Started @1069ms
2025-06-05 06:54:56.214 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@79d2d1fc{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 06:54:56.214 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 06:54:56.224 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7144655b{/,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.272 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 06:54:56.276 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 06:54:56.288 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35323.
2025-06-05 06:54:56.288 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:35323
2025-06-05 06:54:56.290 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 06:54:56.293 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 35323, None)
2025-06-05 06:54:56.296 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:35323 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 35323, None)
2025-06-05 06:54:56.298 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 35323, None)
2025-06-05 06:54:56.299 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 35323, None)
2025-06-05 06:54:56.376 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7144655b{/,null,STOPPED,@Spark}
2025-06-05 06:54:56.377 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241f05b{/jobs,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.377 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.378 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c9320c2{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.378 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cc9385{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.379 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7915bca3{/stages,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.379 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.380 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79fd6f95{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.380 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.381 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.381 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.381 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/storage,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.382 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.383 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.383 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.384 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/environment,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.384 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.384 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/executors,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.385 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.385 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397ef2{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.386 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e93c1f{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.390 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9b21bd3{/static,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.391 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@323f3c96{/,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.392 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b6d92e{/api,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.392 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25d93198{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.392 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f951a7f{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.395 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e7f0216{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.478 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 06:54:56.482 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 06:54:56.488 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4190bc8a{/SQL,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.489 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c83ae01{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.490 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@650ae78c{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.490 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79c5460e{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 06:54:56.496 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b495d4{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 06:54:57.034 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 102.91359 ms
2025-06-05 06:54:57.054 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 13.875626 ms
2025-06-05 06:54:58.140 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 06:54:58.167 [main INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 06:54:58.167 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 06:54:58.167 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 06:54:58.167 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081298167
2025-06-05 06:54:58.321 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 06:54:58.323 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 06:54:58.323 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 06:54:58.323 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 06:54:58.324 [main INFO ] o.a.spark.sql.kafka010.KafkaRelation - GetBatch generating RDD of offset range: KafkaOffsetRange(clickstream-events-0,-2,-1,None), KafkaOffsetRange(clickstream-events-1,-2,-1,None)
2025-06-05 06:54:58.490 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.138129 ms
2025-06-05 06:54:58.620 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 27.625053 ms
2025-06-05 06:54:58.629 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.314583 ms
2025-06-05 06:54:58.643 [main INFO ] org.apache.spark.SparkContext - Starting job: save at SparkClickstreamProcessor.java:232
2025-06-05 06:54:58.656 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (save at SparkClickstreamProcessor.java:232) with 1 output partitions
2025-06-05 06:54:58.656 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (save at SparkClickstreamProcessor.java:232)
2025-06-05 06:54:58.656 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-05 06:54:58.657 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-05 06:54:58.661 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[10] at save at SparkClickstreamProcessor.java:232), which has no missing parents
2025-06-05 06:54:58.761 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 63.0 KiB, free 9.2 GiB)
2025-06-05 06:54:58.779 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.3 KiB, free 9.2 GiB)
2025-06-05 06:54:58.781 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:35323 (size: 22.3 KiB, free: 9.2 GiB)
2025-06-05 06:54:58.783 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 06:54:58.793 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[10] at save at SparkClickstreamProcessor.java:232) (first 15 tasks are for partitions Vector(0))
2025-06-05 06:54:58.794 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
2025-06-05 06:54:58.824 [dispatcher-event-loop-6 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes) 
2025-06-05 06:54:58.831 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 06:54:58.919 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 06:54:58.942 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 06:54:58.972 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 06:54:58.972 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 06:54:58.972 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081298972
2025-06-05 06:54:58.974 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor-1, groupId=spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 06:54:58.977 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor-1, groupId=spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 06:54:58.986 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor-1, groupId=spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 06:54:58.994 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor-1, groupId=spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 06:54:58.994 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor-1, groupId=spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 06:54:58.996 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor-1, groupId=spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 06:54:59.025 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 16.43315 ms
2025-06-05 06:54:59.042 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.448706 ms
2025-06-05 06:54:59.047 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor-1, groupId=spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 06:54:59.067 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor-1, groupId=spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 06:54:59.569 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor-1, groupId=spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 06:54:59.570 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor-1, groupId=spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 06:54:59.570 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor-1, groupId=spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 06:54:59.641 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 19.627224 ms
2025-06-05 06:54:59.678 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 4157 bytes result sent to driver
2025-06-05 06:54:59.683 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 865 ms on phamviethoa (executor driver) (1/1)
2025-06-05 06:54:59.684 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 06:54:59.687 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (save at SparkClickstreamProcessor.java:232) finished in 1.017 s
2025-06-05 06:54:59.688 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 06:54:59.688 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-05 06:54:59.689 [main INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: save at SparkClickstreamProcessor.java:232, took 1.046018 s
2025-06-05 06:54:59.705 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.167131 ms
2025-06-05 06:54:59.719 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor-1, groupId=spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 06:54:59.719 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor-1, groupId=spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 06:54:59.722 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 06:54:59.722 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 06:54:59.722 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 06:54:59.722 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 06:54:59.724 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-relation-34ce763a-a6e9-472a-896d-69c05083e7ba-executor-1 unregistered
2025-06-05 06:54:59.725 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 06:54:59.725 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 06:54:59.729 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@79d2d1fc{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 06:54:59.731 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 06:54:59.739 [dispatcher-event-loop-10 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 06:54:59.745 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 06:54:59.745 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 06:54:59.748 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 06:54:59.749 [dispatcher-event-loop-14 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 06:54:59.752 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 06:54:59.752 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 06:54:59.752 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-6f93e2bd-038a-4016-adbf-976f4e6efa43
2025-06-05 06:55:17.539 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 06:55:17.621 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 06:55:17.666 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 06:55:17.667 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 06:55:17.667 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 06:55:17.667 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 06:55:17.678 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 06:55:17.684 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 06:55:17.685 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 06:55:17.713 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 06:55:17.713 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 06:55:17.713 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 06:55:17.713 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 06:55:17.714 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 06:55:17.836 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 33565.
2025-06-05 06:55:17.849 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 06:55:17.865 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 06:55:17.874 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 06:55:17.874 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 06:55:17.876 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 06:55:17.885 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-b72e6dea-6490-4621-b62c-933dd82f270f
2025-06-05 06:55:17.901 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 06:55:17.909 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 06:55:17.928 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1045ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 06:55:17.972 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 06:55:17.977 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 06:55:17.985 [main INFO ] org.sparkproject.jetty.server.Server - Started @1103ms
2025-06-05 06:55:18.001 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@f10c15a{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 06:55:18.001 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 06:55:18.013 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2257fadf{/,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.073 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 06:55:18.077 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 06:55:18.088 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39343.
2025-06-05 06:55:18.088 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:39343
2025-06-05 06:55:18.089 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 06:55:18.092 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 39343, None)
2025-06-05 06:55:18.095 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:39343 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 39343, None)
2025-06-05 06:55:18.097 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 39343, None)
2025-06-05 06:55:18.098 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 39343, None)
2025-06-05 06:55:18.174 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@2257fadf{/,null,STOPPED,@Spark}
2025-06-05 06:55:18.175 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@109a2025{/jobs,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.175 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@761956ac{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.176 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133661d{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.176 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3414a8c3{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.176 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cf518cf{/stages,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.177 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68d651f2{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.178 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@eca6a74{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.178 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48840594{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.179 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14823f76{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.179 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ed16657{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.180 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@113e13f9{/storage,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.181 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7979b8b7{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.182 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bc49bc5{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.182 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f66ffc8{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.183 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2def7a7a{/environment,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.184 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c080ef3{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.184 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ee6291f{/executors,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.185 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37e0292a{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.186 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35267fd4{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.186 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36a6bea6{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.192 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42373389{/static,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.193 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72f8ae0c{/,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.194 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6726cc69{/api,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.195 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@673919a7{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.195 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2436ea2f{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.197 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b790d86{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.279 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 06:55:18.284 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 06:55:18.291 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ec5ea63{/SQL,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.291 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47d023b7{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.292 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e6d4780{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.292 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e73d5eb{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.299 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3bb87d36{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 06:55:18.852 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 109.411703 ms
2025-06-05 06:55:18.872 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 14.609875 ms
2025-06-05 06:55:19.969 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 06:55:19.994 [main INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 06:55:19.995 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 06:55:19.995 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 06:55:19.995 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081319994
2025-06-05 06:55:20.147 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 06:55:20.149 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 06:55:20.149 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 06:55:20.149 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 06:55:20.150 [main INFO ] o.a.spark.sql.kafka010.KafkaRelation - GetBatch generating RDD of offset range: KafkaOffsetRange(clickstream-events-0,-2,-1,None), KafkaOffsetRange(clickstream-events-1,-2,-1,None)
2025-06-05 06:55:20.321 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.333122 ms
2025-06-05 06:55:20.455 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 32.505915 ms
2025-06-05 06:55:20.467 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.258986 ms
2025-06-05 06:55:20.486 [main INFO ] org.apache.spark.SparkContext - Starting job: save at SparkClickstreamProcessor.java:232
2025-06-05 06:55:20.500 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (save at SparkClickstreamProcessor.java:232) with 1 output partitions
2025-06-05 06:55:20.500 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (save at SparkClickstreamProcessor.java:232)
2025-06-05 06:55:20.500 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-05 06:55:20.501 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-05 06:55:20.506 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[10] at save at SparkClickstreamProcessor.java:232), which has no missing parents
2025-06-05 06:55:20.606 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 63.0 KiB, free 9.2 GiB)
2025-06-05 06:55:20.622 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.3 KiB, free 9.2 GiB)
2025-06-05 06:55:20.624 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:39343 (size: 22.3 KiB, free: 9.2 GiB)
2025-06-05 06:55:20.625 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 06:55:20.633 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[10] at save at SparkClickstreamProcessor.java:232) (first 15 tasks are for partitions Vector(0))
2025-06-05 06:55:20.634 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
2025-06-05 06:55:20.660 [dispatcher-event-loop-6 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes) 
2025-06-05 06:55:20.667 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 06:55:20.747 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 06:55:20.765 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 06:55:20.788 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 06:55:20.788 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 06:55:20.788 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081320788
2025-06-05 06:55:20.789 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor-1, groupId=spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 06:55:20.791 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor-1, groupId=spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 06:55:20.798 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor-1, groupId=spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 06:55:20.804 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor-1, groupId=spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 06:55:20.805 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor-1, groupId=spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 06:55:20.805 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor-1, groupId=spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 06:55:20.825 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.142183 ms
2025-06-05 06:55:20.841 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.522502 ms
2025-06-05 06:55:20.845 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor-1, groupId=spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 06:55:20.863 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor-1, groupId=spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 06:55:21.364 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor-1, groupId=spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 06:55:21.364 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor-1, groupId=spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 06:55:21.365 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor-1, groupId=spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 06:55:21.430 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 18.185669 ms
2025-06-05 06:55:21.468 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 4189 bytes result sent to driver
2025-06-05 06:55:21.474 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 818 ms on phamviethoa (executor driver) (1/1)
2025-06-05 06:55:21.475 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 06:55:21.478 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 0 (save at SparkClickstreamProcessor.java:232) finished in 0.961 s
2025-06-05 06:55:21.479 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 06:55:21.480 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
2025-06-05 06:55:21.481 [main INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: save at SparkClickstreamProcessor.java:232, took 0.993935 s
2025-06-05 06:55:21.504 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 14.814966 ms
2025-06-05 06:55:21.563 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor-1, groupId=spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 06:55:21.563 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor-1, groupId=spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 06:55:21.568 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 06:55:21.568 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 06:55:21.568 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 06:55:21.568 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 06:55:21.573 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-relation-53772076-6cee-452c-bd23-cb7667c855cc-executor-1 unregistered
2025-06-05 06:55:21.574 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 06:55:21.574 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 06:55:21.580 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@f10c15a{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 06:55:21.583 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 06:55:21.593 [dispatcher-event-loop-10 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 06:55:21.600 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 06:55:21.601 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 06:55:21.604 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 06:55:21.606 [dispatcher-event-loop-14 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 06:55:21.609 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 06:55:21.609 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 06:55:21.609 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-023617b0-e643-4572-a96a-b5d38268e3ca
2025-06-05 06:56:49.508 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 06:56:49.590 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 06:56:49.642 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 06:56:49.642 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 06:56:49.642 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 06:56:49.642 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 06:56:49.654 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 06:56:49.661 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 06:56:49.661 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 06:56:49.691 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 06:56:49.691 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 06:56:49.691 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 06:56:49.692 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 06:56:49.692 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 06:56:49.822 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 45217.
2025-06-05 06:56:49.836 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 06:56:49.853 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 06:56:49.862 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 06:56:49.862 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 06:56:49.864 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 06:56:49.873 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-3247ebf5-bb13-429e-9289-a7b37bd1f9cc
2025-06-05 06:56:49.890 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 06:56:49.898 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 06:56:49.919 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1027ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 06:56:49.967 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 06:56:49.972 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 06:56:49.980 [main INFO ] org.sparkproject.jetty.server.Server - Started @1088ms
2025-06-05 06:56:49.997 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@4ba2a391{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 06:56:49.997 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 06:56:50.008 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2257fadf{/,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.059 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 06:56:50.062 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 06:56:50.077 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40869.
2025-06-05 06:56:50.077 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:40869
2025-06-05 06:56:50.078 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 06:56:50.082 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 40869, None)
2025-06-05 06:56:50.085 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:40869 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 40869, None)
2025-06-05 06:56:50.087 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 40869, None)
2025-06-05 06:56:50.088 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 40869, None)
2025-06-05 06:56:50.168 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@2257fadf{/,null,STOPPED,@Spark}
2025-06-05 06:56:50.169 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@109a2025{/jobs,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.169 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@761956ac{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.170 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133661d{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.170 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3414a8c3{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.171 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cf518cf{/stages,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.171 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68d651f2{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.172 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@eca6a74{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.172 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48840594{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.173 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14823f76{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.174 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ed16657{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.174 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@113e13f9{/storage,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.175 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7979b8b7{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.175 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bc49bc5{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.176 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f66ffc8{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.176 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2def7a7a{/environment,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.177 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c080ef3{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.177 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ee6291f{/executors,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.177 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37e0292a{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.178 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35267fd4{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.178 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36a6bea6{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.183 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42373389{/static,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.183 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72f8ae0c{/,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.184 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6726cc69{/api,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.185 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@673919a7{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.185 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2436ea2f{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.187 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b790d86{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.270 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 06:56:50.273 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 06:56:50.280 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ec5ea63{/SQL,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.281 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47d023b7{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.281 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e6d4780{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.282 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e73d5eb{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.288 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3bb87d36{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 06:56:50.850 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 110.80599 ms
2025-06-05 06:56:50.875 [main INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 19.488166 ms
2025-06-05 06:56:51.927 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 06:56:51.928 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 06:56:51.933 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@4ba2a391{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 06:56:51.937 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 06:56:51.946 [dispatcher-event-loop-7 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 06:56:51.953 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 06:56:51.953 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 06:56:51.958 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 06:56:51.959 [dispatcher-event-loop-11 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 06:56:51.963 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 06:56:51.963 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 06:56:51.963 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-e8ff304d-7ffc-407d-8973-375286b50954
2025-06-05 06:57:34.661 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 06:57:34.746 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 06:57:34.793 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 06:57:34.794 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 06:57:34.794 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 06:57:34.794 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 06:57:34.805 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 06:57:34.811 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 06:57:34.811 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 06:57:34.838 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 06:57:34.838 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 06:57:34.839 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 06:57:34.839 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 06:57:34.839 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 06:57:34.967 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 41731.
2025-06-05 06:57:34.981 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 06:57:34.999 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 06:57:35.010 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 06:57:35.010 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 06:57:35.012 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 06:57:35.025 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-b0f6e749-eb5c-4e77-835f-9d239650c358
2025-06-05 06:57:35.042 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 06:57:35.051 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 06:57:35.075 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1065ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 06:57:35.126 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 06:57:35.132 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 06:57:35.141 [main INFO ] org.sparkproject.jetty.server.Server - Started @1132ms
2025-06-05 06:57:35.157 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@29e12a9{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 06:57:35.158 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 06:57:35.176 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14379273{/,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.245 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 06:57:35.252 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 06:57:35.264 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44893.
2025-06-05 06:57:35.264 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:44893
2025-06-05 06:57:35.266 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 06:57:35.270 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 44893, None)
2025-06-05 06:57:35.273 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:44893 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 44893, None)
2025-06-05 06:57:35.275 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 44893, None)
2025-06-05 06:57:35.276 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 44893, None)
2025-06-05 06:57:35.361 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@14379273{/,null,STOPPED,@Spark}
2025-06-05 06:57:35.362 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70c53dbe{/jobs,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.362 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21c815e4{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.363 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@743e66f7{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.363 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241f05b{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.364 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/stages,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.364 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.365 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7915bca3{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.365 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.366 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67b4ec{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.366 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f91da5e{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.367 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79fd6f95{/storage,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.367 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.368 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.369 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.369 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/environment,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.369 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.370 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/executors,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.370 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.371 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.371 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.376 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/static,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.376 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51288417{/,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.377 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e0895f5{/api,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.378 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5292ceca{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.379 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e9ef5b6{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.382 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64984b0f{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.477 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 06:57:35.482 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 06:57:35.489 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f1a4795{/SQL,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.489 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c5ddccd{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.490 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@201c3cda{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.490 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d97caa4{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 06:57:35.496 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c83ae01{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 06:57:36.780 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-05 06:57:36.789 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d5ae6aa{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-05 06:57:36.790 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2210e466{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-05 06:57:36.791 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d0dad12{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-05 06:57:36.791 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58a8ea6f{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-05 06:57:36.792 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c3007d{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 06:57:36.795 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-5ba48bd0-b22b-42ce-a6f5-08708b302777. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-05 06:57:36.808 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-5ba48bd0-b22b-42ce-a6f5-08708b302777 resolved to file:/tmp/temporary-5ba48bd0-b22b-42ce-a6f5-08708b302777.
2025-06-05 06:57:36.808 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-05 06:57:36.863 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-5ba48bd0-b22b-42ce-a6f5-08708b302777/metadata using temp file file:/tmp/temporary-5ba48bd0-b22b-42ce-a6f5-08708b302777/.metadata.bdbae645-6bc0-42a1-a7c6-834ee3b49142.tmp
2025-06-05 06:57:36.918 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-5ba48bd0-b22b-42ce-a6f5-08708b302777/.metadata.bdbae645-6bc0-42a1-a7c6-834ee3b49142.tmp to file:/tmp/temporary-5ba48bd0-b22b-42ce-a6f5-08708b302777/metadata
2025-06-05 06:57:36.934 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359]. Use file:/tmp/temporary-5ba48bd0-b22b-42ce-a6f5-08708b302777 to store the query checkpoint.
2025-06-05 06:57:36.939 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@6678e764] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@797d82e9]
2025-06-05 06:57:36.952 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 06:57:36.953 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 06:57:36.953 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-05 06:57:36.954 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-05 06:57:37.125 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 06:57:37.168 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 06:57:37.169 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 06:57:37.169 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 06:57:37.170 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081457168
2025-06-05 06:57:37.360 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-5ba48bd0-b22b-42ce-a6f5-08708b302777/sources/0/0 using temp file file:/tmp/temporary-5ba48bd0-b22b-42ce-a6f5-08708b302777/sources/0/.0.c2d77628-4b57-4f79-896b-123e7c1503ea.tmp
2025-06-05 06:57:37.374 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-5ba48bd0-b22b-42ce-a6f5-08708b302777/sources/0/.0.c2d77628-4b57-4f79-896b-123e7c1503ea.tmp to file:/tmp/temporary-5ba48bd0-b22b-42ce-a6f5-08708b302777/sources/0/0
2025-06-05 06:57:37.375 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-05 06:57:37.391 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-5ba48bd0-b22b-42ce-a6f5-08708b302777/offsets/0 using temp file file:/tmp/temporary-5ba48bd0-b22b-42ce-a6f5-08708b302777/offsets/.0.753c0488-97ae-4947-b633-5a6973ad72f2.tmp
2025-06-05 06:57:37.409 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-5ba48bd0-b22b-42ce-a6f5-08708b302777/offsets/.0.753c0488-97ae-4947-b633-5a6973ad72f2.tmp to file:/tmp/temporary-5ba48bd0-b22b-42ce-a6f5-08708b302777/offsets/0
2025-06-05 06:57:37.410 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749081457384,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-05 06:57:37.579 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749081457384
2025-06-05 06:57:37.633 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 06:57:37.661 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 06:57:37.690 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749081457384
2025-06-05 06:57:37.692 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 06:57:37.694 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 06:57:37.916 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 135.442084 ms
2025-06-05 06:57:38.061 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.527481 ms
2025-06-05 06:57:38.082 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 14.11957 ms
2025-06-05 06:57:38.139 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:248
2025-06-05 06:57:38.149 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:248) as input to shuffle 0
2025-06-05 06:57:38.152 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:248) with 1 output partitions
2025-06-05 06:57:38.152 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:248)
2025-06-05 06:57:38.152 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-05 06:57:38.153 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
2025-06-05 06:57:38.155 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:248), which has no missing parents
2025-06-05 06:57:38.222 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 46.1 KiB, free 9.2 GiB)
2025-06-05 06:57:38.241 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 9.2 GiB)
2025-06-05 06:57:38.243 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:44893 (size: 17.9 KiB, free: 9.2 GiB)
2025-06-05 06:57:38.246 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 06:57:38.255 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:248) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 06:57:38.256 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-05 06:57:38.290 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8301 bytes) 
2025-06-05 06:57:38.292 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8301 bytes) 
2025-06-05 06:57:38.299 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-05 06:57:38.299 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 06:57:38.402 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 21.392702 ms
2025-06-05 06:57:38.438 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.448368 ms
2025-06-05 06:57:38.451 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.927087 ms
2025-06-05 06:57:38.460 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 06:57:38.460 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 06:57:38.484 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 06:57:38.484 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 06:57:38.511 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 06:57:38.511 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 06:57:38.511 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081458511
2025-06-05 06:57:38.511 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 06:57:38.512 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 06:57:38.512 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081458511
2025-06-05 06:57:38.512 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor-1, groupId=spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 06:57:38.512 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor-2, groupId=spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor] Assigned to partition(s): clickstream-events-1
2025-06-05 06:57:38.517 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor-2, groupId=spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 06:57:38.517 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor-1, groupId=spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 06:57:38.522 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor-2, groupId=spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 06:57:38.522 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor-1, groupId=spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 06:57:38.546 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor-2, groupId=spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 06:57:38.546 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor-1, groupId=spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 06:57:39.047 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor-1, groupId=spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 06:57:39.047 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor-2, groupId=spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 06:57:39.048 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor-2, groupId=spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 06:57:39.048 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor-1, groupId=spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 06:57:39.048 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor-2, groupId=spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 06:57:39.048 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor-1, groupId=spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 06:57:39.131 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 2342 bytes result sent to driver
2025-06-05 06:57:39.131 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2342 bytes result sent to driver
2025-06-05 06:57:39.136 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 844 ms on phamviethoa (executor driver) (1/2)
2025-06-05 06:57:39.137 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 854 ms on phamviethoa (executor driver) (2/2)
2025-06-05 06:57:39.138 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 06:57:39.143 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (start at SparkClickstreamProcessor.java:248) finished in 0.982 s
2025-06-05 06:57:39.144 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
2025-06-05 06:57:39.144 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - running: Set()
2025-06-05 06:57:39.144 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
2025-06-05 06:57:39.144 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - failed: Set()
2025-06-05 06:57:39.145 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:248), which has no missing parents
2025-06-05 06:57:39.150 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-05 06:57:39.152 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-05 06:57:39.152 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:44893 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-05 06:57:39.152 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-05 06:57:39.154 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:248) (first 15 tasks are for partitions Vector(0))
2025-06-05 06:57:39.154 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-05 06:57:39.158 [dispatcher-event-loop-12 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
2025-06-05 06:57:39.158 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-05 06:57:39.185 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-05 06:57:39.186 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
2025-06-05 06:57:39.194 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 4038 bytes result sent to driver
2025-06-05 06:57:39.195 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 39 ms on phamviethoa (executor driver) (1/1)
2025-06-05 06:57:39.196 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-05 06:57:39.196 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:248) finished in 0.047 s
2025-06-05 06:57:39.198 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 06:57:39.198 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-05 06:57:39.199 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:248, took 1.059463 s
2025-06-05 06:57:39.349 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] terminated with error
org.apache.spark.SparkIllegalArgumentException: Can't get JDBC type for struct<event_id:string,event_name:string,event_time:string,user_id:string,element_text:string,track:string,app_id:string,platform:string,page_url:string,page_referrer:string,screen_resolution:string,viewport_size:string,element_type:string,element_class:string,language:string,user_agent:string,page_path:string,productId:string,sessionId:string>.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotGetJdbcTypeError(QueryExecutionErrors.scala:1005)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getJdbcType$2(JdbcUtils.scala:168)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getJdbcType(JdbcUtils.scala:168)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$schemaString$4(JdbcUtils.scala:809)
	at scala.collection.immutable.Map$EmptyMap$.getOrElse(Map.scala:110)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$schemaString$3(JdbcUtils.scala:809)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.schemaString(JdbcUtils.scala:806)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:906)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.SparkClickstreamProcessor.lambda$main$a450ce84$1(SparkClickstreamProcessor.java:244)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
2025-06-05 06:57:39.349 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 06:57:39.351 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 06:57:39.351 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 06:57:39.351 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 06:57:39.351 [stream execution thread for [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = ebcc5dd1-4619-4ed0-a02c-b2084351a0a6, runId = 8f1fe7ec-d685-419f-a739-7cd6b5934359] has been shutdown
2025-06-05 06:57:39.363 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor-1, groupId=spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 06:57:39.363 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor-1, groupId=spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 06:57:39.367 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 06:57:39.367 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 06:57:39.367 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 06:57:39.367 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 06:57:39.371 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor-1 unregistered
2025-06-05 06:57:39.371 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor-2, groupId=spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 06:57:39.371 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor-2, groupId=spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 06:57:39.373 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 06:57:39.373 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 06:57:39.373 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 06:57:39.373 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 06:57:39.376 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-04b3eda1-b269-4b61-ae0d-6d93f54d8ba4-189879758-executor-2 unregistered
2025-06-05 06:57:39.377 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 06:57:39.377 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 06:57:39.382 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@29e12a9{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 06:57:39.384 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 06:57:39.392 [dispatcher-event-loop-1 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 06:57:39.398 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 06:57:39.399 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 06:57:39.402 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 06:57:39.403 [dispatcher-event-loop-5 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 06:57:39.407 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 06:57:39.407 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 06:57:39.408 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-5ba48bd0-b22b-42ce-a6f5-08708b302777
2025-06-05 06:57:39.409 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-3faa0b04-b9f3-43fc-ae17-c6e063760a44
2025-06-05 06:59:59.658 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 06:59:59.740 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 06:59:59.792 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 06:59:59.792 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 06:59:59.792 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 06:59:59.792 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 06:59:59.803 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 06:59:59.809 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 06:59:59.809 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 06:59:59.837 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 06:59:59.837 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 06:59:59.837 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 06:59:59.837 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 06:59:59.837 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 06:59:59.960 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 41153.
2025-06-05 06:59:59.976 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 06:59:59.993 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 07:00:00.001 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 07:00:00.002 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 07:00:00.004 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 07:00:00.013 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-2457cdd0-7889-4305-aea1-900b3603ce5b
2025-06-05 07:00:00.029 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 07:00:00.037 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 07:00:00.056 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1070ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 07:00:00.098 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 07:00:00.104 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 07:00:00.111 [main INFO ] org.sparkproject.jetty.server.Server - Started @1126ms
2025-06-05 07:00:00.127 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@1cbac309{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:00:00.127 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 07:00:00.139 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35835e65{/,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.191 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 07:00:00.195 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 07:00:00.207 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39355.
2025-06-05 07:00:00.207 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:39355
2025-06-05 07:00:00.208 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 07:00:00.212 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 39355, None)
2025-06-05 07:00:00.214 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:39355 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 39355, None)
2025-06-05 07:00:00.216 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 39355, None)
2025-06-05 07:00:00.216 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 39355, None)
2025-06-05 07:00:00.293 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@35835e65{/,null,STOPPED,@Spark}
2025-06-05 07:00:00.294 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/jobs,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.294 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.295 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cc9385{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.295 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7915bca3{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.296 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/stages,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.296 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67b4ec{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.297 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.297 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.298 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.298 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.298 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/storage,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.299 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.299 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.300 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.300 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/environment,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.301 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.301 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/executors,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.301 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397ef2{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.302 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e93c1f{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.302 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9b21bd3{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.306 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7661b5a{/static,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.307 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b6d92e{/,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.308 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7899de11{/api,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.308 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f951a7f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.309 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c777e7b{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.311 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62db3891{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.391 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 07:00:00.395 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 07:00:00.403 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6732726{/SQL,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.403 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d64c581{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.404 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d64c100{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.404 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fdf17dc{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:00.410 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ff8a9dc{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:00:01.712 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-05 07:00:01.721 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e6b379c{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-05 07:00:01.721 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ff81b0d{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:01.722 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66b40dd3{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-05 07:00:01.722 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a5066f5{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:01.723 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b795db7{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:00:01.727 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-dfa828b8-0357-4f1d-aea2-032e104e25c9. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-05 07:00:01.739 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-dfa828b8-0357-4f1d-aea2-032e104e25c9 resolved to file:/tmp/temporary-dfa828b8-0357-4f1d-aea2-032e104e25c9.
2025-06-05 07:00:01.740 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-05 07:00:01.795 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-dfa828b8-0357-4f1d-aea2-032e104e25c9/metadata using temp file file:/tmp/temporary-dfa828b8-0357-4f1d-aea2-032e104e25c9/.metadata.848a8ebd-93e1-481c-a82d-6fdd15680358.tmp
2025-06-05 07:00:01.848 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-dfa828b8-0357-4f1d-aea2-032e104e25c9/.metadata.848a8ebd-93e1-481c-a82d-6fdd15680358.tmp to file:/tmp/temporary-dfa828b8-0357-4f1d-aea2-032e104e25c9/metadata
2025-06-05 07:00:01.862 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8]. Use file:/tmp/temporary-dfa828b8-0357-4f1d-aea2-032e104e25c9 to store the query checkpoint.
2025-06-05 07:00:01.867 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@30f55f07] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@1880fbea]
2025-06-05 07:00:01.880 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:00:01.881 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:00:01.881 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-05 07:00:01.882 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-05 07:00:02.036 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 07:00:02.063 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 07:00:02.064 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:00:02.064 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:00:02.064 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081602063
2025-06-05 07:00:02.235 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-dfa828b8-0357-4f1d-aea2-032e104e25c9/sources/0/0 using temp file file:/tmp/temporary-dfa828b8-0357-4f1d-aea2-032e104e25c9/sources/0/.0.17343691-239a-410f-ae5a-52ece7292c05.tmp
2025-06-05 07:00:02.247 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-dfa828b8-0357-4f1d-aea2-032e104e25c9/sources/0/.0.17343691-239a-410f-ae5a-52ece7292c05.tmp to file:/tmp/temporary-dfa828b8-0357-4f1d-aea2-032e104e25c9/sources/0/0
2025-06-05 07:00:02.248 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-05 07:00:02.258 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-dfa828b8-0357-4f1d-aea2-032e104e25c9/offsets/0 using temp file file:/tmp/temporary-dfa828b8-0357-4f1d-aea2-032e104e25c9/offsets/.0.d72219e7-2689-4ff3-ae8b-ac0c4206c122.tmp
2025-06-05 07:00:02.277 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-dfa828b8-0357-4f1d-aea2-032e104e25c9/offsets/.0.d72219e7-2689-4ff3-ae8b-ac0c4206c122.tmp to file:/tmp/temporary-dfa828b8-0357-4f1d-aea2-032e104e25c9/offsets/0
2025-06-05 07:00:02.277 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749081602254,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-05 07:00:02.429 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749081602254
2025-06-05 07:00:02.477 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:00:02.502 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:00:02.532 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749081602254
2025-06-05 07:00:02.534 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:00:02.535 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:00:02.754 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 133.165489 ms
2025-06-05 07:00:02.874 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.639189 ms
2025-06-05 07:00:02.893 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.702211 ms
2025-06-05 07:00:02.944 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:251
2025-06-05 07:00:02.952 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:251) as input to shuffle 0
2025-06-05 07:00:02.955 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:251) with 1 output partitions
2025-06-05 07:00:02.955 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:251)
2025-06-05 07:00:02.955 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-05 07:00:02.957 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
2025-06-05 07:00:02.960 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:251), which has no missing parents
2025-06-05 07:00:03.027 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 46.1 KiB, free 9.2 GiB)
2025-06-05 07:00:03.043 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 9.2 GiB)
2025-06-05 07:00:03.045 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:39355 (size: 17.9 KiB, free: 9.2 GiB)
2025-06-05 07:00:03.047 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:00:03.056 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:251) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:00:03.057 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-05 07:00:03.087 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8303 bytes) 
2025-06-05 07:00:03.089 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8303 bytes) 
2025-06-05 07:00:03.097 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 07:00:03.097 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-05 07:00:03.196 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 17.700681 ms
2025-06-05 07:00:03.227 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.186926 ms
2025-06-05 07:00:03.239 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.908927 ms
2025-06-05 07:00:03.249 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:00:03.249 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:00:03.269 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:00:03.269 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:00:03.296 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:00:03.296 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:00:03.296 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081603296
2025-06-05 07:00:03.296 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:00:03.296 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:00:03.296 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081603296
2025-06-05 07:00:03.297 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor-1, groupId=spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor] Assigned to partition(s): clickstream-events-1
2025-06-05 07:00:03.297 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor-2, groupId=spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 07:00:03.301 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor-1, groupId=spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:00:03.301 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor-2, groupId=spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:00:03.306 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor-1, groupId=spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:00:03.306 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor-2, groupId=spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:00:03.328 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor-2, groupId=spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:00:03.328 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor-1, groupId=spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:00:03.830 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor-2, groupId=spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:00:03.830 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor-1, groupId=spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:00:03.830 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor-2, groupId=spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:00:03.830 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor-1, groupId=spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:00:03.830 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor-2, groupId=spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:00:03.831 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor-1, groupId=spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:00:03.923 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2342 bytes result sent to driver
2025-06-05 07:00:03.923 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 2342 bytes result sent to driver
2025-06-05 07:00:03.929 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 840 ms on phamviethoa (executor driver) (1/2)
2025-06-05 07:00:03.930 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 849 ms on phamviethoa (executor driver) (2/2)
2025-06-05 07:00:03.931 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 07:00:03.935 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (start at SparkClickstreamProcessor.java:251) finished in 0.968 s
2025-06-05 07:00:03.935 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
2025-06-05 07:00:03.936 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - running: Set()
2025-06-05 07:00:03.936 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
2025-06-05 07:00:03.936 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - failed: Set()
2025-06-05 07:00:03.937 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:251), which has no missing parents
2025-06-05 07:00:03.941 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-05 07:00:03.943 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-05 07:00:03.944 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:39355 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-05 07:00:03.944 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:00:03.945 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:251) (first 15 tasks are for partitions Vector(0))
2025-06-05 07:00:03.945 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-05 07:00:03.948 [dispatcher-event-loop-12 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
2025-06-05 07:00:03.949 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-05 07:00:03.979 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-05 07:00:03.980 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
2025-06-05 07:00:03.988 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 4038 bytes result sent to driver
2025-06-05 07:00:03.990 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 43 ms on phamviethoa (executor driver) (1/1)
2025-06-05 07:00:03.990 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-05 07:00:03.990 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:251) finished in 0.050 s
2025-06-05 07:00:03.992 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 07:00:03.992 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-05 07:00:03.993 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:251, took 1.048766 s
2025-06-05 07:00:04.022 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] terminated with error
org.apache.spark.sql.AnalysisException: [FIELD_NOT_FOUND] No such struct field `session_id` in `event_id`, `event_name`, `event_time`, `user_id`, `element_text`, `track`, `app_id`, `platform`, `page_url`, `page_referrer`, `screen_resolution`, `viewport_size`, `element_type`, `element_class`, `language`, `user_agent`, `page_path`, `productId`, `sessionId`.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.noSuchStructFieldInGivenFieldsError(QueryCompilationErrors.scala:2115)
	at org.apache.spark.sql.catalyst.expressions.ExtractValue$.findField(complexTypeExtractors.scala:83)
	at org.apache.spark.sql.catalyst.expressions.ExtractValue$.apply(complexTypeExtractors.scala:56)
	at org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.$anonfun$resolve$3(package.scala:354)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.ArrayBuffer.foldLeft(ArrayBuffer.scala:49)
	at org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:353)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:115)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpressionByPlanChildren$2(ColumnResolutionHelper.scala:376)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$3(ColumnResolutionHelper.scala:156)
	at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:100)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:163)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:134)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$9(ColumnResolutionHelper.scala:186)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:532)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:186)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:134)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpression(ColumnResolutionHelper.scala:193)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpressionByPlanChildren(ColumnResolutionHelper.scala:383)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpressionByPlanChildren$(ColumnResolutionHelper.scala:356)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.resolveExpressionByPlanChildren(Analyzer.scala:1508)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$16.$anonfun$applyOrElse$111(Analyzer.scala:1642)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$16.applyOrElse(Analyzer.scala:1642)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$16.applyOrElse(Analyzer.scala:1533)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:111)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:110)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:1533)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:1508)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:228)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:224)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:224)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)
	at org.apache.spark.sql.Dataset.withPlan(Dataset.scala:4196)
	at org.apache.spark.sql.Dataset.select(Dataset.scala:1578)
	at org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2776)
	at org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2715)
	at com.example.clickstream.processor.SparkClickstreamProcessor.lambda$main$a450ce84$1(SparkClickstreamProcessor.java:236)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
2025-06-05 07:00:04.023 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 07:00:04.024 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:00:04.024 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:00:04.024 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:00:04.025 [stream execution thread for [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = a99353d3-58ff-4c6e-8974-954d96427b76, runId = dbf1621e-1731-4757-86b0-dc2cb5cc07b8] has been shutdown
2025-06-05 07:00:04.035 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor-1, groupId=spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:00:04.035 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor-1, groupId=spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:00:04.038 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:00:04.038 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:00:04.038 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:00:04.038 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:00:04.040 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor-1 unregistered
2025-06-05 07:00:04.041 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor-2, groupId=spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:00:04.041 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor-2, groupId=spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:00:04.043 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:00:04.043 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:00:04.043 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:00:04.043 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:00:04.045 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-7c26b3ca-4924-47e1-bf15-cdfda5b2f143--1945202930-executor-2 unregistered
2025-06-05 07:00:04.045 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 07:00:04.045 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 07:00:04.050 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@1cbac309{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:00:04.052 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 07:00:04.062 [dispatcher-event-loop-1 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 07:00:04.069 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 07:00:04.069 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 07:00:04.072 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 07:00:04.073 [dispatcher-event-loop-5 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 07:00:04.077 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 07:00:04.077 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 07:00:04.077 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-c7266614-318b-4542-99fd-a337c76a4b13
2025-06-05 07:00:04.079 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-dfa828b8-0357-4f1d-aea2-032e104e25c9
2025-06-05 07:00:26.486 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 07:00:26.565 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 07:00:26.609 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:00:26.610 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 07:00:26.610 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:00:26.610 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 07:00:26.621 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 07:00:26.626 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 07:00:26.627 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 07:00:26.654 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 07:00:26.654 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 07:00:26.655 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 07:00:26.655 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 07:00:26.655 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 07:00:26.774 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 37805.
2025-06-05 07:00:26.788 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 07:00:26.805 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 07:00:26.814 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 07:00:26.815 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 07:00:26.817 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 07:00:26.827 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-e6b131ee-f1dd-453f-afda-b593ad2cc27b
2025-06-05 07:00:26.845 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 07:00:26.853 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 07:00:26.872 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1054ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 07:00:26.914 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 07:00:26.920 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 07:00:26.928 [main INFO ] org.sparkproject.jetty.server.Server - Started @1110ms
2025-06-05 07:00:26.943 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@748e9b20{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:00:26.943 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 07:00:26.953 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35835e65{/,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.000 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 07:00:27.004 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 07:00:27.015 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36431.
2025-06-05 07:00:27.015 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:36431
2025-06-05 07:00:27.017 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 07:00:27.020 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 36431, None)
2025-06-05 07:00:27.022 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:36431 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 36431, None)
2025-06-05 07:00:27.024 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 36431, None)
2025-06-05 07:00:27.025 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 36431, None)
2025-06-05 07:00:27.101 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@35835e65{/,null,STOPPED,@Spark}
2025-06-05 07:00:27.101 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/jobs,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.102 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.102 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cc9385{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.103 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7915bca3{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.103 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/stages,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.103 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67b4ec{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.104 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.105 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.105 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.106 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.106 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/storage,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.107 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.107 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.108 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.108 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/environment,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.109 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.109 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/executors,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.109 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397ef2{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.110 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e93c1f{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.110 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9b21bd3{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.115 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7661b5a{/static,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.115 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b6d92e{/,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.116 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7899de11{/api,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.117 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f951a7f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.117 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c777e7b{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.119 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62db3891{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.199 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 07:00:27.203 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 07:00:27.210 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6732726{/SQL,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.211 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d64c581{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.211 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d64c100{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.212 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fdf17dc{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:27.218 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ff8a9dc{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:00:28.614 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-05 07:00:28.623 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e6b379c{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-05 07:00:28.623 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ff81b0d{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:28.624 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66b40dd3{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-05 07:00:28.624 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a5066f5{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-05 07:00:28.625 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b795db7{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:00:28.629 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1ab2b4cd-8ac6-4172-9910-56a70ac48ad9. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-05 07:00:28.640 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-1ab2b4cd-8ac6-4172-9910-56a70ac48ad9 resolved to file:/tmp/temporary-1ab2b4cd-8ac6-4172-9910-56a70ac48ad9.
2025-06-05 07:00:28.641 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-05 07:00:28.685 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-1ab2b4cd-8ac6-4172-9910-56a70ac48ad9/metadata using temp file file:/tmp/temporary-1ab2b4cd-8ac6-4172-9910-56a70ac48ad9/.metadata.9bfcdf8c-ec87-489e-8ac7-584d237f5754.tmp
2025-06-05 07:00:28.741 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-1ab2b4cd-8ac6-4172-9910-56a70ac48ad9/.metadata.9bfcdf8c-ec87-489e-8ac7-584d237f5754.tmp to file:/tmp/temporary-1ab2b4cd-8ac6-4172-9910-56a70ac48ad9/metadata
2025-06-05 07:00:28.758 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5]. Use file:/tmp/temporary-1ab2b4cd-8ac6-4172-9910-56a70ac48ad9 to store the query checkpoint.
2025-06-05 07:00:28.764 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@3345a19d] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@5d98864]
2025-06-05 07:00:28.779 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:00:28.780 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:00:28.780 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-05 07:00:28.781 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-05 07:00:28.942 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 07:00:28.974 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 07:00:28.975 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:00:28.975 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:00:28.975 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081628974
2025-06-05 07:00:29.151 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-1ab2b4cd-8ac6-4172-9910-56a70ac48ad9/sources/0/0 using temp file file:/tmp/temporary-1ab2b4cd-8ac6-4172-9910-56a70ac48ad9/sources/0/.0.b34f2fd9-1da9-43e0-b32a-c4006976250e.tmp
2025-06-05 07:00:29.164 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-1ab2b4cd-8ac6-4172-9910-56a70ac48ad9/sources/0/.0.b34f2fd9-1da9-43e0-b32a-c4006976250e.tmp to file:/tmp/temporary-1ab2b4cd-8ac6-4172-9910-56a70ac48ad9/sources/0/0
2025-06-05 07:00:29.165 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-05 07:00:29.187 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-1ab2b4cd-8ac6-4172-9910-56a70ac48ad9/offsets/0 using temp file file:/tmp/temporary-1ab2b4cd-8ac6-4172-9910-56a70ac48ad9/offsets/.0.bc5cb2e5-c445-48f4-9b7d-987b62b44701.tmp
2025-06-05 07:00:29.205 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-1ab2b4cd-8ac6-4172-9910-56a70ac48ad9/offsets/.0.bc5cb2e5-c445-48f4-9b7d-987b62b44701.tmp to file:/tmp/temporary-1ab2b4cd-8ac6-4172-9910-56a70ac48ad9/offsets/0
2025-06-05 07:00:29.206 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749081629179,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-05 07:00:29.362 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749081629179
2025-06-05 07:00:29.411 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:00:29.436 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:00:29.468 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749081629179
2025-06-05 07:00:29.470 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:00:29.471 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:00:29.707 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 145.336284 ms
2025-06-05 07:00:29.825 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.581064 ms
2025-06-05 07:00:29.843 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.445201 ms
2025-06-05 07:00:29.895 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:251
2025-06-05 07:00:29.903 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:251) as input to shuffle 0
2025-06-05 07:00:29.906 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:251) with 1 output partitions
2025-06-05 07:00:29.906 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:251)
2025-06-05 07:00:29.906 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-05 07:00:29.907 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
2025-06-05 07:00:29.910 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:251), which has no missing parents
2025-06-05 07:00:29.985 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 46.1 KiB, free 9.2 GiB)
2025-06-05 07:00:30.004 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 9.2 GiB)
2025-06-05 07:00:30.006 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:36431 (size: 17.9 KiB, free: 9.2 GiB)
2025-06-05 07:00:30.008 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:00:30.018 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:251) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:00:30.018 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-05 07:00:30.047 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8302 bytes) 
2025-06-05 07:00:30.049 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8302 bytes) 
2025-06-05 07:00:30.056 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-05 07:00:30.056 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 07:00:30.164 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 17.716364 ms
2025-06-05 07:00:30.193 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.004069 ms
2025-06-05 07:00:30.207 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.942974 ms
2025-06-05 07:00:30.217 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:00:30.217 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:00:30.236 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:00:30.236 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:00:30.262 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:00:30.262 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:00:30.262 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081630262
2025-06-05 07:00:30.263 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:00:30.263 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:00:30.263 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081630262
2025-06-05 07:00:30.263 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor-2, groupId=spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor] Assigned to partition(s): clickstream-events-1
2025-06-05 07:00:30.263 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor-1, groupId=spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 07:00:30.267 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor-2, groupId=spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:00:30.267 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor-1, groupId=spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:00:30.273 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor-2, groupId=spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:00:30.273 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor-1, groupId=spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:00:30.294 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor-1, groupId=spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:00:30.294 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor-2, groupId=spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:00:30.797 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor-1, groupId=spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:00:30.797 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor-2, groupId=spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:00:30.797 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor-1, groupId=spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:00:30.797 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor-2, groupId=spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:00:30.798 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor-2, groupId=spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:00:30.798 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor-1, groupId=spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:00:30.888 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2342 bytes result sent to driver
2025-06-05 07:00:30.888 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 2342 bytes result sent to driver
2025-06-05 07:00:30.894 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 852 ms on phamviethoa (executor driver) (1/2)
2025-06-05 07:00:30.894 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 845 ms on phamviethoa (executor driver) (2/2)
2025-06-05 07:00:30.895 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 07:00:30.899 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (start at SparkClickstreamProcessor.java:251) finished in 0.979 s
2025-06-05 07:00:30.899 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
2025-06-05 07:00:30.899 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - running: Set()
2025-06-05 07:00:30.899 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
2025-06-05 07:00:30.899 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - failed: Set()
2025-06-05 07:00:30.900 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:251), which has no missing parents
2025-06-05 07:00:30.905 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-05 07:00:30.907 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-05 07:00:30.907 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:36431 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-05 07:00:30.908 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:00:30.909 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:251) (first 15 tasks are for partitions Vector(0))
2025-06-05 07:00:30.909 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-05 07:00:30.912 [dispatcher-event-loop-12 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
2025-06-05 07:00:30.912 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-05 07:00:30.942 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-05 07:00:30.943 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
2025-06-05 07:00:30.952 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 4038 bytes result sent to driver
2025-06-05 07:00:30.954 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 44 ms on phamviethoa (executor driver) (1/1)
2025-06-05 07:00:30.954 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-05 07:00:30.954 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:251) finished in 0.050 s
2025-06-05 07:00:30.956 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 07:00:30.956 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-05 07:00:30.957 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:251, took 1.062265 s
2025-06-05 07:00:31.098 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] terminated with error
org.apache.spark.SparkIllegalArgumentException: Can't get JDBC type for struct<event_id:string,event_name:string,event_time:string,user_id:string,element_text:string,track:string,app_id:string,platform:string,page_url:string,page_referrer:string,screen_resolution:string,viewport_size:string,element_type:string,element_class:string,language:string,user_agent:string,page_path:string,productId:string,sessionId:string>.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotGetJdbcTypeError(QueryExecutionErrors.scala:1005)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getJdbcType$2(JdbcUtils.scala:168)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getJdbcType(JdbcUtils.scala:168)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$schemaString$4(JdbcUtils.scala:809)
	at scala.collection.immutable.Map$EmptyMap$.getOrElse(Map.scala:110)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$schemaString$3(JdbcUtils.scala:809)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.schemaString(JdbcUtils.scala:806)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:906)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.SparkClickstreamProcessor.lambda$main$a450ce84$1(SparkClickstreamProcessor.java:247)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
2025-06-05 07:00:31.099 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 07:00:31.100 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:00:31.101 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:00:31.101 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:00:31.101 [stream execution thread for [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = 37aba83c-f7a9-4dd6-963d-3b95522c47ca, runId = b94f67ec-a067-4453-bdfa-9016d75dbcb5] has been shutdown
2025-06-05 07:00:31.114 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor-2, groupId=spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:00:31.114 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor-2, groupId=spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:00:31.118 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:00:31.119 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:00:31.119 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:00:31.119 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:00:31.122 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor-2 unregistered
2025-06-05 07:00:31.122 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor-1, groupId=spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:00:31.122 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor-1, groupId=spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:00:31.124 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:00:31.124 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:00:31.124 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:00:31.124 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:00:31.126 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-95057809-04ba-41a9-97ec-7ac9813d5c84--310784746-executor-1 unregistered
2025-06-05 07:00:31.126 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 07:00:31.127 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 07:00:31.131 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@748e9b20{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:00:31.133 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 07:00:31.142 [dispatcher-event-loop-1 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 07:00:31.148 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 07:00:31.148 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 07:00:31.151 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 07:00:31.153 [dispatcher-event-loop-5 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 07:00:31.156 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 07:00:31.156 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 07:00:31.157 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-3b6f5f54-3670-4b7f-ac68-0fc0ebe23519
2025-06-05 07:00:31.159 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-1ab2b4cd-8ac6-4172-9910-56a70ac48ad9
2025-06-05 07:02:34.597 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 07:02:34.682 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 07:02:34.731 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:02:34.731 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 07:02:34.732 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:02:34.732 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 07:02:34.745 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 07:02:34.752 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 07:02:34.752 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 07:02:34.784 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 07:02:34.784 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 07:02:34.784 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 07:02:34.784 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 07:02:34.784 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 07:02:34.910 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 35039.
2025-06-05 07:02:34.923 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 07:02:34.940 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 07:02:34.949 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 07:02:34.949 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 07:02:34.951 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 07:02:34.960 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-e7ab1965-eae7-49f5-ae23-cc2847d4e11f
2025-06-05 07:02:34.976 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 07:02:34.985 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 07:02:35.004 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1010ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 07:02:35.048 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 07:02:35.053 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 07:02:35.061 [main INFO ] org.sparkproject.jetty.server.Server - Started @1068ms
2025-06-05 07:02:35.077 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@1774ac7{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:02:35.077 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 07:02:35.088 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7144655b{/,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.140 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 07:02:35.144 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 07:02:35.155 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33687.
2025-06-05 07:02:35.155 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:33687
2025-06-05 07:02:35.156 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 07:02:35.160 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 33687, None)
2025-06-05 07:02:35.162 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:33687 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 33687, None)
2025-06-05 07:02:35.164 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 33687, None)
2025-06-05 07:02:35.165 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 33687, None)
2025-06-05 07:02:35.243 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7144655b{/,null,STOPPED,@Spark}
2025-06-05 07:02:35.244 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241f05b{/jobs,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.245 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.246 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c9320c2{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.247 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cc9385{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.247 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7915bca3{/stages,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.248 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.249 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79fd6f95{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.249 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.250 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.250 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.251 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/storage,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.251 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.252 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.252 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.252 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/environment,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.253 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.253 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/executors,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.254 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.254 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397ef2{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.255 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e93c1f{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.259 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9b21bd3{/static,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.260 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@323f3c96{/,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.261 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b6d92e{/api,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.261 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25d93198{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.261 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f951a7f{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.264 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e7f0216{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.348 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 07:02:35.354 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 07:02:35.361 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c86da0c{/SQL,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.361 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6732726{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.362 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47d023b7{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.362 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d64c100{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 07:02:35.369 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d904ff1{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:02:36.647 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-05 07:02:36.655 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c6fb501{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-05 07:02:36.656 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e6b379c{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-05 07:02:36.656 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5abc5854{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-05 07:02:36.657 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66b40dd3{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-05 07:02:36.658 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2095c331{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:02:36.661 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1216aecd-43c4-410d-8f68-427e87f327c2. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-05 07:02:36.673 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-1216aecd-43c4-410d-8f68-427e87f327c2 resolved to file:/tmp/temporary-1216aecd-43c4-410d-8f68-427e87f327c2.
2025-06-05 07:02:36.673 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-05 07:02:36.717 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-1216aecd-43c4-410d-8f68-427e87f327c2/metadata using temp file file:/tmp/temporary-1216aecd-43c4-410d-8f68-427e87f327c2/.metadata.8e98a420-7d32-402e-aab6-3cb78e2e333c.tmp
2025-06-05 07:02:36.768 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-1216aecd-43c4-410d-8f68-427e87f327c2/.metadata.8e98a420-7d32-402e-aab6-3cb78e2e333c.tmp to file:/tmp/temporary-1216aecd-43c4-410d-8f68-427e87f327c2/metadata
2025-06-05 07:02:36.783 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b]. Use file:/tmp/temporary-1216aecd-43c4-410d-8f68-427e87f327c2 to store the query checkpoint.
2025-06-05 07:02:36.788 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7316a0a3] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@45c9e0c9]
2025-06-05 07:02:36.802 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:02:36.802 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:02:36.803 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-05 07:02:36.804 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-05 07:02:36.974 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 07:02:37.004 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 07:02:37.004 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:02:37.005 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:02:37.005 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081757004
2025-06-05 07:02:37.178 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-1216aecd-43c4-410d-8f68-427e87f327c2/sources/0/0 using temp file file:/tmp/temporary-1216aecd-43c4-410d-8f68-427e87f327c2/sources/0/.0.a1883b43-943d-4e70-b2e8-5845be2a42af.tmp
2025-06-05 07:02:37.191 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-1216aecd-43c4-410d-8f68-427e87f327c2/sources/0/.0.a1883b43-943d-4e70-b2e8-5845be2a42af.tmp to file:/tmp/temporary-1216aecd-43c4-410d-8f68-427e87f327c2/sources/0/0
2025-06-05 07:02:37.191 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-05 07:02:37.206 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-1216aecd-43c4-410d-8f68-427e87f327c2/offsets/0 using temp file file:/tmp/temporary-1216aecd-43c4-410d-8f68-427e87f327c2/offsets/.0.74f702aa-576d-43c8-96d3-4a90690868d6.tmp
2025-06-05 07:02:37.237 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-1216aecd-43c4-410d-8f68-427e87f327c2/offsets/.0.74f702aa-576d-43c8-96d3-4a90690868d6.tmp to file:/tmp/temporary-1216aecd-43c4-410d-8f68-427e87f327c2/offsets/0
2025-06-05 07:02:37.238 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749081757200,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-05 07:02:37.386 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749081757200
2025-06-05 07:02:37.438 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:02:37.467 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:02:37.507 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749081757200
2025-06-05 07:02:37.510 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:02:37.511 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:02:37.757 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 148.278664 ms
2025-06-05 07:02:37.882 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.974934 ms
2025-06-05 07:02:37.902 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.651095 ms
2025-06-05 07:02:37.953 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:253
2025-06-05 07:02:37.963 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:253) as input to shuffle 0
2025-06-05 07:02:37.966 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:253) with 1 output partitions
2025-06-05 07:02:37.966 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:253)
2025-06-05 07:02:37.966 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-05 07:02:37.967 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
2025-06-05 07:02:37.971 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:253), which has no missing parents
2025-06-05 07:02:38.037 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 46.1 KiB, free 9.2 GiB)
2025-06-05 07:02:38.053 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 9.2 GiB)
2025-06-05 07:02:38.054 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:33687 (size: 17.9 KiB, free: 9.2 GiB)
2025-06-05 07:02:38.057 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:02:38.065 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:253) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:02:38.066 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-05 07:02:38.093 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8303 bytes) 
2025-06-05 07:02:38.096 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8303 bytes) 
2025-06-05 07:02:38.102 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-05 07:02:38.102 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 07:02:38.204 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 16.934199 ms
2025-06-05 07:02:38.240 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.716489 ms
2025-06-05 07:02:38.256 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.618531 ms
2025-06-05 07:02:38.266 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:02:38.266 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:02:38.287 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:02:38.287 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:02:38.314 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:02:38.314 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:02:38.314 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081758314
2025-06-05 07:02:38.314 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:02:38.314 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:02:38.314 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081758314
2025-06-05 07:02:38.315 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor-2, groupId=spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 07:02:38.315 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor-1, groupId=spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor] Assigned to partition(s): clickstream-events-1
2025-06-05 07:02:38.319 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor-2, groupId=spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:02:38.319 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor-1, groupId=spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:02:38.324 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor-2, groupId=spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:02:38.324 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor-1, groupId=spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:02:38.346 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor-2, groupId=spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:02:38.346 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor-1, groupId=spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:02:38.848 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor-1, groupId=spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:02:38.848 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor-2, groupId=spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:02:38.848 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor-1, groupId=spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:02:38.848 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor-2, groupId=spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:02:38.849 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor-1, groupId=spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:02:38.849 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor-2, groupId=spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:02:38.927 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 2342 bytes result sent to driver
2025-06-05 07:02:38.927 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2342 bytes result sent to driver
2025-06-05 07:02:38.933 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 845 ms on phamviethoa (executor driver) (1/2)
2025-06-05 07:02:38.933 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 838 ms on phamviethoa (executor driver) (2/2)
2025-06-05 07:02:38.934 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 07:02:38.938 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (start at SparkClickstreamProcessor.java:253) finished in 0.956 s
2025-06-05 07:02:38.938 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
2025-06-05 07:02:38.938 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - running: Set()
2025-06-05 07:02:38.939 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
2025-06-05 07:02:38.939 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - failed: Set()
2025-06-05 07:02:38.940 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:253), which has no missing parents
2025-06-05 07:02:38.946 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-05 07:02:38.947 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-05 07:02:38.948 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:33687 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-05 07:02:38.948 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:02:38.949 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:253) (first 15 tasks are for partitions Vector(0))
2025-06-05 07:02:38.949 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-05 07:02:38.952 [dispatcher-event-loop-12 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
2025-06-05 07:02:38.952 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-05 07:02:38.981 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-05 07:02:38.981 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
2025-06-05 07:02:38.989 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 4038 bytes result sent to driver
2025-06-05 07:02:38.991 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 40 ms on phamviethoa (executor driver) (1/1)
2025-06-05 07:02:38.991 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-05 07:02:38.992 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:253) finished in 0.047 s
2025-06-05 07:02:38.994 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 07:02:38.994 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-05 07:02:38.995 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:253, took 1.041773 s
2025-06-05 07:02:39.120 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] WARN ] c.c.jdbc.parser.ClickHouseSqlParser - Parse error at line 1, column 14.  Encountered: .. If you believe the SQL is valid, please feel free to open an issue on Github with this warning and the following SQL attached.
CREATE TABLE .events ("eventId" TEXT , "eventName" TEXT , "event_time" TEXT , "userId" TEXT , "sessionId" TEXT , "appId" TEXT , "platform" TEXT , "pageUrl" TEXT , "geoCountry" TEXT , "geoRegion" TEXT , "geoCity" TEXT , "trafficSource" TEXT , "trafficMedium" TEXT , "session_id" TEXT , "kafka_timestamp" TEXT , "processing_time" TEXT NOT NULL) 
2025-06-05 07:02:39.136 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] terminated with error
java.sql.SQLException: Code: 62. DB::Exception: Syntax error: failed at position 14 ('.'): .events ("eventId" TEXT , "eventName" TEXT , "event_time" TEXT , "userId" TEXT , "sessionId" TEXT , "appId" TEXT , "platform" TEXT , "pageUrl" TEXT , "geoCountr. Expected one of: IF NOT EXISTS, compound identifier, list of elements, identifier. (SYNTAX_ERROR) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream_1k]@1608251105
	at com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:85)
	at com.clickhouse.jdbc.SqlExceptionUtils.create(SqlExceptionUtils.java:31)
	at com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:90)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:122)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeLargeUpdate(ClickHouseStatementImpl.java:489)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeUpdate(ClickHouseStatementImpl.java:498)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.executeStatement(JdbcUtils.scala:1083)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:913)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.SparkClickstreamProcessor.lambda$main$a450ce84$1(SparkClickstreamProcessor.java:249)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
Caused by: java.io.IOException: Code: 62. DB::Exception: Syntax error: failed at position 14 ('.'): .events ("eventId" TEXT , "eventName" TEXT , "event_time" TEXT , "userId" TEXT , "sessionId" TEXT , "appId" TEXT , "platform" TEXT , "pageUrl" TEXT , "geoCountr. Expected one of: IF NOT EXISTS, compound identifier, list of elements, identifier. (SYNTAX_ERROR) (version 22.1.3.7 (official build))

	at com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)
	at com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)
	at com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)
	at com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)
	at com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)
	at com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:120)
	... 62 common frames omitted
2025-06-05 07:02:39.137 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 07:02:39.140 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:02:39.140 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:02:39.140 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:02:39.141 [stream execution thread for [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = be5f0b2e-b2cc-4255-97ea-eb6af59104ac, runId = ab1d3b98-1853-4f2e-a109-a4d5829e622b] has been shutdown
2025-06-05 07:02:39.154 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor-1, groupId=spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:02:39.154 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor-1, groupId=spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:02:39.158 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:02:39.158 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:02:39.158 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:02:39.158 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:02:39.160 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor-1 unregistered
2025-06-05 07:02:39.160 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor-2, groupId=spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:02:39.160 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor-2, groupId=spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:02:39.162 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:02:39.162 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:02:39.162 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:02:39.162 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:02:39.165 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-aec3a6b8-87e4-49b3-a8c7-56aa385cfd04--1970045289-executor-2 unregistered
2025-06-05 07:02:39.165 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 07:02:39.166 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 07:02:39.171 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@1774ac7{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:02:39.173 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 07:02:39.181 [dispatcher-event-loop-1 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 07:02:39.187 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 07:02:39.188 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 07:02:39.192 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 07:02:39.193 [dispatcher-event-loop-5 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 07:02:39.197 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 07:02:39.198 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 07:02:39.198 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-1216aecd-43c4-410d-8f68-427e87f327c2
2025-06-05 07:02:39.200 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-c2677a2d-f001-4775-8f9f-320cde218d5a
2025-06-05 07:03:57.841 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 07:03:57.926 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 07:03:57.971 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:03:57.971 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 07:03:57.972 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:03:57.972 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 07:03:57.983 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 07:03:57.989 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 07:03:57.989 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 07:03:58.017 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 07:03:58.017 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 07:03:58.017 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 07:03:58.017 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 07:03:58.017 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 07:03:58.133 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 41047.
2025-06-05 07:03:58.146 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 07:03:58.162 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 07:03:58.171 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 07:03:58.171 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 07:03:58.173 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 07:03:58.183 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-f1e63020-8b87-4796-85b0-a8428af4f423
2025-06-05 07:03:58.200 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 07:03:58.208 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 07:03:58.228 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @983ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 07:03:58.280 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 07:03:58.285 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 07:03:58.293 [main INFO ] org.sparkproject.jetty.server.Server - Started @1048ms
2025-06-05 07:03:58.310 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@69b452ce{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:03:58.310 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 07:03:58.323 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b61a019{/,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.375 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 07:03:58.380 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 07:03:58.391 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34577.
2025-06-05 07:03:58.391 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:34577
2025-06-05 07:03:58.392 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 07:03:58.396 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 34577, None)
2025-06-05 07:03:58.398 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:34577 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 34577, None)
2025-06-05 07:03:58.400 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 34577, None)
2025-06-05 07:03:58.400 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 34577, None)
2025-06-05 07:03:58.475 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@2b61a019{/,null,STOPPED,@Spark}
2025-06-05 07:03:58.476 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cc9385{/jobs,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.476 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7915bca3{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.477 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67b4ec{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.477 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f91da5e{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.478 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79fd6f95{/stages,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.478 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.479 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.479 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.480 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.480 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.481 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/storage,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.481 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.482 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.482 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.483 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397ef2{/environment,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.483 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e93c1f{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.483 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9b21bd3{/executors,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.484 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7661b5a{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.485 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65c33b92{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.485 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e08acf9{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.489 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78cd163b{/static,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.490 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5292ceca{/,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.491 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e9ef5b6{/api,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.491 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59f93db8{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.492 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73c9e8e8{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.494 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a4d1ab7{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.574 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 07:03:58.578 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 07:03:58.584 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c83ae01{/SQL,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.585 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69d45cca{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.586 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79c5460e{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.586 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f94e148{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 07:03:58.592 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f37b6d9{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:03:59.852 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-05 07:03:59.861 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c3007d{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-05 07:03:59.862 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7296fe0b{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-05 07:03:59.862 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d1c63af{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-05 07:03:59.863 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3909a854{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-05 07:03:59.863 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56ba8e8c{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:03:59.867 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-0a4ae291-8d0a-489c-9b87-053300175050. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-05 07:03:59.879 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-0a4ae291-8d0a-489c-9b87-053300175050 resolved to file:/tmp/temporary-0a4ae291-8d0a-489c-9b87-053300175050.
2025-06-05 07:03:59.879 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-05 07:03:59.922 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-0a4ae291-8d0a-489c-9b87-053300175050/metadata using temp file file:/tmp/temporary-0a4ae291-8d0a-489c-9b87-053300175050/.metadata.e7244151-5311-499d-98c0-08adbf208917.tmp
2025-06-05 07:03:59.970 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-0a4ae291-8d0a-489c-9b87-053300175050/.metadata.e7244151-5311-499d-98c0-08adbf208917.tmp to file:/tmp/temporary-0a4ae291-8d0a-489c-9b87-053300175050/metadata
2025-06-05 07:03:59.984 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc]. Use file:/tmp/temporary-0a4ae291-8d0a-489c-9b87-053300175050 to store the query checkpoint.
2025-06-05 07:03:59.991 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@48d1669] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@5fec0d78]
2025-06-05 07:04:00.004 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:04:00.005 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:04:00.005 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-05 07:04:00.006 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-05 07:04:00.179 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 07:04:00.206 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 07:04:00.207 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:04:00.207 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:04:00.207 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081840206
2025-06-05 07:04:00.378 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-0a4ae291-8d0a-489c-9b87-053300175050/sources/0/0 using temp file file:/tmp/temporary-0a4ae291-8d0a-489c-9b87-053300175050/sources/0/.0.df98e20b-1dc3-4820-8f6b-307abb01a4f9.tmp
2025-06-05 07:04:00.394 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-0a4ae291-8d0a-489c-9b87-053300175050/sources/0/.0.df98e20b-1dc3-4820-8f6b-307abb01a4f9.tmp to file:/tmp/temporary-0a4ae291-8d0a-489c-9b87-053300175050/sources/0/0
2025-06-05 07:04:00.395 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-05 07:04:00.407 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-0a4ae291-8d0a-489c-9b87-053300175050/offsets/0 using temp file file:/tmp/temporary-0a4ae291-8d0a-489c-9b87-053300175050/offsets/.0.e14e82ce-e332-4caf-84e8-efe4e08104f5.tmp
2025-06-05 07:04:00.442 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-0a4ae291-8d0a-489c-9b87-053300175050/offsets/.0.e14e82ce-e332-4caf-84e8-efe4e08104f5.tmp to file:/tmp/temporary-0a4ae291-8d0a-489c-9b87-053300175050/offsets/0
2025-06-05 07:04:00.443 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749081840402,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-05 07:04:00.620 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749081840402
2025-06-05 07:04:00.669 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:04:00.697 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:04:00.740 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749081840402
2025-06-05 07:04:00.743 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:04:00.745 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:04:01.026 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 158.543867 ms
2025-06-05 07:04:01.146 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.763187 ms
2025-06-05 07:04:01.166 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 11.438187 ms
2025-06-05 07:04:01.217 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:253
2025-06-05 07:04:01.228 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:253) as input to shuffle 0
2025-06-05 07:04:01.231 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:253) with 1 output partitions
2025-06-05 07:04:01.232 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:253)
2025-06-05 07:04:01.232 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-05 07:04:01.233 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
2025-06-05 07:04:01.236 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:253), which has no missing parents
2025-06-05 07:04:01.304 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 46.1 KiB, free 9.2 GiB)
2025-06-05 07:04:01.322 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 9.2 GiB)
2025-06-05 07:04:01.323 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:34577 (size: 17.9 KiB, free: 9.2 GiB)
2025-06-05 07:04:01.325 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:04:01.333 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:253) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:04:01.334 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-05 07:04:01.369 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8302 bytes) 
2025-06-05 07:04:01.372 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8302 bytes) 
2025-06-05 07:04:01.378 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-05 07:04:01.378 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 07:04:01.482 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 21.159686 ms
2025-06-05 07:04:01.518 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.202204 ms
2025-06-05 07:04:01.535 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.093707 ms
2025-06-05 07:04:01.544 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:04:01.544 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:04:01.565 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:04:01.565 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:04:01.593 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:04:01.593 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:04:01.593 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081841593
2025-06-05 07:04:01.593 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:04:01.593 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:04:01.593 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081841593
2025-06-05 07:04:01.594 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor-1, groupId=spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 07:04:01.594 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor-2, groupId=spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor] Assigned to partition(s): clickstream-events-1
2025-06-05 07:04:01.598 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor-2, groupId=spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:04:01.598 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor-1, groupId=spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:04:01.603 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor-1, groupId=spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:04:01.603 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor-2, groupId=spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:04:01.625 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor-1, groupId=spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:04:01.625 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor-2, groupId=spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:04:02.126 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor-1, groupId=spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:04:02.126 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor-2, groupId=spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:04:02.127 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor-1, groupId=spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:04:02.127 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor-2, groupId=spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:04:02.127 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor-1, groupId=spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:04:02.127 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor-2, groupId=spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:04:02.208 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 2342 bytes result sent to driver
2025-06-05 07:04:02.208 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2342 bytes result sent to driver
2025-06-05 07:04:02.214 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 842 ms on phamviethoa (executor driver) (1/2)
2025-06-05 07:04:02.214 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 852 ms on phamviethoa (executor driver) (2/2)
2025-06-05 07:04:02.215 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 07:04:02.219 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (start at SparkClickstreamProcessor.java:253) finished in 0.975 s
2025-06-05 07:04:02.219 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
2025-06-05 07:04:02.219 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - running: Set()
2025-06-05 07:04:02.219 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
2025-06-05 07:04:02.220 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - failed: Set()
2025-06-05 07:04:02.221 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:253), which has no missing parents
2025-06-05 07:04:02.225 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-05 07:04:02.227 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-05 07:04:02.227 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:34577 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-05 07:04:02.227 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:04:02.228 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:253) (first 15 tasks are for partitions Vector(0))
2025-06-05 07:04:02.228 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-05 07:04:02.231 [dispatcher-event-loop-12 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
2025-06-05 07:04:02.232 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-05 07:04:02.258 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-05 07:04:02.259 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
2025-06-05 07:04:02.267 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 4038 bytes result sent to driver
2025-06-05 07:04:02.269 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 39 ms on phamviethoa (executor driver) (1/1)
2025-06-05 07:04:02.269 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-05 07:04:02.269 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:253) finished in 0.045 s
2025-06-05 07:04:02.271 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 07:04:02.271 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-05 07:04:02.272 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:253, took 1.054894 s
2025-06-05 07:04:02.420 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] terminated with error
org.apache.spark.sql.AnalysisException: Column eventId not found in schema Some(StructType(StructField(event_id,StringType,false),StructField(event_name,StringType,false),StructField(event_time,TimestampType,false),StructField(user_id,StringType,false),StructField(session_id,StringType,false),StructField(app_id,StringType,false),StructField(platform,StringType,false),StructField(page_url,StringType,false),StructField(geo_country,StringType,false),StructField(geo_city,StringType,false),StructField(traffic_source,StringType,false),StructField(traffic_medium,StringType,false),StructField(item_id,StringType,true),StructField(item_price,DoubleType,true),StructField(kafka_timestamp,TimestampType,false),StructField(processing_time,TimestampType,false))).
	at org.apache.spark.sql.errors.QueryCompilationErrors$.columnNotFoundInSchemaError(QueryCompilationErrors.scala:1612)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$4(JdbcUtils.scala:126)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$2(JdbcUtils.scala:126)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getInsertStatement(JdbcUtils.scala:124)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:883)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.SparkClickstreamProcessor.lambda$main$a450ce84$1(SparkClickstreamProcessor.java:249)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
2025-06-05 07:04:02.421 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 07:04:02.423 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:04:02.423 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:04:02.423 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:04:02.423 [stream execution thread for [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = 92bd7bc0-4b71-4bd9-9a9e-77736f37d186, runId = c3200125-aa2d-4b80-a243-51c50118a3fc] has been shutdown
2025-06-05 07:04:02.434 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor-2, groupId=spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:04:02.434 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor-2, groupId=spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:04:02.437 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:04:02.437 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:04:02.437 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:04:02.437 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:04:02.439 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor-2 unregistered
2025-06-05 07:04:02.439 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor-1, groupId=spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:04:02.439 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor-1, groupId=spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:04:02.441 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:04:02.441 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:04:02.441 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:04:02.441 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:04:02.443 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-5347e2e9-3ca2-4d89-9603-46db7ecf9f8d-1908011996-executor-1 unregistered
2025-06-05 07:04:02.443 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 07:04:02.443 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 07:04:02.447 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@69b452ce{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:04:02.449 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 07:04:02.458 [dispatcher-event-loop-1 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 07:04:02.465 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 07:04:02.465 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 07:04:02.469 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 07:04:02.470 [dispatcher-event-loop-5 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 07:04:02.474 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 07:04:02.474 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 07:04:02.474 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-5d4b23b5-a395-4e9a-8c18-7f51bdd7537e
2025-06-05 07:04:02.475 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-0a4ae291-8d0a-489c-9b87-053300175050
2025-06-05 07:04:54.881 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 07:04:54.965 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 07:04:55.009 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:04:55.009 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 07:04:55.009 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:04:55.010 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 07:04:55.020 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 07:04:55.026 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 07:04:55.026 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 07:04:55.053 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 07:04:55.054 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 07:04:55.054 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 07:04:55.054 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 07:04:55.054 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 07:04:55.179 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 46225.
2025-06-05 07:04:55.194 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 07:04:55.211 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 07:04:55.219 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 07:04:55.220 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 07:04:55.222 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 07:04:55.231 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-4b688439-7708-4398-b203-b603d0e7282d
2025-06-05 07:04:55.249 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 07:04:55.258 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 07:04:55.278 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1007ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 07:04:55.323 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 07:04:55.329 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 07:04:55.337 [main INFO ] org.sparkproject.jetty.server.Server - Started @1068ms
2025-06-05 07:04:55.353 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@21a66d45{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:04:55.353 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 07:04:55.366 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@345cf395{/,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.425 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 07:04:55.432 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 07:04:55.443 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43255.
2025-06-05 07:04:55.443 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:43255
2025-06-05 07:04:55.445 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 07:04:55.448 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 43255, None)
2025-06-05 07:04:55.451 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:43255 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 43255, None)
2025-06-05 07:04:55.454 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 43255, None)
2025-06-05 07:04:55.454 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 43255, None)
2025-06-05 07:04:55.541 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@345cf395{/,null,STOPPED,@Spark}
2025-06-05 07:04:55.542 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a331b46{/jobs,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.543 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@743e66f7{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.544 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.544 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.544 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c9320c2{/stages,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.545 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cc9385{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.546 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67b4ec{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.546 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f91da5e{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.547 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79fd6f95{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.547 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.547 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/storage,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.548 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.548 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.549 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.549 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/environment,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.550 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.550 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/executors,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.550 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.551 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.551 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.556 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397ef2{/static,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.557 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fd9ebde{/,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.558 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ee5b2d9{/api,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.558 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4110765e{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.558 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62e93c3a{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.561 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e19755a{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.660 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 07:04:55.664 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 07:04:55.671 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c101cc1{/SQL,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.672 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7fb48179{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.672 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@474821de{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.673 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ec5ea63{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 07:04:55.680 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e6d4780{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:04:57.004 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-05 07:04:57.012 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10408ea{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-05 07:04:57.013 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@186dcb05{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-05 07:04:57.014 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e7e7a7e{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-05 07:04:57.014 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c610f{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-05 07:04:57.015 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@578d472a{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:04:57.019 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d97d2df4-9c82-4f30-bca7-382c16cf11c9. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-05 07:04:57.031 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-d97d2df4-9c82-4f30-bca7-382c16cf11c9 resolved to file:/tmp/temporary-d97d2df4-9c82-4f30-bca7-382c16cf11c9.
2025-06-05 07:04:57.031 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-05 07:04:57.075 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-d97d2df4-9c82-4f30-bca7-382c16cf11c9/metadata using temp file file:/tmp/temporary-d97d2df4-9c82-4f30-bca7-382c16cf11c9/.metadata.ee08387c-57f8-4adb-8ece-f7ae697c784d.tmp
2025-06-05 07:04:57.123 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-d97d2df4-9c82-4f30-bca7-382c16cf11c9/.metadata.ee08387c-57f8-4adb-8ece-f7ae697c784d.tmp to file:/tmp/temporary-d97d2df4-9c82-4f30-bca7-382c16cf11c9/metadata
2025-06-05 07:04:57.137 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85]. Use file:/tmp/temporary-d97d2df4-9c82-4f30-bca7-382c16cf11c9 to store the query checkpoint.
2025-06-05 07:04:57.144 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@2b6cbe7c] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@466875a7]
2025-06-05 07:04:57.157 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:04:57.158 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:04:57.158 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-05 07:04:57.160 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-05 07:04:57.326 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 07:04:57.352 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 07:04:57.353 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:04:57.353 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:04:57.353 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081897353
2025-06-05 07:04:57.521 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-d97d2df4-9c82-4f30-bca7-382c16cf11c9/sources/0/0 using temp file file:/tmp/temporary-d97d2df4-9c82-4f30-bca7-382c16cf11c9/sources/0/.0.1797de53-f6b4-4599-a26b-a889f8486634.tmp
2025-06-05 07:04:57.533 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-d97d2df4-9c82-4f30-bca7-382c16cf11c9/sources/0/.0.1797de53-f6b4-4599-a26b-a889f8486634.tmp to file:/tmp/temporary-d97d2df4-9c82-4f30-bca7-382c16cf11c9/sources/0/0
2025-06-05 07:04:57.533 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-05 07:04:57.545 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-d97d2df4-9c82-4f30-bca7-382c16cf11c9/offsets/0 using temp file file:/tmp/temporary-d97d2df4-9c82-4f30-bca7-382c16cf11c9/offsets/.0.22ec56b7-0c40-46fb-8b9e-9bb3ee6dbcae.tmp
2025-06-05 07:04:57.563 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-d97d2df4-9c82-4f30-bca7-382c16cf11c9/offsets/.0.22ec56b7-0c40-46fb-8b9e-9bb3ee6dbcae.tmp to file:/tmp/temporary-d97d2df4-9c82-4f30-bca7-382c16cf11c9/offsets/0
2025-06-05 07:04:57.563 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749081897540,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-05 07:04:57.717 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749081897540
2025-06-05 07:04:57.768 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:04:57.795 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:04:57.826 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749081897540
2025-06-05 07:04:57.828 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:04:57.829 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:04:58.070 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 143.180011 ms
2025-06-05 07:04:58.191 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.801054 ms
2025-06-05 07:04:58.210 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.828424 ms
2025-06-05 07:04:58.262 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:254
2025-06-05 07:04:58.269 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:254) as input to shuffle 0
2025-06-05 07:04:58.272 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:254) with 1 output partitions
2025-06-05 07:04:58.272 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:254)
2025-06-05 07:04:58.272 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-05 07:04:58.273 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
2025-06-05 07:04:58.276 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:254), which has no missing parents
2025-06-05 07:04:58.343 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 46.3 KiB, free 9.2 GiB)
2025-06-05 07:04:58.360 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 9.2 GiB)
2025-06-05 07:04:58.362 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:43255 (size: 17.9 KiB, free: 9.2 GiB)
2025-06-05 07:04:58.364 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:04:58.371 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:254) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:04:58.372 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-05 07:04:58.409 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8303 bytes) 
2025-06-05 07:04:58.413 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8303 bytes) 
2025-06-05 07:04:58.420 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 07:04:58.420 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-05 07:04:58.536 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 23.827037 ms
2025-06-05 07:04:58.566 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 5.955504 ms
2025-06-05 07:04:58.578 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.712842 ms
2025-06-05 07:04:58.587 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:04:58.587 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:04:58.606 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:04:58.606 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:04:58.631 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:04:58.631 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:04:58.631 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081898631
2025-06-05 07:04:58.631 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:04:58.631 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:04:58.632 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749081898631
2025-06-05 07:04:58.632 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor-1, groupId=spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 07:04:58.632 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor-2, groupId=spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor] Assigned to partition(s): clickstream-events-1
2025-06-05 07:04:58.636 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor-1, groupId=spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:04:58.636 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor-2, groupId=spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:04:58.641 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor-2, groupId=spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:04:58.641 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor-1, groupId=spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:04:58.662 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor-2, groupId=spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:04:58.662 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor-1, groupId=spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:04:59.164 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor-2, groupId=spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:04:59.164 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor-1, groupId=spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:04:59.164 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor-2, groupId=spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:04:59.164 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor-1, groupId=spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:04:59.165 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor-2, groupId=spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:04:59.165 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor-1, groupId=spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:04:59.247 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2342 bytes result sent to driver
2025-06-05 07:04:59.247 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 2342 bytes result sent to driver
2025-06-05 07:04:59.253 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 841 ms on phamviethoa (executor driver) (1/2)
2025-06-05 07:04:59.254 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 854 ms on phamviethoa (executor driver) (2/2)
2025-06-05 07:04:59.255 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 07:04:59.259 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (start at SparkClickstreamProcessor.java:254) finished in 0.976 s
2025-06-05 07:04:59.260 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
2025-06-05 07:04:59.260 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - running: Set()
2025-06-05 07:04:59.260 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
2025-06-05 07:04:59.260 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - failed: Set()
2025-06-05 07:04:59.261 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:254), which has no missing parents
2025-06-05 07:04:59.266 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-05 07:04:59.267 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-05 07:04:59.268 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:43255 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-05 07:04:59.268 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:04:59.269 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:254) (first 15 tasks are for partitions Vector(0))
2025-06-05 07:04:59.269 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-05 07:04:59.272 [dispatcher-event-loop-12 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
2025-06-05 07:04:59.272 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-05 07:04:59.300 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-05 07:04:59.300 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
2025-06-05 07:04:59.311 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 4038 bytes result sent to driver
2025-06-05 07:04:59.312 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 42 ms on phamviethoa (executor driver) (1/1)
2025-06-05 07:04:59.312 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-05 07:04:59.313 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:254) finished in 0.049 s
2025-06-05 07:04:59.316 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 07:04:59.316 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-05 07:04:59.317 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:254, took 1.055550 s
2025-06-05 07:04:59.491 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] terminated with error
org.apache.spark.sql.AnalysisException: Column eventId not found in schema Some(StructType(StructField(event_id,StringType,false),StructField(event_name,StringType,false),StructField(event_time,TimestampType,false),StructField(user_id,StringType,false),StructField(session_id,StringType,false),StructField(app_id,StringType,false),StructField(platform,StringType,false),StructField(page_url,StringType,false),StructField(geo_country,StringType,false),StructField(geo_city,StringType,false),StructField(traffic_source,StringType,false),StructField(traffic_medium,StringType,false),StructField(item_id,StringType,true),StructField(item_price,DoubleType,true),StructField(kafka_timestamp,TimestampType,false),StructField(processing_time,TimestampType,false))).
	at org.apache.spark.sql.errors.QueryCompilationErrors$.columnNotFoundInSchemaError(QueryCompilationErrors.scala:1612)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$4(JdbcUtils.scala:126)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$2(JdbcUtils.scala:126)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getInsertStatement(JdbcUtils.scala:124)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:883)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.SparkClickstreamProcessor.lambda$main$a450ce84$1(SparkClickstreamProcessor.java:250)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
2025-06-05 07:04:59.492 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 07:04:59.494 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:04:59.494 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:04:59.494 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:04:59.495 [stream execution thread for [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = f68822d5-67df-4aca-bc2e-2c2ddc12b024, runId = ed8c77d8-9679-41f3-92de-954f52d92b85] has been shutdown
2025-06-05 07:04:59.505 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor-2, groupId=spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:04:59.505 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor-2, groupId=spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:04:59.508 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:04:59.508 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:04:59.508 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:04:59.508 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:04:59.510 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor-2 unregistered
2025-06-05 07:04:59.510 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor-1, groupId=spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:04:59.510 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor-1, groupId=spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:04:59.512 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:04:59.512 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:04:59.512 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:04:59.512 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:04:59.513 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-07a79423-3834-4a34-8976-a1280a3c30e0--1348688647-executor-1 unregistered
2025-06-05 07:04:59.514 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 07:04:59.514 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 07:04:59.518 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@21a66d45{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:04:59.520 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 07:04:59.528 [dispatcher-event-loop-1 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 07:04:59.534 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 07:04:59.534 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 07:04:59.537 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 07:04:59.538 [dispatcher-event-loop-5 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 07:04:59.541 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 07:04:59.542 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 07:04:59.542 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-d97d2df4-9c82-4f30-bca7-382c16cf11c9
2025-06-05 07:04:59.543 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-b12cd411-469b-48bd-88e2-63aecc3ee71a
2025-06-05 07:07:17.525 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 07:07:17.606 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 07:07:17.650 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:07:17.650 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 07:07:17.650 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:07:17.650 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 07:07:17.662 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 07:07:17.668 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 07:07:17.668 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 07:07:17.695 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 07:07:17.695 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 07:07:17.696 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 07:07:17.696 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 07:07:17.696 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 07:07:17.814 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 35705.
2025-06-05 07:07:17.827 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 07:07:17.844 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 07:07:17.853 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 07:07:17.853 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 07:07:17.855 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 07:07:17.864 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-3d7398b3-47a4-4f3f-b48f-52fcc13ee5b0
2025-06-05 07:07:17.881 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 07:07:17.889 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 07:07:17.909 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @989ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 07:07:17.951 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 07:07:17.956 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 07:07:17.965 [main INFO ] org.sparkproject.jetty.server.Server - Started @1046ms
2025-06-05 07:07:17.980 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@4a68135e{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:07:17.980 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 07:07:17.991 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c82cd4f{/,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.038 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 07:07:18.043 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 07:07:18.054 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38047.
2025-06-05 07:07:18.054 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:38047
2025-06-05 07:07:18.055 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 07:07:18.058 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 38047, None)
2025-06-05 07:07:18.061 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:38047 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 38047, None)
2025-06-05 07:07:18.063 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 38047, None)
2025-06-05 07:07:18.063 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 38047, None)
2025-06-05 07:07:18.142 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@5c82cd4f{/,null,STOPPED,@Spark}
2025-06-05 07:07:18.143 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@761956ac{/jobs,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.144 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@304d0259{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.144 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3414a8c3{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.145 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cf518cf{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.145 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68d651f2{/stages,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.146 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e43e323{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.146 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48840594{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.147 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14823f76{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.148 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ed16657{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.148 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@113e13f9{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.149 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7979b8b7{/storage,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.149 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bc49bc5{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.150 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f66ffc8{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.151 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2def7a7a{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.152 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c080ef3{/environment,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.152 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ee6291f{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.153 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37e0292a{/executors,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.154 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35267fd4{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.155 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36a6bea6{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.155 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42373389{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.162 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a62c7cd{/static,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.162 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6726cc69{/,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.163 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33899f7a{/api,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.163 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2436ea2f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.164 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20cece0b{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.166 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4339baec{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.252 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 07:07:18.256 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 07:07:18.264 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d97caa4{/SQL,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.264 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@474821de{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.265 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c83ae01{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.265 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69d45cca{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:18.271 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f94e148{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:07:19.596 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-05 07:07:19.604 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e6b379c{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-05 07:07:19.605 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ff81b0d{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:19.605 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66b40dd3{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-05 07:07:19.606 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a5066f5{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:19.606 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b795db7{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:07:19.610 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-4fc17f85-fdbe-4c92-8ac8-24f1324dd122. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-05 07:07:19.621 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-4fc17f85-fdbe-4c92-8ac8-24f1324dd122 resolved to file:/tmp/temporary-4fc17f85-fdbe-4c92-8ac8-24f1324dd122.
2025-06-05 07:07:19.621 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-05 07:07:19.663 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-4fc17f85-fdbe-4c92-8ac8-24f1324dd122/metadata using temp file file:/tmp/temporary-4fc17f85-fdbe-4c92-8ac8-24f1324dd122/.metadata.ba684634-e365-4385-b59c-4852835c4c0b.tmp
2025-06-05 07:07:19.710 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-4fc17f85-fdbe-4c92-8ac8-24f1324dd122/.metadata.ba684634-e365-4385-b59c-4852835c4c0b.tmp to file:/tmp/temporary-4fc17f85-fdbe-4c92-8ac8-24f1324dd122/metadata
2025-06-05 07:07:19.724 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662]. Use file:/tmp/temporary-4fc17f85-fdbe-4c92-8ac8-24f1324dd122 to store the query checkpoint.
2025-06-05 07:07:19.730 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@67929fb0] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@693190a3]
2025-06-05 07:07:19.743 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:07:19.743 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:07:19.744 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-05 07:07:19.745 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-05 07:07:19.900 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 07:07:19.930 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 07:07:19.931 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:07:19.931 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:07:19.931 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082039930
2025-06-05 07:07:20.115 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-4fc17f85-fdbe-4c92-8ac8-24f1324dd122/sources/0/0 using temp file file:/tmp/temporary-4fc17f85-fdbe-4c92-8ac8-24f1324dd122/sources/0/.0.bc39595e-9b70-4b23-8f66-1d3b28f6490b.tmp
2025-06-05 07:07:20.128 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-4fc17f85-fdbe-4c92-8ac8-24f1324dd122/sources/0/.0.bc39595e-9b70-4b23-8f66-1d3b28f6490b.tmp to file:/tmp/temporary-4fc17f85-fdbe-4c92-8ac8-24f1324dd122/sources/0/0
2025-06-05 07:07:20.129 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-05 07:07:20.151 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-4fc17f85-fdbe-4c92-8ac8-24f1324dd122/offsets/0 using temp file file:/tmp/temporary-4fc17f85-fdbe-4c92-8ac8-24f1324dd122/offsets/.0.0796d9e0-8edb-4eb1-8a5c-2e2b82b16467.tmp
2025-06-05 07:07:20.168 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-4fc17f85-fdbe-4c92-8ac8-24f1324dd122/offsets/.0.0796d9e0-8edb-4eb1-8a5c-2e2b82b16467.tmp to file:/tmp/temporary-4fc17f85-fdbe-4c92-8ac8-24f1324dd122/offsets/0
2025-06-05 07:07:20.168 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749082040143,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-05 07:07:20.309 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082040143
2025-06-05 07:07:20.361 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:07:20.388 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:07:20.422 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082040143
2025-06-05 07:07:20.424 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:07:20.425 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:07:20.683 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 158.881734 ms
2025-06-05 07:07:20.810 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.502083 ms
2025-06-05 07:07:20.828 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.379305 ms
2025-06-05 07:07:20.881 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:254
2025-06-05 07:07:20.890 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:254) as input to shuffle 0
2025-06-05 07:07:20.893 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:254) with 1 output partitions
2025-06-05 07:07:20.893 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:254)
2025-06-05 07:07:20.894 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-05 07:07:20.895 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
2025-06-05 07:07:20.898 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:254), which has no missing parents
2025-06-05 07:07:20.969 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 46.1 KiB, free 9.2 GiB)
2025-06-05 07:07:20.985 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 9.2 GiB)
2025-06-05 07:07:20.986 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:38047 (size: 17.9 KiB, free: 9.2 GiB)
2025-06-05 07:07:20.989 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:07:20.999 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:254) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:07:20.999 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-05 07:07:21.030 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8303 bytes) 
2025-06-05 07:07:21.033 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8303 bytes) 
2025-06-05 07:07:21.040 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-05 07:07:21.040 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 07:07:21.145 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 18.598483 ms
2025-06-05 07:07:21.180 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.330659 ms
2025-06-05 07:07:21.196 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.367375 ms
2025-06-05 07:07:21.207 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:07:21.207 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:07:21.229 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:07:21.229 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:07:21.258 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:07:21.258 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:07:21.258 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082041258
2025-06-05 07:07:21.259 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:07:21.259 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:07:21.259 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082041258
2025-06-05 07:07:21.260 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor-1, groupId=spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 07:07:21.260 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor-2, groupId=spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor] Assigned to partition(s): clickstream-events-1
2025-06-05 07:07:21.267 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor-2, groupId=spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:07:21.267 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor-1, groupId=spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:07:21.275 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor-2, groupId=spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:07:21.275 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor-1, groupId=spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:07:21.303 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor-2, groupId=spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:07:21.303 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor-1, groupId=spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:07:21.805 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor-1, groupId=spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:07:21.805 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor-2, groupId=spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:07:21.806 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor-1, groupId=spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:07:21.806 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor-2, groupId=spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:07:21.806 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor-2, groupId=spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:07:21.806 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor-1, groupId=spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:07:21.886 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 2342 bytes result sent to driver
2025-06-05 07:07:21.886 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2342 bytes result sent to driver
2025-06-05 07:07:21.892 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 859 ms on phamviethoa (executor driver) (1/2)
2025-06-05 07:07:21.892 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 868 ms on phamviethoa (executor driver) (2/2)
2025-06-05 07:07:21.893 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 07:07:21.898 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (start at SparkClickstreamProcessor.java:254) finished in 0.991 s
2025-06-05 07:07:21.899 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
2025-06-05 07:07:21.899 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - running: Set()
2025-06-05 07:07:21.899 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
2025-06-05 07:07:21.899 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - failed: Set()
2025-06-05 07:07:21.900 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:254), which has no missing parents
2025-06-05 07:07:21.905 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-05 07:07:21.907 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-05 07:07:21.907 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:38047 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-05 07:07:21.908 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:07:21.909 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:254) (first 15 tasks are for partitions Vector(0))
2025-06-05 07:07:21.909 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-05 07:07:21.912 [dispatcher-event-loop-12 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
2025-06-05 07:07:21.913 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-05 07:07:21.941 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-05 07:07:21.942 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
2025-06-05 07:07:21.950 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 4038 bytes result sent to driver
2025-06-05 07:07:21.951 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 40 ms on phamviethoa (executor driver) (1/1)
2025-06-05 07:07:21.951 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-05 07:07:21.952 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:254) finished in 0.049 s
2025-06-05 07:07:21.953 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 07:07:21.953 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-05 07:07:21.954 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:254, took 1.073167 s
2025-06-05 07:07:22.109 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] terminated with error
org.apache.spark.sql.AnalysisException: Column eventId not found in schema Some(StructType(StructField(event_id,StringType,false),StructField(event_name,StringType,false),StructField(event_time,TimestampType,false),StructField(user_id,StringType,false),StructField(session_id,StringType,false),StructField(app_id,StringType,false),StructField(platform,StringType,false),StructField(page_url,StringType,false),StructField(geo_country,StringType,false),StructField(geo_city,StringType,false),StructField(traffic_source,StringType,false),StructField(traffic_medium,StringType,false),StructField(item_id,StringType,true),StructField(item_price,DoubleType,true),StructField(kafka_timestamp,TimestampType,false),StructField(processing_time,TimestampType,false))).
	at org.apache.spark.sql.errors.QueryCompilationErrors$.columnNotFoundInSchemaError(QueryCompilationErrors.scala:1612)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$4(JdbcUtils.scala:126)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$2(JdbcUtils.scala:126)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getInsertStatement(JdbcUtils.scala:124)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:883)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.SparkClickstreamProcessor.lambda$main$a450ce84$1(SparkClickstreamProcessor.java:250)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
2025-06-05 07:07:22.110 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 07:07:22.112 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:07:22.112 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:07:22.112 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:07:22.113 [stream execution thread for [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = a8c8f3a9-3ed1-4d90-ae6d-589289d87234, runId = 4e6a9a3b-93a8-4f6d-9e67-0bedb9fac662] has been shutdown
2025-06-05 07:07:22.124 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor-1, groupId=spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:07:22.124 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor-1, groupId=spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:07:22.127 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:07:22.127 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:07:22.128 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:07:22.128 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:07:22.130 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor-1 unregistered
2025-06-05 07:07:22.131 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor-2, groupId=spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:07:22.131 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor-2, groupId=spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:07:22.133 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:07:22.133 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:07:22.133 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:07:22.133 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:07:22.135 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-5e3ed204-8e02-411c-99b9-af645b4dbc29--1200043955-executor-2 unregistered
2025-06-05 07:07:22.136 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 07:07:22.136 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 07:07:22.141 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@4a68135e{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:07:22.143 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 07:07:22.150 [dispatcher-event-loop-1 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 07:07:22.156 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 07:07:22.156 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 07:07:22.160 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 07:07:22.162 [dispatcher-event-loop-5 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 07:07:22.166 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 07:07:22.166 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 07:07:22.167 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-42274b3c-679d-4a92-b5f3-2716d33e1d6f
2025-06-05 07:07:22.168 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-4fc17f85-fdbe-4c92-8ac8-24f1324dd122
2025-06-05 07:07:54.797 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 07:07:54.883 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 07:07:54.927 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:07:54.927 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 07:07:54.927 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:07:54.927 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 07:07:54.938 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 07:07:54.944 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 07:07:54.944 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 07:07:54.971 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 07:07:54.971 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 07:07:54.972 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 07:07:54.972 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 07:07:54.972 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 07:07:55.093 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 39685.
2025-06-05 07:07:55.106 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 07:07:55.121 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 07:07:55.130 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 07:07:55.130 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 07:07:55.132 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 07:07:55.142 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-162dc9c7-66af-45b7-b283-9533628aef9d
2025-06-05 07:07:55.157 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 07:07:55.165 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 07:07:55.184 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @954ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 07:07:55.227 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 07:07:55.233 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 07:07:55.240 [main INFO ] org.sparkproject.jetty.server.Server - Started @1010ms
2025-06-05 07:07:55.256 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@37fe1824{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:07:55.256 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 07:07:55.268 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35835e65{/,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.312 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 07:07:55.316 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 07:07:55.330 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41807.
2025-06-05 07:07:55.330 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:41807
2025-06-05 07:07:55.331 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 07:07:55.336 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 41807, None)
2025-06-05 07:07:55.339 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:41807 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 41807, None)
2025-06-05 07:07:55.341 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 41807, None)
2025-06-05 07:07:55.342 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 41807, None)
2025-06-05 07:07:55.420 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@35835e65{/,null,STOPPED,@Spark}
2025-06-05 07:07:55.421 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/jobs,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.422 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.422 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cc9385{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.423 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7915bca3{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.423 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/stages,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.423 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67b4ec{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.424 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.425 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.425 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.425 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.426 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/storage,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.426 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.427 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.427 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.427 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/environment,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.428 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.428 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/executors,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.429 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397ef2{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.429 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e93c1f{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.430 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9b21bd3{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.434 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7661b5a{/static,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.435 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b6d92e{/,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.435 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7899de11{/api,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.436 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f951a7f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.436 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c777e7b{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.438 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62db3891{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.518 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 07:07:55.522 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 07:07:55.529 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6732726{/SQL,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.530 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d64c581{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.530 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d64c100{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.531 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fdf17dc{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:55.537 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ff8a9dc{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:07:56.869 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-05 07:07:56.878 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ff81b0d{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-05 07:07:56.878 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@773c7147{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:56.879 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a5066f5{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-05 07:07:56.879 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1191029d{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-05 07:07:56.880 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b849fa6{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:07:56.884 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-9b839fde-a528-4fe6-820c-70df68b35e14. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-05 07:07:56.896 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-9b839fde-a528-4fe6-820c-70df68b35e14 resolved to file:/tmp/temporary-9b839fde-a528-4fe6-820c-70df68b35e14.
2025-06-05 07:07:56.896 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-05 07:07:56.941 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-9b839fde-a528-4fe6-820c-70df68b35e14/metadata using temp file file:/tmp/temporary-9b839fde-a528-4fe6-820c-70df68b35e14/.metadata.6c243b68-d0a7-439a-80dc-13671f9da799.tmp
2025-06-05 07:07:56.992 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-9b839fde-a528-4fe6-820c-70df68b35e14/.metadata.6c243b68-d0a7-439a-80dc-13671f9da799.tmp to file:/tmp/temporary-9b839fde-a528-4fe6-820c-70df68b35e14/metadata
2025-06-05 07:07:57.007 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571]. Use file:/tmp/temporary-9b839fde-a528-4fe6-820c-70df68b35e14 to store the query checkpoint.
2025-06-05 07:07:57.014 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@212e8041] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@57b00a30]
2025-06-05 07:07:57.028 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:07:57.028 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:07:57.029 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-05 07:07:57.030 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-05 07:07:57.188 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 07:07:57.218 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 07:07:57.219 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:07:57.219 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:07:57.219 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082077218
2025-06-05 07:07:57.374 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-9b839fde-a528-4fe6-820c-70df68b35e14/sources/0/0 using temp file file:/tmp/temporary-9b839fde-a528-4fe6-820c-70df68b35e14/sources/0/.0.44eedd9f-16ba-4801-9719-7d67d0923c3f.tmp
2025-06-05 07:07:57.385 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-9b839fde-a528-4fe6-820c-70df68b35e14/sources/0/.0.44eedd9f-16ba-4801-9719-7d67d0923c3f.tmp to file:/tmp/temporary-9b839fde-a528-4fe6-820c-70df68b35e14/sources/0/0
2025-06-05 07:07:57.386 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-05 07:07:57.408 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-9b839fde-a528-4fe6-820c-70df68b35e14/offsets/0 using temp file file:/tmp/temporary-9b839fde-a528-4fe6-820c-70df68b35e14/offsets/.0.7c071d6e-9a87-4cec-82d9-4e9523770cae.tmp
2025-06-05 07:07:57.426 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-9b839fde-a528-4fe6-820c-70df68b35e14/offsets/.0.7c071d6e-9a87-4cec-82d9-4e9523770cae.tmp to file:/tmp/temporary-9b839fde-a528-4fe6-820c-70df68b35e14/offsets/0
2025-06-05 07:07:57.427 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749082077400,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-05 07:07:57.571 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082077400
2025-06-05 07:07:57.618 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:07:57.645 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:07:57.679 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082077400
2025-06-05 07:07:57.681 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:07:57.682 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:07:57.907 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 137.104827 ms
2025-06-05 07:07:58.018 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.287167 ms
2025-06-05 07:07:58.035 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.596965 ms
2025-06-05 07:07:58.083 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:254
2025-06-05 07:07:58.091 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:254) as input to shuffle 0
2025-06-05 07:07:58.094 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:254) with 1 output partitions
2025-06-05 07:07:58.094 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:254)
2025-06-05 07:07:58.094 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-05 07:07:58.095 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
2025-06-05 07:07:58.097 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:254), which has no missing parents
2025-06-05 07:07:58.158 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 46.1 KiB, free 9.2 GiB)
2025-06-05 07:07:58.175 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 9.2 GiB)
2025-06-05 07:07:58.176 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:41807 (size: 17.9 KiB, free: 9.2 GiB)
2025-06-05 07:07:58.178 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:07:58.188 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:254) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:07:58.189 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-05 07:07:58.223 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8302 bytes) 
2025-06-05 07:07:58.225 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8302 bytes) 
2025-06-05 07:07:58.232 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-05 07:07:58.232 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 07:07:58.327 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 13.713602 ms
2025-06-05 07:07:58.358 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.234058 ms
2025-06-05 07:07:58.371 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.405935 ms
2025-06-05 07:07:58.381 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:07:58.381 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:07:58.400 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:07:58.400 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:07:58.427 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:07:58.427 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:07:58.427 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082078427
2025-06-05 07:07:58.428 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:07:58.428 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:07:58.428 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082078427
2025-06-05 07:07:58.428 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor-1, groupId=spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor] Assigned to partition(s): clickstream-events-1
2025-06-05 07:07:58.428 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor-2, groupId=spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 07:07:58.433 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor-2, groupId=spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:07:58.433 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor-1, groupId=spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:07:58.438 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor-2, groupId=spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:07:58.438 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor-1, groupId=spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:07:58.460 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor-1, groupId=spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:07:58.460 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor-2, groupId=spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:07:58.962 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor-1, groupId=spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:07:58.962 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor-2, groupId=spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:07:58.963 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor-1, groupId=spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:07:58.963 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor-2, groupId=spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:07:58.963 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor-2, groupId=spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:07:58.963 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor-1, groupId=spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:07:59.043 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2342 bytes result sent to driver
2025-06-05 07:07:59.043 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 2342 bytes result sent to driver
2025-06-05 07:07:59.049 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 824 ms on phamviethoa (executor driver) (1/2)
2025-06-05 07:07:59.050 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 833 ms on phamviethoa (executor driver) (2/2)
2025-06-05 07:07:59.051 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 07:07:59.057 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (start at SparkClickstreamProcessor.java:254) finished in 0.952 s
2025-06-05 07:07:59.057 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
2025-06-05 07:07:59.058 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - running: Set()
2025-06-05 07:07:59.058 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
2025-06-05 07:07:59.058 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - failed: Set()
2025-06-05 07:07:59.060 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:254), which has no missing parents
2025-06-05 07:07:59.066 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-05 07:07:59.068 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-05 07:07:59.069 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:41807 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-05 07:07:59.069 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:07:59.070 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:254) (first 15 tasks are for partitions Vector(0))
2025-06-05 07:07:59.070 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-05 07:07:59.074 [dispatcher-event-loop-12 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
2025-06-05 07:07:59.074 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-05 07:07:59.106 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-05 07:07:59.107 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
2025-06-05 07:07:59.115 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 4038 bytes result sent to driver
2025-06-05 07:07:59.117 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 45 ms on phamviethoa (executor driver) (1/1)
2025-06-05 07:07:59.117 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-05 07:07:59.117 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:254) finished in 0.052 s
2025-06-05 07:07:59.119 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 07:07:59.119 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-05 07:07:59.120 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:254, took 1.036572 s
2025-06-05 07:07:59.270 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] terminated with error
org.apache.spark.sql.AnalysisException: Column eventName not found in schema Some(StructType(StructField(event_id,StringType,false),StructField(event_name,StringType,false),StructField(event_time,TimestampType,false),StructField(user_id,StringType,false),StructField(session_id,StringType,false),StructField(app_id,StringType,false),StructField(platform,StringType,false),StructField(page_url,StringType,false),StructField(geo_country,StringType,false),StructField(geo_city,StringType,false),StructField(traffic_source,StringType,false),StructField(traffic_medium,StringType,false),StructField(item_id,StringType,true),StructField(item_price,DoubleType,true),StructField(kafka_timestamp,TimestampType,false),StructField(processing_time,TimestampType,false))).
	at org.apache.spark.sql.errors.QueryCompilationErrors$.columnNotFoundInSchemaError(QueryCompilationErrors.scala:1612)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$4(JdbcUtils.scala:126)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$2(JdbcUtils.scala:126)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getInsertStatement(JdbcUtils.scala:124)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:883)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.SparkClickstreamProcessor.lambda$main$a450ce84$1(SparkClickstreamProcessor.java:250)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
2025-06-05 07:07:59.271 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 07:07:59.272 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:07:59.273 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:07:59.273 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:07:59.273 [stream execution thread for [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = e1563ba2-f266-4c54-86fe-8ee478b44025, runId = 045dbf8f-46d4-4dba-9eca-0d9f52867571] has been shutdown
2025-06-05 07:07:59.284 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor-1, groupId=spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:07:59.284 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor-1, groupId=spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:07:59.289 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:07:59.289 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:07:59.289 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:07:59.289 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:07:59.292 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor-1 unregistered
2025-06-05 07:07:59.292 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor-2, groupId=spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:07:59.292 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor-2, groupId=spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:07:59.294 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:07:59.295 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:07:59.295 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:07:59.295 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:07:59.298 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-b87a6cd8-8485-4178-ab50-0b89790d2a53-1906282558-executor-2 unregistered
2025-06-05 07:07:59.299 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 07:07:59.299 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 07:07:59.305 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@37fe1824{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:07:59.308 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 07:07:59.317 [dispatcher-event-loop-1 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 07:07:59.322 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 07:07:59.323 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 07:07:59.326 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 07:07:59.328 [dispatcher-event-loop-5 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 07:07:59.332 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 07:07:59.332 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 07:07:59.333 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-9b839fde-a528-4fe6-820c-70df68b35e14
2025-06-05 07:07:59.335 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-724e6145-2ad5-43a7-b295-00f1fa47f6b1
2025-06-05 07:08:14.867 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 07:08:14.955 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 07:08:14.999 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:08:14.999 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 07:08:14.999 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:08:14.999 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 07:08:15.010 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 07:08:15.016 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 07:08:15.016 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 07:08:15.044 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 07:08:15.044 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 07:08:15.044 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 07:08:15.045 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 07:08:15.045 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 07:08:15.167 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 41279.
2025-06-05 07:08:15.180 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 07:08:15.196 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 07:08:15.205 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 07:08:15.205 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 07:08:15.207 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 07:08:15.217 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-e6ebfb03-271e-4b00-a01d-6410cda6c6e1
2025-06-05 07:08:15.233 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 07:08:15.241 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 07:08:15.260 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1010ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 07:08:15.303 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 07:08:15.308 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 07:08:15.317 [main INFO ] org.sparkproject.jetty.server.Server - Started @1066ms
2025-06-05 07:08:15.344 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@57e2896e{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:08:15.344 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 07:08:15.354 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14379273{/,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.401 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 07:08:15.406 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 07:08:15.417 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43633.
2025-06-05 07:08:15.417 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:43633
2025-06-05 07:08:15.418 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 07:08:15.422 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 43633, None)
2025-06-05 07:08:15.424 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:43633 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 43633, None)
2025-06-05 07:08:15.427 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 43633, None)
2025-06-05 07:08:15.427 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 43633, None)
2025-06-05 07:08:15.502 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@14379273{/,null,STOPPED,@Spark}
2025-06-05 07:08:15.503 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70c53dbe{/jobs,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.503 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21c815e4{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.504 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@743e66f7{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.504 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241f05b{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.505 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/stages,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.506 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.507 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7915bca3{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.507 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.508 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67b4ec{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.508 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f91da5e{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.509 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79fd6f95{/storage,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.509 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.510 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.510 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.511 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/environment,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.511 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.512 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/executors,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.512 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.513 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.513 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.518 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/static,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.519 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51288417{/,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.520 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e0895f5{/api,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.520 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5292ceca{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.521 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e9ef5b6{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.523 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64984b0f{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.607 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 07:08:15.611 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 07:08:15.618 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f1a4795{/SQL,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.618 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c5ddccd{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.619 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@201c3cda{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.619 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d97caa4{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:15.625 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c83ae01{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:08:16.899 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-05 07:08:16.908 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@be1c08a{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-05 07:08:16.908 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d7b3b18{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:16.909 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e6b379c{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-05 07:08:16.910 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ff81b0d{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:16.910 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a5066f5{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:08:16.914 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-5fc41f1a-849d-4845-9c6a-aba40d1026eb. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-05 07:08:16.926 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-5fc41f1a-849d-4845-9c6a-aba40d1026eb resolved to file:/tmp/temporary-5fc41f1a-849d-4845-9c6a-aba40d1026eb.
2025-06-05 07:08:16.927 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-05 07:08:16.972 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-5fc41f1a-849d-4845-9c6a-aba40d1026eb/metadata using temp file file:/tmp/temporary-5fc41f1a-849d-4845-9c6a-aba40d1026eb/.metadata.7a5b294b-9919-418c-bb43-1edff95323b9.tmp
2025-06-05 07:08:17.027 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-5fc41f1a-849d-4845-9c6a-aba40d1026eb/.metadata.7a5b294b-9919-418c-bb43-1edff95323b9.tmp to file:/tmp/temporary-5fc41f1a-849d-4845-9c6a-aba40d1026eb/metadata
2025-06-05 07:08:17.045 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2]. Use file:/tmp/temporary-5fc41f1a-849d-4845-9c6a-aba40d1026eb to store the query checkpoint.
2025-06-05 07:08:17.050 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@18b3a317] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@28a4b000]
2025-06-05 07:08:17.064 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:08:17.065 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:08:17.065 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-05 07:08:17.066 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-05 07:08:17.234 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 07:08:17.263 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 07:08:17.264 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:08:17.264 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:08:17.264 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082097263
2025-06-05 07:08:17.445 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-5fc41f1a-849d-4845-9c6a-aba40d1026eb/sources/0/0 using temp file file:/tmp/temporary-5fc41f1a-849d-4845-9c6a-aba40d1026eb/sources/0/.0.fadeb29a-2ee5-41f1-a7a2-f5d6ca21f518.tmp
2025-06-05 07:08:17.459 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-5fc41f1a-849d-4845-9c6a-aba40d1026eb/sources/0/.0.fadeb29a-2ee5-41f1-a7a2-f5d6ca21f518.tmp to file:/tmp/temporary-5fc41f1a-849d-4845-9c6a-aba40d1026eb/sources/0/0
2025-06-05 07:08:17.460 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-05 07:08:17.471 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-5fc41f1a-849d-4845-9c6a-aba40d1026eb/offsets/0 using temp file file:/tmp/temporary-5fc41f1a-849d-4845-9c6a-aba40d1026eb/offsets/.0.a9d57343-f285-4c6c-9a71-e85a39d37888.tmp
2025-06-05 07:08:17.489 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-5fc41f1a-849d-4845-9c6a-aba40d1026eb/offsets/.0.a9d57343-f285-4c6c-9a71-e85a39d37888.tmp to file:/tmp/temporary-5fc41f1a-849d-4845-9c6a-aba40d1026eb/offsets/0
2025-06-05 07:08:17.489 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749082097467,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-05 07:08:17.654 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082097467
2025-06-05 07:08:17.708 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:08:17.735 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:08:17.767 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082097467
2025-06-05 07:08:17.770 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:08:17.771 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:08:18.021 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 146.480156 ms
2025-06-05 07:08:18.149 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.003673 ms
2025-06-05 07:08:18.169 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 11.946504 ms
2025-06-05 07:08:18.220 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:256
2025-06-05 07:08:18.228 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:256) as input to shuffle 0
2025-06-05 07:08:18.232 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:256) with 1 output partitions
2025-06-05 07:08:18.232 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:256)
2025-06-05 07:08:18.232 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-05 07:08:18.233 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
2025-06-05 07:08:18.235 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:256), which has no missing parents
2025-06-05 07:08:18.312 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 46.1 KiB, free 9.2 GiB)
2025-06-05 07:08:18.332 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 9.2 GiB)
2025-06-05 07:08:18.333 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:43633 (size: 17.9 KiB, free: 9.2 GiB)
2025-06-05 07:08:18.336 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:08:18.344 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:256) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:08:18.345 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-05 07:08:18.380 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8302 bytes) 
2025-06-05 07:08:18.382 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8302 bytes) 
2025-06-05 07:08:18.389 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-05 07:08:18.389 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 07:08:18.488 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 15.776887 ms
2025-06-05 07:08:18.520 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.198121 ms
2025-06-05 07:08:18.532 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.064432 ms
2025-06-05 07:08:18.541 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:08:18.541 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:08:18.561 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:08:18.561 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:08:18.589 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:08:18.589 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:08:18.589 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082098589
2025-06-05 07:08:18.589 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:08:18.590 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:08:18.590 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082098589
2025-06-05 07:08:18.590 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor-1, groupId=spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 07:08:18.590 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor-2, groupId=spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor] Assigned to partition(s): clickstream-events-1
2025-06-05 07:08:18.595 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor-1, groupId=spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:08:18.595 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor-2, groupId=spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:08:18.600 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor-1, groupId=spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:08:18.601 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor-2, groupId=spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:08:18.623 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor-1, groupId=spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:08:18.623 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor-2, groupId=spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:08:19.125 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor-1, groupId=spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:08:19.125 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor-2, groupId=spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:08:19.125 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor-2, groupId=spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:08:19.125 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor-1, groupId=spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:08:19.126 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor-2, groupId=spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:08:19.126 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor-1, groupId=spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:08:19.207 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 2342 bytes result sent to driver
2025-06-05 07:08:19.207 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2342 bytes result sent to driver
2025-06-05 07:08:19.212 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 831 ms on phamviethoa (executor driver) (1/2)
2025-06-05 07:08:19.213 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 840 ms on phamviethoa (executor driver) (2/2)
2025-06-05 07:08:19.214 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 07:08:19.218 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (start at SparkClickstreamProcessor.java:256) finished in 0.976 s
2025-06-05 07:08:19.218 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
2025-06-05 07:08:19.219 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - running: Set()
2025-06-05 07:08:19.219 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
2025-06-05 07:08:19.219 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - failed: Set()
2025-06-05 07:08:19.220 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:256), which has no missing parents
2025-06-05 07:08:19.225 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-05 07:08:19.227 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-05 07:08:19.228 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:43633 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-05 07:08:19.228 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:08:19.229 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:256) (first 15 tasks are for partitions Vector(0))
2025-06-05 07:08:19.230 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-05 07:08:19.234 [dispatcher-event-loop-12 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
2025-06-05 07:08:19.234 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-05 07:08:19.259 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-05 07:08:19.260 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
2025-06-05 07:08:19.268 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 4038 bytes result sent to driver
2025-06-05 07:08:19.269 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 37 ms on phamviethoa (executor driver) (1/1)
2025-06-05 07:08:19.269 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-05 07:08:19.270 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:256) finished in 0.046 s
2025-06-05 07:08:19.272 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 07:08:19.273 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-05 07:08:19.274 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:256, took 1.053581 s
2025-06-05 07:08:19.430 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] terminated with error
org.apache.spark.sql.AnalysisException: Column sessionId not found in schema Some(StructType(StructField(event_id,StringType,false),StructField(event_name,StringType,false),StructField(event_time,TimestampType,false),StructField(user_id,StringType,false),StructField(session_id,StringType,false),StructField(app_id,StringType,false),StructField(platform,StringType,false),StructField(page_url,StringType,false),StructField(geo_country,StringType,false),StructField(geo_city,StringType,false),StructField(traffic_source,StringType,false),StructField(traffic_medium,StringType,false),StructField(item_id,StringType,true),StructField(item_price,DoubleType,true),StructField(kafka_timestamp,TimestampType,false),StructField(processing_time,TimestampType,false))).
	at org.apache.spark.sql.errors.QueryCompilationErrors$.columnNotFoundInSchemaError(QueryCompilationErrors.scala:1612)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$4(JdbcUtils.scala:126)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$2(JdbcUtils.scala:126)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getInsertStatement(JdbcUtils.scala:124)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:883)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.SparkClickstreamProcessor.lambda$main$a450ce84$1(SparkClickstreamProcessor.java:252)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
2025-06-05 07:08:19.431 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 07:08:19.433 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:08:19.433 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:08:19.433 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:08:19.434 [stream execution thread for [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = 45a1c6aa-9048-4609-8de4-a73a216ac1b3, runId = 15b9a7ee-3042-42ff-bb8a-8538a5a127d2] has been shutdown
2025-06-05 07:08:19.448 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor-1, groupId=spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:08:19.448 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor-1, groupId=spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:08:19.453 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:08:19.456 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:08:19.456 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:08:19.456 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:08:19.460 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor-1 unregistered
2025-06-05 07:08:19.461 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor-2, groupId=spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:08:19.461 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor-2, groupId=spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:08:19.463 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:08:19.463 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:08:19.463 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:08:19.463 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:08:19.466 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-85e1bf49-1160-4305-b086-6b960870fd95--844819993-executor-2 unregistered
2025-06-05 07:08:19.467 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 07:08:19.467 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 07:08:19.473 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@57e2896e{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:08:19.475 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 07:08:19.483 [dispatcher-event-loop-1 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 07:08:19.491 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 07:08:19.491 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 07:08:19.495 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 07:08:19.496 [dispatcher-event-loop-5 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 07:08:19.499 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 07:08:19.499 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 07:08:19.500 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-5fc41f1a-849d-4845-9c6a-aba40d1026eb
2025-06-05 07:08:19.502 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-c121fa5e-0704-4bb1-8910-9ca60ca40bdc
2025-06-05 07:08:36.784 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 07:08:36.864 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 07:08:36.908 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:08:36.908 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 07:08:36.908 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:08:36.908 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 07:08:36.919 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 07:08:36.925 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 07:08:36.925 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 07:08:36.952 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 07:08:36.953 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 07:08:36.953 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 07:08:36.953 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 07:08:36.953 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 07:08:37.072 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 39215.
2025-06-05 07:08:37.086 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 07:08:37.102 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 07:08:37.111 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 07:08:37.112 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 07:08:37.113 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 07:08:37.124 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-82d18408-5a4c-4fcb-bb66-cc19bc6b2799
2025-06-05 07:08:37.140 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 07:08:37.148 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 07:08:37.168 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1055ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 07:08:37.211 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 07:08:37.217 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 07:08:37.225 [main INFO ] org.sparkproject.jetty.server.Server - Started @1113ms
2025-06-05 07:08:37.241 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@398a9fd2{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:08:37.241 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 07:08:37.253 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46d9aec8{/,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.302 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 07:08:37.307 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 07:08:37.318 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38225.
2025-06-05 07:08:37.318 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:38225
2025-06-05 07:08:37.319 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 07:08:37.322 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 38225, None)
2025-06-05 07:08:37.325 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:38225 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 38225, None)
2025-06-05 07:08:37.327 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 38225, None)
2025-06-05 07:08:37.328 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 38225, None)
2025-06-05 07:08:37.404 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@46d9aec8{/,null,STOPPED,@Spark}
2025-06-05 07:08:37.405 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d51e129{/jobs,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.405 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1894e40d{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.406 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15383681{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.406 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@109a2025{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.407 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@761956ac{/stages,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.407 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@304d0259{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.408 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cf518cf{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.408 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68d651f2{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.409 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e43e323{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.409 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10643593{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.409 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@eca6a74{/storage,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.410 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48840594{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.410 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14823f76{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.411 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ed16657{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.411 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@113e13f9{/environment,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.411 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7979b8b7{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.412 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bc49bc5{/executors,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.412 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f66ffc8{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.413 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2def7a7a{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.413 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c080ef3{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.417 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ee6291f{/static,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.418 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46185a1b{/,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.419 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60cf62ad{/api,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.420 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@644ded04{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.420 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13d9261f{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.423 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@582a764a{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.504 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 07:08:37.508 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 07:08:37.515 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58d6b7b9{/SQL,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.515 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a6f6c7e{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.516 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7fb48179{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.517 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c86da0c{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:37.523 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47d023b7{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:08:38.837 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-05 07:08:38.846 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2210e466{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-05 07:08:38.847 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10408ea{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:38.847 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58a8ea6f{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-05 07:08:38.848 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e7e7a7e{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-05 07:08:38.849 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7296fe0b{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:08:38.852 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-217ee8a9-9381-4c83-b84e-ad4af0752e69. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-05 07:08:38.870 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-217ee8a9-9381-4c83-b84e-ad4af0752e69 resolved to file:/tmp/temporary-217ee8a9-9381-4c83-b84e-ad4af0752e69.
2025-06-05 07:08:38.870 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-05 07:08:38.914 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-217ee8a9-9381-4c83-b84e-ad4af0752e69/metadata using temp file file:/tmp/temporary-217ee8a9-9381-4c83-b84e-ad4af0752e69/.metadata.30c0ebcb-0271-43b0-8bc7-39d64abbf8e7.tmp
2025-06-05 07:08:38.964 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-217ee8a9-9381-4c83-b84e-ad4af0752e69/.metadata.30c0ebcb-0271-43b0-8bc7-39d64abbf8e7.tmp to file:/tmp/temporary-217ee8a9-9381-4c83-b84e-ad4af0752e69/metadata
2025-06-05 07:08:38.979 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5]. Use file:/tmp/temporary-217ee8a9-9381-4c83-b84e-ad4af0752e69 to store the query checkpoint.
2025-06-05 07:08:38.986 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@63c16bc1] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@64e08921]
2025-06-05 07:08:38.999 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:08:39.000 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:08:39.000 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-05 07:08:39.001 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-05 07:08:39.150 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 07:08:39.178 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 07:08:39.178 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:08:39.178 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:08:39.178 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082119178
2025-06-05 07:08:39.340 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-217ee8a9-9381-4c83-b84e-ad4af0752e69/sources/0/0 using temp file file:/tmp/temporary-217ee8a9-9381-4c83-b84e-ad4af0752e69/sources/0/.0.589efcfd-39aa-44c9-8a8b-eda237f8c7e7.tmp
2025-06-05 07:08:39.352 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-217ee8a9-9381-4c83-b84e-ad4af0752e69/sources/0/.0.589efcfd-39aa-44c9-8a8b-eda237f8c7e7.tmp to file:/tmp/temporary-217ee8a9-9381-4c83-b84e-ad4af0752e69/sources/0/0
2025-06-05 07:08:39.352 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-05 07:08:39.364 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-217ee8a9-9381-4c83-b84e-ad4af0752e69/offsets/0 using temp file file:/tmp/temporary-217ee8a9-9381-4c83-b84e-ad4af0752e69/offsets/.0.2c119a88-30a1-439c-afbf-57a881e150b4.tmp
2025-06-05 07:08:39.382 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-217ee8a9-9381-4c83-b84e-ad4af0752e69/offsets/.0.2c119a88-30a1-439c-afbf-57a881e150b4.tmp to file:/tmp/temporary-217ee8a9-9381-4c83-b84e-ad4af0752e69/offsets/0
2025-06-05 07:08:39.383 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749082119360,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-05 07:08:39.550 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082119360
2025-06-05 07:08:39.601 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:08:39.627 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:08:39.660 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082119360
2025-06-05 07:08:39.662 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:08:39.663 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:08:39.907 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 146.954778 ms
2025-06-05 07:08:40.025 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.364675 ms
2025-06-05 07:08:40.041 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.944454 ms
2025-06-05 07:08:40.088 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:257
2025-06-05 07:08:40.098 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:257) as input to shuffle 0
2025-06-05 07:08:40.101 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:257) with 1 output partitions
2025-06-05 07:08:40.101 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:257)
2025-06-05 07:08:40.101 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-05 07:08:40.102 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
2025-06-05 07:08:40.104 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:257), which has no missing parents
2025-06-05 07:08:40.171 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 46.1 KiB, free 9.2 GiB)
2025-06-05 07:08:40.191 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 9.2 GiB)
2025-06-05 07:08:40.193 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:38225 (size: 17.9 KiB, free: 9.2 GiB)
2025-06-05 07:08:40.196 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:08:40.205 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:257) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:08:40.205 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-05 07:08:40.235 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8302 bytes) 
2025-06-05 07:08:40.237 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8302 bytes) 
2025-06-05 07:08:40.244 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 07:08:40.244 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-05 07:08:40.341 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 21.074476 ms
2025-06-05 07:08:40.369 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.113572 ms
2025-06-05 07:08:40.385 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.471379 ms
2025-06-05 07:08:40.401 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:08:40.401 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:08:40.421 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:08:40.421 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:08:40.452 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:08:40.452 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:08:40.452 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082120452
2025-06-05 07:08:40.452 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:08:40.453 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:08:40.453 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082120452
2025-06-05 07:08:40.453 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor-2, groupId=spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor] Assigned to partition(s): clickstream-events-1
2025-06-05 07:08:40.453 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor-1, groupId=spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 07:08:40.458 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor-1, groupId=spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:08:40.458 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor-2, groupId=spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:08:40.464 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor-1, groupId=spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:08:40.464 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor-2, groupId=spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:08:40.487 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor-2, groupId=spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:08:40.487 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor-1, groupId=spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:08:40.989 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor-1, groupId=spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:08:40.989 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor-1, groupId=spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:08:40.989 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor-2, groupId=spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:08:40.989 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor-2, groupId=spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:08:40.990 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor-2, groupId=spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:08:40.990 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor-1, groupId=spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:08:41.074 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2342 bytes result sent to driver
2025-06-05 07:08:41.074 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 2342 bytes result sent to driver
2025-06-05 07:08:41.086 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 849 ms on phamviethoa (executor driver) (1/2)
2025-06-05 07:08:41.087 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 858 ms on phamviethoa (executor driver) (2/2)
2025-06-05 07:08:41.087 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 07:08:41.092 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (start at SparkClickstreamProcessor.java:257) finished in 0.981 s
2025-06-05 07:08:41.093 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
2025-06-05 07:08:41.093 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - running: Set()
2025-06-05 07:08:41.093 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
2025-06-05 07:08:41.093 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - failed: Set()
2025-06-05 07:08:41.094 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:257), which has no missing parents
2025-06-05 07:08:41.098 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-05 07:08:41.102 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-05 07:08:41.104 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:38225 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-05 07:08:41.104 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:08:41.105 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:257) (first 15 tasks are for partitions Vector(0))
2025-06-05 07:08:41.105 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-05 07:08:41.109 [dispatcher-event-loop-12 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
2025-06-05 07:08:41.109 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-05 07:08:41.135 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-05 07:08:41.136 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
2025-06-05 07:08:41.144 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 4038 bytes result sent to driver
2025-06-05 07:08:41.146 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 39 ms on phamviethoa (executor driver) (1/1)
2025-06-05 07:08:41.146 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-05 07:08:41.146 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:257) finished in 0.049 s
2025-06-05 07:08:41.148 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 07:08:41.148 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-05 07:08:41.149 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:257, took 1.060794 s
2025-06-05 07:08:41.310 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] terminated with error
org.apache.spark.sql.AnalysisException: Column appId not found in schema Some(StructType(StructField(event_id,StringType,false),StructField(event_name,StringType,false),StructField(event_time,TimestampType,false),StructField(user_id,StringType,false),StructField(session_id,StringType,false),StructField(app_id,StringType,false),StructField(platform,StringType,false),StructField(page_url,StringType,false),StructField(geo_country,StringType,false),StructField(geo_city,StringType,false),StructField(traffic_source,StringType,false),StructField(traffic_medium,StringType,false),StructField(item_id,StringType,true),StructField(item_price,DoubleType,true),StructField(kafka_timestamp,TimestampType,false),StructField(processing_time,TimestampType,false))).
	at org.apache.spark.sql.errors.QueryCompilationErrors$.columnNotFoundInSchemaError(QueryCompilationErrors.scala:1612)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$4(JdbcUtils.scala:126)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$2(JdbcUtils.scala:126)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getInsertStatement(JdbcUtils.scala:124)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:883)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.SparkClickstreamProcessor.lambda$main$a450ce84$1(SparkClickstreamProcessor.java:253)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
2025-06-05 07:08:41.311 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 07:08:41.313 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:08:41.313 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:08:41.313 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:08:41.314 [stream execution thread for [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = 1c724d7b-a74e-45e1-84c5-a008fe0bb424, runId = 7687bddf-a55c-4cd6-b8ea-c9cf3616d6a5] has been shutdown
2025-06-05 07:08:41.326 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor-2, groupId=spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:08:41.326 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor-2, groupId=spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:08:41.329 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:08:41.330 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:08:41.330 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:08:41.330 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:08:41.331 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor-2 unregistered
2025-06-05 07:08:41.332 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor-1, groupId=spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:08:41.332 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor-1, groupId=spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:08:41.336 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:08:41.336 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:08:41.336 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:08:41.336 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:08:41.337 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-a38f2a6e-2d1a-4f80-bf5b-ad813aa68766-1545338597-executor-1 unregistered
2025-06-05 07:08:41.338 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 07:08:41.338 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 07:08:41.342 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@398a9fd2{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:08:41.344 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 07:08:41.352 [dispatcher-event-loop-1 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 07:08:41.358 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 07:08:41.359 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 07:08:41.362 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 07:08:41.363 [dispatcher-event-loop-5 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 07:08:41.366 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 07:08:41.366 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 07:08:41.367 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-12da00b2-8b7d-4212-bcf0-80575b5130d8
2025-06-05 07:08:41.368 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-217ee8a9-9381-4c83-b84e-ad4af0752e69
2025-06-05 07:11:05.259 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 07:11:05.345 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 07:11:05.389 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:11:05.389 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 07:11:05.390 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:11:05.390 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 07:11:05.401 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 07:11:05.407 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 07:11:05.407 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 07:11:05.435 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 07:11:05.435 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 07:11:05.435 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 07:11:05.435 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 07:11:05.435 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 07:11:05.553 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 43615.
2025-06-05 07:11:05.566 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 07:11:05.583 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 07:11:05.592 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 07:11:05.593 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 07:11:05.594 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 07:11:05.604 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-faff84f3-935c-4368-a8cf-473afb9ddec3
2025-06-05 07:11:05.621 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 07:11:05.629 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 07:11:05.648 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1001ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 07:11:05.690 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 07:11:05.695 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 07:11:05.703 [main INFO ] org.sparkproject.jetty.server.Server - Started @1057ms
2025-06-05 07:11:05.718 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@28955922{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:11:05.718 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 07:11:05.729 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35835e65{/,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.774 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 07:11:05.778 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 07:11:05.789 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43039.
2025-06-05 07:11:05.789 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:43039
2025-06-05 07:11:05.791 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 07:11:05.795 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 43039, None)
2025-06-05 07:11:05.798 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:43039 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 43039, None)
2025-06-05 07:11:05.800 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 43039, None)
2025-06-05 07:11:05.800 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 43039, None)
2025-06-05 07:11:05.877 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@35835e65{/,null,STOPPED,@Spark}
2025-06-05 07:11:05.878 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/jobs,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.878 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.879 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cc9385{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.879 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7915bca3{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.879 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/stages,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.880 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67b4ec{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.881 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.881 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.882 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.882 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.883 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/storage,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.883 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.884 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.884 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.885 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/environment,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.885 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.886 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/executors,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.886 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397ef2{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.887 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e93c1f{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.887 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9b21bd3{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.892 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7661b5a{/static,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.892 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b6d92e{/,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.893 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7899de11{/api,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.893 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f951a7f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.894 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c777e7b{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.897 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62db3891{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.978 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 07:11:05.981 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 07:11:05.988 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6732726{/SQL,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.989 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d64c581{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.989 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d64c100{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.990 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fdf17dc{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:05.996 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ff8a9dc{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:11:07.318 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-05 07:11:07.326 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c610f{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-05 07:11:07.326 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5abc5854{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:07.327 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@578d472a{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-05 07:11:07.327 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54b2d002{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:07.328 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7fbbdd8a{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:11:07.332 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-7bc93bc0-353e-494b-bf68-05162dc1d466. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-05 07:11:07.343 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-7bc93bc0-353e-494b-bf68-05162dc1d466 resolved to file:/tmp/temporary-7bc93bc0-353e-494b-bf68-05162dc1d466.
2025-06-05 07:11:07.344 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-05 07:11:07.390 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-7bc93bc0-353e-494b-bf68-05162dc1d466/metadata using temp file file:/tmp/temporary-7bc93bc0-353e-494b-bf68-05162dc1d466/.metadata.68c32f88-ea5d-4435-9948-a6e60921d19b.tmp
2025-06-05 07:11:07.442 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-7bc93bc0-353e-494b-bf68-05162dc1d466/.metadata.68c32f88-ea5d-4435-9948-a6e60921d19b.tmp to file:/tmp/temporary-7bc93bc0-353e-494b-bf68-05162dc1d466/metadata
2025-06-05 07:11:07.456 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb]. Use file:/tmp/temporary-7bc93bc0-353e-494b-bf68-05162dc1d466 to store the query checkpoint.
2025-06-05 07:11:07.461 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7316a0a3] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@45c9e0c9]
2025-06-05 07:11:07.475 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:11:07.475 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:11:07.475 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-05 07:11:07.477 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-05 07:11:07.638 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 07:11:07.669 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 07:11:07.670 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:11:07.670 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:11:07.670 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082267669
2025-06-05 07:11:07.828 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-7bc93bc0-353e-494b-bf68-05162dc1d466/sources/0/0 using temp file file:/tmp/temporary-7bc93bc0-353e-494b-bf68-05162dc1d466/sources/0/.0.3249c8c9-fff3-4b96-9ce1-6c3be33a03bd.tmp
2025-06-05 07:11:07.840 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-7bc93bc0-353e-494b-bf68-05162dc1d466/sources/0/.0.3249c8c9-fff3-4b96-9ce1-6c3be33a03bd.tmp to file:/tmp/temporary-7bc93bc0-353e-494b-bf68-05162dc1d466/sources/0/0
2025-06-05 07:11:07.840 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-05 07:11:07.852 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-7bc93bc0-353e-494b-bf68-05162dc1d466/offsets/0 using temp file file:/tmp/temporary-7bc93bc0-353e-494b-bf68-05162dc1d466/offsets/.0.29ddf22e-78c0-448e-9a13-10a53c1a5791.tmp
2025-06-05 07:11:07.871 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-7bc93bc0-353e-494b-bf68-05162dc1d466/offsets/.0.29ddf22e-78c0-448e-9a13-10a53c1a5791.tmp to file:/tmp/temporary-7bc93bc0-353e-494b-bf68-05162dc1d466/offsets/0
2025-06-05 07:11:07.871 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749082267847,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-05 07:11:08.045 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082267847
2025-06-05 07:11:08.094 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:11:08.125 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:11:08.156 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082267847
2025-06-05 07:11:08.157 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:11:08.158 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:11:08.396 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 143.291302 ms
2025-06-05 07:11:08.506 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.652329 ms
2025-06-05 07:11:08.523 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.122921 ms
2025-06-05 07:11:08.572 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:268
2025-06-05 07:11:08.579 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:268) as input to shuffle 0
2025-06-05 07:11:08.582 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:268) with 1 output partitions
2025-06-05 07:11:08.582 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:268)
2025-06-05 07:11:08.582 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-05 07:11:08.583 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
2025-06-05 07:11:08.585 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:268), which has no missing parents
2025-06-05 07:11:08.651 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 46.1 KiB, free 9.2 GiB)
2025-06-05 07:11:08.670 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 9.2 GiB)
2025-06-05 07:11:08.671 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:43039 (size: 17.9 KiB, free: 9.2 GiB)
2025-06-05 07:11:08.674 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:11:08.682 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:268) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:11:08.683 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-05 07:11:08.717 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8301 bytes) 
2025-06-05 07:11:08.720 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8301 bytes) 
2025-06-05 07:11:08.727 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-05 07:11:08.727 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 07:11:08.836 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 19.731923 ms
2025-06-05 07:11:08.866 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.600905 ms
2025-06-05 07:11:08.884 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 11.472998 ms
2025-06-05 07:11:08.893 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:11:08.893 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:11:08.913 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:11:08.913 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:11:08.940 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:11:08.940 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:11:08.940 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082268940
2025-06-05 07:11:08.941 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:11:08.941 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:11:08.941 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082268940
2025-06-05 07:11:08.942 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor-2, groupId=spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 07:11:08.942 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor-1, groupId=spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor] Assigned to partition(s): clickstream-events-1
2025-06-05 07:11:08.946 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor-1, groupId=spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:11:08.946 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor-2, groupId=spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:11:08.951 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor-2, groupId=spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:11:08.951 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor-1, groupId=spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:11:08.974 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor-1, groupId=spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:11:08.974 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor-2, groupId=spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:11:09.475 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor-1, groupId=spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:11:09.475 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor-2, groupId=spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:11:09.475 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor-2, groupId=spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:11:09.475 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor-1, groupId=spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:11:09.476 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor-2, groupId=spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:11:09.476 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor-1, groupId=spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:11:09.559 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2342 bytes result sent to driver
2025-06-05 07:11:09.559 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 2342 bytes result sent to driver
2025-06-05 07:11:09.565 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 855 ms on phamviethoa (executor driver) (1/2)
2025-06-05 07:11:09.566 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 847 ms on phamviethoa (executor driver) (2/2)
2025-06-05 07:11:09.566 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 07:11:09.572 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (start at SparkClickstreamProcessor.java:268) finished in 0.980 s
2025-06-05 07:11:09.573 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
2025-06-05 07:11:09.573 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - running: Set()
2025-06-05 07:11:09.573 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
2025-06-05 07:11:09.574 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - failed: Set()
2025-06-05 07:11:09.576 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:268), which has no missing parents
2025-06-05 07:11:09.583 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-05 07:11:09.585 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-05 07:11:09.586 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:43039 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-05 07:11:09.586 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:11:09.587 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:268) (first 15 tasks are for partitions Vector(0))
2025-06-05 07:11:09.587 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-05 07:11:09.590 [dispatcher-event-loop-12 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
2025-06-05 07:11:09.591 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-05 07:11:09.617 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-05 07:11:09.618 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
2025-06-05 07:11:09.627 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 4038 bytes result sent to driver
2025-06-05 07:11:09.628 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 39 ms on phamviethoa (executor driver) (1/1)
2025-06-05 07:11:09.628 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-05 07:11:09.629 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:268) finished in 0.048 s
2025-06-05 07:11:09.631 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 07:11:09.631 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-05 07:11:09.632 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:268, took 1.059995 s
2025-06-05 07:11:09.789 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] terminated with error
org.apache.spark.sql.AnalysisException: Column pageUrl not found in schema Some(StructType(StructField(event_id,StringType,false),StructField(event_name,StringType,false),StructField(event_time,TimestampType,false),StructField(user_id,StringType,false),StructField(session_id,StringType,false),StructField(app_id,StringType,false),StructField(platform,StringType,false),StructField(page_url,StringType,false),StructField(geo_country,StringType,false),StructField(geo_city,StringType,false),StructField(traffic_source,StringType,false),StructField(traffic_medium,StringType,false),StructField(item_id,StringType,true),StructField(item_price,DoubleType,true),StructField(kafka_timestamp,TimestampType,false),StructField(processing_time,TimestampType,false))).
	at org.apache.spark.sql.errors.QueryCompilationErrors$.columnNotFoundInSchemaError(QueryCompilationErrors.scala:1612)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$4(JdbcUtils.scala:126)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$2(JdbcUtils.scala:126)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getInsertStatement(JdbcUtils.scala:124)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:883)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.SparkClickstreamProcessor.lambda$main$a450ce84$1(SparkClickstreamProcessor.java:264)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
2025-06-05 07:11:09.790 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 07:11:09.792 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:11:09.792 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:11:09.792 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:11:09.793 [stream execution thread for [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = d1e1ebe5-5ae2-4797-84c5-2418c1c0bde4, runId = 1111ccd8-38d8-4bc5-ac5f-0415e24c1bcb] has been shutdown
2025-06-05 07:11:09.806 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor-1, groupId=spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:11:09.806 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor-1, groupId=spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:11:09.810 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:11:09.810 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:11:09.811 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:11:09.811 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:11:09.813 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor-1 unregistered
2025-06-05 07:11:09.814 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor-2, groupId=spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:11:09.814 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor-2, groupId=spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:11:09.815 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:11:09.816 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:11:09.816 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:11:09.816 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:11:09.817 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-0a188521-5268-4294-9558-d8d17437751e-870796607-executor-2 unregistered
2025-06-05 07:11:09.818 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 07:11:09.818 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 07:11:09.822 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@28955922{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:11:09.825 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 07:11:09.835 [dispatcher-event-loop-1 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 07:11:09.842 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 07:11:09.842 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 07:11:09.847 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 07:11:09.849 [dispatcher-event-loop-5 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 07:11:09.853 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 07:11:09.853 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 07:11:09.854 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-dacf35f8-ac01-4cb0-bf59-f7298b953d3f
2025-06-05 07:11:09.855 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-7bc93bc0-353e-494b-bf68-05162dc1d466
2025-06-05 07:11:43.260 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 07:11:43.372 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 07:11:43.423 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:11:43.424 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 07:11:43.424 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:11:43.424 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 07:11:43.436 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 07:11:43.442 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 07:11:43.442 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 07:11:43.474 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 07:11:43.475 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 07:11:43.475 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 07:11:43.475 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 07:11:43.476 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 07:11:43.611 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 34939.
2025-06-05 07:11:43.625 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 07:11:43.643 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 07:11:43.652 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 07:11:43.652 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 07:11:43.654 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 07:11:43.664 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-b2ddf34c-a064-44da-a248-72b3457a4175
2025-06-05 07:11:43.683 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 07:11:43.692 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 07:11:43.716 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1015ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 07:11:43.767 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 07:11:43.773 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 07:11:43.785 [main INFO ] org.sparkproject.jetty.server.Server - Started @1085ms
2025-06-05 07:11:43.805 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@6686b7f1{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:11:43.806 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 07:11:43.823 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c82cd4f{/,null,AVAILABLE,@Spark}
2025-06-05 07:11:43.882 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 07:11:43.888 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 07:11:43.901 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46057.
2025-06-05 07:11:43.901 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:46057
2025-06-05 07:11:43.902 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 07:11:43.905 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 46057, None)
2025-06-05 07:11:43.908 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:46057 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 46057, None)
2025-06-05 07:11:43.911 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 46057, None)
2025-06-05 07:11:43.912 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 46057, None)
2025-06-05 07:11:43.992 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@5c82cd4f{/,null,STOPPED,@Spark}
2025-06-05 07:11:43.993 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@761956ac{/jobs,null,AVAILABLE,@Spark}
2025-06-05 07:11:43.994 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@304d0259{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:43.995 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3414a8c3{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 07:11:43.995 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cf518cf{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:43.996 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68d651f2{/stages,null,AVAILABLE,@Spark}
2025-06-05 07:11:43.997 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e43e323{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:43.998 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48840594{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 07:11:43.999 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14823f76{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:43.999 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ed16657{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 07:11:44.000 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@113e13f9{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:44.001 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7979b8b7{/storage,null,AVAILABLE,@Spark}
2025-06-05 07:11:44.002 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bc49bc5{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:44.002 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f66ffc8{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 07:11:44.002 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2def7a7a{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:44.003 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c080ef3{/environment,null,AVAILABLE,@Spark}
2025-06-05 07:11:44.003 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ee6291f{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:44.004 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37e0292a{/executors,null,AVAILABLE,@Spark}
2025-06-05 07:11:44.004 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35267fd4{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:44.005 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36a6bea6{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 07:11:44.005 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42373389{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:44.009 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a62c7cd{/static,null,AVAILABLE,@Spark}
2025-06-05 07:11:44.010 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6726cc69{/,null,AVAILABLE,@Spark}
2025-06-05 07:11:44.011 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33899f7a{/api,null,AVAILABLE,@Spark}
2025-06-05 07:11:44.011 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2436ea2f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 07:11:44.012 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20cece0b{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 07:11:44.014 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4339baec{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:44.093 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 07:11:44.097 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 07:11:44.104 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d97caa4{/SQL,null,AVAILABLE,@Spark}
2025-06-05 07:11:44.104 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@474821de{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:44.105 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c83ae01{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 07:11:44.105 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69d45cca{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:44.112 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f94e148{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:11:45.452 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-05 07:11:45.461 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ff81b0d{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-05 07:11:45.462 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@773c7147{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:45.462 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a5066f5{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-05 07:11:45.463 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1191029d{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:45.464 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b849fa6{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:11:45.468 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1db7f88c-a95c-46d3-bf3f-e9844b96b708. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-05 07:11:45.481 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-1db7f88c-a95c-46d3-bf3f-e9844b96b708 resolved to file:/tmp/temporary-1db7f88c-a95c-46d3-bf3f-e9844b96b708.
2025-06-05 07:11:45.481 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-05 07:11:45.529 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-1db7f88c-a95c-46d3-bf3f-e9844b96b708/metadata using temp file file:/tmp/temporary-1db7f88c-a95c-46d3-bf3f-e9844b96b708/.metadata.b66a47d9-2f6f-48f6-b074-1b65e258244c.tmp
2025-06-05 07:11:45.580 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-1db7f88c-a95c-46d3-bf3f-e9844b96b708/.metadata.b66a47d9-2f6f-48f6-b074-1b65e258244c.tmp to file:/tmp/temporary-1db7f88c-a95c-46d3-bf3f-e9844b96b708/metadata
2025-06-05 07:11:45.595 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01]. Use file:/tmp/temporary-1db7f88c-a95c-46d3-bf3f-e9844b96b708 to store the query checkpoint.
2025-06-05 07:11:45.600 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@73774ddb] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@7203b4e]
2025-06-05 07:11:45.620 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:11:45.621 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:11:45.622 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-05 07:11:45.623 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-05 07:11:45.789 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 07:11:45.819 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 07:11:45.820 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:11:45.820 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:11:45.820 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082305819
2025-06-05 07:11:46.002 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-1db7f88c-a95c-46d3-bf3f-e9844b96b708/sources/0/0 using temp file file:/tmp/temporary-1db7f88c-a95c-46d3-bf3f-e9844b96b708/sources/0/.0.62f53a55-3587-4013-9b65-d91f093a2812.tmp
2025-06-05 07:11:46.015 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-1db7f88c-a95c-46d3-bf3f-e9844b96b708/sources/0/.0.62f53a55-3587-4013-9b65-d91f093a2812.tmp to file:/tmp/temporary-1db7f88c-a95c-46d3-bf3f-e9844b96b708/sources/0/0
2025-06-05 07:11:46.015 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-05 07:11:46.027 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-1db7f88c-a95c-46d3-bf3f-e9844b96b708/offsets/0 using temp file file:/tmp/temporary-1db7f88c-a95c-46d3-bf3f-e9844b96b708/offsets/.0.910356ec-0416-495c-aa34-46f2395752a8.tmp
2025-06-05 07:11:46.046 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-1db7f88c-a95c-46d3-bf3f-e9844b96b708/offsets/.0.910356ec-0416-495c-aa34-46f2395752a8.tmp to file:/tmp/temporary-1db7f88c-a95c-46d3-bf3f-e9844b96b708/offsets/0
2025-06-05 07:11:46.046 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749082306022,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-05 07:11:46.232 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082306022
2025-06-05 07:11:46.284 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:11:46.309 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:11:46.338 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082306022
2025-06-05 07:11:46.340 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:11:46.341 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:11:46.575 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 133.106724 ms
2025-06-05 07:11:46.689 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.754069 ms
2025-06-05 07:11:46.706 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.79356 ms
2025-06-05 07:11:46.756 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:269
2025-06-05 07:11:46.765 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:269) as input to shuffle 0
2025-06-05 07:11:46.768 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:269) with 1 output partitions
2025-06-05 07:11:46.768 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:269)
2025-06-05 07:11:46.768 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-05 07:11:46.769 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
2025-06-05 07:11:46.771 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:269), which has no missing parents
2025-06-05 07:11:46.841 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 46.1 KiB, free 9.2 GiB)
2025-06-05 07:11:46.858 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 9.2 GiB)
2025-06-05 07:11:46.859 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:46057 (size: 17.9 KiB, free: 9.2 GiB)
2025-06-05 07:11:46.862 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:11:46.870 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:269) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:11:46.871 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-05 07:11:46.902 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8302 bytes) 
2025-06-05 07:11:46.905 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8302 bytes) 
2025-06-05 07:11:46.912 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 07:11:46.912 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-05 07:11:47.020 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 14.282137 ms
2025-06-05 07:11:47.048 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.139406 ms
2025-06-05 07:11:47.061 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.571908 ms
2025-06-05 07:11:47.072 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:11:47.072 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:11:47.092 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:11:47.092 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:11:47.119 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:11:47.119 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:11:47.119 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082307119
2025-06-05 07:11:47.120 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:11:47.120 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:11:47.120 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082307119
2025-06-05 07:11:47.121 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor-1, groupId=spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 07:11:47.121 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor-2, groupId=spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor] Assigned to partition(s): clickstream-events-1
2025-06-05 07:11:47.126 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor-2, groupId=spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:11:47.126 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor-1, groupId=spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:11:47.132 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor-2, groupId=spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:11:47.132 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor-1, groupId=spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:11:47.159 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor-2, groupId=spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:11:47.159 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor-1, groupId=spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:11:47.661 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor-2, groupId=spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:11:47.661 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor-1, groupId=spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:11:47.661 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor-2, groupId=spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:11:47.661 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor-1, groupId=spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:11:47.662 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor-1, groupId=spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:11:47.662 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor-2, groupId=spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:11:47.742 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2342 bytes result sent to driver
2025-06-05 07:11:47.742 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 2342 bytes result sent to driver
2025-06-05 07:11:47.748 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 843 ms on phamviethoa (executor driver) (1/2)
2025-06-05 07:11:47.748 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 853 ms on phamviethoa (executor driver) (2/2)
2025-06-05 07:11:47.749 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 07:11:47.755 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (start at SparkClickstreamProcessor.java:269) finished in 0.976 s
2025-06-05 07:11:47.755 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
2025-06-05 07:11:47.756 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - running: Set()
2025-06-05 07:11:47.756 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
2025-06-05 07:11:47.756 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - failed: Set()
2025-06-05 07:11:47.758 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:269), which has no missing parents
2025-06-05 07:11:47.765 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-05 07:11:47.767 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-05 07:11:47.768 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:46057 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-05 07:11:47.768 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:11:47.769 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:269) (first 15 tasks are for partitions Vector(0))
2025-06-05 07:11:47.769 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-05 07:11:47.773 [dispatcher-event-loop-12 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
2025-06-05 07:11:47.774 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-05 07:11:47.804 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-05 07:11:47.804 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
2025-06-05 07:11:47.813 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 4038 bytes result sent to driver
2025-06-05 07:11:47.814 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 42 ms on phamviethoa (executor driver) (1/1)
2025-06-05 07:11:47.815 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-05 07:11:47.815 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:269) finished in 0.052 s
2025-06-05 07:11:47.817 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 07:11:47.817 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-05 07:11:47.818 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:269, took 1.061953 s
2025-06-05 07:11:47.973 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] terminated with error
org.apache.spark.sql.AnalysisException: Column pageUrl not found in schema Some(StructType(StructField(event_id,StringType,false),StructField(event_name,StringType,false),StructField(event_time,TimestampType,false),StructField(user_id,StringType,false),StructField(session_id,StringType,false),StructField(app_id,StringType,false),StructField(platform,StringType,false),StructField(page_url,StringType,false),StructField(geo_country,StringType,false),StructField(geo_city,StringType,false),StructField(traffic_source,StringType,false),StructField(traffic_medium,StringType,false),StructField(item_id,StringType,true),StructField(item_price,DoubleType,true),StructField(kafka_timestamp,TimestampType,false),StructField(processing_time,TimestampType,false))).
	at org.apache.spark.sql.errors.QueryCompilationErrors$.columnNotFoundInSchemaError(QueryCompilationErrors.scala:1612)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$4(JdbcUtils.scala:126)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$2(JdbcUtils.scala:126)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getInsertStatement(JdbcUtils.scala:124)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:883)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.SparkClickstreamProcessor.lambda$main$a450ce84$1(SparkClickstreamProcessor.java:265)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
2025-06-05 07:11:47.974 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 07:11:47.977 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:11:47.977 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:11:47.977 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:11:47.978 [stream execution thread for [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = 8266448b-c29a-4523-a4c2-6e26de76188f, runId = 91b2e3e9-0b56-442a-b045-0cadc7addb01] has been shutdown
2025-06-05 07:11:47.990 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor-1, groupId=spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:11:47.991 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor-1, groupId=spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:11:47.995 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:11:47.995 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:11:47.995 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:11:47.995 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:11:47.998 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor-1 unregistered
2025-06-05 07:11:47.999 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor-2, groupId=spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:11:47.999 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor-2, groupId=spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:11:48.001 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:11:48.001 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:11:48.001 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:11:48.001 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:11:48.004 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-e1444a75-bc11-4968-ac8d-d33a14586da6-2013551104-executor-2 unregistered
2025-06-05 07:11:48.005 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 07:11:48.005 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 07:11:48.011 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@6686b7f1{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:11:48.013 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 07:11:48.021 [dispatcher-event-loop-1 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 07:11:48.027 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 07:11:48.028 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 07:11:48.031 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 07:11:48.033 [dispatcher-event-loop-5 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 07:11:48.037 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 07:11:48.038 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 07:11:48.038 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-e1108d27-dafe-49db-94da-52b74f273ca3
2025-06-05 07:11:48.040 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-1db7f88c-a95c-46d3-bf3f-e9844b96b708
2025-06-05 07:11:56.768 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 07:11:56.851 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 07:11:56.898 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:11:56.899 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 07:11:56.899 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:11:56.899 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 07:11:56.911 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 07:11:56.917 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 07:11:56.917 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 07:11:56.945 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 07:11:56.945 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 07:11:56.946 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 07:11:56.946 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 07:11:56.946 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 07:11:57.068 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 43271.
2025-06-05 07:11:57.080 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 07:11:57.097 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 07:11:57.105 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 07:11:57.106 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 07:11:57.107 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 07:11:57.117 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-6e7344cc-46f4-49ee-9391-07b4bd8e24eb
2025-06-05 07:11:57.133 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 07:11:57.141 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 07:11:57.161 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1049ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 07:11:57.204 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 07:11:57.209 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 07:11:57.218 [main INFO ] org.sparkproject.jetty.server.Server - Started @1106ms
2025-06-05 07:11:57.233 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@748e9b20{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:11:57.234 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 07:11:57.244 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@289fdb08{/,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.293 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 07:11:57.297 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 07:11:57.310 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36463.
2025-06-05 07:11:57.310 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:36463
2025-06-05 07:11:57.311 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 07:11:57.315 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 36463, None)
2025-06-05 07:11:57.318 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:36463 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 36463, None)
2025-06-05 07:11:57.320 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 36463, None)
2025-06-05 07:11:57.321 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 36463, None)
2025-06-05 07:11:57.396 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@289fdb08{/,null,STOPPED,@Spark}
2025-06-05 07:11:57.397 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@304d0259{/jobs,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.398 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133661d{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.398 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cf518cf{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.398 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68d651f2{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.399 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e43e323{/stages,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.399 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10643593{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.400 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14823f76{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.400 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ed16657{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.401 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@113e13f9{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.401 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7979b8b7{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.402 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bc49bc5{/storage,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.402 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f66ffc8{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.403 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2def7a7a{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.403 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c080ef3{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.404 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ee6291f{/environment,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.404 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37e0292a{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.405 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35267fd4{/executors,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.405 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36a6bea6{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.406 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42373389{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.406 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a62c7cd{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.410 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c36db44{/static,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.411 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33899f7a{/,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.412 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@290d10ef{/api,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.412 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20cece0b{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.412 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f038248{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.415 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61cd1c71{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.497 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 07:11:57.501 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 07:11:57.508 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@474821de{/SQL,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.509 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ec5ea63{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.509 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69d45cca{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.510 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e6d4780{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:57.516 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2cde651b{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:11:58.858 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-05 07:11:58.867 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@773c7147{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-05 07:11:58.868 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c3007d{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:58.869 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1191029d{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-05 07:11:58.869 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d1c63af{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-05 07:11:58.870 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e9ea32f{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:11:58.875 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ba748078-3651-4992-872a-f4ec2e1b5168. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-05 07:11:58.887 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-ba748078-3651-4992-872a-f4ec2e1b5168 resolved to file:/tmp/temporary-ba748078-3651-4992-872a-f4ec2e1b5168.
2025-06-05 07:11:58.887 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-05 07:11:58.935 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-ba748078-3651-4992-872a-f4ec2e1b5168/metadata using temp file file:/tmp/temporary-ba748078-3651-4992-872a-f4ec2e1b5168/.metadata.c29177fd-0889-48dd-b857-eb992a29e52b.tmp
2025-06-05 07:11:58.988 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-ba748078-3651-4992-872a-f4ec2e1b5168/.metadata.c29177fd-0889-48dd-b857-eb992a29e52b.tmp to file:/tmp/temporary-ba748078-3651-4992-872a-f4ec2e1b5168/metadata
2025-06-05 07:11:59.005 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc]. Use file:/tmp/temporary-ba748078-3651-4992-872a-f4ec2e1b5168 to store the query checkpoint.
2025-06-05 07:11:59.010 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@519b1d3a] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@836ac9d]
2025-06-05 07:11:59.024 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:11:59.025 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:11:59.025 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-05 07:11:59.027 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-05 07:11:59.203 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 07:11:59.231 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 07:11:59.232 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:11:59.232 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:11:59.232 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082319231
2025-06-05 07:11:59.402 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-ba748078-3651-4992-872a-f4ec2e1b5168/sources/0/0 using temp file file:/tmp/temporary-ba748078-3651-4992-872a-f4ec2e1b5168/sources/0/.0.89d5f92a-a489-4b97-b4c1-20a9a2b9e108.tmp
2025-06-05 07:11:59.414 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-ba748078-3651-4992-872a-f4ec2e1b5168/sources/0/.0.89d5f92a-a489-4b97-b4c1-20a9a2b9e108.tmp to file:/tmp/temporary-ba748078-3651-4992-872a-f4ec2e1b5168/sources/0/0
2025-06-05 07:11:59.414 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-05 07:11:59.426 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-ba748078-3651-4992-872a-f4ec2e1b5168/offsets/0 using temp file file:/tmp/temporary-ba748078-3651-4992-872a-f4ec2e1b5168/offsets/.0.f17f9cf1-2fd5-4f39-9e35-e2f0b36bc61d.tmp
2025-06-05 07:11:59.446 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-ba748078-3651-4992-872a-f4ec2e1b5168/offsets/.0.f17f9cf1-2fd5-4f39-9e35-e2f0b36bc61d.tmp to file:/tmp/temporary-ba748078-3651-4992-872a-f4ec2e1b5168/offsets/0
2025-06-05 07:11:59.447 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749082319421,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-05 07:11:59.630 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082319421
2025-06-05 07:11:59.679 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:11:59.704 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:11:59.732 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082319421
2025-06-05 07:11:59.733 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:11:59.734 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:11:59.962 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 137.548977 ms
2025-06-05 07:12:00.077 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.412305 ms
2025-06-05 07:12:00.094 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.107888 ms
2025-06-05 07:12:00.143 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:269
2025-06-05 07:12:00.154 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:269) as input to shuffle 0
2025-06-05 07:12:00.157 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:269) with 1 output partitions
2025-06-05 07:12:00.158 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:269)
2025-06-05 07:12:00.158 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-05 07:12:00.160 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
2025-06-05 07:12:00.163 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:269), which has no missing parents
2025-06-05 07:12:00.233 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 46.1 KiB, free 9.2 GiB)
2025-06-05 07:12:00.252 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 9.2 GiB)
2025-06-05 07:12:00.254 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:36463 (size: 17.9 KiB, free: 9.2 GiB)
2025-06-05 07:12:00.256 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:12:00.268 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:269) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:12:00.269 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-05 07:12:00.296 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8302 bytes) 
2025-06-05 07:12:00.298 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8302 bytes) 
2025-06-05 07:12:00.305 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-05 07:12:00.305 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 07:12:00.401 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 19.057595 ms
2025-06-05 07:12:00.433 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.92284 ms
2025-06-05 07:12:00.446 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.115175 ms
2025-06-05 07:12:00.456 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:12:00.456 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:12:00.475 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:12:00.475 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:12:00.502 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:12:00.502 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:12:00.502 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082320502
2025-06-05 07:12:00.502 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:12:00.502 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:12:00.502 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082320502
2025-06-05 07:12:00.503 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor-1, groupId=spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor] Assigned to partition(s): clickstream-events-1
2025-06-05 07:12:00.503 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor-2, groupId=spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 07:12:00.508 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor-2, groupId=spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:12:00.508 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor-1, groupId=spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:12:00.513 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor-2, groupId=spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:12:00.513 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor-1, groupId=spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:12:00.536 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor-2, groupId=spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:12:00.536 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor-1, groupId=spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:12:01.039 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor-1, groupId=spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:12:01.039 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor-2, groupId=spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:12:01.039 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor-1, groupId=spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:12:01.039 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor-2, groupId=spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:12:01.040 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor-2, groupId=spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:12:01.040 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor-1, groupId=spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:12:01.123 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 2342 bytes result sent to driver
2025-06-05 07:12:01.123 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2342 bytes result sent to driver
2025-06-05 07:12:01.131 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 839 ms on phamviethoa (executor driver) (1/2)
2025-06-05 07:12:01.131 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 833 ms on phamviethoa (executor driver) (2/2)
2025-06-05 07:12:01.132 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 07:12:01.137 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (start at SparkClickstreamProcessor.java:269) finished in 0.966 s
2025-06-05 07:12:01.138 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
2025-06-05 07:12:01.138 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - running: Set()
2025-06-05 07:12:01.138 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
2025-06-05 07:12:01.138 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - failed: Set()
2025-06-05 07:12:01.140 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:269), which has no missing parents
2025-06-05 07:12:01.147 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-05 07:12:01.149 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-05 07:12:01.149 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:36463 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-05 07:12:01.149 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:12:01.150 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:269) (first 15 tasks are for partitions Vector(0))
2025-06-05 07:12:01.150 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-05 07:12:01.154 [dispatcher-event-loop-12 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
2025-06-05 07:12:01.155 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-05 07:12:01.181 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-05 07:12:01.182 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
2025-06-05 07:12:01.189 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 3995 bytes result sent to driver
2025-06-05 07:12:01.191 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 39 ms on phamviethoa (executor driver) (1/1)
2025-06-05 07:12:01.191 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-05 07:12:01.191 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:269) finished in 0.046 s
2025-06-05 07:12:01.194 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 07:12:01.194 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-05 07:12:01.195 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:269, took 1.051669 s
2025-06-05 07:12:01.354 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] terminated with error
org.apache.spark.sql.AnalysisException: Column geoCountry not found in schema Some(StructType(StructField(event_id,StringType,false),StructField(event_name,StringType,false),StructField(event_time,TimestampType,false),StructField(user_id,StringType,false),StructField(session_id,StringType,false),StructField(app_id,StringType,false),StructField(platform,StringType,false),StructField(page_url,StringType,false),StructField(geo_country,StringType,false),StructField(geo_city,StringType,false),StructField(traffic_source,StringType,false),StructField(traffic_medium,StringType,false),StructField(item_id,StringType,true),StructField(item_price,DoubleType,true),StructField(kafka_timestamp,TimestampType,false),StructField(processing_time,TimestampType,false))).
	at org.apache.spark.sql.errors.QueryCompilationErrors$.columnNotFoundInSchemaError(QueryCompilationErrors.scala:1612)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$4(JdbcUtils.scala:126)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$2(JdbcUtils.scala:126)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getInsertStatement(JdbcUtils.scala:124)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:883)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.SparkClickstreamProcessor.lambda$main$a450ce84$1(SparkClickstreamProcessor.java:265)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
2025-06-05 07:12:01.355 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 07:12:01.357 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:12:01.357 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:12:01.357 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:12:01.357 [stream execution thread for [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = 7d221e96-3c1f-4f3c-bb63-6ef6ff17950e, runId = d0a1491e-1523-43b1-888f-68112cc63fbc] has been shutdown
2025-06-05 07:12:01.367 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor-1, groupId=spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:12:01.367 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor-1, groupId=spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:12:01.370 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:12:01.370 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:12:01.370 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:12:01.370 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:12:01.372 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor-1 unregistered
2025-06-05 07:12:01.372 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor-2, groupId=spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:12:01.373 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor-2, groupId=spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:12:01.374 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:12:01.374 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:12:01.374 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:12:01.374 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:12:01.376 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-d806770b-2bc1-4a79-b560-5ff8680350a1-1314284355-executor-2 unregistered
2025-06-05 07:12:01.377 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 07:12:01.377 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 07:12:01.382 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@748e9b20{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:12:01.385 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 07:12:01.393 [dispatcher-event-loop-1 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 07:12:01.399 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 07:12:01.399 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 07:12:01.403 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 07:12:01.404 [dispatcher-event-loop-5 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 07:12:01.407 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 07:12:01.407 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 07:12:01.408 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-ba748078-3651-4992-872a-f4ec2e1b5168
2025-06-05 07:12:01.409 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-d3967f5b-83e1-4a7c-9d67-bbd96f6ff44b
2025-06-05 07:12:14.896 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 07:12:14.973 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 07:12:15.017 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:12:15.017 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 07:12:15.017 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:12:15.017 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 07:12:15.028 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 07:12:15.034 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 07:12:15.034 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 07:12:15.062 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 07:12:15.062 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 07:12:15.062 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 07:12:15.062 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 07:12:15.063 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 07:12:15.179 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 37065.
2025-06-05 07:12:15.191 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 07:12:15.208 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 07:12:15.216 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 07:12:15.217 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 07:12:15.219 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 07:12:15.229 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-fcb801ab-f1eb-4109-b412-a03d4ca11853
2025-06-05 07:12:15.250 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 07:12:15.258 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 07:12:15.280 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1015ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 07:12:15.324 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 07:12:15.335 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 07:12:15.345 [main INFO ] org.sparkproject.jetty.server.Server - Started @1081ms
2025-06-05 07:12:15.360 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@5d2d8112{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:12:15.361 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 07:12:15.371 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c504e66{/,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.423 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 07:12:15.427 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 07:12:15.439 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44699.
2025-06-05 07:12:15.439 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:44699
2025-06-05 07:12:15.440 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 07:12:15.444 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 44699, None)
2025-06-05 07:12:15.446 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:44699 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 44699, None)
2025-06-05 07:12:15.449 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 44699, None)
2025-06-05 07:12:15.450 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 44699, None)
2025-06-05 07:12:15.522 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1c504e66{/,null,STOPPED,@Spark}
2025-06-05 07:12:15.523 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7342e05d{/jobs,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.524 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15383681{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.524 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@761956ac{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.525 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@304d0259{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.525 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2133661d{/stages,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.525 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3414a8c3{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.526 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e43e323{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.527 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10643593{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.527 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@eca6a74{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.527 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48840594{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.528 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14823f76{/storage,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.528 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ed16657{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.529 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@113e13f9{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.529 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7979b8b7{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.529 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bc49bc5{/environment,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.530 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f66ffc8{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.530 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2def7a7a{/executors,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.531 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c080ef3{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.531 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ee6291f{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.532 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37e0292a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.536 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35267fd4{/static,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.536 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ac4ccad{/,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.537 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14982a82{/api,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.538 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5300cac{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.538 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ba359bd{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.540 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58ff8d79{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.620 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 07:12:15.623 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 07:12:15.630 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1dbd580{/SQL,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.631 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d0d91a1{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.631 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6732726{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.632 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d64c581{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 07:12:15.638 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fdf17dc{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:12:17.129 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-05 07:12:17.139 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@186dcb05{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-05 07:12:17.140 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c6fb501{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-05 07:12:17.140 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c610f{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-05 07:12:17.141 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5abc5854{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-05 07:12:17.142 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54b2d002{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:12:17.147 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d9c3ce39-9a8a-4bf3-9787-db7ea3640d92. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-05 07:12:17.160 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-d9c3ce39-9a8a-4bf3-9787-db7ea3640d92 resolved to file:/tmp/temporary-d9c3ce39-9a8a-4bf3-9787-db7ea3640d92.
2025-06-05 07:12:17.161 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-05 07:12:17.206 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-d9c3ce39-9a8a-4bf3-9787-db7ea3640d92/metadata using temp file file:/tmp/temporary-d9c3ce39-9a8a-4bf3-9787-db7ea3640d92/.metadata.62e9c20f-04d7-4f78-9ae3-4e8afb3e2fbd.tmp
2025-06-05 07:12:17.254 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-d9c3ce39-9a8a-4bf3-9787-db7ea3640d92/.metadata.62e9c20f-04d7-4f78-9ae3-4e8afb3e2fbd.tmp to file:/tmp/temporary-d9c3ce39-9a8a-4bf3-9787-db7ea3640d92/metadata
2025-06-05 07:12:17.268 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c]. Use file:/tmp/temporary-d9c3ce39-9a8a-4bf3-9787-db7ea3640d92 to store the query checkpoint.
2025-06-05 07:12:17.275 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@3ea5aa29] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@266be8b0]
2025-06-05 07:12:17.288 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:12:17.289 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:12:17.289 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-05 07:12:17.290 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-05 07:12:17.460 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 07:12:17.491 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 07:12:17.491 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:12:17.491 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:12:17.492 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082337491
2025-06-05 07:12:17.661 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-d9c3ce39-9a8a-4bf3-9787-db7ea3640d92/sources/0/0 using temp file file:/tmp/temporary-d9c3ce39-9a8a-4bf3-9787-db7ea3640d92/sources/0/.0.908f542d-f653-4238-a40f-86b61a2762ad.tmp
2025-06-05 07:12:17.675 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-d9c3ce39-9a8a-4bf3-9787-db7ea3640d92/sources/0/.0.908f542d-f653-4238-a40f-86b61a2762ad.tmp to file:/tmp/temporary-d9c3ce39-9a8a-4bf3-9787-db7ea3640d92/sources/0/0
2025-06-05 07:12:17.676 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-05 07:12:17.688 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-d9c3ce39-9a8a-4bf3-9787-db7ea3640d92/offsets/0 using temp file file:/tmp/temporary-d9c3ce39-9a8a-4bf3-9787-db7ea3640d92/offsets/.0.ece0a842-3f40-40ad-b691-7d78e8dbeacf.tmp
2025-06-05 07:12:17.708 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-d9c3ce39-9a8a-4bf3-9787-db7ea3640d92/offsets/.0.ece0a842-3f40-40ad-b691-7d78e8dbeacf.tmp to file:/tmp/temporary-d9c3ce39-9a8a-4bf3-9787-db7ea3640d92/offsets/0
2025-06-05 07:12:17.709 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749082337683,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-05 07:12:17.891 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082337683
2025-06-05 07:12:17.943 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:12:17.969 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:12:17.999 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082337683
2025-06-05 07:12:18.000 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:12:18.001 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:12:18.234 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 142.299127 ms
2025-06-05 07:12:18.346 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.471186 ms
2025-06-05 07:12:18.366 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 12.521147 ms
2025-06-05 07:12:18.414 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:272
2025-06-05 07:12:18.423 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:272) as input to shuffle 0
2025-06-05 07:12:18.425 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:272) with 1 output partitions
2025-06-05 07:12:18.425 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:272)
2025-06-05 07:12:18.426 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-05 07:12:18.426 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
2025-06-05 07:12:18.429 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:272), which has no missing parents
2025-06-05 07:12:18.496 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 46.1 KiB, free 9.2 GiB)
2025-06-05 07:12:18.517 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 9.2 GiB)
2025-06-05 07:12:18.519 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:44699 (size: 17.9 KiB, free: 9.2 GiB)
2025-06-05 07:12:18.522 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:12:18.534 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:272) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:12:18.534 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-05 07:12:18.564 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8303 bytes) 
2025-06-05 07:12:18.566 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8303 bytes) 
2025-06-05 07:12:18.572 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-05 07:12:18.572 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 07:12:18.664 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 14.769129 ms
2025-06-05 07:12:18.692 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.273411 ms
2025-06-05 07:12:18.705 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.212849 ms
2025-06-05 07:12:18.714 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:12:18.714 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:12:18.736 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:12:18.736 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:12:18.765 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:12:18.765 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:12:18.765 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082338765
2025-06-05 07:12:18.766 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:12:18.766 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:12:18.766 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082338765
2025-06-05 07:12:18.766 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor-2, groupId=spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 07:12:18.766 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor-1, groupId=spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor] Assigned to partition(s): clickstream-events-1
2025-06-05 07:12:18.771 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor-2, groupId=spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:12:18.771 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor-1, groupId=spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:12:18.776 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor-1, groupId=spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:12:18.776 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor-2, groupId=spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:12:18.799 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor-2, groupId=spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:12:18.799 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor-1, groupId=spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:12:19.301 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor-2, groupId=spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:12:19.301 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor-1, groupId=spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:12:19.301 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor-1, groupId=spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:12:19.301 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor-2, groupId=spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:12:19.301 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor-2, groupId=spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:12:19.301 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor-1, groupId=spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:12:19.387 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2342 bytes result sent to driver
2025-06-05 07:12:19.387 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 2342 bytes result sent to driver
2025-06-05 07:12:19.392 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 826 ms on phamviethoa (executor driver) (1/2)
2025-06-05 07:12:19.393 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 836 ms on phamviethoa (executor driver) (2/2)
2025-06-05 07:12:19.394 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 07:12:19.398 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (start at SparkClickstreamProcessor.java:272) finished in 0.962 s
2025-06-05 07:12:19.398 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
2025-06-05 07:12:19.399 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - running: Set()
2025-06-05 07:12:19.399 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
2025-06-05 07:12:19.399 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - failed: Set()
2025-06-05 07:12:19.400 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:272), which has no missing parents
2025-06-05 07:12:19.404 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-05 07:12:19.406 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-05 07:12:19.407 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:44699 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-05 07:12:19.407 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:12:19.408 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:272) (first 15 tasks are for partitions Vector(0))
2025-06-05 07:12:19.408 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-05 07:12:19.411 [dispatcher-event-loop-12 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
2025-06-05 07:12:19.412 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-05 07:12:19.441 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-05 07:12:19.442 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
2025-06-05 07:12:19.450 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 3995 bytes result sent to driver
2025-06-05 07:12:19.451 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 42 ms on phamviethoa (executor driver) (1/1)
2025-06-05 07:12:19.452 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-05 07:12:19.452 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:272) finished in 0.049 s
2025-06-05 07:12:19.454 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 07:12:19.455 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-05 07:12:19.456 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:272, took 1.041174 s
2025-06-05 07:12:19.611 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] terminated with error
org.apache.spark.sql.AnalysisException: Column geo_region not found in schema Some(StructType(StructField(event_id,StringType,false),StructField(event_name,StringType,false),StructField(event_time,TimestampType,false),StructField(user_id,StringType,false),StructField(session_id,StringType,false),StructField(app_id,StringType,false),StructField(platform,StringType,false),StructField(page_url,StringType,false),StructField(geo_country,StringType,false),StructField(geo_city,StringType,false),StructField(traffic_source,StringType,false),StructField(traffic_medium,StringType,false),StructField(item_id,StringType,true),StructField(item_price,DoubleType,true),StructField(kafka_timestamp,TimestampType,false),StructField(processing_time,TimestampType,false))).
	at org.apache.spark.sql.errors.QueryCompilationErrors$.columnNotFoundInSchemaError(QueryCompilationErrors.scala:1612)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$4(JdbcUtils.scala:126)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$2(JdbcUtils.scala:126)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getInsertStatement(JdbcUtils.scala:124)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:883)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.SparkClickstreamProcessor.lambda$main$a450ce84$1(SparkClickstreamProcessor.java:268)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
2025-06-05 07:12:19.612 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 07:12:19.614 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:12:19.614 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:12:19.614 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:12:19.615 [stream execution thread for [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = 95efa62f-781f-4a82-a7b0-194d9f7dbc49, runId = d6b1ca70-a629-46b4-b33a-dba7dfaa006c] has been shutdown
2025-06-05 07:12:19.625 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor-1, groupId=spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:12:19.625 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor-1, groupId=spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:12:19.628 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:12:19.628 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:12:19.628 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:12:19.628 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:12:19.630 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor-1 unregistered
2025-06-05 07:12:19.631 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor-2, groupId=spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:12:19.631 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor-2, groupId=spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:12:19.632 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:12:19.633 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:12:19.633 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:12:19.633 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:12:19.634 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-649ce338-bc9c-48cd-b6ea-5a5e752f7b28--1130350606-executor-2 unregistered
2025-06-05 07:12:19.635 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 07:12:19.635 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 07:12:19.639 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@5d2d8112{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:12:19.641 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 07:12:19.649 [dispatcher-event-loop-1 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 07:12:19.654 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 07:12:19.655 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 07:12:19.657 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 07:12:19.659 [dispatcher-event-loop-5 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 07:12:19.661 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 07:12:19.662 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 07:12:19.662 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-2b0661c1-2c13-4559-b855-ad69b0b38dd4
2025-06-05 07:12:19.663 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-d9c3ce39-9a8a-4bf3-9787-db7ea3640d92
2025-06-05 07:13:03.540 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 07:13:03.624 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 07:13:03.668 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:13:03.668 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 07:13:03.668 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:13:03.668 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 07:13:03.680 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 07:13:03.686 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 07:13:03.686 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 07:13:03.713 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 07:13:03.714 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 07:13:03.714 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 07:13:03.714 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 07:13:03.714 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 07:13:03.833 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 45241.
2025-06-05 07:13:03.846 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 07:13:03.862 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 07:13:03.871 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 07:13:03.871 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 07:13:03.873 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 07:13:03.883 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-45f8612b-719b-4825-92c9-7f78afdf2762
2025-06-05 07:13:03.899 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 07:13:03.907 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 07:13:03.926 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @975ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 07:13:03.969 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 07:13:03.974 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 07:13:03.982 [main INFO ] org.sparkproject.jetty.server.Server - Started @1032ms
2025-06-05 07:13:03.998 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@5967d476{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:13:03.999 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 07:13:04.009 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35835e65{/,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.059 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 07:13:04.063 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 07:13:04.075 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34467.
2025-06-05 07:13:04.075 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:34467
2025-06-05 07:13:04.076 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 07:13:04.080 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 34467, None)
2025-06-05 07:13:04.083 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:34467 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 34467, None)
2025-06-05 07:13:04.085 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 34467, None)
2025-06-05 07:13:04.086 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 34467, None)
2025-06-05 07:13:04.168 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@35835e65{/,null,STOPPED,@Spark}
2025-06-05 07:13:04.169 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/jobs,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.170 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.170 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cc9385{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.171 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7915bca3{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.171 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/stages,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.171 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67b4ec{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.172 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.173 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.173 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.174 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.175 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/storage,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.175 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.176 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.176 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.177 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/environment,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.177 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.177 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/executors,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.178 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397ef2{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.178 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e93c1f{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.179 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9b21bd3{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.183 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7661b5a{/static,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.184 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b6d92e{/,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.185 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7899de11{/api,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.185 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f951a7f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.186 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c777e7b{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.188 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62db3891{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.273 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 07:13:04.277 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 07:13:04.284 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6732726{/SQL,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.285 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d64c581{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.285 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d64c100{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.286 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fdf17dc{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 07:13:04.292 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ff8a9dc{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:13:05.684 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-05 07:13:05.694 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c610f{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-05 07:13:05.695 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5abc5854{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-05 07:13:05.696 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@578d472a{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-05 07:13:05.697 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54b2d002{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-05 07:13:05.699 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7fbbdd8a{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:13:05.706 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-3986b33f-1049-4502-bf81-6f07f2a9c10f. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-05 07:13:05.725 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-3986b33f-1049-4502-bf81-6f07f2a9c10f resolved to file:/tmp/temporary-3986b33f-1049-4502-bf81-6f07f2a9c10f.
2025-06-05 07:13:05.726 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-05 07:13:05.773 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-3986b33f-1049-4502-bf81-6f07f2a9c10f/metadata using temp file file:/tmp/temporary-3986b33f-1049-4502-bf81-6f07f2a9c10f/.metadata.86884a41-b614-4cd9-bfd6-e5d720c93ec1.tmp
2025-06-05 07:13:05.833 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-3986b33f-1049-4502-bf81-6f07f2a9c10f/.metadata.86884a41-b614-4cd9-bfd6-e5d720c93ec1.tmp to file:/tmp/temporary-3986b33f-1049-4502-bf81-6f07f2a9c10f/metadata
2025-06-05 07:13:05.847 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27]. Use file:/tmp/temporary-3986b33f-1049-4502-bf81-6f07f2a9c10f to store the query checkpoint.
2025-06-05 07:13:05.852 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@11fbc9f3] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@26724f05]
2025-06-05 07:13:05.865 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:13:05.866 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:13:05.866 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-05 07:13:05.867 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-05 07:13:06.022 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 07:13:06.051 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 07:13:06.052 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:13:06.052 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:13:06.052 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082386051
2025-06-05 07:13:06.217 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-3986b33f-1049-4502-bf81-6f07f2a9c10f/sources/0/0 using temp file file:/tmp/temporary-3986b33f-1049-4502-bf81-6f07f2a9c10f/sources/0/.0.94bcb5a2-0e26-4b55-b92d-751ceb25016e.tmp
2025-06-05 07:13:06.229 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-3986b33f-1049-4502-bf81-6f07f2a9c10f/sources/0/.0.94bcb5a2-0e26-4b55-b92d-751ceb25016e.tmp to file:/tmp/temporary-3986b33f-1049-4502-bf81-6f07f2a9c10f/sources/0/0
2025-06-05 07:13:06.229 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-05 07:13:06.241 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-3986b33f-1049-4502-bf81-6f07f2a9c10f/offsets/0 using temp file file:/tmp/temporary-3986b33f-1049-4502-bf81-6f07f2a9c10f/offsets/.0.8cf6021c-14af-42d0-9a71-dcb4fc1e795d.tmp
2025-06-05 07:13:06.259 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-3986b33f-1049-4502-bf81-6f07f2a9c10f/offsets/.0.8cf6021c-14af-42d0-9a71-dcb4fc1e795d.tmp to file:/tmp/temporary-3986b33f-1049-4502-bf81-6f07f2a9c10f/offsets/0
2025-06-05 07:13:06.259 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749082386236,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-05 07:13:06.457 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082386236
2025-06-05 07:13:06.512 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:13:06.541 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:13:06.573 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082386236
2025-06-05 07:13:06.574 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:13:06.575 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:13:06.821 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 146.24543 ms
2025-06-05 07:13:06.933 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.723196 ms
2025-06-05 07:13:06.950 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.80829 ms
2025-06-05 07:13:06.999 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:271
2025-06-05 07:13:07.007 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:271) as input to shuffle 0
2025-06-05 07:13:07.010 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:271) with 1 output partitions
2025-06-05 07:13:07.010 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:271)
2025-06-05 07:13:07.010 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-05 07:13:07.011 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
2025-06-05 07:13:07.013 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:271), which has no missing parents
2025-06-05 07:13:07.082 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 46.1 KiB, free 9.2 GiB)
2025-06-05 07:13:07.099 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 9.2 GiB)
2025-06-05 07:13:07.101 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:34467 (size: 17.9 KiB, free: 9.2 GiB)
2025-06-05 07:13:07.104 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:13:07.113 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:271) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:13:07.114 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-05 07:13:07.142 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8301 bytes) 
2025-06-05 07:13:07.145 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8301 bytes) 
2025-06-05 07:13:07.151 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 07:13:07.151 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-05 07:13:07.243 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 14.198339 ms
2025-06-05 07:13:07.270 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 5.742004 ms
2025-06-05 07:13:07.282 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.758825 ms
2025-06-05 07:13:07.292 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:13:07.292 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:13:07.312 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:13:07.312 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:13:07.339 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:13:07.339 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:13:07.339 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082387339
2025-06-05 07:13:07.340 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:13:07.340 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:13:07.340 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082387339
2025-06-05 07:13:07.340 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor-2, groupId=spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor] Assigned to partition(s): clickstream-events-1
2025-06-05 07:13:07.340 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor-1, groupId=spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 07:13:07.345 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor-2, groupId=spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:13:07.345 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor-1, groupId=spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:13:07.351 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor-1, groupId=spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:13:07.351 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor-2, groupId=spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:13:07.374 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor-2, groupId=spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:13:07.374 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor-1, groupId=spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:13:07.876 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor-2, groupId=spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:13:07.876 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor-1, groupId=spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:13:07.876 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor-2, groupId=spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:13:07.876 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor-1, groupId=spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:13:07.876 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor-1, groupId=spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:13:07.876 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor-2, groupId=spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:13:07.957 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 2342 bytes result sent to driver
2025-06-05 07:13:07.957 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2342 bytes result sent to driver
2025-06-05 07:13:07.963 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 827 ms on phamviethoa (executor driver) (1/2)
2025-06-05 07:13:07.964 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 819 ms on phamviethoa (executor driver) (2/2)
2025-06-05 07:13:07.965 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 07:13:07.971 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (start at SparkClickstreamProcessor.java:271) finished in 0.951 s
2025-06-05 07:13:07.971 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
2025-06-05 07:13:07.972 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - running: Set()
2025-06-05 07:13:07.972 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
2025-06-05 07:13:07.972 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - failed: Set()
2025-06-05 07:13:07.973 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:271), which has no missing parents
2025-06-05 07:13:07.978 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-05 07:13:07.980 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-05 07:13:07.981 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:34467 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-05 07:13:07.981 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:13:07.982 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:271) (first 15 tasks are for partitions Vector(0))
2025-06-05 07:13:07.982 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-05 07:13:07.985 [dispatcher-event-loop-12 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
2025-06-05 07:13:07.985 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-05 07:13:08.015 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-05 07:13:08.016 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
2025-06-05 07:13:08.026 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 4038 bytes result sent to driver
2025-06-05 07:13:08.027 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 44 ms on phamviethoa (executor driver) (1/1)
2025-06-05 07:13:08.027 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-05 07:13:08.028 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:271) finished in 0.050 s
2025-06-05 07:13:08.030 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 07:13:08.030 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-05 07:13:08.031 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:271, took 1.032419 s
2025-06-05 07:13:08.197 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] terminated with error
org.apache.spark.sql.AnalysisException: Column geoRegion not found in schema Some(StructType(StructField(event_id,StringType,false),StructField(event_name,StringType,false),StructField(event_time,TimestampType,false),StructField(user_id,StringType,false),StructField(session_id,StringType,false),StructField(app_id,StringType,false),StructField(platform,StringType,false),StructField(page_url,StringType,false),StructField(geo_country,StringType,false),StructField(geo_city,StringType,false),StructField(traffic_source,StringType,false),StructField(traffic_medium,StringType,false),StructField(item_id,StringType,true),StructField(item_price,DoubleType,true),StructField(kafka_timestamp,TimestampType,false),StructField(processing_time,TimestampType,false))).
	at org.apache.spark.sql.errors.QueryCompilationErrors$.columnNotFoundInSchemaError(QueryCompilationErrors.scala:1612)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$4(JdbcUtils.scala:126)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$2(JdbcUtils.scala:126)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getInsertStatement(JdbcUtils.scala:124)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:883)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.SparkClickstreamProcessor.lambda$main$a450ce84$1(SparkClickstreamProcessor.java:267)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
2025-06-05 07:13:08.198 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 07:13:08.200 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:13:08.200 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:13:08.200 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:13:08.201 [stream execution thread for [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = fd8dbb8c-b957-4fa5-a22e-bb93c2e795d6, runId = 37776588-962b-459f-b932-af2a37897c27] has been shutdown
2025-06-05 07:13:08.212 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor-1, groupId=spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:13:08.212 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor-1, groupId=spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:13:08.216 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:13:08.216 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:13:08.216 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:13:08.216 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:13:08.218 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor-1 unregistered
2025-06-05 07:13:08.218 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor-2, groupId=spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:13:08.218 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor-2, groupId=spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:13:08.221 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:13:08.221 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:13:08.221 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:13:08.221 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:13:08.222 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-78f3a169-f444-4fb9-beca-10db038e9cba--44108264-executor-2 unregistered
2025-06-05 07:13:08.223 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 07:13:08.223 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 07:13:08.228 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@5967d476{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:13:08.230 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 07:13:08.238 [dispatcher-event-loop-1 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 07:13:08.245 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 07:13:08.246 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 07:13:08.250 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 07:13:08.252 [dispatcher-event-loop-5 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 07:13:08.256 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 07:13:08.256 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 07:13:08.257 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-27ef21a5-5be2-422b-8565-00cad5da9fb5
2025-06-05 07:13:08.258 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-3986b33f-1049-4502-bf81-6f07f2a9c10f
2025-06-05 07:14:20.645 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 07:14:20.723 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 07:14:20.767 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:14:20.767 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 07:14:20.767 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:14:20.768 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 07:14:20.778 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 07:14:20.784 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 07:14:20.785 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 07:14:20.812 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 07:14:20.812 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 07:14:20.812 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 07:14:20.812 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 07:14:20.812 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 07:14:20.935 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 40635.
2025-06-05 07:14:20.948 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 07:14:20.965 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 07:14:20.977 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 07:14:20.977 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 07:14:20.979 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 07:14:20.989 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-356a76c2-a6e7-4552-9724-f9d706ed1d76
2025-06-05 07:14:21.005 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 07:14:21.013 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 07:14:21.031 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1001ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 07:14:21.076 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 07:14:21.084 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 07:14:21.092 [main INFO ] org.sparkproject.jetty.server.Server - Started @1063ms
2025-06-05 07:14:21.108 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@34a2041b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:14:21.108 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 07:14:21.119 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35835e65{/,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.170 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 07:14:21.174 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 07:14:21.186 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37751.
2025-06-05 07:14:21.187 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:37751
2025-06-05 07:14:21.188 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 07:14:21.191 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 37751, None)
2025-06-05 07:14:21.194 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:37751 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 37751, None)
2025-06-05 07:14:21.196 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 37751, None)
2025-06-05 07:14:21.197 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 37751, None)
2025-06-05 07:14:21.275 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@35835e65{/,null,STOPPED,@Spark}
2025-06-05 07:14:21.276 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/jobs,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.276 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.277 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cc9385{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.277 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7915bca3{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.278 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/stages,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.278 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67b4ec{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.279 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.280 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.281 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.281 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.282 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/storage,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.282 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.283 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.283 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.284 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/environment,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.284 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.285 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/executors,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.285 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397ef2{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.286 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e93c1f{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.287 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9b21bd3{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.291 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7661b5a{/static,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.291 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b6d92e{/,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.292 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7899de11{/api,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.293 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f951a7f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.293 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c777e7b{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.296 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62db3891{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.375 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 07:14:21.379 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 07:14:21.387 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6732726{/SQL,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.387 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d64c581{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.388 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d64c100{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.388 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fdf17dc{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 07:14:21.394 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ff8a9dc{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:14:22.787 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-05 07:14:22.795 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c3007d{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-05 07:14:22.796 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7296fe0b{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-05 07:14:22.796 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d1c63af{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-05 07:14:22.797 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3909a854{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-05 07:14:22.798 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56ba8e8c{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:14:22.802 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-fe3e1db4-d470-475d-a970-8e6bc25645e0. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-05 07:14:22.814 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-fe3e1db4-d470-475d-a970-8e6bc25645e0 resolved to file:/tmp/temporary-fe3e1db4-d470-475d-a970-8e6bc25645e0.
2025-06-05 07:14:22.815 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-05 07:14:22.858 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-fe3e1db4-d470-475d-a970-8e6bc25645e0/metadata using temp file file:/tmp/temporary-fe3e1db4-d470-475d-a970-8e6bc25645e0/.metadata.8a9aaaf7-98ed-45dd-b27e-efb4990c7970.tmp
2025-06-05 07:14:22.906 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-fe3e1db4-d470-475d-a970-8e6bc25645e0/.metadata.8a9aaaf7-98ed-45dd-b27e-efb4990c7970.tmp to file:/tmp/temporary-fe3e1db4-d470-475d-a970-8e6bc25645e0/metadata
2025-06-05 07:14:22.921 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1]. Use file:/tmp/temporary-fe3e1db4-d470-475d-a970-8e6bc25645e0 to store the query checkpoint.
2025-06-05 07:14:22.927 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@700d2376] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@7cb4ad7a]
2025-06-05 07:14:22.941 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:14:22.942 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:14:22.942 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-05 07:14:22.944 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-05 07:14:23.097 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 07:14:23.127 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 07:14:23.128 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:14:23.128 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:14:23.128 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082463127
2025-06-05 07:14:23.294 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-fe3e1db4-d470-475d-a970-8e6bc25645e0/sources/0/0 using temp file file:/tmp/temporary-fe3e1db4-d470-475d-a970-8e6bc25645e0/sources/0/.0.c8dd35b5-669b-410a-8267-71b4a027fa71.tmp
2025-06-05 07:14:23.307 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-fe3e1db4-d470-475d-a970-8e6bc25645e0/sources/0/.0.c8dd35b5-669b-410a-8267-71b4a027fa71.tmp to file:/tmp/temporary-fe3e1db4-d470-475d-a970-8e6bc25645e0/sources/0/0
2025-06-05 07:14:23.307 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-05 07:14:23.320 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-fe3e1db4-d470-475d-a970-8e6bc25645e0/offsets/0 using temp file file:/tmp/temporary-fe3e1db4-d470-475d-a970-8e6bc25645e0/offsets/.0.47cd0d9d-0148-4a3e-831a-64ea69de329d.tmp
2025-06-05 07:14:23.337 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-fe3e1db4-d470-475d-a970-8e6bc25645e0/offsets/.0.47cd0d9d-0148-4a3e-831a-64ea69de329d.tmp to file:/tmp/temporary-fe3e1db4-d470-475d-a970-8e6bc25645e0/offsets/0
2025-06-05 07:14:23.338 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749082463315,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-05 07:14:23.519 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082463315
2025-06-05 07:14:23.565 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:14:23.591 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:14:23.619 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082463315
2025-06-05 07:14:23.621 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:14:23.622 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:14:23.837 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 131.850368 ms
2025-06-05 07:14:23.943 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.287764 ms
2025-06-05 07:14:23.958 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.042484 ms
2025-06-05 07:14:24.004 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:272
2025-06-05 07:14:24.014 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:272) as input to shuffle 0
2025-06-05 07:14:24.018 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:272) with 1 output partitions
2025-06-05 07:14:24.018 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:272)
2025-06-05 07:14:24.019 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-05 07:14:24.020 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
2025-06-05 07:14:24.022 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:272), which has no missing parents
2025-06-05 07:14:24.084 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 45.7 KiB, free 9.2 GiB)
2025-06-05 07:14:24.101 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.8 KiB, free 9.2 GiB)
2025-06-05 07:14:24.102 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:37751 (size: 17.8 KiB, free: 9.2 GiB)
2025-06-05 07:14:24.106 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:14:24.114 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:272) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:14:24.115 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-05 07:14:24.148 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8302 bytes) 
2025-06-05 07:14:24.150 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8302 bytes) 
2025-06-05 07:14:24.157 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-05 07:14:24.157 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 07:14:24.261 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 17.203656 ms
2025-06-05 07:14:24.297 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.063671 ms
2025-06-05 07:14:24.316 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 12.930416 ms
2025-06-05 07:14:24.331 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:14:24.331 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:14:24.354 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:14:24.354 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:14:24.381 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:14:24.381 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:14:24.381 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082464381
2025-06-05 07:14:24.381 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:14:24.381 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:14:24.381 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082464381
2025-06-05 07:14:24.382 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor-1, groupId=spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 07:14:24.382 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor-2, groupId=spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor] Assigned to partition(s): clickstream-events-1
2025-06-05 07:14:24.386 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor-1, groupId=spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:14:24.386 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor-2, groupId=spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:14:24.391 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor-2, groupId=spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:14:24.391 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor-1, groupId=spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:14:24.416 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor-1, groupId=spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:14:24.416 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor-2, groupId=spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:14:24.918 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor-2, groupId=spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:14:24.918 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor-1, groupId=spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:14:24.919 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor-2, groupId=spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:14:24.919 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor-1, groupId=spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:14:24.919 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor-2, groupId=spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:14:24.919 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor-1, groupId=spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:14:25.001 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 2342 bytes result sent to driver
2025-06-05 07:14:25.001 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2342 bytes result sent to driver
2025-06-05 07:14:25.009 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 859 ms on phamviethoa (executor driver) (1/2)
2025-06-05 07:14:25.010 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 867 ms on phamviethoa (executor driver) (2/2)
2025-06-05 07:14:25.011 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 07:14:25.017 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (start at SparkClickstreamProcessor.java:272) finished in 0.988 s
2025-06-05 07:14:25.017 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
2025-06-05 07:14:25.017 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - running: Set()
2025-06-05 07:14:25.018 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
2025-06-05 07:14:25.018 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - failed: Set()
2025-06-05 07:14:25.020 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:272), which has no missing parents
2025-06-05 07:14:25.024 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-05 07:14:25.026 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-05 07:14:25.027 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:37751 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-05 07:14:25.027 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:14:25.028 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:272) (first 15 tasks are for partitions Vector(0))
2025-06-05 07:14:25.028 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-05 07:14:25.031 [dispatcher-event-loop-12 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
2025-06-05 07:14:25.032 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-05 07:14:25.058 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-05 07:14:25.059 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
2025-06-05 07:14:25.067 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 4038 bytes result sent to driver
2025-06-05 07:14:25.068 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 38 ms on phamviethoa (executor driver) (1/1)
2025-06-05 07:14:25.069 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-05 07:14:25.069 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:272) finished in 0.046 s
2025-06-05 07:14:25.071 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 07:14:25.071 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-05 07:14:25.072 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:272, took 1.067245 s
2025-06-05 07:14:25.227 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] terminated with error
org.apache.spark.sql.AnalysisException: Column sessionId not found in schema Some(StructType(StructField(event_id,StringType,false),StructField(event_name,StringType,false),StructField(event_time,TimestampType,false),StructField(user_id,StringType,false),StructField(session_id,StringType,false),StructField(app_id,StringType,false),StructField(platform,StringType,false),StructField(page_url,StringType,false),StructField(geo_country,StringType,false),StructField(geo_city,StringType,false),StructField(traffic_source,StringType,false),StructField(traffic_medium,StringType,false),StructField(item_id,StringType,true),StructField(item_price,DoubleType,true),StructField(kafka_timestamp,TimestampType,false),StructField(processing_time,TimestampType,false))).
	at org.apache.spark.sql.errors.QueryCompilationErrors$.columnNotFoundInSchemaError(QueryCompilationErrors.scala:1612)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$4(JdbcUtils.scala:126)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$2(JdbcUtils.scala:126)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getInsertStatement(JdbcUtils.scala:124)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:883)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.SparkClickstreamProcessor.lambda$main$a450ce84$1(SparkClickstreamProcessor.java:268)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
2025-06-05 07:14:25.228 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 07:14:25.229 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:14:25.229 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:14:25.229 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:14:25.230 [stream execution thread for [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = 436129fc-ea06-4645-9cd9-258d87f4ec7a, runId = d838eb99-f85b-4e4c-a095-b07e266b6ec1] has been shutdown
2025-06-05 07:14:25.241 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor-2, groupId=spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:14:25.241 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor-2, groupId=spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:14:25.245 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:14:25.245 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:14:25.245 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:14:25.245 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:14:25.248 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor-2 unregistered
2025-06-05 07:14:25.248 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor-1, groupId=spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:14:25.248 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor-1, groupId=spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:14:25.250 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:14:25.251 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:14:25.251 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:14:25.251 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:14:25.253 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-8ca430b1-110c-4587-9499-12c6389782ef-1343998162-executor-1 unregistered
2025-06-05 07:14:25.254 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 07:14:25.254 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 07:14:25.259 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@34a2041b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:14:25.262 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 07:14:25.270 [dispatcher-event-loop-1 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 07:14:25.276 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 07:14:25.276 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 07:14:25.279 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 07:14:25.280 [dispatcher-event-loop-5 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 07:14:25.284 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 07:14:25.284 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 07:14:25.285 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-fe3e1db4-d470-475d-a970-8e6bc25645e0
2025-06-05 07:14:25.287 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-e0dee564-6a48-4546-9c89-8e956c0e9c48
2025-06-05 07:15:08.262 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 07:15:08.353 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 07:15:08.397 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:15:08.397 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 07:15:08.397 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:15:08.397 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 07:15:08.408 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 07:15:08.414 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 07:15:08.414 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 07:15:08.442 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 07:15:08.442 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 07:15:08.442 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 07:15:08.442 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 07:15:08.442 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 07:15:08.562 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 33303.
2025-06-05 07:15:08.576 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 07:15:08.592 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 07:15:08.601 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 07:15:08.602 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 07:15:08.603 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 07:15:08.613 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-cbc6e161-ae73-485f-8504-4a826c6ae333
2025-06-05 07:15:08.629 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 07:15:08.637 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 07:15:08.656 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @977ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 07:15:08.699 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 07:15:08.704 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 07:15:08.713 [main INFO ] org.sparkproject.jetty.server.Server - Started @1034ms
2025-06-05 07:15:08.732 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@4a68135e{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:15:08.732 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 07:15:08.748 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c82cd4f{/,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.800 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 07:15:08.804 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 07:15:08.816 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36757.
2025-06-05 07:15:08.816 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:36757
2025-06-05 07:15:08.818 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 07:15:08.822 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 36757, None)
2025-06-05 07:15:08.825 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:36757 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 36757, None)
2025-06-05 07:15:08.828 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 36757, None)
2025-06-05 07:15:08.828 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 36757, None)
2025-06-05 07:15:08.905 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@5c82cd4f{/,null,STOPPED,@Spark}
2025-06-05 07:15:08.906 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@761956ac{/jobs,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.907 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@304d0259{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.907 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3414a8c3{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.907 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cf518cf{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.908 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68d651f2{/stages,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.908 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e43e323{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.909 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48840594{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.910 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14823f76{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.910 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ed16657{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.911 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@113e13f9{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.912 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7979b8b7{/storage,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.912 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bc49bc5{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.913 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f66ffc8{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.913 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2def7a7a{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.914 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c080ef3{/environment,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.914 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ee6291f{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.915 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37e0292a{/executors,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.915 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35267fd4{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.916 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36a6bea6{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.916 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42373389{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.920 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a62c7cd{/static,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.921 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6726cc69{/,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.922 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33899f7a{/api,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.922 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2436ea2f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.923 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20cece0b{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 07:15:08.925 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4339baec{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:09.004 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 07:15:09.008 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 07:15:09.015 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d97caa4{/SQL,null,AVAILABLE,@Spark}
2025-06-05 07:15:09.016 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@474821de{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:09.017 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c83ae01{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 07:15:09.017 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69d45cca{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:09.023 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f94e148{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:15:10.406 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-05 07:15:10.415 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5abc5854{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-05 07:15:10.416 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66b40dd3{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:10.417 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54b2d002{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-05 07:15:10.417 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2095c331{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:10.418 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79ec57b8{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:15:10.422 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-8708d058-2e75-4b08-8eed-227b16fb42a2. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-05 07:15:10.435 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-8708d058-2e75-4b08-8eed-227b16fb42a2 resolved to file:/tmp/temporary-8708d058-2e75-4b08-8eed-227b16fb42a2.
2025-06-05 07:15:10.435 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-05 07:15:10.485 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-8708d058-2e75-4b08-8eed-227b16fb42a2/metadata using temp file file:/tmp/temporary-8708d058-2e75-4b08-8eed-227b16fb42a2/.metadata.df92e393-443f-428c-97cf-6330d8e559f7.tmp
2025-06-05 07:15:10.534 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-8708d058-2e75-4b08-8eed-227b16fb42a2/.metadata.df92e393-443f-428c-97cf-6330d8e559f7.tmp to file:/tmp/temporary-8708d058-2e75-4b08-8eed-227b16fb42a2/metadata
2025-06-05 07:15:10.549 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0]. Use file:/tmp/temporary-8708d058-2e75-4b08-8eed-227b16fb42a2 to store the query checkpoint.
2025-06-05 07:15:10.561 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@3118bfb5] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@16eed76]
2025-06-05 07:15:10.574 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:15:10.575 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:15:10.575 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-05 07:15:10.577 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-05 07:15:10.740 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 07:15:10.768 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 07:15:10.769 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:15:10.769 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:15:10.769 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082510768
2025-06-05 07:15:10.938 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-8708d058-2e75-4b08-8eed-227b16fb42a2/sources/0/0 using temp file file:/tmp/temporary-8708d058-2e75-4b08-8eed-227b16fb42a2/sources/0/.0.b4a681c5-89ac-42fd-beca-a08eb6ca15bd.tmp
2025-06-05 07:15:10.951 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-8708d058-2e75-4b08-8eed-227b16fb42a2/sources/0/.0.b4a681c5-89ac-42fd-beca-a08eb6ca15bd.tmp to file:/tmp/temporary-8708d058-2e75-4b08-8eed-227b16fb42a2/sources/0/0
2025-06-05 07:15:10.952 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-05 07:15:10.965 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-8708d058-2e75-4b08-8eed-227b16fb42a2/offsets/0 using temp file file:/tmp/temporary-8708d058-2e75-4b08-8eed-227b16fb42a2/offsets/.0.07330c98-f634-4f21-b68a-1ec8e3ba4698.tmp
2025-06-05 07:15:10.983 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-8708d058-2e75-4b08-8eed-227b16fb42a2/offsets/.0.07330c98-f634-4f21-b68a-1ec8e3ba4698.tmp to file:/tmp/temporary-8708d058-2e75-4b08-8eed-227b16fb42a2/offsets/0
2025-06-05 07:15:10.984 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749082510959,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-05 07:15:11.160 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082510959
2025-06-05 07:15:11.209 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:15:11.235 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:15:11.265 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082510959
2025-06-05 07:15:11.267 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:15:11.268 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:15:11.505 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 150.456926 ms
2025-06-05 07:15:11.627 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.046818 ms
2025-06-05 07:15:11.645 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.301448 ms
2025-06-05 07:15:11.694 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:272
2025-06-05 07:15:11.704 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:272) as input to shuffle 0
2025-06-05 07:15:11.708 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:272) with 1 output partitions
2025-06-05 07:15:11.709 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:272)
2025-06-05 07:15:11.709 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-05 07:15:11.711 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
2025-06-05 07:15:11.714 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:272), which has no missing parents
2025-06-05 07:15:11.786 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 45.5 KiB, free 9.2 GiB)
2025-06-05 07:15:11.809 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.8 KiB, free 9.2 GiB)
2025-06-05 07:15:11.811 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:36757 (size: 17.8 KiB, free: 9.2 GiB)
2025-06-05 07:15:11.814 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:15:11.822 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:272) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:15:11.822 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-05 07:15:11.854 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8302 bytes) 
2025-06-05 07:15:11.857 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8302 bytes) 
2025-06-05 07:15:11.863 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 07:15:11.863 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-05 07:15:11.960 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 18.692786 ms
2025-06-05 07:15:11.989 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.570168 ms
2025-06-05 07:15:12.003 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.064357 ms
2025-06-05 07:15:12.014 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:15:12.014 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:15:12.035 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:15:12.035 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:15:12.062 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:15:12.062 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:15:12.062 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082512062
2025-06-05 07:15:12.064 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:15:12.064 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:15:12.064 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082512062
2025-06-05 07:15:12.065 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-1, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Assigned to partition(s): clickstream-events-1
2025-06-05 07:15:12.065 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-2, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 07:15:12.069 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-1, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:15:12.069 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-2, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:15:12.075 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-1, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:15:12.075 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-2, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:15:12.101 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-2, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:15:12.101 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-1, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:15:12.602 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-2, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:15:12.602 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-1, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:15:12.602 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-2, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:15:12.602 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-1, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:15:12.603 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-1, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:15:12.603 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-2, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:15:12.701 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 2342 bytes result sent to driver
2025-06-05 07:15:12.701 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2342 bytes result sent to driver
2025-06-05 07:15:12.707 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 851 ms on phamviethoa (executor driver) (1/2)
2025-06-05 07:15:12.708 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 863 ms on phamviethoa (executor driver) (2/2)
2025-06-05 07:15:12.708 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 07:15:12.714 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (start at SparkClickstreamProcessor.java:272) finished in 0.991 s
2025-06-05 07:15:12.714 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
2025-06-05 07:15:12.714 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - running: Set()
2025-06-05 07:15:12.714 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
2025-06-05 07:15:12.714 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - failed: Set()
2025-06-05 07:15:12.715 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:272), which has no missing parents
2025-06-05 07:15:12.720 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-05 07:15:12.722 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-05 07:15:12.723 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:36757 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-05 07:15:12.723 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:15:12.724 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:272) (first 15 tasks are for partitions Vector(0))
2025-06-05 07:15:12.724 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-05 07:15:12.728 [dispatcher-event-loop-12 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
2025-06-05 07:15:12.729 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-05 07:15:12.768 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-05 07:15:12.769 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
2025-06-05 07:15:12.779 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 4038 bytes result sent to driver
2025-06-05 07:15:12.781 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 55 ms on phamviethoa (executor driver) (1/1)
2025-06-05 07:15:12.781 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-05 07:15:12.782 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:272) finished in 0.062 s
2025-06-05 07:15:12.784 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 07:15:12.784 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-05 07:15:12.785 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:272, took 1.091629 s
2025-06-05 07:15:12.977 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 14.684033 ms
2025-06-05 07:15:13.020 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:272
2025-06-05 07:15:13.021 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 1 (start at SparkClickstreamProcessor.java:272) with 2 output partitions
2025-06-05 07:15:13.021 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (start at SparkClickstreamProcessor.java:272)
2025-06-05 07:15:13.021 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-05 07:15:13.022 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-05 07:15:13.023 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[13] at start at SparkClickstreamProcessor.java:272), which has no missing parents
2025-06-05 07:15:13.044 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 54.6 KiB, free 9.2 GiB)
2025-06-05 07:15:13.047 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.8 KiB, free 9.2 GiB)
2025-06-05 07:15:13.048 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on phamviethoa:36757 (size: 20.8 KiB, free: 9.2 GiB)
2025-06-05 07:15:13.049 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:15:13.049 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at start at SparkClickstreamProcessor.java:272) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:15:13.050 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 2 tasks resource profile 0
2025-06-05 07:15:13.051 [dispatcher-event-loop-15 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 3) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8313 bytes) 
2025-06-05 07:15:13.051 [dispatcher-event-loop-15 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 4) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8313 bytes) 
2025-06-05 07:15:13.052 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 3)
2025-06-05 07:15:13.052 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 4)
2025-06-05 07:15:13.101 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 12.512018 ms
2025-06-05 07:15:13.109 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-2, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:15:13.111 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-1, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:15:13.114 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-2, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:15:13.115 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-1, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:15:13.615 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-2, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:15:13.615 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-2, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:15:13.616 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-2, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:15:13.616 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-1, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:15:13.616 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-1, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:15:13.617 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-1, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:15:13.627 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-05 07:15:13.627 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-05 07:15:13.635 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-05 07:15:13.635 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-05 07:15:13.635 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [9158b64e-a88a-4b44-a1d4-cfd32fbaf971] (1 queries & 0 savepoints) is rolled back.
2025-06-05 07:15:13.635 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [365b3d9c-a0af-4ffd-b1b7-a47c4631db16] (1 queries & 0 savepoints) is rolled back.
2025-06-05 07:15:13.635 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [20c8971e-5ada-463e-87c2-fb70c97c57d2] (0 queries & 0 savepoints) is committed.
2025-06-05 07:15:13.635 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [d7bf8a24-ab78-47e2-b7a9-9f5be27a12ff] (0 queries & 0 savepoints) is committed.
2025-06-05 07:15:13.637 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) ERROR] org.apache.spark.executor.Executor - Exception in task 1.0 in stage 2.0 (TID 4)
java.sql.SQLException: Cannot set null to non-nullable column #3 [event_time DateTime64(3)]
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:340)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-05 07:15:13.637 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) ERROR] org.apache.spark.executor.Executor - Exception in task 0.0 in stage 2.0 (TID 3)
java.sql.SQLException: Cannot set null to non-nullable column #3 [event_time DateTime64(3)]
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:340)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-05 07:15:13.645 [task-result-getter-3 WARN ] o.a.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 2.0 (TID 3) (phamviethoa executor driver): java.sql.SQLException: Cannot set null to non-nullable column #3 [event_time DateTime64(3)]
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:340)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

2025-06-05 07:15:13.646 [task-result-getter-3 ERROR] o.a.spark.scheduler.TaskSetManager - Task 0 in stage 2.0 failed 1 times; aborting job
2025-06-05 07:15:13.647 [task-result-getter-3 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2025-06-05 07:15:13.647 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 2.0 (TID 4) on phamviethoa, executor driver: java.sql.SQLException (Cannot set null to non-nullable column #3 [event_time DateTime64(3)]) [duplicate 1]
2025-06-05 07:15:13.647 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2025-06-05 07:15:13.648 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Cancelling stage 2
2025-06-05 07:15:13.648 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage cancelled
2025-06-05 07:15:13.649 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 2 (start at SparkClickstreamProcessor.java:272) failed in 0.626 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 3) (phamviethoa executor driver): java.sql.SQLException: Cannot set null to non-nullable column #3 [event_time DateTime64(3)]
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:340)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Driver stacktrace:
2025-06-05 07:15:13.650 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.spark.scheduler.DAGScheduler - Job 1 failed: start at SparkClickstreamProcessor.java:272, took 0.629460 s
2025-06-05 07:15:13.662 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] terminated with error
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 3) (phamviethoa executor driver): java.sql.SQLException: Cannot set null to non-nullable column #3 [event_time DateTime64(3)]
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:340)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1009)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1007)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:890)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.SparkClickstreamProcessor.lambda$main$a450ce84$1(SparkClickstreamProcessor.java:268)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
Caused by: java.sql.SQLException: Cannot set null to non-nullable column #3 [event_time DateTime64(3)]
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:340)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-05 07:15:13.663 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 07:15:13.665 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:15:13.665 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:15:13.665 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:15:13.666 [stream execution thread for [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = ed666df4-dd70-4fba-8590-8df7133b683e, runId = c792f70c-591c-4008-95d2-d1f6a4ba01b0] has been shutdown
2025-06-05 07:15:13.677 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-2, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:15:13.677 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-2, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:15:13.680 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:15:13.680 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:15:13.680 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:15:13.681 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:15:13.684 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-2 unregistered
2025-06-05 07:15:13.684 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-1, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:15:13.684 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-1, groupId=spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:15:13.685 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:15:13.685 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:15:13.685 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:15:13.685 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:15:13.687 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-4adb319e-136a-4494-931a-537f1c7bbf77-1785866553-executor-1 unregistered
2025-06-05 07:15:13.687 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 07:15:13.687 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 07:15:13.691 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@4a68135e{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:15:13.693 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 07:15:13.700 [dispatcher-event-loop-6 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 07:15:13.717 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 07:15:13.718 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 07:15:13.721 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 07:15:13.722 [dispatcher-event-loop-11 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 07:15:13.725 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 07:15:13.725 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 07:15:13.725 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-a1f89bc0-257f-4512-a8c2-621d67f339ed
2025-06-05 07:15:13.727 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-8708d058-2e75-4b08-8eed-227b16fb42a2
2025-06-05 07:15:45.819 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 07:15:45.898 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 07:15:45.942 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:15:45.943 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 07:15:45.943 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:15:45.943 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 07:15:45.954 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 07:15:45.960 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 07:15:45.961 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 07:15:45.988 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 07:15:45.988 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 07:15:45.989 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 07:15:45.989 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 07:15:45.989 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 07:15:46.109 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 39615.
2025-06-05 07:15:46.122 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 07:15:46.138 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 07:15:46.148 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 07:15:46.149 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 07:15:46.151 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 07:15:46.162 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-2bd1805b-2658-46d4-bd5d-b06bbb199ae8
2025-06-05 07:15:46.181 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 07:15:46.189 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 07:15:46.208 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1029ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 07:15:46.249 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 07:15:46.254 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 07:15:46.262 [main INFO ] org.sparkproject.jetty.server.Server - Started @1084ms
2025-06-05 07:15:46.278 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@57e2896e{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:15:46.278 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 07:15:46.288 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49cf9028{/,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.336 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 07:15:46.341 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 07:15:46.352 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45089.
2025-06-05 07:15:46.352 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:45089
2025-06-05 07:15:46.354 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 07:15:46.358 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 45089, None)
2025-06-05 07:15:46.360 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:45089 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 45089, None)
2025-06-05 07:15:46.362 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 45089, None)
2025-06-05 07:15:46.363 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 45089, None)
2025-06-05 07:15:46.441 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@49cf9028{/,null,STOPPED,@Spark}
2025-06-05 07:15:46.441 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3414a8c3{/jobs,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.442 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cf518cf{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.443 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e43e323{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.443 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10643593{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.443 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@eca6a74{/stages,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.444 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48840594{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.444 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@113e13f9{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.445 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7979b8b7{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.445 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bc49bc5{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.446 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f66ffc8{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.446 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2def7a7a{/storage,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.447 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c080ef3{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.447 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ee6291f{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.448 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37e0292a{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.448 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35267fd4{/environment,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.449 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36a6bea6{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.449 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42373389{/executors,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.449 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a62c7cd{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.450 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c36db44{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.451 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7903d448{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.455 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42ea287{/static,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.456 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@644ded04{/,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.456 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13d9261f{/api,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.457 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e8a1ab4{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.457 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1aabf50d{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.460 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bc7e78e{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.543 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 07:15:46.546 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 07:15:46.553 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47d023b7{/SQL,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.554 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d64c100{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.554 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e73d5eb{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.555 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d904ff1{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:46.561 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e1dde44{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:15:47.993 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-05 07:15:48.009 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1191029d{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-05 07:15:48.010 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d1c63af{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:48.011 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b849fa6{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-05 07:15:48.012 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e9ea32f{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-05 07:15:48.013 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40ddf339{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:15:48.020 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-fd63046f-eaa8-4e56-822f-da4e2f578c84. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-05 07:15:48.035 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-fd63046f-eaa8-4e56-822f-da4e2f578c84 resolved to file:/tmp/temporary-fd63046f-eaa8-4e56-822f-da4e2f578c84.
2025-06-05 07:15:48.035 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-05 07:15:48.080 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-fd63046f-eaa8-4e56-822f-da4e2f578c84/metadata using temp file file:/tmp/temporary-fd63046f-eaa8-4e56-822f-da4e2f578c84/.metadata.fa86b789-5456-4bf0-a84d-35d741efa559.tmp
2025-06-05 07:15:48.129 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-fd63046f-eaa8-4e56-822f-da4e2f578c84/.metadata.fa86b789-5456-4bf0-a84d-35d741efa559.tmp to file:/tmp/temporary-fd63046f-eaa8-4e56-822f-da4e2f578c84/metadata
2025-06-05 07:15:48.143 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db]. Use file:/tmp/temporary-fd63046f-eaa8-4e56-822f-da4e2f578c84 to store the query checkpoint.
2025-06-05 07:15:48.148 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@59dc4ca4] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@77623e7f]
2025-06-05 07:15:48.161 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:15:48.162 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:15:48.162 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-05 07:15:48.164 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-05 07:15:48.323 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 07:15:48.352 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 07:15:48.353 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:15:48.353 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:15:48.353 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082548353
2025-06-05 07:15:48.524 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-fd63046f-eaa8-4e56-822f-da4e2f578c84/sources/0/0 using temp file file:/tmp/temporary-fd63046f-eaa8-4e56-822f-da4e2f578c84/sources/0/.0.2123dc62-4659-411e-8db0-5c1e7d9d6432.tmp
2025-06-05 07:15:48.537 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-fd63046f-eaa8-4e56-822f-da4e2f578c84/sources/0/.0.2123dc62-4659-411e-8db0-5c1e7d9d6432.tmp to file:/tmp/temporary-fd63046f-eaa8-4e56-822f-da4e2f578c84/sources/0/0
2025-06-05 07:15:48.538 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-05 07:15:48.549 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-fd63046f-eaa8-4e56-822f-da4e2f578c84/offsets/0 using temp file file:/tmp/temporary-fd63046f-eaa8-4e56-822f-da4e2f578c84/offsets/.0.e85ba6bf-bfd3-432f-aca8-9bac20642d3f.tmp
2025-06-05 07:15:48.567 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-fd63046f-eaa8-4e56-822f-da4e2f578c84/offsets/.0.e85ba6bf-bfd3-432f-aca8-9bac20642d3f.tmp to file:/tmp/temporary-fd63046f-eaa8-4e56-822f-da4e2f578c84/offsets/0
2025-06-05 07:15:48.568 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749082548544,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-05 07:15:48.735 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082548544
2025-06-05 07:15:48.785 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:15:48.811 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:15:48.843 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082548544
2025-06-05 07:15:48.844 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:15:48.846 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:15:49.071 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 136.440139 ms
2025-06-05 07:15:49.184 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.551252 ms
2025-06-05 07:15:49.201 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.205936 ms
2025-06-05 07:15:49.252 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:273
2025-06-05 07:15:49.261 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:273) as input to shuffle 0
2025-06-05 07:15:49.265 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:273) with 1 output partitions
2025-06-05 07:15:49.265 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:273)
2025-06-05 07:15:49.265 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-05 07:15:49.266 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
2025-06-05 07:15:49.268 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:273), which has no missing parents
2025-06-05 07:15:49.338 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 45.4 KiB, free 9.2 GiB)
2025-06-05 07:15:49.355 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.8 KiB, free 9.2 GiB)
2025-06-05 07:15:49.356 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:45089 (size: 17.8 KiB, free: 9.2 GiB)
2025-06-05 07:15:49.360 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:15:49.374 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:273) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:15:49.375 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-05 07:15:49.413 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8301 bytes) 
2025-06-05 07:15:49.416 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8301 bytes) 
2025-06-05 07:15:49.421 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-05 07:15:49.422 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 07:15:49.514 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 16.20521 ms
2025-06-05 07:15:49.549 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.770296 ms
2025-06-05 07:15:49.564 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.613432 ms
2025-06-05 07:15:49.576 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:15:49.576 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:15:49.599 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:15:49.599 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:15:49.632 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:15:49.632 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:15:49.632 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082549632
2025-06-05 07:15:49.633 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:15:49.633 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:15:49.633 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082549632
2025-06-05 07:15:49.634 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-2, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 07:15:49.634 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-1, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Assigned to partition(s): clickstream-events-1
2025-06-05 07:15:49.639 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-2, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:15:49.639 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-1, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:15:49.645 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-2, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:15:49.645 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-1, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:15:49.671 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-1, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:15:49.671 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-2, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:15:50.173 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-2, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:15:50.173 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-1, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:15:50.173 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-1, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:15:50.173 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-2, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:15:50.174 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-2, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:15:50.174 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-1, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:15:50.258 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 2342 bytes result sent to driver
2025-06-05 07:15:50.258 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2342 bytes result sent to driver
2025-06-05 07:15:50.264 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 849 ms on phamviethoa (executor driver) (1/2)
2025-06-05 07:15:50.265 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 858 ms on phamviethoa (executor driver) (2/2)
2025-06-05 07:15:50.266 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 07:15:50.272 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (start at SparkClickstreamProcessor.java:273) finished in 0.997 s
2025-06-05 07:15:50.272 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
2025-06-05 07:15:50.273 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - running: Set()
2025-06-05 07:15:50.273 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
2025-06-05 07:15:50.273 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - failed: Set()
2025-06-05 07:15:50.275 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:273), which has no missing parents
2025-06-05 07:15:50.280 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-05 07:15:50.282 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-05 07:15:50.282 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:45089 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-05 07:15:50.283 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:15:50.284 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:273) (first 15 tasks are for partitions Vector(0))
2025-06-05 07:15:50.284 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-05 07:15:50.287 [dispatcher-event-loop-12 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
2025-06-05 07:15:50.287 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-05 07:15:50.314 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-05 07:15:50.315 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
2025-06-05 07:15:50.324 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 4038 bytes result sent to driver
2025-06-05 07:15:50.325 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 40 ms on phamviethoa (executor driver) (1/1)
2025-06-05 07:15:50.325 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-05 07:15:50.326 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:273) finished in 0.047 s
2025-06-05 07:15:50.328 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 07:15:50.328 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-05 07:15:50.329 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:273, took 1.076779 s
2025-06-05 07:15:50.478 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.40007 ms
2025-06-05 07:15:50.509 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:273
2025-06-05 07:15:50.510 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 1 (start at SparkClickstreamProcessor.java:273) with 2 output partitions
2025-06-05 07:15:50.510 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (start at SparkClickstreamProcessor.java:273)
2025-06-05 07:15:50.510 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-05 07:15:50.510 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-05 07:15:50.511 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[13] at start at SparkClickstreamProcessor.java:273), which has no missing parents
2025-06-05 07:15:50.529 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 54.6 KiB, free 9.2 GiB)
2025-06-05 07:15:50.532 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.9 KiB, free 9.2 GiB)
2025-06-05 07:15:50.533 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on phamviethoa:45089 (size: 20.9 KiB, free: 9.2 GiB)
2025-06-05 07:15:50.533 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:15:50.534 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at start at SparkClickstreamProcessor.java:273) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:15:50.534 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 2 tasks resource profile 0
2025-06-05 07:15:50.535 [dispatcher-event-loop-15 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 3) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8312 bytes) 
2025-06-05 07:15:50.535 [dispatcher-event-loop-15 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 4) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8312 bytes) 
2025-06-05 07:15:50.536 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 4)
2025-06-05 07:15:50.536 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 3)
2025-06-05 07:15:50.595 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.297522 ms
2025-06-05 07:15:50.606 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-1, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:15:50.606 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-2, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:15:50.609 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-2, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:15:50.610 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-1, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:15:51.112 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-2, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:15:51.112 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-2, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:15:51.112 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-1, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:15:51.112 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-1, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:15:51.112 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-2, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:15:51.113 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-1, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:15:51.125 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-05 07:15:51.125 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-05 07:15:51.134 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-05 07:15:51.134 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-05 07:15:51.134 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [de233213-ffe6-428b-95f4-6adfa488d3f6] (1 queries & 0 savepoints) is rolled back.
2025-06-05 07:15:51.134 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [5a9b2747-f091-4791-b1d5-f04f54c17e4c] (1 queries & 0 savepoints) is rolled back.
2025-06-05 07:15:51.134 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [0fb61f9e-d07a-4617-bb6a-be542977b590] (0 queries & 0 savepoints) is committed.
2025-06-05 07:15:51.134 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [4135f49d-6ed8-4d79-80cc-4d7ae9e02393] (0 queries & 0 savepoints) is committed.
2025-06-05 07:15:51.136 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) ERROR] org.apache.spark.executor.Executor - Exception in task 1.0 in stage 2.0 (TID 4)
java.sql.SQLException: Cannot set null to non-nullable column #11 [traffic_source String]
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:340)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-05 07:15:51.136 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) ERROR] org.apache.spark.executor.Executor - Exception in task 0.0 in stage 2.0 (TID 3)
java.sql.SQLException: Cannot set null to non-nullable column #11 [traffic_source String]
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:340)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-05 07:15:51.144 [task-result-getter-3 WARN ] o.a.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 2.0 (TID 4) (phamviethoa executor driver): java.sql.SQLException: Cannot set null to non-nullable column #11 [traffic_source String]
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:340)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

2025-06-05 07:15:51.145 [task-result-getter-3 ERROR] o.a.spark.scheduler.TaskSetManager - Task 1 in stage 2.0 failed 1 times; aborting job
2025-06-05 07:15:51.145 [task-result-getter-3 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2025-06-05 07:15:51.146 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 2.0 (TID 3) on phamviethoa, executor driver: java.sql.SQLException (Cannot set null to non-nullable column #11 [traffic_source String]) [duplicate 1]
2025-06-05 07:15:51.146 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2025-06-05 07:15:51.147 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Cancelling stage 2
2025-06-05 07:15:51.147 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage cancelled
2025-06-05 07:15:51.148 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 2 (start at SparkClickstreamProcessor.java:273) failed in 0.636 s due to Job aborted due to stage failure: Task 1 in stage 2.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2.0 (TID 4) (phamviethoa executor driver): java.sql.SQLException: Cannot set null to non-nullable column #11 [traffic_source String]
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:340)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Driver stacktrace:
2025-06-05 07:15:51.149 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.spark.scheduler.DAGScheduler - Job 1 failed: start at SparkClickstreamProcessor.java:273, took 0.639675 s
2025-06-05 07:15:51.163 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] terminated with error
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2.0 (TID 4) (phamviethoa executor driver): java.sql.SQLException: Cannot set null to non-nullable column #11 [traffic_source String]
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:340)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1009)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1007)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:890)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.SparkClickstreamProcessor.lambda$main$a450ce84$1(SparkClickstreamProcessor.java:269)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
Caused by: java.sql.SQLException: Cannot set null to non-nullable column #11 [traffic_source String]
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:340)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-05 07:15:51.164 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 07:15:51.166 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:15:51.166 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:15:51.166 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:15:51.166 [stream execution thread for [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = e3b23d3f-4a66-4b02-884f-4822b1b386bf, runId = f232bd33-8bff-40fc-95d3-166bc2eb14db] has been shutdown
2025-06-05 07:15:51.179 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-1, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:15:51.179 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-1, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:15:51.182 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:15:51.182 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:15:51.182 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:15:51.182 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:15:51.184 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-1 unregistered
2025-06-05 07:15:51.184 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-2, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:15:51.184 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-2, groupId=spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:15:51.186 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:15:51.186 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:15:51.186 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:15:51.186 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:15:51.187 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-7ad6dd97-e2e3-4a37-9f37-124c01df8f9a-326510522-executor-2 unregistered
2025-06-05 07:15:51.188 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 07:15:51.188 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 07:15:51.191 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@57e2896e{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:15:51.193 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 07:15:51.199 [dispatcher-event-loop-6 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 07:15:51.205 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 07:15:51.205 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 07:15:51.209 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 07:15:51.210 [dispatcher-event-loop-11 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 07:15:51.214 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 07:15:51.214 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 07:15:51.214 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-fd63046f-eaa8-4e56-822f-da4e2f578c84
2025-06-05 07:15:51.216 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-351aabb7-ddbb-4d2e-b136-865245fe1d42
2025-06-05 07:17:24.792 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 07:17:24.890 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 07:17:24.936 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:17:24.937 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 07:17:24.937 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:17:24.937 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 07:17:24.949 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 07:17:24.956 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 07:17:24.956 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 07:17:24.984 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 07:17:24.985 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 07:17:24.985 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 07:17:24.985 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 07:17:24.985 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 07:17:25.110 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 46585.
2025-06-05 07:17:25.123 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 07:17:25.140 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 07:17:25.149 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 07:17:25.149 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 07:17:25.151 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 07:17:25.161 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-705b14c3-1155-406d-8c2a-3130fe50f545
2025-06-05 07:17:25.177 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 07:17:25.185 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 07:17:25.204 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1064ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 07:17:25.248 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 07:17:25.253 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 07:17:25.261 [main INFO ] org.sparkproject.jetty.server.Server - Started @1121ms
2025-06-05 07:17:25.277 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@50f40653{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:17:25.277 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 07:17:25.287 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35835e65{/,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.339 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 07:17:25.343 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 07:17:25.356 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38327.
2025-06-05 07:17:25.356 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:38327
2025-06-05 07:17:25.357 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 07:17:25.361 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 38327, None)
2025-06-05 07:17:25.364 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:38327 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 38327, None)
2025-06-05 07:17:25.367 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 38327, None)
2025-06-05 07:17:25.367 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 38327, None)
2025-06-05 07:17:25.448 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@35835e65{/,null,STOPPED,@Spark}
2025-06-05 07:17:25.449 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/jobs,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.449 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.450 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cc9385{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.450 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7915bca3{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.451 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/stages,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.451 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67b4ec{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.452 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.452 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.453 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.453 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.453 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/storage,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.454 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.454 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.455 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.455 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/environment,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.456 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.456 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/executors,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.456 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397ef2{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.457 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e93c1f{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.458 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9b21bd3{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.462 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7661b5a{/static,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.463 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b6d92e{/,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.464 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7899de11{/api,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.464 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f951a7f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.465 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c777e7b{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.467 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62db3891{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.550 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 07:17:25.554 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 07:17:25.561 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6732726{/SQL,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.561 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d64c581{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.562 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d64c100{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.562 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fdf17dc{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 07:17:25.568 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ff8a9dc{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:17:27.101 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-05 07:17:27.112 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c3007d{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-05 07:17:27.113 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7296fe0b{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-05 07:17:27.113 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d1c63af{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-05 07:17:27.114 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3909a854{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-05 07:17:27.115 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56ba8e8c{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:17:27.119 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-6c8b2657-4b25-441f-9ec6-450406d8a817. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-05 07:17:27.132 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-6c8b2657-4b25-441f-9ec6-450406d8a817 resolved to file:/tmp/temporary-6c8b2657-4b25-441f-9ec6-450406d8a817.
2025-06-05 07:17:27.132 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-05 07:17:27.181 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-6c8b2657-4b25-441f-9ec6-450406d8a817/metadata using temp file file:/tmp/temporary-6c8b2657-4b25-441f-9ec6-450406d8a817/.metadata.f9f9cb06-6ec6-462b-a0fa-19bd5d086ed7.tmp
2025-06-05 07:17:27.227 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-6c8b2657-4b25-441f-9ec6-450406d8a817/.metadata.f9f9cb06-6ec6-462b-a0fa-19bd5d086ed7.tmp to file:/tmp/temporary-6c8b2657-4b25-441f-9ec6-450406d8a817/metadata
2025-06-05 07:17:27.241 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136]. Use file:/tmp/temporary-6c8b2657-4b25-441f-9ec6-450406d8a817 to store the query checkpoint.
2025-06-05 07:17:27.246 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@64b41e4d] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@124c5c84]
2025-06-05 07:17:27.259 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:17:27.260 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:17:27.260 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-05 07:17:27.261 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-05 07:17:27.419 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 07:17:27.449 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 07:17:27.450 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:17:27.450 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:17:27.450 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082647449
2025-06-05 07:17:27.620 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-6c8b2657-4b25-441f-9ec6-450406d8a817/sources/0/0 using temp file file:/tmp/temporary-6c8b2657-4b25-441f-9ec6-450406d8a817/sources/0/.0.f934e960-b3b7-47ca-9813-5ceec42af648.tmp
2025-06-05 07:17:27.635 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-6c8b2657-4b25-441f-9ec6-450406d8a817/sources/0/.0.f934e960-b3b7-47ca-9813-5ceec42af648.tmp to file:/tmp/temporary-6c8b2657-4b25-441f-9ec6-450406d8a817/sources/0/0
2025-06-05 07:17:27.635 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-05 07:17:27.646 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-6c8b2657-4b25-441f-9ec6-450406d8a817/offsets/0 using temp file file:/tmp/temporary-6c8b2657-4b25-441f-9ec6-450406d8a817/offsets/.0.d94a3e9f-3387-4dcb-8ca9-548e6236bc14.tmp
2025-06-05 07:17:27.665 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-6c8b2657-4b25-441f-9ec6-450406d8a817/offsets/.0.d94a3e9f-3387-4dcb-8ca9-548e6236bc14.tmp to file:/tmp/temporary-6c8b2657-4b25-441f-9ec6-450406d8a817/offsets/0
2025-06-05 07:17:27.666 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749082647642,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-05 07:17:27.847 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082647642
2025-06-05 07:17:27.897 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:17:27.924 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:17:27.958 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082647642
2025-06-05 07:17:27.960 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:17:27.961 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:17:28.210 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 146.486959 ms
2025-06-05 07:17:28.331 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.92908 ms
2025-06-05 07:17:28.347 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.025124 ms
2025-06-05 07:17:28.397 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:273
2025-06-05 07:17:28.408 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:273) as input to shuffle 0
2025-06-05 07:17:28.412 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:273) with 1 output partitions
2025-06-05 07:17:28.413 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:273)
2025-06-05 07:17:28.413 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-05 07:17:28.415 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
2025-06-05 07:17:28.418 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:273), which has no missing parents
2025-06-05 07:17:28.486 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 45.4 KiB, free 9.2 GiB)
2025-06-05 07:17:28.510 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.8 KiB, free 9.2 GiB)
2025-06-05 07:17:28.512 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:38327 (size: 17.8 KiB, free: 9.2 GiB)
2025-06-05 07:17:28.514 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:17:28.523 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:273) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:17:28.524 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-05 07:17:28.564 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8301 bytes) 
2025-06-05 07:17:28.567 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8301 bytes) 
2025-06-05 07:17:28.575 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-05 07:17:28.575 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 07:17:28.707 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 24.838698 ms
2025-06-05 07:17:28.739 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.128318 ms
2025-06-05 07:17:28.755 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.806894 ms
2025-06-05 07:17:28.765 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:17:28.765 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:17:28.785 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:17:28.785 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:17:28.811 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:17:28.812 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:17:28.812 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082648811
2025-06-05 07:17:28.812 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:17:28.812 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:17:28.812 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082648811
2025-06-05 07:17:28.813 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-2, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 07:17:28.813 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-1, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Assigned to partition(s): clickstream-events-1
2025-06-05 07:17:28.818 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-2, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:17:28.818 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-1, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:17:28.823 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-1, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:17:28.823 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-2, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:17:28.847 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-2, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:17:28.847 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-1, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:17:29.350 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-1, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:17:29.350 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-2, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:17:29.350 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-2, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:17:29.350 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-1, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:17:29.351 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-2, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:17:29.351 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-1, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:17:29.448 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 2342 bytes result sent to driver
2025-06-05 07:17:29.448 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2342 bytes result sent to driver
2025-06-05 07:17:29.454 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 887 ms on phamviethoa (executor driver) (1/2)
2025-06-05 07:17:29.455 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 900 ms on phamviethoa (executor driver) (2/2)
2025-06-05 07:17:29.456 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 07:17:29.459 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (start at SparkClickstreamProcessor.java:273) finished in 1.032 s
2025-06-05 07:17:29.459 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
2025-06-05 07:17:29.460 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - running: Set()
2025-06-05 07:17:29.460 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
2025-06-05 07:17:29.460 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - failed: Set()
2025-06-05 07:17:29.461 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:273), which has no missing parents
2025-06-05 07:17:29.465 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-05 07:17:29.467 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-05 07:17:29.467 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:38327 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-05 07:17:29.468 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:17:29.469 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:273) (first 15 tasks are for partitions Vector(0))
2025-06-05 07:17:29.469 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-05 07:17:29.472 [dispatcher-event-loop-12 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
2025-06-05 07:17:29.473 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-05 07:17:29.503 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-05 07:17:29.504 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
2025-06-05 07:17:29.512 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 4038 bytes result sent to driver
2025-06-05 07:17:29.514 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 44 ms on phamviethoa (executor driver) (1/1)
2025-06-05 07:17:29.514 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-05 07:17:29.515 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:273) finished in 0.050 s
2025-06-05 07:17:29.516 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 07:17:29.516 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-05 07:17:29.518 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:273, took 1.120569 s
2025-06-05 07:17:29.673 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.716439 ms
2025-06-05 07:17:29.706 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:273
2025-06-05 07:17:29.706 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 1 (start at SparkClickstreamProcessor.java:273) with 2 output partitions
2025-06-05 07:17:29.706 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (start at SparkClickstreamProcessor.java:273)
2025-06-05 07:17:29.706 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-05 07:17:29.707 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-05 07:17:29.707 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[13] at start at SparkClickstreamProcessor.java:273), which has no missing parents
2025-06-05 07:17:29.726 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 54.6 KiB, free 9.2 GiB)
2025-06-05 07:17:29.729 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.9 KiB, free 9.2 GiB)
2025-06-05 07:17:29.729 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on phamviethoa:38327 (size: 20.9 KiB, free: 9.2 GiB)
2025-06-05 07:17:29.730 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:17:29.730 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at start at SparkClickstreamProcessor.java:273) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:17:29.730 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 2 tasks resource profile 0
2025-06-05 07:17:29.731 [dispatcher-event-loop-15 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 3) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8312 bytes) 
2025-06-05 07:17:29.731 [dispatcher-event-loop-15 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 4) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8312 bytes) 
2025-06-05 07:17:29.732 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 4)
2025-06-05 07:17:29.732 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 3)
2025-06-05 07:17:29.794 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.980671 ms
2025-06-05 07:17:29.802 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-2, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:17:29.802 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-1, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:17:29.806 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-2, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:17:29.806 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-1, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:17:30.307 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-2, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:17:30.307 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-2, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:17:30.308 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-2, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:17:30.308 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-1, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:17:30.308 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-1, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:17:30.309 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-1, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:17:30.320 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-05 07:17:30.320 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-05 07:17:30.339 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-05 07:17:30.339 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-05 07:17:30.339 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [08e7ff6b-4225-447f-9a08-369013611c86] (1 queries & 0 savepoints) is rolled back.
2025-06-05 07:17:30.339 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [a229207c-2ec9-41cb-9d2c-6cd02616d446] (1 queries & 0 savepoints) is rolled back.
2025-06-05 07:17:30.339 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [307fe49d-2d11-4df6-97d3-433d84fc4d80] (0 queries & 0 savepoints) is committed.
2025-06-05 07:17:30.339 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [de71ce7a-9afd-4c66-b299-6aca561aa3ca] (0 queries & 0 savepoints) is committed.
2025-06-05 07:17:30.342 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) ERROR] org.apache.spark.executor.Executor - Exception in task 1.0 in stage 2.0 (TID 4)
java.sql.SQLException: Code: 47. DB::Exception: Missing columns: 'kafka_timestamp' 'processing_time' while processing query: 'SELECT event_id, event_name, event_time, user_id, session_id, app_id, platform, page_url, geo_country, geo_city, traffic_source, traffic_medium, session_id, kafka_timestamp, processing_time FROM events WHERE 0', required columns: 'event_time' 'user_id' 'event_name' 'event_id' 'traffic_medium' 'session_id' 'app_id' 'platform' 'page_url' 'geo_country' 'processing_time' 'geo_city' 'traffic_source' 'kafka_timestamp', maybe you meant: ['event_time','user_id','event_name','event_id','traffic_medium','session_id','app_id','platform','page_url','geo_country','geo_city','traffic_source']. (UNKNOWN_IDENTIFIER) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream_1k]@-1551307789
	at com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:85)
	at com.clickhouse.jdbc.SqlExceptionUtils.create(SqlExceptionUtils.java:31)
	at com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:90)
	at com.clickhouse.jdbc.internal.ClickHouseConnectionImpl.getTableColumns(ClickHouseConnectionImpl.java:267)
	at com.clickhouse.jdbc.internal.ClickHouseConnectionImpl.prepareStatement(ClickHouseConnectionImpl.java:843)
	at com.clickhouse.jdbc.ClickHouseConnection.prepareStatement(ClickHouseConnection.java:121)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:710)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.io.IOException: Code: 47. DB::Exception: Missing columns: 'kafka_timestamp' 'processing_time' while processing query: 'SELECT event_id, event_name, event_time, user_id, session_id, app_id, platform, page_url, geo_country, geo_city, traffic_source, traffic_medium, session_id, kafka_timestamp, processing_time FROM events WHERE 0', required columns: 'event_time' 'user_id' 'event_name' 'event_id' 'traffic_medium' 'session_id' 'app_id' 'platform' 'page_url' 'geo_country' 'processing_time' 'geo_city' 'traffic_source' 'kafka_timestamp', maybe you meant: ['event_time','user_id','event_name','event_id','traffic_medium','session_id','app_id','platform','page_url','geo_country','geo_city','traffic_source']. (UNKNOWN_IDENTIFIER) (version 22.1.3.7 (official build))

	at com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)
	at com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)
	at com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)
	at com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)
	at com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)
	at com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)
	at com.clickhouse.jdbc.internal.ClickHouseConnectionImpl.getTableColumns(ClickHouseConnectionImpl.java:264)
	... 17 common frames omitted
2025-06-05 07:17:30.342 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) ERROR] org.apache.spark.executor.Executor - Exception in task 0.0 in stage 2.0 (TID 3)
java.sql.SQLException: Code: 47. DB::Exception: Missing columns: 'kafka_timestamp' 'processing_time' while processing query: 'SELECT event_id, event_name, event_time, user_id, session_id, app_id, platform, page_url, geo_country, geo_city, traffic_source, traffic_medium, session_id, kafka_timestamp, processing_time FROM events WHERE 0', required columns: 'event_time' 'user_id' 'event_name' 'event_id' 'traffic_medium' 'session_id' 'app_id' 'platform' 'page_url' 'geo_country' 'processing_time' 'geo_city' 'traffic_source' 'kafka_timestamp', maybe you meant: ['event_time','user_id','event_name','event_id','traffic_medium','session_id','app_id','platform','page_url','geo_country','geo_city','traffic_source']. (UNKNOWN_IDENTIFIER) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream_1k]@-1551307789
	at com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:85)
	at com.clickhouse.jdbc.SqlExceptionUtils.create(SqlExceptionUtils.java:31)
	at com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:90)
	at com.clickhouse.jdbc.internal.ClickHouseConnectionImpl.getTableColumns(ClickHouseConnectionImpl.java:267)
	at com.clickhouse.jdbc.internal.ClickHouseConnectionImpl.prepareStatement(ClickHouseConnectionImpl.java:843)
	at com.clickhouse.jdbc.ClickHouseConnection.prepareStatement(ClickHouseConnection.java:121)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:710)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.io.IOException: Code: 47. DB::Exception: Missing columns: 'kafka_timestamp' 'processing_time' while processing query: 'SELECT event_id, event_name, event_time, user_id, session_id, app_id, platform, page_url, geo_country, geo_city, traffic_source, traffic_medium, session_id, kafka_timestamp, processing_time FROM events WHERE 0', required columns: 'event_time' 'user_id' 'event_name' 'event_id' 'traffic_medium' 'session_id' 'app_id' 'platform' 'page_url' 'geo_country' 'processing_time' 'geo_city' 'traffic_source' 'kafka_timestamp', maybe you meant: ['event_time','user_id','event_name','event_id','traffic_medium','session_id','app_id','platform','page_url','geo_country','geo_city','traffic_source']. (UNKNOWN_IDENTIFIER) (version 22.1.3.7 (official build))

	at com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)
	at com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)
	at com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)
	at com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)
	at com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)
	at com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)
	at com.clickhouse.jdbc.internal.ClickHouseConnectionImpl.getTableColumns(ClickHouseConnectionImpl.java:264)
	... 17 common frames omitted
2025-06-05 07:17:30.351 [task-result-getter-1 WARN ] o.a.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 2.0 (TID 4) (phamviethoa executor driver): java.sql.SQLException: Code: 47. DB::Exception: Missing columns: 'kafka_timestamp' 'processing_time' while processing query: 'SELECT event_id, event_name, event_time, user_id, session_id, app_id, platform, page_url, geo_country, geo_city, traffic_source, traffic_medium, session_id, kafka_timestamp, processing_time FROM events WHERE 0', required columns: 'event_time' 'user_id' 'event_name' 'event_id' 'traffic_medium' 'session_id' 'app_id' 'platform' 'page_url' 'geo_country' 'processing_time' 'geo_city' 'traffic_source' 'kafka_timestamp', maybe you meant: ['event_time','user_id','event_name','event_id','traffic_medium','session_id','app_id','platform','page_url','geo_country','geo_city','traffic_source']. (UNKNOWN_IDENTIFIER) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream_1k]@-1551307789
	at com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:85)
	at com.clickhouse.jdbc.SqlExceptionUtils.create(SqlExceptionUtils.java:31)
	at com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:90)
	at com.clickhouse.jdbc.internal.ClickHouseConnectionImpl.getTableColumns(ClickHouseConnectionImpl.java:267)
	at com.clickhouse.jdbc.internal.ClickHouseConnectionImpl.prepareStatement(ClickHouseConnectionImpl.java:843)
	at com.clickhouse.jdbc.ClickHouseConnection.prepareStatement(ClickHouseConnection.java:121)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:710)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.io.IOException: Code: 47. DB::Exception: Missing columns: 'kafka_timestamp' 'processing_time' while processing query: 'SELECT event_id, event_name, event_time, user_id, session_id, app_id, platform, page_url, geo_country, geo_city, traffic_source, traffic_medium, session_id, kafka_timestamp, processing_time FROM events WHERE 0', required columns: 'event_time' 'user_id' 'event_name' 'event_id' 'traffic_medium' 'session_id' 'app_id' 'platform' 'page_url' 'geo_country' 'processing_time' 'geo_city' 'traffic_source' 'kafka_timestamp', maybe you meant: ['event_time','user_id','event_name','event_id','traffic_medium','session_id','app_id','platform','page_url','geo_country','geo_city','traffic_source']. (UNKNOWN_IDENTIFIER) (version 22.1.3.7 (official build))

	at com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)
	at com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)
	at com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)
	at com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)
	at com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)
	at com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)
	at com.clickhouse.jdbc.internal.ClickHouseConnectionImpl.getTableColumns(ClickHouseConnectionImpl.java:264)
	... 17 more

2025-06-05 07:17:30.351 [task-result-getter-1 ERROR] o.a.spark.scheduler.TaskSetManager - Task 1 in stage 2.0 failed 1 times; aborting job
2025-06-05 07:17:30.352 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2025-06-05 07:17:30.352 [task-result-getter-3 INFO ] o.a.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 2.0 (TID 3) on phamviethoa, executor driver: java.sql.SQLException (Code: 47. DB::Exception: Missing columns: 'kafka_timestamp' 'processing_time' while processing query: 'SELECT event_id, event_name, event_time, user_id, session_id, app_id, platform, page_url, geo_country, geo_city, traffic_source, traffic_medium, session_id, kafka_timestamp, processing_time FROM events WHERE 0', required columns: 'event_time' 'user_id' 'event_name' 'event_id' 'traffic_medium' 'session_id' 'app_id' 'platform' 'page_url' 'geo_country' 'processing_time' 'geo_city' 'traffic_source' 'kafka_timestamp', maybe you meant: ['event_time','user_id','event_name','event_id','traffic_medium','session_id','app_id','platform','page_url','geo_country','geo_city','traffic_source']. (UNKNOWN_IDENTIFIER) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream_1k]@-1551307789) [duplicate 1]
2025-06-05 07:17:30.352 [task-result-getter-3 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2025-06-05 07:17:30.353 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Cancelling stage 2
2025-06-05 07:17:30.353 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage cancelled
2025-06-05 07:17:30.354 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 2 (start at SparkClickstreamProcessor.java:273) failed in 0.646 s due to Job aborted due to stage failure: Task 1 in stage 2.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2.0 (TID 4) (phamviethoa executor driver): java.sql.SQLException: Code: 47. DB::Exception: Missing columns: 'kafka_timestamp' 'processing_time' while processing query: 'SELECT event_id, event_name, event_time, user_id, session_id, app_id, platform, page_url, geo_country, geo_city, traffic_source, traffic_medium, session_id, kafka_timestamp, processing_time FROM events WHERE 0', required columns: 'event_time' 'user_id' 'event_name' 'event_id' 'traffic_medium' 'session_id' 'app_id' 'platform' 'page_url' 'geo_country' 'processing_time' 'geo_city' 'traffic_source' 'kafka_timestamp', maybe you meant: ['event_time','user_id','event_name','event_id','traffic_medium','session_id','app_id','platform','page_url','geo_country','geo_city','traffic_source']. (UNKNOWN_IDENTIFIER) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream_1k]@-1551307789
	at com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:85)
	at com.clickhouse.jdbc.SqlExceptionUtils.create(SqlExceptionUtils.java:31)
	at com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:90)
	at com.clickhouse.jdbc.internal.ClickHouseConnectionImpl.getTableColumns(ClickHouseConnectionImpl.java:267)
	at com.clickhouse.jdbc.internal.ClickHouseConnectionImpl.prepareStatement(ClickHouseConnectionImpl.java:843)
	at com.clickhouse.jdbc.ClickHouseConnection.prepareStatement(ClickHouseConnection.java:121)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:710)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.io.IOException: Code: 47. DB::Exception: Missing columns: 'kafka_timestamp' 'processing_time' while processing query: 'SELECT event_id, event_name, event_time, user_id, session_id, app_id, platform, page_url, geo_country, geo_city, traffic_source, traffic_medium, session_id, kafka_timestamp, processing_time FROM events WHERE 0', required columns: 'event_time' 'user_id' 'event_name' 'event_id' 'traffic_medium' 'session_id' 'app_id' 'platform' 'page_url' 'geo_country' 'processing_time' 'geo_city' 'traffic_source' 'kafka_timestamp', maybe you meant: ['event_time','user_id','event_name','event_id','traffic_medium','session_id','app_id','platform','page_url','geo_country','geo_city','traffic_source']. (UNKNOWN_IDENTIFIER) (version 22.1.3.7 (official build))

	at com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)
	at com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)
	at com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)
	at com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)
	at com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)
	at com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)
	at com.clickhouse.jdbc.internal.ClickHouseConnectionImpl.getTableColumns(ClickHouseConnectionImpl.java:264)
	... 17 more

Driver stacktrace:
2025-06-05 07:17:30.355 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.spark.scheduler.DAGScheduler - Job 1 failed: start at SparkClickstreamProcessor.java:273, took 0.648992 s
2025-06-05 07:17:30.368 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] terminated with error
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2.0 (TID 4) (phamviethoa executor driver): java.sql.SQLException: Code: 47. DB::Exception: Missing columns: 'kafka_timestamp' 'processing_time' while processing query: 'SELECT event_id, event_name, event_time, user_id, session_id, app_id, platform, page_url, geo_country, geo_city, traffic_source, traffic_medium, session_id, kafka_timestamp, processing_time FROM events WHERE 0', required columns: 'event_time' 'user_id' 'event_name' 'event_id' 'traffic_medium' 'session_id' 'app_id' 'platform' 'page_url' 'geo_country' 'processing_time' 'geo_city' 'traffic_source' 'kafka_timestamp', maybe you meant: ['event_time','user_id','event_name','event_id','traffic_medium','session_id','app_id','platform','page_url','geo_country','geo_city','traffic_source']. (UNKNOWN_IDENTIFIER) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream_1k]@-1551307789
	at com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:85)
	at com.clickhouse.jdbc.SqlExceptionUtils.create(SqlExceptionUtils.java:31)
	at com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:90)
	at com.clickhouse.jdbc.internal.ClickHouseConnectionImpl.getTableColumns(ClickHouseConnectionImpl.java:267)
	at com.clickhouse.jdbc.internal.ClickHouseConnectionImpl.prepareStatement(ClickHouseConnectionImpl.java:843)
	at com.clickhouse.jdbc.ClickHouseConnection.prepareStatement(ClickHouseConnection.java:121)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:710)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.io.IOException: Code: 47. DB::Exception: Missing columns: 'kafka_timestamp' 'processing_time' while processing query: 'SELECT event_id, event_name, event_time, user_id, session_id, app_id, platform, page_url, geo_country, geo_city, traffic_source, traffic_medium, session_id, kafka_timestamp, processing_time FROM events WHERE 0', required columns: 'event_time' 'user_id' 'event_name' 'event_id' 'traffic_medium' 'session_id' 'app_id' 'platform' 'page_url' 'geo_country' 'processing_time' 'geo_city' 'traffic_source' 'kafka_timestamp', maybe you meant: ['event_time','user_id','event_name','event_id','traffic_medium','session_id','app_id','platform','page_url','geo_country','geo_city','traffic_source']. (UNKNOWN_IDENTIFIER) (version 22.1.3.7 (official build))

	at com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)
	at com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)
	at com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)
	at com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)
	at com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)
	at com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)
	at com.clickhouse.jdbc.internal.ClickHouseConnectionImpl.getTableColumns(ClickHouseConnectionImpl.java:264)
	... 17 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1009)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1007)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:890)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.SparkClickstreamProcessor.lambda$main$a450ce84$1(SparkClickstreamProcessor.java:269)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
Caused by: java.sql.SQLException: Code: 47. DB::Exception: Missing columns: 'kafka_timestamp' 'processing_time' while processing query: 'SELECT event_id, event_name, event_time, user_id, session_id, app_id, platform, page_url, geo_country, geo_city, traffic_source, traffic_medium, session_id, kafka_timestamp, processing_time FROM events WHERE 0', required columns: 'event_time' 'user_id' 'event_name' 'event_id' 'traffic_medium' 'session_id' 'app_id' 'platform' 'page_url' 'geo_country' 'processing_time' 'geo_city' 'traffic_source' 'kafka_timestamp', maybe you meant: ['event_time','user_id','event_name','event_id','traffic_medium','session_id','app_id','platform','page_url','geo_country','geo_city','traffic_source']. (UNKNOWN_IDENTIFIER) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream_1k]@-1551307789
	at com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:85)
	at com.clickhouse.jdbc.SqlExceptionUtils.create(SqlExceptionUtils.java:31)
	at com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:90)
	at com.clickhouse.jdbc.internal.ClickHouseConnectionImpl.getTableColumns(ClickHouseConnectionImpl.java:267)
	at com.clickhouse.jdbc.internal.ClickHouseConnectionImpl.prepareStatement(ClickHouseConnectionImpl.java:843)
	at com.clickhouse.jdbc.ClickHouseConnection.prepareStatement(ClickHouseConnection.java:121)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:710)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.io.IOException: Code: 47. DB::Exception: Missing columns: 'kafka_timestamp' 'processing_time' while processing query: 'SELECT event_id, event_name, event_time, user_id, session_id, app_id, platform, page_url, geo_country, geo_city, traffic_source, traffic_medium, session_id, kafka_timestamp, processing_time FROM events WHERE 0', required columns: 'event_time' 'user_id' 'event_name' 'event_id' 'traffic_medium' 'session_id' 'app_id' 'platform' 'page_url' 'geo_country' 'processing_time' 'geo_city' 'traffic_source' 'kafka_timestamp', maybe you meant: ['event_time','user_id','event_name','event_id','traffic_medium','session_id','app_id','platform','page_url','geo_country','geo_city','traffic_source']. (UNKNOWN_IDENTIFIER) (version 22.1.3.7 (official build))

	at com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)
	at com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)
	at com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)
	at com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)
	at com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)
	at com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)
	at com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)
	at com.clickhouse.jdbc.internal.ClickHouseConnectionImpl.getTableColumns(ClickHouseConnectionImpl.java:264)
	... 17 common frames omitted
2025-06-05 07:17:30.370 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 07:17:30.371 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:17:30.371 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:17:30.371 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:17:30.372 [stream execution thread for [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = 23635d44-8d06-45d7-97e5-752682c3a0d2, runId = 6c9df609-47d4-4df2-8b39-890d8bcee136] has been shutdown
2025-06-05 07:17:30.383 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-2, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:17:30.383 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-2, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:17:30.387 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:17:30.387 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:17:30.387 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:17:30.387 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:17:30.389 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-2 unregistered
2025-06-05 07:17:30.389 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-1, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:17:30.389 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-1, groupId=spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:17:30.391 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:17:30.391 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:17:30.391 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:17:30.391 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:17:30.393 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-845163de-bb9c-4f93-aaf0-7d13b6a305b6-297817677-executor-1 unregistered
2025-06-05 07:17:30.394 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 07:17:30.394 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 07:17:30.415 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@50f40653{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:17:30.419 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 07:17:30.427 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on phamviethoa:38327 in memory (size: 5.8 KiB, free: 9.2 GiB)
2025-06-05 07:17:30.435 [dispatcher-event-loop-6 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 07:17:30.441 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 07:17:30.441 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 07:17:30.443 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 07:17:30.444 [dispatcher-event-loop-11 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 07:17:30.447 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 07:17:30.447 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 07:17:30.447 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-6c8b2657-4b25-441f-9ec6-450406d8a817
2025-06-05 07:17:30.449 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-54424e39-895d-403a-95a4-600ab28653c5
2025-06-05 07:19:42.162 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 07:19:42.240 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 07:19:42.283 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:19:42.283 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 07:19:42.283 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:19:42.284 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 07:19:42.295 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 07:19:42.301 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 07:19:42.301 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 07:19:42.328 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 07:19:42.328 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 07:19:42.329 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 07:19:42.329 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 07:19:42.329 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 07:19:42.450 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 37305.
2025-06-05 07:19:42.463 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 07:19:42.479 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 07:19:42.489 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 07:19:42.490 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 07:19:42.492 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 07:19:42.502 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-9e49608e-cd15-4e6c-a6d7-cc81ac350cde
2025-06-05 07:19:42.518 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 07:19:42.527 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 07:19:42.547 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1001ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 07:19:42.590 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 07:19:42.596 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 07:19:42.604 [main INFO ] org.sparkproject.jetty.server.Server - Started @1059ms
2025-06-05 07:19:42.621 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@14f90d92{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:19:42.621 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 07:19:42.632 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35835e65{/,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.686 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 07:19:42.691 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 07:19:42.702 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33359.
2025-06-05 07:19:42.702 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:33359
2025-06-05 07:19:42.704 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 07:19:42.709 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 33359, None)
2025-06-05 07:19:42.712 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:33359 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 33359, None)
2025-06-05 07:19:42.714 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 33359, None)
2025-06-05 07:19:42.715 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 33359, None)
2025-06-05 07:19:42.796 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@35835e65{/,null,STOPPED,@Spark}
2025-06-05 07:19:42.797 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/jobs,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.797 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d23ff23{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.798 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cc9385{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.798 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7915bca3{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.799 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/stages,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.799 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67b4ec{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.800 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.801 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.801 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.802 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.802 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/storage,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.803 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.803 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.804 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.804 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/environment,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.805 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.805 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/executors,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.805 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397ef2{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.806 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e93c1f{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.806 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9b21bd3{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.811 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7661b5a{/static,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.812 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b6d92e{/,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.813 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7899de11{/api,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.813 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f951a7f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.814 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c777e7b{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.816 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62db3891{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.898 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 07:19:42.902 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 07:19:42.909 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6732726{/SQL,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.910 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d64c581{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.910 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d64c100{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.910 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fdf17dc{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 07:19:42.917 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ff8a9dc{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:19:44.342 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-05 07:19:44.351 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c3007d{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-05 07:19:44.352 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7296fe0b{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-05 07:19:44.353 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d1c63af{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-05 07:19:44.353 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3909a854{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-05 07:19:44.354 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56ba8e8c{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:19:44.358 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-c0fdb620-98cf-4456-a212-360ef6b2ec03. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-05 07:19:44.370 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-c0fdb620-98cf-4456-a212-360ef6b2ec03 resolved to file:/tmp/temporary-c0fdb620-98cf-4456-a212-360ef6b2ec03.
2025-06-05 07:19:44.371 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-05 07:19:44.417 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-c0fdb620-98cf-4456-a212-360ef6b2ec03/metadata using temp file file:/tmp/temporary-c0fdb620-98cf-4456-a212-360ef6b2ec03/.metadata.5a0f3d8b-b895-4ee9-89d1-7359d1295702.tmp
2025-06-05 07:19:44.464 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-c0fdb620-98cf-4456-a212-360ef6b2ec03/.metadata.5a0f3d8b-b895-4ee9-89d1-7359d1295702.tmp to file:/tmp/temporary-c0fdb620-98cf-4456-a212-360ef6b2ec03/metadata
2025-06-05 07:19:44.479 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2]. Use file:/tmp/temporary-c0fdb620-98cf-4456-a212-360ef6b2ec03 to store the query checkpoint.
2025-06-05 07:19:44.483 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@3882d9] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@3f0ed8bc]
2025-06-05 07:19:44.503 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:19:44.504 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:19:44.504 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-05 07:19:44.506 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-05 07:19:44.667 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 07:19:44.694 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 07:19:44.695 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:19:44.695 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:19:44.695 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082784694
2025-06-05 07:19:44.868 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-c0fdb620-98cf-4456-a212-360ef6b2ec03/sources/0/0 using temp file file:/tmp/temporary-c0fdb620-98cf-4456-a212-360ef6b2ec03/sources/0/.0.d58b6b7c-c79c-4105-9ad1-e6ee739aa8a4.tmp
2025-06-05 07:19:44.883 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-c0fdb620-98cf-4456-a212-360ef6b2ec03/sources/0/.0.d58b6b7c-c79c-4105-9ad1-e6ee739aa8a4.tmp to file:/tmp/temporary-c0fdb620-98cf-4456-a212-360ef6b2ec03/sources/0/0
2025-06-05 07:19:44.883 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-05 07:19:44.894 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-c0fdb620-98cf-4456-a212-360ef6b2ec03/offsets/0 using temp file file:/tmp/temporary-c0fdb620-98cf-4456-a212-360ef6b2ec03/offsets/.0.346ef20a-0090-4a4a-a653-c12b8bcb3a6c.tmp
2025-06-05 07:19:44.913 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-c0fdb620-98cf-4456-a212-360ef6b2ec03/offsets/.0.346ef20a-0090-4a4a-a653-c12b8bcb3a6c.tmp to file:/tmp/temporary-c0fdb620-98cf-4456-a212-360ef6b2ec03/offsets/0
2025-06-05 07:19:44.914 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749082784890,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-05 07:19:45.080 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082784890
2025-06-05 07:19:45.129 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:19:45.154 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:19:45.187 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082784890
2025-06-05 07:19:45.189 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:19:45.190 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:19:45.420 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 138.573901 ms
2025-06-05 07:19:45.532 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.855351 ms
2025-06-05 07:19:45.555 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 14.743513 ms
2025-06-05 07:19:45.605 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:273
2025-06-05 07:19:45.614 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:273) as input to shuffle 0
2025-06-05 07:19:45.617 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:273) with 1 output partitions
2025-06-05 07:19:45.617 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:273)
2025-06-05 07:19:45.617 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-05 07:19:45.618 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
2025-06-05 07:19:45.620 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:273), which has no missing parents
2025-06-05 07:19:45.695 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 45.4 KiB, free 9.2 GiB)
2025-06-05 07:19:45.714 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 17.8 KiB, free 9.2 GiB)
2025-06-05 07:19:45.716 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:33359 (size: 17.8 KiB, free: 9.2 GiB)
2025-06-05 07:19:45.718 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:19:45.726 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:273) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:19:45.727 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-05 07:19:45.757 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8301 bytes) 
2025-06-05 07:19:45.759 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8301 bytes) 
2025-06-05 07:19:45.765 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-05 07:19:45.765 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 07:19:45.858 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 15.697016 ms
2025-06-05 07:19:45.887 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.019411 ms
2025-06-05 07:19:45.900 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.243037 ms
2025-06-05 07:19:45.914 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:19:45.914 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:19:45.937 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:19:45.937 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:19:45.965 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:19:45.965 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:19:45.965 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082785965
2025-06-05 07:19:45.966 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:19:45.966 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:19:45.966 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082785965
2025-06-05 07:19:45.967 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-2, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 07:19:45.967 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-1, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Assigned to partition(s): clickstream-events-1
2025-06-05 07:19:45.971 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-2, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:19:45.971 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-1, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:19:45.976 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-1, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:19:45.976 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-2, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:19:46.001 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-1, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:19:46.001 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-2, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:19:46.502 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-2, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:19:46.502 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-1, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:19:46.503 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-2, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:19:46.503 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-1, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:19:46.503 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-1, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:19:46.503 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-2, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:19:46.590 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 2342 bytes result sent to driver
2025-06-05 07:19:46.590 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2342 bytes result sent to driver
2025-06-05 07:19:46.596 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 836 ms on phamviethoa (executor driver) (1/2)
2025-06-05 07:19:46.598 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 848 ms on phamviethoa (executor driver) (2/2)
2025-06-05 07:19:46.599 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 07:19:46.602 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (start at SparkClickstreamProcessor.java:273) finished in 0.976 s
2025-06-05 07:19:46.603 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
2025-06-05 07:19:46.603 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - running: Set()
2025-06-05 07:19:46.603 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
2025-06-05 07:19:46.603 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - failed: Set()
2025-06-05 07:19:46.605 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:273), which has no missing parents
2025-06-05 07:19:46.610 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-05 07:19:46.612 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-05 07:19:46.613 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:33359 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-05 07:19:46.614 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:19:46.615 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:273) (first 15 tasks are for partitions Vector(0))
2025-06-05 07:19:46.615 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-05 07:19:46.619 [dispatcher-event-loop-12 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
2025-06-05 07:19:46.619 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-05 07:19:46.649 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-05 07:19:46.651 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
2025-06-05 07:19:46.670 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 4038 bytes result sent to driver
2025-06-05 07:19:46.671 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 54 ms on phamviethoa (executor driver) (1/1)
2025-06-05 07:19:46.671 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-05 07:19:46.672 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:273) finished in 0.063 s
2025-06-05 07:19:46.674 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 07:19:46.674 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-05 07:19:46.675 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:273, took 1.070053 s
2025-06-05 07:19:46.872 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.418373 ms
2025-06-05 07:19:46.909 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:273
2025-06-05 07:19:46.910 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 1 (start at SparkClickstreamProcessor.java:273) with 2 output partitions
2025-06-05 07:19:46.910 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (start at SparkClickstreamProcessor.java:273)
2025-06-05 07:19:46.911 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-05 07:19:46.911 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-05 07:19:46.912 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[13] at start at SparkClickstreamProcessor.java:273), which has no missing parents
2025-06-05 07:19:46.932 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 54.6 KiB, free 9.2 GiB)
2025-06-05 07:19:46.934 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.9 KiB, free 9.2 GiB)
2025-06-05 07:19:46.935 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on phamviethoa:33359 (size: 20.9 KiB, free: 9.2 GiB)
2025-06-05 07:19:46.936 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:19:46.936 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at start at SparkClickstreamProcessor.java:273) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:19:46.936 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 2 tasks resource profile 0
2025-06-05 07:19:46.938 [dispatcher-event-loop-15 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 3) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8312 bytes) 
2025-06-05 07:19:46.938 [dispatcher-event-loop-15 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 4) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8312 bytes) 
2025-06-05 07:19:46.939 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 4)
2025-06-05 07:19:46.939 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 3)
2025-06-05 07:19:46.997 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 13.824342 ms
2025-06-05 07:19:47.007 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-1, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:19:47.007 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-2, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:19:47.013 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-2, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:19:47.013 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-1, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:19:47.514 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-2, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:19:47.514 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-1, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:19:47.514 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-2, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:19:47.514 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-1, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:19:47.515 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-1, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:19:47.515 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-2, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:19:47.526 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-05 07:19:47.526 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-05 07:19:47.557 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-05 07:19:47.557 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [e36f2dd4-edc6-476a-b7d0-114ca2c90a60] (2 queries & 0 savepoints) is rolled back.
2025-06-05 07:19:47.557 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [e60b2997-8516-4c39-b21d-ddd979655939] (0 queries & 0 savepoints) is committed.
2025-06-05 07:19:47.560 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) ERROR] org.apache.spark.executor.Executor - Exception in task 0.0 in stage 2.0 (TID 3)
java.sql.BatchUpdateException: Code: 15. DB::Exception: Column session_id specified more than once. (DUPLICATE_COLUMN) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream_1k]@1834800568
	at com.clickhouse.jdbc.SqlExceptionUtils.batchUpdateError(SqlExceptionUtils.java:107)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.executeAny(InputBasedPreparedStatement.java:154)
	at com.clickhouse.jdbc.internal.AbstractPreparedStatement.executeLargeBatch(AbstractPreparedStatement.java:85)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeBatch(ClickHouseStatementImpl.java:754)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:740)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-05 07:19:47.560 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-05 07:19:47.560 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [78d41eb0-c51b-40e6-906b-c16ebd9d514e] (2 queries & 0 savepoints) is rolled back.
2025-06-05 07:19:47.561 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [9b980e45-5d57-4420-b786-cd4c49a637f9] (0 queries & 0 savepoints) is committed.
2025-06-05 07:19:47.561 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) ERROR] org.apache.spark.executor.Executor - Exception in task 1.0 in stage 2.0 (TID 4)
java.sql.BatchUpdateException: Code: 15. DB::Exception: Column session_id specified more than once. (DUPLICATE_COLUMN) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream_1k]@1834800568
	at com.clickhouse.jdbc.SqlExceptionUtils.batchUpdateError(SqlExceptionUtils.java:107)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.executeAny(InputBasedPreparedStatement.java:154)
	at com.clickhouse.jdbc.internal.AbstractPreparedStatement.executeLargeBatch(AbstractPreparedStatement.java:85)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeBatch(ClickHouseStatementImpl.java:754)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:740)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-05 07:19:47.569 [task-result-getter-1 WARN ] o.a.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 2.0 (TID 4) (phamviethoa executor driver): java.sql.BatchUpdateException: Code: 15. DB::Exception: Column session_id specified more than once. (DUPLICATE_COLUMN) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream_1k]@1834800568
	at com.clickhouse.jdbc.SqlExceptionUtils.batchUpdateError(SqlExceptionUtils.java:107)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.executeAny(InputBasedPreparedStatement.java:154)
	at com.clickhouse.jdbc.internal.AbstractPreparedStatement.executeLargeBatch(AbstractPreparedStatement.java:85)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeBatch(ClickHouseStatementImpl.java:754)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:740)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

2025-06-05 07:19:47.570 [task-result-getter-1 ERROR] o.a.spark.scheduler.TaskSetManager - Task 1 in stage 2.0 failed 1 times; aborting job
2025-06-05 07:19:47.570 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2025-06-05 07:19:47.571 [task-result-getter-3 INFO ] o.a.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 2.0 (TID 3) on phamviethoa, executor driver: java.sql.BatchUpdateException (Code: 15. DB::Exception: Column session_id specified more than once. (DUPLICATE_COLUMN) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream_1k]@1834800568) [duplicate 1]
2025-06-05 07:19:47.571 [task-result-getter-3 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2025-06-05 07:19:47.572 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Cancelling stage 2
2025-06-05 07:19:47.572 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage cancelled
2025-06-05 07:19:47.573 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 2 (start at SparkClickstreamProcessor.java:273) failed in 0.659 s due to Job aborted due to stage failure: Task 1 in stage 2.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2.0 (TID 4) (phamviethoa executor driver): java.sql.BatchUpdateException: Code: 15. DB::Exception: Column session_id specified more than once. (DUPLICATE_COLUMN) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream_1k]@1834800568
	at com.clickhouse.jdbc.SqlExceptionUtils.batchUpdateError(SqlExceptionUtils.java:107)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.executeAny(InputBasedPreparedStatement.java:154)
	at com.clickhouse.jdbc.internal.AbstractPreparedStatement.executeLargeBatch(AbstractPreparedStatement.java:85)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeBatch(ClickHouseStatementImpl.java:754)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:740)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Driver stacktrace:
2025-06-05 07:19:47.574 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.spark.scheduler.DAGScheduler - Job 1 failed: start at SparkClickstreamProcessor.java:273, took 0.664397 s
2025-06-05 07:19:47.587 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] terminated with error
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2.0 (TID 4) (phamviethoa executor driver): java.sql.BatchUpdateException: Code: 15. DB::Exception: Column session_id specified more than once. (DUPLICATE_COLUMN) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream_1k]@1834800568
	at com.clickhouse.jdbc.SqlExceptionUtils.batchUpdateError(SqlExceptionUtils.java:107)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.executeAny(InputBasedPreparedStatement.java:154)
	at com.clickhouse.jdbc.internal.AbstractPreparedStatement.executeLargeBatch(AbstractPreparedStatement.java:85)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeBatch(ClickHouseStatementImpl.java:754)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:740)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1009)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1007)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:890)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.SparkClickstreamProcessor.lambda$main$a450ce84$1(SparkClickstreamProcessor.java:269)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
Caused by: java.sql.BatchUpdateException: Code: 15. DB::Exception: Column session_id specified more than once. (DUPLICATE_COLUMN) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream_1k]@1834800568
	at com.clickhouse.jdbc.SqlExceptionUtils.batchUpdateError(SqlExceptionUtils.java:107)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.executeAny(InputBasedPreparedStatement.java:154)
	at com.clickhouse.jdbc.internal.AbstractPreparedStatement.executeLargeBatch(AbstractPreparedStatement.java:85)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeBatch(ClickHouseStatementImpl.java:754)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:740)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-05 07:19:47.589 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 07:19:47.591 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:19:47.591 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:19:47.591 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:19:47.592 [stream execution thread for [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = c164187f-0a65-481a-ad00-7ab62f20ab2f, runId = f8951c79-2e3d-4189-a23f-22f8faeab1a2] has been shutdown
2025-06-05 07:19:47.605 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-1, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:19:47.605 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-1, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:19:47.609 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:19:47.609 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:19:47.609 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:19:47.609 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:19:47.613 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-1 unregistered
2025-06-05 07:19:47.613 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-2, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:19:47.613 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-2, groupId=spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:19:47.614 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:19:47.614 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:19:47.614 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:19:47.615 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:19:47.616 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-c518921a-8755-4a0b-a0eb-c1c632e3f009-282173394-executor-2 unregistered
2025-06-05 07:19:47.617 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 07:19:47.617 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 07:19:47.621 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@14f90d92{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:19:47.623 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 07:19:47.632 [dispatcher-event-loop-6 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 07:19:47.640 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 07:19:47.640 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 07:19:47.644 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 07:19:47.645 [dispatcher-event-loop-11 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 07:19:47.649 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 07:19:47.649 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 07:19:47.650 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-97050e9d-0625-465f-8e17-b628ba17ca06
2025-06-05 07:19:47.652 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-c0fdb620-98cf-4456-a212-360ef6b2ec03
2025-06-05 07:21:34.497 [main INFO ] org.apache.spark.SparkContext - Running Spark version 3.4.0
2025-06-05 07:21:34.575 [main WARN ] o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-06-05 07:21:34.618 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:21:34.618 [main INFO ] o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-06-05 07:21:34.618 [main INFO ] o.a.spark.resource.ResourceUtils - ==============================================================
2025-06-05 07:21:34.619 [main INFO ] org.apache.spark.SparkContext - Submitted application: Clickstream Processor
2025-06-05 07:21:34.630 [main INFO ] o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-06-05 07:21:34.636 [main INFO ] o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-06-05 07:21:34.636 [main INFO ] o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-06-05 07:21:34.665 [main INFO ] org.apache.spark.SecurityManager - Changing view acls to: phamviethoa
2025-06-05 07:21:34.666 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls to: phamviethoa
2025-06-05 07:21:34.666 [main INFO ] org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-06-05 07:21:34.666 [main INFO ] org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-06-05 07:21:34.666 [main INFO ] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: phamviethoa; groups with view permissions: EMPTY; users with modify permissions: phamviethoa; groups with modify permissions: EMPTY
2025-06-05 07:21:34.782 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 46707.
2025-06-05 07:21:34.798 [main INFO ] org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-06-05 07:21:34.813 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-06-05 07:21:34.822 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-06-05 07:21:34.823 [main INFO ] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-06-05 07:21:34.824 [main INFO ] org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-06-05 07:21:34.834 [main INFO ] o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-60075460-3606-4a7c-a951-9cbc1845db58
2025-06-05 07:21:34.850 [main INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 9.2 GiB
2025-06-05 07:21:34.858 [main INFO ] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-06-05 07:21:34.877 [main INFO ] org.sparkproject.jetty.util.log - Logging initialized @1050ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-06-05 07:21:34.921 [main INFO ] org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-06-05 07:21:34.926 [main INFO ] org.sparkproject.jetty.server.Server - jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.27+6-post-Ubuntu-0ubuntu124.04
2025-06-05 07:21:34.935 [main INFO ] org.sparkproject.jetty.server.Server - Started @1108ms
2025-06-05 07:21:34.952 [main INFO ] o.s.jetty.server.AbstractConnector - Started ServerConnector@11dcbd53{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:21:34.952 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-06-05 07:21:34.963 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7144655b{/,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.020 [main INFO ] org.apache.spark.executor.Executor - Starting executor ID driver on host phamviethoa
2025-06-05 07:21:35.024 [main INFO ] org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-06-05 07:21:35.040 [main INFO ] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42747.
2025-06-05 07:21:35.040 [main INFO ] o.a.s.n.n.NettyBlockTransferService - Server created on phamviethoa:42747
2025-06-05 07:21:35.042 [main INFO ] o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-06-05 07:21:35.046 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, phamviethoa, 42747, None)
2025-06-05 07:21:35.050 [dispatcher-BlockManagerMaster INFO ] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager phamviethoa:42747 with 9.2 GiB RAM, BlockManagerId(driver, phamviethoa, 42747, None)
2025-06-05 07:21:35.052 [main INFO ] o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, phamviethoa, 42747, None)
2025-06-05 07:21:35.054 [main INFO ] o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, phamviethoa, 42747, None)
2025-06-05 07:21:35.133 [main INFO ] o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7144655b{/,null,STOPPED,@Spark}
2025-06-05 07:21:35.134 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2241f05b{/jobs,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.135 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71978f46{/jobs/json,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.136 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c9320c2{/jobs/job,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.136 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36cc9385{/jobs/job/json,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.137 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7915bca3{/stages,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.137 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad4a7d6{/stages/json,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.138 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79fd6f95{/stages/stage,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.139 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c675f0{/stages/stage/json,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.139 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6917bb4{/stages/pool,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.140 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1442f788{/stages/pool/json,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.140 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c7f96b1{/storage,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.141 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a04fea7{/storage/json,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.141 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6e5c12{/storage/rdd,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.142 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124ac145{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.143 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e83d19{/environment,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.143 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188cbcde{/environment/json,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.144 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b03d52f{/executors,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.144 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af70944{/executors/json,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.145 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397ef2{/executors/threadDump,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.145 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e93c1f{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.150 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9b21bd3{/static,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.151 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@323f3c96{/,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.152 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b6d92e{/api,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.152 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25d93198{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.153 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f951a7f{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.155 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e7f0216{/metrics/json,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.238 [main INFO ] o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-06-05 07:21:35.242 [main INFO ] o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/phamviethoa/Idea%20Projects/clickstream-analysis/spark-warehouse'.
2025-06-05 07:21:35.249 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c86da0c{/SQL,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.249 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6732726{/SQL/json,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.250 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47d023b7{/SQL/execution,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.250 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d64c100{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-06-05 07:21:35.256 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d904ff1{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:21:36.666 [main INFO ] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2025-06-05 07:21:36.675 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@773c7147{/StreamingQuery,null,AVAILABLE,@Spark}
2025-06-05 07:21:36.675 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c3007d{/StreamingQuery/json,null,AVAILABLE,@Spark}
2025-06-05 07:21:36.676 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1191029d{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2025-06-05 07:21:36.676 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d1c63af{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2025-06-05 07:21:36.677 [main INFO ] o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e9ea32f{/static/sql,null,AVAILABLE,@Spark}
2025-06-05 07:21:36.683 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-b74090fe-8e73-4f81-b368-10f789ac7fde. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
2025-06-05 07:21:36.700 [main INFO ] o.a.s.s.e.s.ResolveWriteToStream - Checkpoint root /tmp/temporary-b74090fe-8e73-4f81-b368-10f789ac7fde resolved to file:/tmp/temporary-b74090fe-8e73-4f81-b368-10f789ac7fde.
2025-06-05 07:21:36.700 [main WARN ] o.a.s.s.e.s.ResolveWriteToStream - spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2025-06-05 07:21:36.745 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-b74090fe-8e73-4f81-b368-10f789ac7fde/metadata using temp file file:/tmp/temporary-b74090fe-8e73-4f81-b368-10f789ac7fde/.metadata.518acaf4-70f3-4cff-bca8-78ecb993c3c9.tmp
2025-06-05 07:21:36.790 [main INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-b74090fe-8e73-4f81-b368-10f789ac7fde/.metadata.518acaf4-70f3-4cff-bca8-78ecb993c3c9.tmp to file:/tmp/temporary-b74090fe-8e73-4f81-b368-10f789ac7fde/metadata
2025-06-05 07:21:36.804 [main INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512]. Use file:/tmp/temporary-b74090fe-8e73-4f81-b368-10f789ac7fde to store the query checkpoint.
2025-06-05 07:21:36.811 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.s.s.e.s.MicroBatchExecution - Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@3ea5aa29] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@266be8b0]
2025-06-05 07:21:36.825 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:21:36.826 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.s.s.e.streaming.OffsetSeqLog - BatchIds found from listing: 
2025-06-05 07:21:36.826 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.s.s.e.s.MicroBatchExecution - Starting new streaming query.
2025-06-05 07:21:36.828 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.s.s.e.s.MicroBatchExecution - Stream started from {}
2025-06-05 07:21:36.987 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 07:21:37.014 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.k.c.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
2025-06-05 07:21:37.015 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:21:37.015 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:21:37.015 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082897014
2025-06-05 07:21:37.187 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-b74090fe-8e73-4f81-b368-10f789ac7fde/sources/0/0 using temp file file:/tmp/temporary-b74090fe-8e73-4f81-b368-10f789ac7fde/sources/0/.0.5d00f72b-55c0-4b7c-9381-fa7ceac48ea3.tmp
2025-06-05 07:21:37.201 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-b74090fe-8e73-4f81-b368-10f789ac7fde/sources/0/.0.5d00f72b-55c0-4b7c-9381-fa7ceac48ea3.tmp to file:/tmp/temporary-b74090fe-8e73-4f81-b368-10f789ac7fde/sources/0/0
2025-06-05 07:21:37.202 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.s.s.k.KafkaMicroBatchStream - Initial offsets: {"clickstream-events":{"1":0,"0":0}}
2025-06-05 07:21:37.214 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.s.s.e.s.CheckpointFileManager - Writing atomically to file:/tmp/temporary-b74090fe-8e73-4f81-b368-10f789ac7fde/offsets/0 using temp file file:/tmp/temporary-b74090fe-8e73-4f81-b368-10f789ac7fde/offsets/.0.942f4a8b-557b-4313-93dd-da6f8ccc1a2a.tmp
2025-06-05 07:21:37.237 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.s.s.e.s.CheckpointFileManager - Renamed temp file file:/tmp/temporary-b74090fe-8e73-4f81-b368-10f789ac7fde/offsets/.0.942f4a8b-557b-4313-93dd-da6f8ccc1a2a.tmp to file:/tmp/temporary-b74090fe-8e73-4f81-b368-10f789ac7fde/offsets/0
2025-06-05 07:21:37.237 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.s.s.e.s.MicroBatchExecution - Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1749082897209,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 3))
2025-06-05 07:21:37.416 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082897209
2025-06-05 07:21:37.463 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:21:37.488 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:21:37.519 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.s.s.e.s.IncrementalExecution - Current batch timestamp = 1749082897209
2025-06-05 07:21:37.521 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:21:37.522 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.s.s.k.KafkaOffsetReaderAdmin - Partitions added: Map()
2025-06-05 07:21:37.726 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 122.603662 ms
2025-06-05 07:21:37.847 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.382069 ms
2025-06-05 07:21:37.859 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 6.034664 ms
2025-06-05 07:21:37.906 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:270
2025-06-05 07:21:37.913 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (start at SparkClickstreamProcessor.java:270) as input to shuffle 0
2025-06-05 07:21:37.915 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 0 (start at SparkClickstreamProcessor.java:270) with 1 output partitions
2025-06-05 07:21:37.916 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (start at SparkClickstreamProcessor.java:270)
2025-06-05 07:21:37.916 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
2025-06-05 07:21:37.916 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
2025-06-05 07:21:37.918 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:270), which has no missing parents
2025-06-05 07:21:37.985 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 37.5 KiB, free 9.2 GiB)
2025-06-05 07:21:38.003 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 9.2 GiB)
2025-06-05 07:21:38.006 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on phamviethoa:42747 (size: 16.3 KiB, free: 9.2 GiB)
2025-06-05 07:21:38.009 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:21:38.020 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at start at SparkClickstreamProcessor.java:270) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:21:38.021 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 2 tasks resource profile 0
2025-06-05 07:21:38.049 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8303 bytes) 
2025-06-05 07:21:38.051 [dispatcher-event-loop-7 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8303 bytes) 
2025-06-05 07:21:38.058 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2025-06-05 07:21:38.058 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2025-06-05 07:21:38.161 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 20.677603 ms
2025-06-05 07:21:38.191 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.882491 ms
2025-06-05 07:21:38.209 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 11.794836 ms
2025-06-05 07:21:38.220 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:21:38.220 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-06-05 07:21:38.240 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:21:38.240 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:21:38.270 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:21:38.270 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:21:38.270 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082898270
2025-06-05 07:21:38.271 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:21:38.271 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:21:38.271 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749082898270
2025-06-05 07:21:38.271 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-2, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Assigned to partition(s): clickstream-events-1
2025-06-05 07:21:38.271 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-1, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Assigned to partition(s): clickstream-events-0
2025-06-05 07:21:38.276 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-1, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:21:38.276 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-2, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:21:38.282 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-2, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:21:38.282 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-1, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:21:38.306 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-2, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:21:38.306 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-1, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:21:38.808 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-1, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:21:38.808 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-2, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:21:38.808 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-1, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:21:38.808 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-2, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:21:38.809 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-1, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:21:38.809 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-2, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:21:38.921 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1) INFO ] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 2342 bytes result sent to driver
2025-06-05 07:21:38.921 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2342 bytes result sent to driver
2025-06-05 07:21:38.929 [task-result-getter-1 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 885 ms on phamviethoa (executor driver) (1/2)
2025-06-05 07:21:38.930 [task-result-getter-0 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 878 ms on phamviethoa (executor driver) (2/2)
2025-06-05 07:21:38.930 [task-result-getter-0 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-06-05 07:21:38.937 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (start at SparkClickstreamProcessor.java:270) finished in 1.012 s
2025-06-05 07:21:38.937 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
2025-06-05 07:21:38.938 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - running: Set()
2025-06-05 07:21:38.938 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
2025-06-05 07:21:38.938 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - failed: Set()
2025-06-05 07:21:38.939 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:270), which has no missing parents
2025-06-05 07:21:38.946 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 9.2 GiB)
2025-06-05 07:21:38.948 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 9.2 GiB)
2025-06-05 07:21:38.948 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on phamviethoa:42747 (size: 5.8 KiB, free: 9.2 GiB)
2025-06-05 07:21:38.949 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:21:38.950 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at SparkClickstreamProcessor.java:270) (first 15 tasks are for partitions Vector(0))
2025-06-05 07:21:38.950 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
2025-06-05 07:21:38.955 [dispatcher-event-loop-12 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 2) (phamviethoa, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
2025-06-05 07:21:38.955 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 2)
2025-06-05 07:21:38.992 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-06-05 07:21:38.992 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
2025-06-05 07:21:39.002 [Executor task launch worker for task 0.0 in stage 1.0 (TID 2) INFO ] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 2). 4038 bytes result sent to driver
2025-06-05 07:21:39.003 [task-result-getter-2 INFO ] o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 2) in 50 ms on phamviethoa (executor driver) (1/1)
2025-06-05 07:21:39.003 [task-result-getter-2 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-06-05 07:21:39.004 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 1 (start at SparkClickstreamProcessor.java:270) finished in 0.059 s
2025-06-05 07:21:39.006 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-06-05 07:21:39.006 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
2025-06-05 07:21:39.007 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.spark.scheduler.DAGScheduler - Job 0 finished: start at SparkClickstreamProcessor.java:270, took 1.100886 s
2025-06-05 07:21:39.196 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 13.077914 ms
2025-06-05 07:21:39.241 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] org.apache.spark.SparkContext - Starting job: start at SparkClickstreamProcessor.java:270
2025-06-05 07:21:39.242 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Got job 1 (start at SparkClickstreamProcessor.java:270) with 2 output partitions
2025-06-05 07:21:39.242 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (start at SparkClickstreamProcessor.java:270)
2025-06-05 07:21:39.242 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
2025-06-05 07:21:39.243 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Missing parents: List()
2025-06-05 07:21:39.244 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[13] at start at SparkClickstreamProcessor.java:270), which has no missing parents
2025-06-05 07:21:39.266 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 46.4 KiB, free 9.2 GiB)
2025-06-05 07:21:39.269 [dag-scheduler-event-loop INFO ] o.a.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 19.3 KiB, free 9.2 GiB)
2025-06-05 07:21:39.270 [dispatcher-BlockManagerMaster INFO ] o.a.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on phamviethoa:42747 (size: 19.3 KiB, free: 9.2 GiB)
2025-06-05 07:21:39.270 [dag-scheduler-event-loop INFO ] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1535
2025-06-05 07:21:39.271 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at start at SparkClickstreamProcessor.java:270) (first 15 tasks are for partitions Vector(0, 1))
2025-06-05 07:21:39.271 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 2 tasks resource profile 0
2025-06-05 07:21:39.272 [dispatcher-event-loop-15 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 3) (phamviethoa, executor driver, partition 0, PROCESS_LOCAL, 8314 bytes) 
2025-06-05 07:21:39.272 [dispatcher-event-loop-15 INFO ] o.a.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 4) (phamviethoa, executor driver, partition 1, PROCESS_LOCAL, 8314 bytes) 
2025-06-05 07:21:39.273 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 4)
2025-06-05 07:21:39.273 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 3)
2025-06-05 07:21:39.333 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.583438 ms
2025-06-05 07:21:39.347 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-1, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Seeking to offset 0 for partition clickstream-events-0
2025-06-05 07:21:39.347 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-2, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Seeking to offset 0 for partition clickstream-events-1
2025-06-05 07:21:39.352 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-1, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Seeking to earliest offset of partition clickstream-events-0
2025-06-05 07:21:39.352 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-2, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Seeking to earliest offset of partition clickstream-events-1
2025-06-05 07:21:39.854 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-1, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:21:39.854 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-1, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Seeking to latest offset of partition clickstream-events-0
2025-06-05 07:21:39.854 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-2, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:21:39.854 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-2, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Seeking to latest offset of partition clickstream-events-1
2025-06-05 07:21:39.855 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-1, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Resetting offset for partition clickstream-events-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:21:39.855 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) INFO ] o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-2, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Resetting offset for partition clickstream-events-1 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.
2025-06-05 07:21:39.867 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-05 07:21:39.867 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-05 07:21:39.896 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-05 07:21:39.896 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction is not supported. You may change jdbcCompliant to false to throw SQLException instead.
2025-06-05 07:21:39.896 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [1a8a8d7c-a7a3-4720-be54-c08a781314b4] (2 queries & 0 savepoints) is rolled back.
2025-06-05 07:21:39.896 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [a3124e31-27ae-40f4-99a6-49fbc75f933f] (2 queries & 0 savepoints) is rolled back.
2025-06-05 07:21:39.896 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [7d658f03-df6f-4015-83ca-c3587f2e0658] (0 queries & 0 savepoints) is committed.
2025-06-05 07:21:39.896 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) WARN ] c.c.j.i.ClickHouseConnectionImpl - [JDBC Compliant Mode] Transaction [298eaa19-78b5-4b57-ac9c-ae2fa7d88859] (0 queries & 0 savepoints) is committed.
2025-06-05 07:21:39.900 [Executor task launch worker for task 1.0 in stage 2.0 (TID 4) ERROR] org.apache.spark.executor.Executor - Exception in task 1.0 in stage 2.0 (TID 4)
java.sql.BatchUpdateException: Code: 15. DB::Exception: Column session_id specified more than once. (DUPLICATE_COLUMN) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream_1k]@743023620
	at com.clickhouse.jdbc.SqlExceptionUtils.batchUpdateError(SqlExceptionUtils.java:107)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.executeAny(InputBasedPreparedStatement.java:154)
	at com.clickhouse.jdbc.internal.AbstractPreparedStatement.executeLargeBatch(AbstractPreparedStatement.java:85)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeBatch(ClickHouseStatementImpl.java:754)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:740)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-05 07:21:39.900 [Executor task launch worker for task 0.0 in stage 2.0 (TID 3) ERROR] org.apache.spark.executor.Executor - Exception in task 0.0 in stage 2.0 (TID 3)
java.sql.BatchUpdateException: Code: 15. DB::Exception: Column session_id specified more than once. (DUPLICATE_COLUMN) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream_1k]@743023620
	at com.clickhouse.jdbc.SqlExceptionUtils.batchUpdateError(SqlExceptionUtils.java:107)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.executeAny(InputBasedPreparedStatement.java:154)
	at com.clickhouse.jdbc.internal.AbstractPreparedStatement.executeLargeBatch(AbstractPreparedStatement.java:85)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeBatch(ClickHouseStatementImpl.java:754)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:740)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-05 07:21:39.909 [task-result-getter-1 WARN ] o.a.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 2.0 (TID 3) (phamviethoa executor driver): java.sql.BatchUpdateException: Code: 15. DB::Exception: Column session_id specified more than once. (DUPLICATE_COLUMN) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream_1k]@743023620
	at com.clickhouse.jdbc.SqlExceptionUtils.batchUpdateError(SqlExceptionUtils.java:107)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.executeAny(InputBasedPreparedStatement.java:154)
	at com.clickhouse.jdbc.internal.AbstractPreparedStatement.executeLargeBatch(AbstractPreparedStatement.java:85)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeBatch(ClickHouseStatementImpl.java:754)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:740)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

2025-06-05 07:21:39.910 [task-result-getter-1 ERROR] o.a.spark.scheduler.TaskSetManager - Task 0 in stage 2.0 failed 1 times; aborting job
2025-06-05 07:21:39.910 [task-result-getter-1 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2025-06-05 07:21:39.911 [task-result-getter-3 INFO ] o.a.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 2.0 (TID 4) on phamviethoa, executor driver: java.sql.BatchUpdateException (Code: 15. DB::Exception: Column session_id specified more than once. (DUPLICATE_COLUMN) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream_1k]@743023620) [duplicate 1]
2025-06-05 07:21:39.911 [task-result-getter-3 INFO ] o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2025-06-05 07:21:39.911 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Cancelling stage 2
2025-06-05 07:21:39.911 [dag-scheduler-event-loop INFO ] o.a.s.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage cancelled
2025-06-05 07:21:39.912 [dag-scheduler-event-loop INFO ] o.a.spark.scheduler.DAGScheduler - ResultStage 2 (start at SparkClickstreamProcessor.java:270) failed in 0.666 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 3) (phamviethoa executor driver): java.sql.BatchUpdateException: Code: 15. DB::Exception: Column session_id specified more than once. (DUPLICATE_COLUMN) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream_1k]@743023620
	at com.clickhouse.jdbc.SqlExceptionUtils.batchUpdateError(SqlExceptionUtils.java:107)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.executeAny(InputBasedPreparedStatement.java:154)
	at com.clickhouse.jdbc.internal.AbstractPreparedStatement.executeLargeBatch(AbstractPreparedStatement.java:85)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeBatch(ClickHouseStatementImpl.java:754)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:740)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Driver stacktrace:
2025-06-05 07:21:39.912 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.spark.scheduler.DAGScheduler - Job 1 failed: start at SparkClickstreamProcessor.java:270, took 0.671696 s
2025-06-05 07:21:39.926 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] ERROR] o.a.s.s.e.s.MicroBatchExecution - Query [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] terminated with error
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 3) (phamviethoa executor driver): java.sql.BatchUpdateException: Code: 15. DB::Exception: Column session_id specified more than once. (DUPLICATE_COLUMN) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream_1k]@743023620
	at com.clickhouse.jdbc.SqlExceptionUtils.batchUpdateError(SqlExceptionUtils.java:107)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.executeAny(InputBasedPreparedStatement.java:154)
	at com.clickhouse.jdbc.internal.AbstractPreparedStatement.executeLargeBatch(AbstractPreparedStatement.java:85)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeBatch(ClickHouseStatementImpl.java:754)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:740)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1009)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1007)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:890)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at com.example.clickstream.processor.SparkClickstreamProcessor.lambda$main$a450ce84$1(SparkClickstreamProcessor.java:266)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1(DataStreamWriter.scala:496)
	at org.apache.spark.sql.streaming.DataStreamWriter.$anonfun$foreachBatch$1$adapted(DataStreamWriter.scala:496)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)
Caused by: java.sql.BatchUpdateException: Code: 15. DB::Exception: Column session_id specified more than once. (DUPLICATE_COLUMN) (version 22.1.3.7 (official build))
, server ClickHouseNode [uri=http://localhost:8123/clickstream_1k]@743023620
	at com.clickhouse.jdbc.SqlExceptionUtils.batchUpdateError(SqlExceptionUtils.java:107)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.executeAny(InputBasedPreparedStatement.java:154)
	at com.clickhouse.jdbc.internal.AbstractPreparedStatement.executeLargeBatch(AbstractPreparedStatement.java:85)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeBatch(ClickHouseStatementImpl.java:754)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:740)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
2025-06-05 07:21:39.927 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 07:21:39.929 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:21:39.929 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:21:39.929 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:21:39.929 [stream execution thread for [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] INFO ] o.a.s.s.e.s.MicroBatchExecution - Async log purge executor pool for query [id = 2c8df6cf-ae54-4538-88ae-2b5dde1057ab, runId = ee60c0de-dc4d-4a2d-8c61-b37e4bf1e512] has been shutdown
2025-06-05 07:21:39.942 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-1, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:21:39.942 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-1, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:21:39.946 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:21:39.946 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:21:39.946 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:21:39.946 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:21:39.949 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-1 unregistered
2025-06-05 07:21:39.949 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-2, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-05 07:21:39.949 [shutdown-hook-0 INFO ] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-2, groupId=spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor] Request joining group due to: consumer pro-actively leaving the group
2025-06-05 07:21:39.951 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:21:39.951 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:21:39.951 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:21:39.951 [shutdown-hook-0 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:21:39.952 [shutdown-hook-0 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spark-kafka-source-7086dbcd-321d-4d2d-af02-086f9a7419f2--1626006840-executor-2 unregistered
2025-06-05 07:21:39.953 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-06-05 07:21:39.953 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-06-05 07:21:39.956 [shutdown-hook-0 INFO ] o.s.jetty.server.AbstractConnector - Stopped Spark@11dcbd53{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-06-05 07:21:39.958 [shutdown-hook-0 INFO ] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://phamviethoa:4040
2025-06-05 07:21:39.966 [dispatcher-event-loop-6 INFO ] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-06-05 07:21:39.990 [shutdown-hook-0 INFO ] o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-06-05 07:21:39.990 [shutdown-hook-0 INFO ] o.apache.spark.storage.BlockManager - BlockManager stopped
2025-06-05 07:21:39.994 [shutdown-hook-0 INFO ] o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-06-05 07:21:39.996 [dispatcher-event-loop-11 INFO ] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-06-05 07:21:40.000 [shutdown-hook-0 INFO ] org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-06-05 07:21:40.000 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-06-05 07:21:40.001 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/temporary-b74090fe-8e73-4f81-b368-10f789ac7fde
2025-06-05 07:21:40.003 [shutdown-hook-0 INFO ] o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-05b06c0c-0f5e-4836-a2c3-576d3a91dc72
2025-06-05 07:24:14.014 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 137315 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-05 07:24:14.015 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-05 07:24:14.516 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-05 07:24:14.518 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-05 07:24:14.519 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-05 07:24:14.519 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-05 07:24:14.562 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-05 07:24:14.562 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 527 ms
2025-06-05 07:24:14.706 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-05 07:24:14.856 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-05 07:24:14.869 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 07:24:14.882 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:24:14.883 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:24:14.883 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749083054882
2025-06-05 07:24:14.967 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 07:24:14.969 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:24:14.970 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:24:14.970 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:24:14.974 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-05 07:24:14.978 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-05 07:24:14.987 [main INFO ] com.example.Application - Started Application in 1.15 seconds (JVM running for 1.46)
2025-06-05 07:24:15.189 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-05 07:24:15.189 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-05 07:24:15.190 [RMI TCP Connection(3)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-05 07:24:21.747 [http-nio-8080-exec-1 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=04d80e65-1659-475d-bf0b-168c8b155c83, event_name=page_view, event_time=2025-06-04T23:57:29.450Z, user_id=user_4q6eh9x, sessionId=session_3cfg0cy, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/}, {event_id=f73453d1-30ec-48c2-91f4-ee460cce9a1a, event_name=scroll, event_time=2025-06-04T23:57:29.468Z, user_id=user_4q6eh9x, sessionId=session_3cfg0cy, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=1}, {event_id=5f30f595-dfa0-4e6e-b2d6-56b59fc086cb, event_name=scroll, event_time=2025-06-04T23:57:30.468Z, user_id=user_4q6eh9x, sessionId=session_3cfg0cy, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=77}, {event_id=ce620e61-2382-4c52-874f-a87e4bf851a4, event_name=tab_change, event_time=2025-06-04T23:58:13.476Z, user_id=user_4q6eh9x, sessionId=session_3cfg0cy, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, is_visible=false, time_visible=0}, {event_id=4d0503d9-6f32-49b5-9495-6c6587b756e9, event_name=tab_change, event_time=2025-06-05T00:24:20.545Z, user_id=user_4q6eh9x, sessionId=session_3cfg0cy, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, is_visible=true, time_visible=0}, {event_id=7e9715d2-9802-4fff-a656-c56f9d665d73, event_name=scroll, event_time=2025-06-05T00:24:21.704Z, user_id=user_4q6eh9x, sessionId=session_3cfg0cy, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=78}]}
2025-06-05 07:24:21.747 [http-nio-8080-exec-1 INFO ] c.e.controller.ClickstreamController - Received 6 events
2025-06-05 07:24:21.753 [http-nio-8080-exec-1 INFO ] o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 1000
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-06-05 07:24:21.760 [http-nio-8080-exec-1 INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:24:21.763 [http-nio-8080-exec-1 INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-06-05 07:24:21.772 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:24:21.772 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:24:21.772 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749083061772
2025-06-05 07:24:21.776 [kafka-producer-network-thread | producer-1 INFO ] org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:24:21.778 [kafka-producer-network-thread | producer-1 INFO ] o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 3005 with epoch 0
2025-06-05 07:24:21.782 [http-nio-8080-exec-1 INFO ] c.e.controller.ClickstreamController - Successfully processed 6 events
2025-06-05 07:24:22.924 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=798dd471-8731-4785-9506-ebdac9e852d2, event_name=click, event_time=2025-06-05T00:24:22.428Z, user_id=user_4q6eh9x, sessionId=session_3cfg0cy, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 2
                        Durable and styl, element_type=div, element_name=null, track=product_click, productId=2}, {event_id=2d9e4fc8-5e63-4ce0-a39f-8cb8e6b856c4, event_name=scroll, event_time=2025-06-05T00:24:22.704Z, user_id=user_4q6eh9x, sessionId=session_3cfg0cy, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=100}, {event_id=629d194c-2aa0-426f-924f-0f34bf4d75db, event_name=click, event_time=2025-06-05T00:24:22.921Z, user_id=user_4q6eh9x, sessionId=session_3cfg0cy, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 3
                        Customer favorit, element_type=div, element_name=null, track=product_click, productId=3}]}
2025-06-05 07:24:22.925 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-05 07:24:22.926 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-05 07:24:24.063 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=d19ac957-a640-4d72-924b-e6726cf103a0, event_name=click, event_time=2025-06-05T00:24:23.363Z, user_id=user_4q6eh9x, sessionId=session_3cfg0cy, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 6
                        Sleek design and, element_type=div, element_name=null, track=product_click, productId=6}, {event_id=8a9e2db7-bb1e-4ff4-9fe2-517e195744c5, event_name=click, event_time=2025-06-05T00:24:23.734Z, user_id=user_4q6eh9x, sessionId=session_3cfg0cy, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 5
                        Bestseller with , element_type=div, element_name=null, track=product_click, productId=5}, {event_id=e3fab3c5-9d73-4d7d-a7bc-a5eff9b98dfa, event_name=click, event_time=2025-06-05T00:24:24.059Z, user_id=user_4q6eh9x, sessionId=session_3cfg0cy, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 1
                        High-quality ite, element_type=div, element_name=null, track=product_click, productId=1}]}
2025-06-05 07:24:24.063 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-05 07:24:24.064 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-05 07:24:25.020 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=43097aa6-4b90-4e37-8c52-c122c49668f4, event_name=click, event_time=2025-06-05T00:24:24.373Z, user_id=user_4q6eh9x, sessionId=session_3cfg0cy, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 4
                        Reliable and aff, element_type=div, element_name=null, track=product_click, productId=4}, {event_id=0db57ccb-8ec2-4f54-9e7d-1fda7d70b87f, event_name=click, event_time=2025-06-05T00:24:24.718Z, user_id=user_4q6eh9x, sessionId=session_3cfg0cy, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 7
                        Compact and effi, element_type=div, element_name=null, track=product_click, productId=7}, {event_id=78be7564-755c-4cb2-8c11-3746d35f8f19, event_name=click, event_time=2025-06-05T00:24:25.017Z, user_id=user_4q6eh9x, sessionId=session_3cfg0cy, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/#productList, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 8
                        Eco-friendly and, element_type=div, element_name=null, track=product_click, productId=8}]}
2025-06-05 07:24:25.020 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-05 07:24:25.021 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-05 07:25:01.504 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:25:01.506 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node -1 disconnected.
2025-06-05 07:25:02.508 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:25:02.508 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:25:03.510 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:25:03.510 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:25:04.512 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:25:04.512 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:25:05.514 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:25:05.514 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:25:06.516 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:25:06.516 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:25:07.517 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:25:07.517 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:25:08.519 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:25:08.519 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:25:09.520 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:25:09.520 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:25:10.521 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:25:10.521 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:25:11.523 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:25:11.523 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:25:12.524 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:25:12.524 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:25:13.526 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:25:13.526 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:25:14.528 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:25:14.528 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:25:15.529 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:25:15.529 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:25:16.530 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:25:16.530 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:25:17.532 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:25:17.532 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:25:18.533 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:25:18.534 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:25:19.534 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:25:19.534 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Cancelled in-flight API_VERSIONS request with correlation id 13 due to node 1 being disconnected (elapsed time since creation: 0ms, elapsed time since send: 0ms, request timeout: 30000ms)
2025-06-05 07:25:20.535 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:25:20.535 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Cancelled in-flight API_VERSIONS request with correlation id 14 due to node 1 being disconnected (elapsed time since creation: 0ms, elapsed time since send: 0ms, request timeout: 30000ms)
2025-06-05 07:25:21.537 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:25:21.538 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Cancelled in-flight API_VERSIONS request with correlation id 15 due to node 1 being disconnected (elapsed time since creation: 0ms, elapsed time since send: 0ms, request timeout: 30000ms)
2025-06-05 07:25:22.538 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:25:22.539 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Cancelled in-flight API_VERSIONS request with correlation id 16 due to node 1 being disconnected (elapsed time since creation: 0ms, elapsed time since send: 0ms, request timeout: 30000ms)
2025-06-05 07:25:23.910 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Error while fetching metadata with correlation id 18 : {clickstream-events=LEADER_NOT_AVAILABLE}
2025-06-05 07:25:24.478 [SpringApplicationShutdownHook INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-06-05 07:25:24.482 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:25:24.482 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:25:24.482 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:25:24.482 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:25:24.483 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-1 unregistered
2025-06-05 07:25:46.232 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 141492 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-05 07:25:46.233 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-05 07:25:46.743 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-05 07:25:46.746 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-05 07:25:46.747 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-05 07:25:46.747 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-05 07:25:46.790 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-05 07:25:46.790 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 536 ms
2025-06-05 07:25:46.933 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-05 07:25:47.087 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-05 07:25:47.101 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 07:25:47.114 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:25:47.114 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:25:47.115 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749083147114
2025-06-05 07:25:47.205 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 07:25:47.208 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:25:47.208 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:25:47.209 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:25:47.213 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-05 07:25:47.217 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-05 07:25:47.226 [main INFO ] com.example.Application - Started Application in 1.159 seconds (JVM running for 1.472)
2025-06-05 07:25:47.321 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-05 07:25:47.322 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-05 07:25:47.322 [RMI TCP Connection(2)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 0 ms
2025-06-05 07:25:49.641 [http-nio-8080-exec-1 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=d78009a1-1311-4ae3-a0d8-b607f4634b06, event_name=page_view, event_time=2025-06-05T00:25:37.256Z, user_id=user_4q6eh9x, sessionId=session_n64h5fs, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/}, {event_id=1618b7b2-37fb-428a-9ab4-05c6f4f8d305, event_name=tab_change, event_time=2025-06-05T00:25:37.693Z, user_id=user_4q6eh9x, sessionId=session_n64h5fs, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, is_visible=true, time_visible=0}, {event_id=8a3e7509-063d-40c5-b204-22166df81512, event_name=scroll, event_time=2025-06-05T00:25:42.334Z, user_id=user_4q6eh9x, sessionId=session_n64h5fs, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=1}, {event_id=9604f938-b36a-4e4a-82d6-075ffc752c0b, event_name=scroll, event_time=2025-06-05T00:25:43.335Z, user_id=user_4q6eh9x, sessionId=session_n64h5fs, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=31}, {event_id=3dc7c1ff-0482-4a24-bb76-d1b519d569e5, event_name=scroll, event_time=2025-06-05T00:25:49.598Z, user_id=user_4q6eh9x, sessionId=session_n64h5fs, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=32}]}
2025-06-05 07:25:49.641 [http-nio-8080-exec-1 INFO ] c.e.controller.ClickstreamController - Received 5 events
2025-06-05 07:25:49.647 [http-nio-8080-exec-1 INFO ] o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 1000
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-06-05 07:25:49.655 [http-nio-8080-exec-1 INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:25:49.658 [http-nio-8080-exec-1 INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-06-05 07:25:49.667 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:25:49.667 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:25:49.667 [http-nio-8080-exec-1 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749083149667
2025-06-05 07:25:49.672 [kafka-producer-network-thread | producer-1 INFO ] org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:25:49.678 [http-nio-8080-exec-1 INFO ] c.e.controller.ClickstreamController - Successfully processed 5 events
2025-06-05 07:25:49.686 [kafka-producer-network-thread | producer-1 INFO ] o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 4000 with epoch 0
2025-06-05 07:25:53.240 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=119fa16f-41b0-419d-b648-8e36f866a8da, event_name=scroll, event_time=2025-06-05T00:25:50.599Z, user_id=user_4q6eh9x, sessionId=session_n64h5fs, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=94}, {event_id=68512fee-5fa6-4df8-995e-446b405cd96f, event_name=click, event_time=2025-06-05T00:25:51.970Z, user_id=user_4q6eh9x, sessionId=session_n64h5fs, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 3
                        Customer favorit, element_type=div, element_name=null, track=product_click, productId=3}, {event_id=34d141cb-a391-4b67-b796-3bcf3eb04585, event_name=click, event_time=2025-06-05T00:25:53.236Z, user_id=user_4q6eh9x, sessionId=session_n64h5fs, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 2
                        Durable and styl, element_type=div, element_name=null, track=product_click, productId=2}]}
2025-06-05 07:25:53.240 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-05 07:25:53.241 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-05 07:25:56.145 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=b76781e9-d312-42b3-9a84-93e24ca9c704, event_name=click, event_time=2025-06-05T00:25:54.513Z, user_id=user_4q6eh9x, sessionId=session_n64h5fs, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 5
                        Bestseller with , element_type=div, element_name=null, track=product_click, productId=5}, {event_id=7d5a7fd7-22a5-4a7a-8311-d4ce7cae95f0, event_name=click, event_time=2025-06-05T00:25:55.707Z, user_id=user_4q6eh9x, sessionId=session_n64h5fs, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 6
                        Sleek design and, element_type=div, element_name=null, track=product_click, productId=6}, {event_id=78c49f34-7047-46f4-aeb1-869491ce26c7, event_name=click, event_time=2025-06-05T00:25:56.143Z, user_id=user_4q6eh9x, sessionId=session_n64h5fs, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 9
                        Top-rated with e, element_type=div, element_name=null, track=product_click, productId=9}]}
2025-06-05 07:25:56.146 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-05 07:25:56.146 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-05 07:27:03.442 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node -1 disconnected.
2025-06-05 07:27:03.443 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:27:03.444 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Cancelled in-flight METADATA request with correlation id 9 due to node 1 being disconnected (elapsed time since creation: 0ms, elapsed time since send: 0ms, request timeout: 30000ms)
2025-06-05 07:27:04.444 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:27:04.444 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:27:05.446 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:27:05.446 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:27:06.447 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:27:06.447 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:27:07.448 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:27:07.449 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:27:08.449 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:27:08.449 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:27:09.450 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:27:09.450 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:27:10.451 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:27:10.451 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:27:11.452 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:27:11.452 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:27:12.453 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:27:12.453 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:27:13.454 [kafka-producer-network-thread | producer-1 INFO ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 1 disconnected.
2025-06-05 07:27:13.455 [kafka-producer-network-thread | producer-1 WARN ] o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available.
2025-06-05 07:27:13.865 [SpringApplicationShutdownHook INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-06-05 07:27:13.867 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:27:13.868 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:27:13.868 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:27:13.868 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:27:13.868 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-1 unregistered
2025-06-05 07:27:25.747 [main INFO ] com.example.Application - Starting Application using Java 11.0.27 on phamviethoa with PID 145599 (/home/phamviethoa/Idea Projects/clickstream-analysis/target/classes started by phamviethoa in /home/phamviethoa/Idea Projects/clickstream-analysis)
2025-06-05 07:27:25.748 [main INFO ] com.example.Application - No active profile set, falling back to 1 default profile: "default"
2025-06-05 07:27:26.359 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8080 (http)
2025-06-05 07:27:26.362 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-05 07:27:26.363 [main INFO ] o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-05 07:27:26.363 [main INFO ] o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.83]
2025-06-05 07:27:26.415 [main INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-05 07:27:26.415 [main INFO ] o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 644 ms
2025-06-05 07:27:26.571 [main INFO ] o.s.b.a.w.s.WelcomePageHandlerMapping - Adding welcome page: class path resource [static/index.html]
2025-06-05 07:27:26.736 [main INFO ] o.s.b.a.e.web.EndpointLinksResolver - Exposing 4 endpoint(s) beneath base path '/actuator'
2025-06-05 07:27:26.752 [main INFO ] o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-05 07:27:26.770 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:27:26.771 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:27:26.771 [main INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749083246770
2025-06-05 07:27:26.863 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-06-05 07:27:26.866 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:27:26.866 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:27:26.866 [kafka-admin-client-thread | adminclient-1 INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:27:26.871 [main INFO ] o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-05 07:27:26.876 [main INFO ] o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port(s): 8080 (http) with context path ''
2025-06-05 07:27:26.885 [main INFO ] com.example.Application - Started Application in 1.332 seconds (JVM running for 1.68)
2025-06-05 07:27:27.000 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-05 07:27:27.000 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-06-05 07:27:27.001 [RMI TCP Connection(1)-192.168.0.103 INFO ] o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-06-05 07:27:29.618 [http-nio-8080-exec-1 WARN ] o.s.w.s.m.s.DefaultHandlerExceptionResolver - Resolved [org.springframework.web.HttpMediaTypeNotSupportedException: Content type 'text/plain;charset=UTF-8' not supported]
2025-06-05 07:27:36.378 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=e679aac9-199b-4b5b-974a-6d13e666b925, event_name=page_view, event_time=2025-06-05T00:27:30.513Z, user_id=user_4q6eh9x, sessionId=session_odlu0ya, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/}, {event_id=b2c33d38-6f98-4b95-b342-7cdba2a2927c, event_name=tab_change, event_time=2025-06-05T00:27:31.026Z, user_id=user_4q6eh9x, sessionId=session_odlu0ya, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=1846x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, is_visible=true, time_visible=0}, {event_id=864879a4-9d3f-45e3-a938-89976e1ea5f5, event_name=scroll, event_time=2025-06-05T00:27:36.369Z, user_id=user_4q6eh9x, sessionId=session_odlu0ya, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=1}]}
2025-06-05 07:27:36.378 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-05 07:27:36.384 [http-nio-8080-exec-2 INFO ] o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 1000
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-06-05 07:27:36.391 [http-nio-8080-exec-2 INFO ] o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-05 07:27:36.394 [http-nio-8080-exec-2 INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-06-05 07:27:36.403 [http-nio-8080-exec-2 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-05 07:27:36.403 [http-nio-8080-exec-2 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-05 07:27:36.403 [http-nio-8080-exec-2 INFO ] o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1749083256403
2025-06-05 07:27:36.409 [kafka-producer-network-thread | producer-1 INFO ] org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: PYlIuVMhSkOfVzwSbK_l7A
2025-06-05 07:27:36.415 [http-nio-8080-exec-2 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-05 07:27:36.422 [kafka-producer-network-thread | producer-1 INFO ] o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 5000 with epoch 0
2025-06-05 07:27:39.717 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=cf7e2003-7fa5-4d9a-951d-cf7efa55c636, event_name=scroll, event_time=2025-06-05T00:27:37.370Z, user_id=user_4q6eh9x, sessionId=session_odlu0ya, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=31}, {event_id=b86b99b3-c323-4b41-92db-757556017f5a, event_name=click, event_time=2025-06-05T00:27:38.848Z, user_id=user_4q6eh9x, sessionId=session_odlu0ya, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 3
                        Customer favorit, element_type=div, element_name=null, track=product_click, productId=3}, {event_id=5db4ec4e-369e-49ee-bf7b-32b67153af96, event_name=click, event_time=2025-06-05T00:27:39.714Z, user_id=user_4q6eh9x, sessionId=session_odlu0ya, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 2
                        Durable and styl, element_type=div, element_name=null, track=product_click, productId=2}]}
2025-06-05 07:27:39.717 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-05 07:27:39.718 [http-nio-8080-exec-3 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-05 07:27:41.956 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=d990f526-d020-419a-8c7f-4a74a47a4d11, event_name=click, event_time=2025-06-05T00:27:40.871Z, user_id=user_4q6eh9x, sessionId=session_odlu0ya, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 1
                        High-quality ite, element_type=div, element_name=null, track=product_click, productId=1}, {event_id=4a0462a4-5243-4674-8c60-d94ddb33b66e, event_name=click, event_time=2025-06-05T00:27:41.490Z, user_id=user_4q6eh9x, sessionId=session_odlu0ya, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 4
                        Reliable and aff, element_type=div, element_name=null, track=product_click, productId=4}, {event_id=ad3e7018-d2ba-41ca-b942-66f0b6c9ddd8, event_name=click, event_time=2025-06-05T00:27:41.952Z, user_id=user_4q6eh9x, sessionId=session_odlu0ya, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 5
                        Bestseller with , element_type=div, element_name=null, track=product_click, productId=5}]}
2025-06-05 07:27:41.956 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-05 07:27:41.958 [http-nio-8080-exec-4 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-05 07:27:44.455 [http-nio-8080-exec-5 INFO ] c.e.controller.ClickstreamController - Received payload: {events=[{event_id=9c2a2ac9-9c7b-4e06-b3b3-0e40faff6566, event_name=click, event_time=2025-06-05T00:27:43.191Z, user_id=user_4q6eh9x, sessionId=session_odlu0ya, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, element_id=null, element_class=card product_card, element_text=Product 6
                        Sleek design and, element_type=div, element_name=null, track=product_click, productId=6}, {event_id=84e05a11-00c9-481a-9b89-cadbff2e0237, event_name=scroll, event_time=2025-06-05T00:27:43.451Z, user_id=user_4q6eh9x, sessionId=session_odlu0ya, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=32}, {event_id=6a663c33-515c-4e63-ba23-a9a8594c9731, event_name=scroll, event_time=2025-06-05T00:27:44.451Z, user_id=user_4q6eh9x, sessionId=session_odlu0ya, app_id=ecommerce-app, platform=web, screen_resolution=1920x1080, viewport_size=850x927, user_agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36, language=en-US, geo_country=unknown, geo_city=unknown, page_url=http://localhost/, page_title=E-Commerce Store, page_referrer=direct, page_path=/, scrollDepth=100}]}
2025-06-05 07:27:44.455 [http-nio-8080-exec-5 INFO ] c.e.controller.ClickstreamController - Received 3 events
2025-06-05 07:27:44.456 [http-nio-8080-exec-5 INFO ] c.e.controller.ClickstreamController - Successfully processed 3 events
2025-06-05 07:28:51.270 [SpringApplicationShutdownHook INFO ] o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-06-05 07:28:51.272 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-05 07:28:51.273 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-05 07:28:51.273 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-05 07:28:51.273 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-05 07:28:51.273 [SpringApplicationShutdownHook INFO ] o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-1 unregistered
